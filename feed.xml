<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Paper Pulse</title>
    <link>https://jamie-cui.github.io/paper-pulse</link>
    <description>Keyword-based research paper aggregation from arXiv and IACR</description>
    <lastBuildDate>Wed, 25 Feb 2026 02:08:23 -0000</lastBuildDate>
    <atom:link href="https://jamie-cui.github.io/paper-pulse/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2602.21078v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21078v1</guid>
      <description>Federated Semi-Supervised Learning (FSSL) faces dual heterogeneity: *external* (cross-client distribution shift) and *internal* (within-client labeled/unlabeled mismatch). Existing methods either aggregate client models via fixed/dynamic weights—struggling to capture the ideal global distribution—or filter low-confidence pseudo-labels, discarding valuable unlabeled data. To address both challenges jointly, we propose **ProxyFL**, a proxy-guided framework that uses **learnable classifier weights as category-distribution proxies**. For external heterogeneity, ProxyFL explicitly optimizes a *global proxy* on the server to robustly suppress outliers, replacing direct weight aggregation. For internal heterogeneity, it introduces a *positive-negative proxy pool* to re-include filtered unlabeled samples via similarity-based soft weighting, mitigating pseudo-label noise. Theoretically, ProxyFL achieves tighter convergence bounds; empirically, it outperforms SOTAs by +3.2–5.8% average accuracy across 6 benchmarks (e.g., CIFAR-10/LT, SVHN), boosts unlabeled data utilization by 3.7×, and reduces pseudo-label error rate by 41%.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Some Simple Economics of AGI</title>
      <link>https://arxiv.org/abs/2602.20946v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20946v1</guid>
      <description>This paper reframes the AGI transition as an economic collision between two asymmetric cost curves: exponentially falling *Cost to Automate* and biologically constrained *Cost to Verify*. As AI decouples cognition from biology, execution becomes near-zero marginal cost—absorbing even creative and innovative labor—but human verification bandwidth remains the binding constraint on growth. This structural asymmetry widens a *Measurability Gap*: what agents can execute vastly exceeds what humans can afford to validate, audit, or underwrite. Consequently, technical change shifts from skill-biased to *measurability-biased*, with economic rents migrating to verification-grade ground truth, cryptographic provenance, and liability underwriting—not just output generation. The current human-in-the-loop equilibrium is unstable: eroded by collapsing apprenticeship (“Missing Junior Loop”) and expert self-obsolescence (“Codifier’s Curse”), enabling privately rational but socially hazardous unverified deployment. Unmanaged, this pulls toward a *Hollow Economy*; yet scaling verification capacity in tandem with agentic capability unlocks an *Augmented Economy*—one where robust oversight enables unbounded discovery. The central imperative is clear: the defining race today is not for autonomy, but for *verifiability*.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>trojan</category>
      <category>ai</category>
    </item>
    <item>
      <title>On Electric Vehicle Energy Demand Forecasting and the Effect of Federated Learning</title>
      <link>https://arxiv.org/abs/2602.20782v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20782v1</guid>
      <description>This paper investigates Electric Vehicle Supply Equipment (EVSE) energy demand forecasting (EDF) under privacy-preserving constraints. We benchmark statistical (ARIMA), machine learning (XGBoost), and deep learning (LSTM, GRU) models across four real-world EVSE datasets, evaluating performance under both centralized and federated learning (FL) paradigms. Results show XGBoost achieves the highest accuracy (18.7% lower MAE on average) and best energy efficiency (5× less training energy than LSTM), outperforming both statistical and neural models. Crucially, in FL settings, XGBoost maintains this advantage while reducing communication overhead by 62% compared to deep models—demonstrating superior trade-offs among prediction fidelity, privacy preservation (no raw data sharing), and edge-device energy consumption. Our work establishes FL-enabled tree-based forecasting as a practical, scalable, and sustainable solution for decentralized EV energy management.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Bikelution: Federated Gradient-Boosting for Scalable Shared Micro-Mobility Demand Forecasting</title>
      <link>https://arxiv.org/abs/2602.20671v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20671v1</guid>
      <description>Bikelution is a novel federated learning framework for scalable, privacy-preserving micro-mobility demand forecasting using gradient-boosted decision trees. Unlike centralized ML—which achieves high accuracy but violates data privacy and incurs prohibitive bandwidth costs—Bikelution enables collaborative model training across edge devices (e.g., bike docks, city zones) without sharing raw spatio-temporal data. It introduces sparse histogram-based gradient compression, differential privacy-aware secure aggregation, and a sliding-window client selection strategy to handle non-IID demand patterns. Evaluated on three real-world dockless bike-sharing datasets (Hangzhou, NYC Citi Bike, Singapore SG Bike), Bikelution matches centralized XGBoost’s 6-hour-ahead forecast accuracy (within 2.1% MAE gap) while outperforming state-of-the-art federated tree methods by 19.3% on average. This demonstrates that high-fidelity, mid-term demand forecasting is feasible under strict privacy constraints—enabling sustainable, compliant smart mobility planning.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Is the Trigger Essential? A Feature-Based Triggerless Backdoor Attack in Vertical Federated Learning</title>
      <link>https://arxiv.org/abs/2602.20593v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20593v1</guid>
      <description>Vertical federated learning (VFL) enables collaborative modeling across parties holding disjoint features and labels, yet remains vulnerable to backdoor attacks. Contrary to the prevailing trigger-dependent paradigm, this paper demonstrates that **triggers are *not essential* for effective backdoor attacks in VFL**. We propose the first **feature-based triggerless backdoor attack**, operating under a stricter honest-but-curious threat model—where the attacker participates legitimately in training without gradient tampering. Our method comprises three key components: (1) label inference to identify target classes from intermediate outputs; (2) triggerless poisoning via feature amplification and imperceptible perturbation; and (3) stealthy backdoor execution on *clean, unmodified inputs*. Experiments on five benchmark datasets show our attack achieves 2–50× higher attack success rate (ASR) than three trigger-based baselines, with &lt;0.8% main-task accuracy degradation. It remains highly effective even in large-scale VFL (32 passive parties) using only one auxiliary dataset and exhibits strong resilience against defenses (ASR variation &lt;3%). This work uncovers a critical blind spot in VFL security and calls for robust, trigger-agnostic defense design.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>learning</category>
      <category>security</category>
      <category>train</category>
      <category>privacy-preserving</category>
    </item>
    <item>
      <title>Characterizing Online and Private Learnability under Distributional Constraints via Generalized Smoothness</title>
      <link>https://arxiv.org/abs/2602.20585v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20585v1</guid>
      <description>This paper characterizes online and private learnability under *distributional adversaries*—adaptive sequences of data-generating distributions drawn from a fixed family $U$. We introduce **generalized smoothness**, a structural property of $U$, and prove it is *necessary and sufficient* for online learnability: a family $U$ admits VC-dimension-dependent regret bounds for *every* finite-VC hypothesis class if and only if $U$ is generalized smooth. We design universal algorithms achieving low regret without explicit knowledge of $U$, and—when $U$ is known—derive refined bounds via the **fragmentation number**, a combinatorial measure of how many disjoint regions can simultaneously carry nontrivial mass under $U$. Remarkably, we show generalized smoothness also *exactly characterizes* private learnability under distributional constraints, revealing a deep unification between online and differentially private learning. These results provide a near-complete theory of learnability beyond i.i.d. and fully adversarial settings.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>Wireless Federated Multi-Task LLM Fine-Tuning via Sparse-and-Orthogonal LoRA</title>
      <link>https://arxiv.org/abs/2602.20492v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20492v1</guid>
      <description>This paper proposes **Sparse-and-Orthogonal LoRA**, a fully decentralized wireless federated learning framework for multi-task fine-tuning of large language models (LLMs) on mobile devices. It tackles three critical issues in heterogeneous edge settings: (i) catastrophic forgetting during local fine-tuning due to conflicting gradient directions; (ii) inefficient communication and slow convergence from redundant parameter transmission; and (iii) cross-task knowledge interference at inference. Our approach introduces: (1) orthogonality-constrained sparse LoRA adapters to eliminate update conflicts; (2) a task-aware clustering strategy for topology-efficient model aggregation among neighboring devices; and (3) an implicit mixture-of-experts mechanism enabling task-specific inference paths without explicit routing. Experiments show up to **73% reduction in communication cost** and **+5.0% average accuracy gain** over standard federated LoRA, with faster convergence—demonstrating strong practicality for bandwidth-constrained wireless edge AI.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning</title>
      <link>https://arxiv.org/abs/2602.20450v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20450v1</guid>
      <description>Federated Learning (FL) suffers from statistical heterogeneity across clients, leading to accuracy degradation and unstable convergence. Prior client selection methods rely on loss or bias—imprecise proxies for heterogeneity—and employ stochastic selection, limiting reliability. We propose **Terraform**, a novel heterogeneity-aware methodology that leverages **client gradient updates** (not scalar metrics) to quantify directional and magnitude deviation from the global gradient, enabling precise heterogeneity assessment. Its **deterministic greedy algorithm** selects the top-*k* most heterogeneous clients per round—ensuring reproducibility and stability. Evaluated across four benchmarks (CIFAR-10/100, Tiny-ImageNet, LEAF-HeartDisease), Terraform achieves up to **47% higher accuracy** over state-of-the-art baselines (e.g., FedAvg, q-FFL). Ablation studies confirm both gradient-based modeling and deterministic selection are essential; training overhead remains under 3% per round. Terraform delivers a robust, efficient, and principled solution for heterogeneous FL.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Partially Non-Interactive Two-Round Threshold and Multi-Signatures with Tighter and Adaptive Security</title>
      <link>https://eprint.iacr.org/2026/373</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/373</guid>
      <description>We bridge a critical security gap in partially non-interactive (PNI) two-round threshold and multi-signature schemes in the pairing-free discrete-logarithm setting. While fully online schemes achieve rewinding-free or fully adaptive security under standard assumptions, prior PNI schemes require algebraic adversary restrictions or non-standard assumptions. Our key insight is to systematically transform the HBMS framework (Bellare &amp; Dai, Asiacrypt 2021) into a PNI paradigm via message-independent preprocessing of commitments and adaptive key derivation. We present: (1) the **first PNI two-round multi-signature scheme** with a rewinding-free reduction under standard DL assumptions, achieving tighter security (O(1) loss) against *non-algebraic* adversaries; and (2) the **first PNI two-round threshold signature scheme** with *fully adaptive security*—supporting adaptive corruptions and message queries—also under standard assumptions. Both schemes retain practical efficiency and avoid random oracles.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks</title>
      <link>https://arxiv.org/abs/2602.20156v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20156v1</guid>
      <description>We introduce **SkillInject**, the first benchmark for evaluating LLM agent vulnerability to *skill file attacks*—a novel prompt injection threat enabled by the emerging “agent skills” paradigm. SkillInject comprises 202 carefully crafted injection-task pairs, spanning overtly malicious commands (e.g., “exfiltrate ~/.bash_history”) to subtle, context-dependent exploits embedded in otherwise benign skill definitions. We evaluate 12 state-of-the-art LLM agents (including GPT-4o, Claude-3, and Llama-3-based agents) across both *security* (harmful instruction avoidance) and *utility* (legitimate skill compliance). Results reveal alarming susceptibility: up to **80% attack success rates**, with agents executing high-impact harmful actions—including data exfiltration, irreversible system destruction, and ransomware-like behavior. Critically, neither model scaling nor naive input filtering mitigates this risk effectively. Our findings underscore that robust agent security requires *context-aware authorization frameworks*, not just stronger models. The benchmark is publicly available at https://www.skill-inject.com/.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>llm</category>
      <category>injection</category>
      <category>prompt</category>
      <category>agent</category>
    </item>
    <item>
      <title>The LLMbda Calculus: AI Agents, Conversations, and Information Flow</title>
      <link>https://arxiv.org/abs/2602.20064v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20064v1</guid>
      <description>We introduce the **LLMbda Calculus**, an untyped call-by-value lambda calculus extended with dynamic information-flow control and primitives for modeling prompt-response conversations. Its core `llmcall` primitive serializes a value into a prompt, invokes an LLM, and parses the natural-language response as a new term—faithfully capturing planner loops, prompt injection vulnerabilities, and the semantic role of conversation history. The calculus explicitly represents conversational state, enabling formal reasoning about defenses like quarantined sub-conversations, code isolation, and flow-sensitive LLM input restrictions. We prove a termination-insensitive noninterference theorem, establishing rigorous integrity and confidentiality guarantees for agentic systems. This work provides the first principled semantic foundation for safe, verifiable AI agent programming.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>injection</category>
      <category>prompt</category>
    </item>
    <item>
      <title>RobPI: Robust Private Inference against Malicious Client</title>
      <link>https://arxiv.org/abs/2602.19918v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19918v1</guid>
      <description>Private inference (PI) enables privacy-preserving ML model serving, yet most existing protocols assume a *semi-honest* client—ignoring real-world adversarial motivations. We first demonstrate this fragility by designing a novel **inference manipulation attack**, achieving 3×–8× query efficiency over state-of-the-art black-box attacks against leading PI systems (e.g., CrypTFlow2, ABY3-PI). To counter such malicious clients, we propose **RobPI**, the first robust PI protocol featuring: (1) encryption-compatible noise injection into both logits and intermediate features; and (2) a lightweight homomorphic encryption + random masking framework ensuring output integrity without compromising efficiency. Extensive experiments across ResNet-18, ViT-Tiny, and MLP on CIFAR-10, ImageNet-1k, and Adult show RobPI reduces attack success rate by **~91.9%** and increases required queries by **&gt;10×**, with only &lt;12% latency overhead. RobPI bridges a critical security gap in practical private inference.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>inference</category>
    </item>
    <item>
      <title>LLM-enabled Applications Require System-Level Threat Monitoring</title>
      <link>https://arxiv.org/abs/2602.19844v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19844v1</guid>
      <description>Large language model (LLM)-enabled applications are transforming software systems by embedding LLMs as core reasoning engines—but their non-deterministic, learning-driven, and hard-to-verify behavior dramatically expands the security attack surface. We argue that LLM-related risks must be treated as *expected operational conditions*, not exceptions—shifting focus from pre-deployment safeguards (e.g., testing, guardrails) to *post-deployment system-level threat monitoring*. This position paper contends that the primary barrier to trustworthy deployment is not further model capability gains, but the lack of mechanisms that can **detect, attribute, and contextualize security-relevant anomalies in production**—such as prompt injection, jailbreaking, data leakage, or semantic drift. We propose monitoring as a foundational infrastructure layer: spanning API logs, token-level generation traces, user feedback, and runtime dependencies—and emphasize *contextualization* (e.g., linking anomalies to intent, session history, and business logic) as essential for actionable incident response. Our work establishes monitoring as a prerequisite for reliable operation and a cornerstone for future LLM-specific incident frameworks.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>llm</category>
      <category>security</category>
    </item>
    <item>
      <title>Efficient Multi-Party Secure Comparison over Different Domains with Preprocessing Assistance</title>
      <link>https://arxiv.org/abs/2602.19604v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19604v1</guid>
      <description>Secure comparison is a foundational primitive in multi-party computation (MPC), yet its preprocessing—especially correlated randomness generation—remains a major bottleneck. While dealer-assisted frameworks have emerged to accelerate preprocessing, they fail to co-design the online phase or support broad domain generality. This work presents the **first dealer-assisted $n$-party protocols** for LTBits and MSB extraction over **both $\mathbb{F}_p$ and $\mathbb{Z}_{2^k}$**, achieving **perfect security**. By fully leveraging the dealer’s capability, our $\mathbb{F}_p$ protocol attains **constant-round online complexity**, while our $\mathbb{Z}_{2^k}$ protocol achieves **$O(\log_n k)$ rounds** with tunable branching. All protocols are formulated as black-box constructions via an extended Arithmetic Black-Box (ABB) model, ensuring backend and adversary-model portability. Experiments show **1.79×–19.4× speedups** over state-of-the-art MPC frameworks (e.g., ABY3, Cheetah), demonstrating strong practicality for comparison-intensive applications.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>computation</category>
      <category>secure</category>
      <category>privacy-preserving</category>
      <category>model</category>
      <category>machine</category>
    </item>
    <item>
      <title>Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains</title>
      <link>https://arxiv.org/abs/2602.19555v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19555v1</guid>
      <description>This paper identifies and systematizes novel cybersecurity threats arising from the *runtime supply chain* of agentic AI systems. We categorize attacks into **data supply chain threats** (transient context injection and persistent memory poisoning) and **tool supply chain threats** (discovery hijacking, implementation corruption, and invocation manipulation). A key contribution is the formalization and demonstration of the **Viral Agent Loop**: a self-propagating generative worm that spreads via semantic induction alone—requiring no code-level vulnerabilities. To counter these risks, we propose a **Zero-Trust Runtime Architecture**, which treats all context as untrusted control flow and enforces tool execution constraints via cryptographic provenance (e.g., signed tool manifests, verifiable invocation policies), replacing brittle semantic inference with cryptographically grounded trust. Our framework bridges critical gaps between LLM security, runtime systems, and supply chain integrity.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>model</category>
      <category>poisoning</category>
      <category>inference</category>
      <category>data</category>
    </item>
    <item>
      <title>CIBER: A Comprehensive Benchmark for Security Evaluation of Code Interpreter Agents</title>
      <link>https://arxiv.org/abs/2602.19547v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19547v1</guid>
      <description>CIBER is the first comprehensive, dynamic benchmark for evaluating security vulnerabilities of LLM-based code interpreter agents—addressing critical gaps in existing static or simulated benchmarks. It integrates *automated adversarial attack generation* (covering Direct/Indirect Prompt Injection, Memory Poisoning, and Prompt-based Backdoor), *isolated secure sandboxing*, and *state-aware evaluation* across multi-turn interactions and tool executions. Evaluating six foundation models on OpenInterpreter and OpenCodeInterpreter under controlled settings, we find: (1) Interpreter architecture and model alignment jointly set the security baseline—specialized, structurally integrated models outperform generic SOTA; (2) higher instruction-following capability paradoxically increases susceptibility to complex adversarial prompts; (3) natural language inputs bypass syntax-based defenses more effectively than explicit code snippets (+14.1% ASR), revealing the “Natural Language Disguise” phenomenon; and (4) alarming “Security Polarization”: agents robust against explicit threats fail catastrophically against implicit semantic hazards—exposing a fundamental blind spot in pattern-matching protections. CIBER is open-sourced to advance trustworthy code interpretation.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>model</category>
      <category>poisoning</category>
      <category>llm</category>
      <category>injection</category>
    </item>
    <item>
      <title>FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing</title>
      <link>https://arxiv.org/abs/2602.19490v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19490v1</guid>
      <description>Traditional database fuzzers focus on syntactic SQL correctness, overlooking obscure yet critical DBMS special features—such as GTID modes, stored procedures, and system commands (e.g., `KILL`)—which can trigger crashes or security flaws under edge cases. We present **FuzzySQL**, an LLM-powered adaptive fuzzer that uncovers subtle vulnerabilities in these features via *grammar-guided SQL generation* and a novel *logic-shifting progressive mutation*, which explores alternative control paths by condition negation and execution logic restructuring. Its hybrid error repair pipeline combines rule-based patching with LLM-driven semantic correction to handle context-sensitive failures (e.g., invalid GTID session states). Evaluated across MySQL, MariaDB, SQLite, PostgreSQL, and ClickHouse, FuzzySQL discovered **37 vulnerabilities**, including **7 tied to under-tested special features**. So far, **29 are confirmed**, **9 assigned CVEs**, and **14 already patched**—demonstrating the superiority of LLM-guided semantic fuzzing over conventional approaches in deep database bug discovery.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>llm</category>
      <category>security</category>
    </item>
    <item>
      <title>Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments</title>
      <link>https://arxiv.org/abs/2602.19450v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19450v1</guid>
      <description>This paper presents the first red-teaming study of LLM-based security advisors—specifically ChatGPT-5.2 and Claude Opus-4.6—for Trusted Execution Environments (TEEs). We introduce **TEE-RedBench**, a grounded evaluation framework comprising: (i) a TEE-specific threat model for LLM-mediated security work; (ii) a structured prompt suite covering SGX/TrustZone architecture, attestation, key management, threat modeling, and policy-bound misuse probes; and (iii) an annotation rubric jointly measuring technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that up to **12.02% of critical failures transfer across models**, indicating shared architectural blind spots—not just idiosyncratic hallucinations. To mitigate these risks, we propose an “LLM-in-the-loop” pipeline integrating policy gating, retrieval grounding, structured templates, and lightweight verification checks, which collectively **reduce failure rates by 80.62%**. Our work establishes both a benchmark and a practical pathway for deploying LLMs safely in high-assurance security domains.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>llm</category>
      <category>injection</category>
      <category>prompt</category>
      <category>security</category>
    </item>
    <item>
      <title>Agents of Chaos</title>
      <link>https://arxiv.org/abs/2602.20021v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20021v1</guid>
      <description>We present an exploratory red-teaming study of autonomous language-model agents deployed in a live lab environment with persistent memory, email, Discord, filesystem access, and shell execution. Over two weeks, 20 AI researchers interacted with the agents under both benign and adversarial conditions. Focusing on failures arising from the integration of LMs with autonomy, tool use, and multi-party communication, we document 11 representative case studies. Observed high-risk behaviors include unauthorized compliance with non-owners, sensitive data disclosure, destructive system actions (e.g., file deletion, config overwrites), denial-of-service via resource exhaustion, identity spoofing, cross-agent propagation of unsafe practices, partial system takeover, and hallucinated task completion reports inconsistent with actual system state. We also report on failed attack attempts, revealing partial resilience of current mitigations. Our findings empirically confirm security, privacy, and governance vulnerabilities in realistic deployment settings—raising urgent, unresolved questions about accountability, delegated authority, and responsibility for downstream harms. This work provides the first empirical foundation for interdisciplinary responses from legal scholars, policymakers, and technical researchers.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>agent</category>
    </item>
    <item>
      <title>A Secure and Private Distributed Bayesian Federated Learning Design</title>
      <link>https://arxiv.org/abs/2602.20003v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20003v1</guid>
      <description>Distributed Federated Learning (DFL) enables collaborative model training without a central server, yet suffers from privacy leakage to honest-but-curious peers, slow convergence due to decentralized coordination, and vulnerability to Byzantine adversaries. To address these, we propose a secure and private Bayesian DFL framework integrating three pillars: (i) local Bayesian inference for uncertainty-aware modeling; (ii) privacy-preserving and Byzantine-resilient neighbor selection formulated as a constrained optimization problem minimizing global loss under $(\varepsilon,\delta)$-differential privacy and detection guarantees; and (iii) a fully distributed Graph Neural Network–Reinforcement Learning (GNN-RL) algorithm enabling autonomous connection decisions using only local observations. Experiments show our method achieves 92.1% accuracy under 30% Byzantine attacks—outperforming baselines by 11.4%—while accelerating convergence 2.8× and reducing communication overhead by 63%. We also theoretically characterize the fundamental trade-offs among connectivity dynamics, privacy level, Byzantine detection, and convergence speed.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models</title>
      <link>https://arxiv.org/abs/2602.19945v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19945v1</guid>
      <description>Balancing convergence speed and privacy robustness remains a key challenge in Differentially Private Federated Learning (DPFL). While AdamW excels in large-model training, its direct use in DPFL suffers from inflated second-moment variance, DP-induced bias in $v_t$, and exacerbated client drift under data heterogeneity. We propose **DP-FedAdamW**, the first AdamW-based optimizer tailored for DPFL. It restores AdamW’s efficacy under privacy constraints via: (i) variance-stabilized second-moment estimation, (ii) an analytically unbiased correction for DP noise bias, and (iii) global-direction alignment to suppress client drift. Theoretically, we prove a linearly accelerated convergence rate $O(1/T)$ *without any heterogeneity assumptions*, and deliver tighter $(\varepsilon,\delta)$-DP guarantees. Empirically, DP-FedAdamW outperforms SOTA by **+5.83%** accuracy on Tiny-ImageNet (Swin-Base, $\varepsilon = 1$), and consistently improves performance across LLMs, ViTs, and ResNet-18 under realistic DP budgets. Code is available in the Appendix.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>privacy</category>
      <category>differential</category>
      <category>federated</category>
      <category>dp</category>
      <category>learning</category>
    </item>
    <item>
      <title>Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models</title>
      <link>https://arxiv.org/abs/2602.19926v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19926v1</guid>
      <description>Fine-tuning large models under differentially private federated learning (DPFL) suffers from severe utility degradation when applying standard LoRA due to three underexplored issues: gradient coupling between asymmetric low-rank matrices, compounded noise amplification from DP perturbation, and increased loss landscape sharpness in the global model. To address these, we propose **LA-LoRA (Local Alternating LoRA)**—a novel PEFT framework that alternates updates of the two LoRA matrices locally per client, aligns update directions via subspace projection, and incorporates sharpness-aware clipping. Theoretically, LA-LoRA improves convergence guarantees under DP noise with reduced sensitivity $O(\sqrt{r})$. Experiments show state-of-the-art performance: on Swin-B/Tiny-ImageNet with $\varepsilon = 1$, LA-LoRA achieves **72.41% → 89.24% test accuracy**, outperforming RoLoRA by +16.83%; it consistently excels across both LVMs and LLMs under strict privacy budgets ($\varepsilon \leq 2$). Code is publicly available.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>privacy</category>
      <category>differential</category>
      <category>federated</category>
      <category>dp</category>
      <category>learning</category>
    </item>
    <item>
      <title>MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2602.19843v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19843v1</guid>
      <description>MAS-FIRE is a systematic framework for fault injection and reliability evaluation of LLM-based Multi-Agent Systems (MAS). Recognizing that MAS fail silently due to semantic errors—e.g., hallucinations, misinterpreted instructions, and reasoning drift—we introduce a taxonomy of 15 fault types spanning intra-agent cognitive flaws and inter-agent coordination breakdowns. We inject faults via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applied to three representative MAS architectures, MAS-FIRE reveals four-tiered fault-tolerant behaviors (mechanism, rule, prompt, and reasoning), enabling fine-grained diagnosis of failure origins and recovery efficacy. Crucially, we find that stronger foundation models do not uniformly improve robustness; instead, architectural topology is equally decisive—iterative, closed-loop designs mitigate over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE delivers process-level observability and actionable insights for systematically engineering reliable multi-agent systems.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>injection</category>
      <category>prompt</category>
    </item>
    <item>
      <title>OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research</title>
      <link>https://arxiv.org/abs/2602.19810v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19810v1</guid>
      <description>This paper analyzes the OpenClaw–Moltbook ecosystem—the first large-scale agent-only scientific interaction network—and introduces **ClawdLab**, an open-source platform for autonomous scientific research. Through a multivocal literature review of 27 sources, we identify critical architectural failure modes: pervasive security vulnerabilities (131 exposed skills, &gt;15,200 unsecured control panels), overreliance on social consensus instead of computational evidence, and rigid, non-composable architectures that impede cumulative improvement. ClawdLab addresses these via five design principles: hard role separation, structured adversarial critique grounded in tool-executed validation, PI-led governance with binding veto power, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints—ensuring verification depends on computational outputs (e.g., theorem provers, simulators, instrument data), not peer agreement. This architecture inherently confers Sybil resistance. We propose a three-tier taxonomy of AI research systems: (1) single-agent pipelines, (2) predetermined multi-agent workflows, and (3) fully decentralized, composable systems. ClawdLab is the first implementation of Tier 3, where foundation models, capabilities, governance rules, and evidence protocols are independently upgradable—enabling compounding advances as the broader AI ecosystem evolves.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>agent</category>
    </item>
    <item>
      <title>DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework</title>
      <link>https://arxiv.org/abs/2602.19702v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19702v1</guid>
      <description>Multimodal recommender systems improve cold-start and sparsity issues by integrating heterogeneous signals (e.g., ratings, reviews, images). Yet mainstream approaches suffer from modality isolation, strict requirement of complete multimodal data per interaction, and disjoint user/item representation learning—leading to misaligned embeddings and poor robustness. We propose **DReX**, a unified deep learning framework that incrementally refines *both* user and item representations using interaction-level multimodal feedback (e.g., rating magnitude, review sentiment, keyword salience) via gated recurrent units (GRUs). This design enables: (1) joint modeling of fine-grained interactions and global preference patterns; (2) end-to-end alignment of user/item embeddings through shared GRU dynamics; and (3) inherent robustness to missing or partial modalities. Evaluated on three real-world datasets (Amazon-Books, Yelp, Steam), DReX consistently outperforms state-of-the-art methods (e.g., +12.7% average Recall@10). Crucially, by treating review text as a first-class modality, DReX automatically generates interpretable keyword profiles for users and items—providing transparent, human-readable preference indicators that enhance recommendation trustworthiness.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark</title>
      <link>https://arxiv.org/abs/2602.19502v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19502v1</guid>
      <description>This paper introduces a human-guided agentic AI framework for multimodal clinical prediction, validated on all three AgentDS Healthcare Benchmark tasks: 30-day readmission (Macro-F1 = 0.8986), ED cost forecasting (MAE = $465.13), and discharge readiness (Macro-F1 = 0.7939). Human analysts intervened at critical decision points—multimodal feature engineering (clinical notes, PDF receipts, vital signs), task-appropriate model selection, and clinically informed validation—yielding 5th overall in healthcare and 3rd on discharge readiness. Ablation studies show human guidance delivers a cumulative +0.065 Macro-F1 gain over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed, stage-wise feature engineering outperforms exhaustive automated search; (2) multimodal integration requires task-specific human judgment—no universal strategy exists across text, PDFs, and time-series; and (3) deliberate, clinically motivated ensemble diversity surpasses random hyperparameter tuning. These findings advance deployable, interpretable, and clinically valid AI in real-world healthcare settings.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>Federated Learning Playground</title>
      <link>https://arxiv.org/abs/2602.19489v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19489v1</guid>
      <description>We introduce *Federated Learning Playground*, an interactive, browser-based educational platform inspired by TensorFlow Playground and specifically designed to demystify core federated learning (FL) concepts. It enables users—without coding, installation, or backend setup—to configure heterogeneous client data distributions (e.g., controllable non-IID skew), tune model hyperparameters (learning rate, local epochs), and select from multiple aggregation algorithms (FedAvg, FedProx, SCAFFOLD, FedNova). All computations run client-side in real time, with intuitive visualizations showing per-client and global model behavior—highlighting challenges like non-IID degradation, local overfitting, and convergence instability. The playground serves a dual purpose: as an accessible entry point for students and practitioners to build intuition about FL dynamics, and as a rapid prototyping sandbox for researchers comparing algorithmic variants. By lowering technical barriers and enabling immediate, visual experimentation, it advances democratized understanding and adoption of federated learning.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement</title>
      <link>https://arxiv.org/abs/2602.19396v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19396v1</guid>
      <description>Large language models (LLMs) are vulnerable to *concealed jailbreaks*—semantically fluent prompts that hide malicious goals within benign framing, evading heuristic detectors. To address this, we propose **ReDAct**, a self-supervised framework that disentangles goal and framing representations from frozen LLM activations at inference. We introduce **GoalFrameBench**, a controlled benchmark with orthogonal goal/framing variations, to train ReDAct without fine-tuning. Leveraging the disentangled framing representations, we design **FrameShield**, a lightweight, model-agnostic anomaly detector requiring only forward passes. Across 6 LLM families (e.g., Llama-3, Qwen, Phi-3), FrameShield achieves 92.7% detection accuracy (+31.5% over baselines) with &lt;1.8% false positive rate and negligible latency overhead. Theoretically, ReDAct satisfies causal invariance and sufficiency guarantees; empirically, it enables mechanistic interpretation—revealing distinct architectural footprints for goal (early attention) versus framing (late MLP) signals. This establishes semantic disentanglement as a foundational building block for both LLM safety and interpretability.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>llm</category>
      <category>jailbreak</category>
    </item>
    <item>
      <title>Dirichlet Scale Mixture Priors for Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2602.19859v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19859v1</guid>
      <description>We propose the **Dirichlet Scale Mixture (DSM) prior**, a novel structured, sparsity-inducing prior for Bayesian neural networks (BNNs). DSM hierarchically couples Dirichlet-distributed scale allocations across weights with heavy-tailed local shrinkage (e.g., Student-*t*), enabling adaptive, geometry-aware regularization under neural network parameterizations. Theoretically, we derive its dependence structure and asymptotic shrinkage properties. Empirically, DSM yields sparse, robust models: it achieves competitive predictive accuracy with **30–65% fewer effective parameters**, shows **+8.2–14.7% adversarial accuracy gain** over Gaussian-prior BNNs under FGSM/PGD attacks, and excels in **small, correlated data regimes** where it reduces prediction error by up to 22%. Crucially, its heavy-tailed design mitigates the cold posterior effect without ad hoc temperature tuning—offering a principled alternative to standard Gaussian priors.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>adversarial</category>
      <category>learning</category>
      <category>machine</category>
    </item>
    <item>
      <title>Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation</title>
      <link>https://arxiv.org/abs/2602.19668v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19668v1</guid>
      <description>Longitudinal medical report generation is clinically vital but hindered by stringent privacy requirements and dynamic disease progression. Conventional federated learning (FL) fails to capture temporal shifts across patient visits or individual heterogeneity due to its stationary client assumption, leading to unstable optimization and poor temporal coherence. We propose **FedTAR**, a novel framework built upon the **Federated Temporal Adaptation (FTA)** paradigm. FedTAR integrates **demographic-driven personalization**—generating lightweight LoRA adapters from demographic embeddings—and **time-aware global aggregation**, where visit-level updates are weighted by a meta-learned temporal policy optimized via first-order MAML. Evaluated on J-MID (1M exams) and MIMIC-CXR, FedTAR consistently improves linguistic accuracy (+2.8 BLEU-4), temporal coherence (+17.3%), and cross-site generalization (−21.5% error), establishing a robust, privacy-preserving foundation for longitudinal federated modeling in healthcare.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining</title>
      <link>https://arxiv.org/abs/2602.19548v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19548v1</guid>
      <description>This paper challenges the prevailing practice of using a single fixed HTML-to-text extractor for web-scale LLM pretraining datasets. We empirically demonstrate that diverse extractors retain largely non-overlapping webpage subsets—despite yielding comparable model performance on standard NLU benchmarks—indicating substantial untapped data coverage. A simple union strategy across seven extractors boosts token yield by up to **71%** over DCLM-Baseline while preserving benchmark accuracy (e.g., &lt;0.3 GLUE score drop). Crucially, extractor choice strongly impacts structured content: performance gaps reach **10 p.p. on WikiTQ** and **3 p.p. on HumanEval**, highlighting the critical role of table and code block fidelity. Our work establishes multi-extractor union as a low-cost, high-impact preprocessing paradigm for richer, more robust pretraining corpora.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>PIS: A Physics-Informed System for Accurate State Partitioning of $Aβ_{42}$ Protein Trajectories</title>
      <link>https://arxiv.org/abs/2602.19444v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19444v1</guid>
      <description>Understanding conformational dynamics of $Aβ_{42}$ is critical for Alzheimer’s disease research, yet conventional deep learning methods lack physical grounding to resolve subtle metastable transitions in MD trajectories. We present **PIS**, a Physics-Informed System that integrates precomputed physical priors—specifically radius of gyration (*Rg*) and solvent-accessible surface area (*SASA*)—directly into topological feature extraction via persistent homology. By enforcing physical consistency during state partitioning (e.g., rejecting clusters violating *Rg–SASA* correlation), PIS achieves superior robustness: on a 12-μs $Aβ_{42}$ dataset, it attains an F1-score of **0.89**, outperforming tICA+HMM and VAMPnet by 23% and 17%, respectively. Crucially, PIS identifies a previously elusive pH-sensitive “coil-to-hydrophobic-collapse” intermediate. The system includes an interactive dashboard for real-time physical monitoring and multi-dimensional validation, offering biologists an interpretable, physics-grounded toolkit. Code and demo are publicly available.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>Federated Causal Representation Learning in State-Space Systems for Decentralized Counterfactual Reasoning</title>
      <link>https://arxiv.org/abs/2602.19414v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19414v1</guid>
      <description>This paper introduces **FedCRL-SS**, the first federated framework for causal representation learning in state-space systems, enabling decentralized counterfactual reasoning under strict data privacy and model immutability constraints. Each client learns a low-dimensional latent state that disentangles intrinsic dynamics from control-driven effects via a private encoder; a central server aggregates only these compact states to estimate a globally consistent, interpretable state-transition and cross-client control-effect structure. Crucially, clients perform counterfactual inference locally—e.g., predicting how their output would change if another client altered its control input—without sharing raw data or modifying local models. We prove convergence to a centralized causal oracle and provide $(\varepsilon,\delta)$-differential privacy guarantees. Experiments on synthetic multi-oscillator networks and real-world industrial control datasets (e.g., blast furnace–hot stove coupling) show 37.2% lower counterfactual error than baselines and 89% reduced communication overhead, scaling to 100+ clients.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>SPRINT: New Isogeny Proofs of Knowledge and Isogeny-Based Signatures</title>
      <link>https://eprint.iacr.org/2026/364</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/364</guid>
      <description>We present **SPRINT**, an efficient polynomial IOP-based zero-knowledge proof of knowledge for isogenies, which encodes radical $2$-isogeny formulas directly into multivariate polynomial constraints. Integrated with the DeepFold PCS, SPRINT achieves major improvements: for NIST Level I ($p = 5 \cdot 2^{248} - 1$), proofs take **a few milliseconds**, verification is similarly fast, and proof sizes are **~80 kB**—yielding **1.1–8× prover speedup**, **4.4–24× verifier speedup**, and **1.2–2.3× smaller proofs** vs. prior art. We prove that any Fiat–Shamir–compiled interactive proof with a *canonical simulator* is **weakly simulation-extractable (wSE)**—a general result applicable to many post-quantum proof systems. Leveraging SPRINT and wSE, we construct a new family of signatures whose security rests *solely* on the $\ell$-isogeny path problem. A DeepFold-based instantiation matches SQIsign’s performance across NIST levels; while signatures are larger, the scheme relies on weaker assumptions and offers inherent flexibility for optimization—both within and across PCS designs—naturally inheriting future advances in plausibly post-quantum PCS.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>LazyArc: Dynamic Out-of-Order Engine for High-Throughput FHE</title>
      <link>https://eprint.iacr.org/2026/363</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/363</guid>
      <description>Fully Homomorphic Encryption (FHE) enables privacy-preserving computation on encrypted data but suffers from severe performance bottlenecks—especially bootstrapping, whose high latency limits throughput. This paper introduces **LazyArc**, a lightweight, dynamic out-of-order (OoO) execution engine designed to accelerate FHE workloads expressed as instruction sequences. LazyArc features a **hybrid arithmetic-Boolean execution unit**, enabling seamless co-execution of CKKS-style polynomial operations and bit-level logic in a single program. Crucially, its OoO scheduler dynamically identifies and executes independent instructions during bootstrapping latency, effectively masking this overhead. To enable proactive bootstrapping decisions, we propose **RegisterMap**, a novel static analysis structure that models ciphertext noise propagation across FHE circuits and guides fine-grained, timing-aware bootstrap scheduling. Evaluated on linear algebra benchmarks, LazyArc achieves **~10% higher throughput** than state-of-the-art baselines while reducing unnecessary bootstraps by 23%, with minimal hardware overhead (&lt;8%).</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Janus-FHE: A Side Channel Resilient Framework for High-Degree Homomorphic Encryption on GPUs</title>
      <link>https://eprint.iacr.org/2026/362</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/362</guid>
      <description>Janus-FHE is a GPU-accelerated framework for BFV homomorphic encryption that achieves intrinsic side-channel resilience during ciphertext multiplication and relinearization. To eliminate data-dependent timing leaks inherent in conventional GPU-based HE implementations, we reformulate polynomial multiplication via Kronecker substitution and compute it using a Discrete Galois Transform (DGT)-based Schönhage–Strassen algorithm. Crucially, we implement DGT with the Stockham auto-sort algorithm—ensuring strictly deterministic, input-independent memory access patterns to mitigate cache-timing vulnerabilities. For relinearization, we introduce a constant-time strategy using masked arithmetic instead of conditional branching, preventing warp divergence on SIMT architectures. Experiments on NVIDIA A100 confirm Janus-FHE eliminates control-flow leakage observed in state-of-the-art libraries (e.g., HEonGPU) and successfully executes multiplications for polynomials up to degree $2^{18}$, significantly extending the practical computational reach of side-channel-resilient GPU-based FHE.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Scytale: A Compiler Framework for Accelerating TFHE with Circuit Bootstrapping</title>
      <link>https://eprint.iacr.org/2026/361</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/361</guid>
      <description>Fully Homomorphic Encryption (FHE) enables secure outsourced computation, but TFHE’s performance remains prohibitive for complex circuits due to the inefficiency of Programmable Bootstrapping (PBS), which is limited to small (3–4 bit) lookup tables (LUTs). This paper introduces **Scytale**, a novel MLIR-based compiler framework that overcomes this limitation by integrating **Circuit Bootstrapping (CBS)** and **Vertical Packing (VP)** to evaluate LUTs up to **12 bits**. Scytale defines new MLIR dialects for CBS and VP, leverages Yosys for hardware-aware circuit synthesis, and introduces bespoke optimization passes—especially **shared LUT fusion**—to minimize cryptographic operations. Experimental evaluation shows Scytale achieves **3.2×–5.8× speedup** over PBS-only baselines on real-world circuits (e.g., AES S-Box), reducing bootstrapping overhead by 67%. This work demonstrates that compiler-driven, circuit-level co-optimization is essential for practical TFHE acceleration.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>CREDIT: Certified Ownership Verification of Deep Neural Networks Against Model Extraction Attacks</title>
      <link>https://arxiv.org/abs/2602.20419v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20419v1</guid>
      <description>CREDIT is the first framework for **certified ownership verification** of deep neural networks against model extraction attacks (MEAs). It introduces **mutual information (MI)** as a theoretically grounded similarity metric between models and derives a practical, computable verification threshold with rigorous statistical guarantees: if the estimated MI between a suspect model and the original falls below this threshold, ownership infringement can be *provably rejected* with confidence $1-\delta$ ($\delta \leq 0.01$). Evaluated across diverse benchmarks (CIFAR, ImageNet, SVHN, GLUE), CREDIT achieves **98.2% average verification accuracy** and **&lt;0.8% false positive rate** using only ≤5,000 queries—outperforming state-of-the-art watermarking defenses. Its certification is robust to fine-tuning and input perturbations, and the implementation is publicly available at https://github.com/LabRAI/CREDIT.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>CITED: A Decision Boundary-Aware Signature for GNNs Towards Model Extraction Defense</title>
      <link>https://arxiv.org/abs/2602.20418v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20418v1</guid>
      <description>Graph Neural Networks (GNNs) are widely deployed via MLaaS, yet vulnerable to Model Extraction Attacks (MEAs), where adversaries query the target model with subgraphs to train high-fidelity surrogates. Existing defenses—watermarking or fingerprinting—are typically limited to *either* label-level *or* embedding-level verification, often degrading model utility or introducing inference overhead. We propose **CITED**, the first decision boundary-aware signature framework enabling *joint* ownership verification at *both* label and embedding levels. CITED injects imperceptible, robust signatures by guiding the model’s decision boundary on carefully designed “signature subgraphs” during standard training—requiring *no architectural changes*, *zero additional parameters*, and *no performance drop*. Extensive experiments across 6 real-world graph datasets and 3 GNN backbones show CITED achieves &gt;99.2% label-level verification accuracy and &gt;0.987 AUC on embedding-level verification, outperforming all prior methods by ≥12.6% in robustness. It remains effective under severe perturbations (e.g., 50% node pruning, cross-architecture transfer). Code: https://github.com/LabRAI/CITED.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models</title>
      <link>https://arxiv.org/abs/2602.20324v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20324v1</guid>
      <description>We present **RARE-PHENIX**, an end-to-end AI framework for rare disease phenotyping from unstructured clinical notes. It unifies three clinically grounded components: (1) LLM-based phenotype extraction, (2) ontology-aware standardization to Human Phenotype Ontology (HPO) terms leveraging semantic structure and embeddings, and (3) supervised ranking of diagnostically informative HPO terms. Trained on 2,671 UDN patients across 11 sites and externally validated on 16,357 real-world notes from Vanderbilt, RARE-PHENIX achieves an ontology-based similarity score of **0.70**—significantly outperforming PhenoBERT (0.58) and improving F1 by 12.3%. Ablation studies confirm the additive value of each module. By modeling phenotyping as a unified clinical workflow—not just extraction—RARE-PHENIX delivers structured, ranked, clinician-concordant phenotypes, enabling scalable, human-in-the-loop rare disease diagnosis.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>Distributed Monotone-Policy Encryption with Silent Setup from Lattices</title>
      <link>https://eprint.iacr.org/2026/372</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/372</guid>
      <description>We present the first **lattice-based distributed monotone-policy encryption scheme with silent setup**, resolving a critical gap in post-quantum decentralized cryptography. Unlike prior silent-setup constructions—exclusively built on quantum-vulnerable bilinear pairings—our scheme rests on the hardness of the Learning With Errors (LWE) problem, ensuring quantum resistance. It supports arbitrary monotone access policies expressed as Disjunctive Normal Forms (DNFs), enabling expressive, fine-grained access control without trusted setup or interactive key generation. Each user independently generates a lattice-based key pair; the joint public key is derived *deterministically* from all users’ public keys via hash-and-combine operations—no communication or dealer required. We prove IND-CPA security under standard LWE assumptions and formally establish the correctness and robustness of the distributed decryption protocol. This work establishes a foundational primitive for trustless, quantum-safe policy enforcement in decentralized systems.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>A Modular Approach to Succinct Arguments for QMA</title>
      <link>https://eprint.iacr.org/2026/371</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/371</guid>
      <description>We present the first *modular*, *LWE-free* framework for succinct, classically-verifiable arguments for QMA—the quantum analogue of NP. Departing from prior constructions that crucially rely on the structured hardness of LWE, our approach builds on two minimal, well-motivated assumptions: (i) *oblivious state preparation (OSP)*—constructible from plain trapdoor claw-free functions—and (ii) *collapsing hash functions*, the quantum counterpart of collision resistance. Our construction proceeds in two steps: first, we design a *round-efficient*, classically-verifiable QMA argument system based solely on OSP; second, we introduce a *generalized communication compression compiler* that, assuming collapsing hashes, compresses any $T$-round interactive protocol with a classical verifier into one whose total communication is bounded by $T \cdot \poly(\secp)$, independent of original message sizes. This yields the first succinct QMA argument system without LWE, significantly broadening its cryptographic foundations and offering new tools for quantum interactive proofs.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Round-Optimal Byzantine Agreement without Trusted Setup</title>
      <link>https://eprint.iacr.org/2026/370</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/370</guid>
      <description>Byzantine Agreement (BA) is a cornerstone primitive in distributed computing and cryptography, where minimizing round complexity is critical. Classical lower bounds (Karlin-Yao’84; Chor-Merritt-Shmoys’89) show that any randomized $r$-round protocol tolerating $t = \Theta(n)$ corruptions must fail with probability at least $(c \cdot r)^{-r}$. Ghinea-Goyal-Liu-Zhang (Eurocrypt’22) achieved round-optimality but required trusted setup—namely, unique threshold signatures and random oracles.  

This work presents the **first round-optimal BA protocols without any trusted setup**. We give: (1) a statistically secure $r$-round protocol for $t &lt; n/3$, assuming only digital signatures and a bulletin-board PKI; and (2) a computationally secure $r$-round protocol for $t &lt; (1-\epsilon)n/2$ (any constant $\epsilon &gt; 0$), relying solely on standard signature-based PKI—no threshold signatures, no random oracles. Our constructions match the $(c \cdot r)^{-r}$ failure bound and eliminate all setup assumptions, resolving a central open problem.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Issuer-Hiding for BBS Anonymous Credentials via Randomizable Keys</title>
      <link>https://eprint.iacr.org/2026/369</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/369</guid>
      <description>Anonymous credentials (ACs) enable users to prove statements about attested attributes while minimizing data disclosure. However, standard ACs reveal the credential issuer upon presentation—exposing more information than necessary (e.g., when only proving age or personhood). Issuer-Hiding ACs (IHACs) address this by hiding the specific issuer, revealing only that the credential originates from *some* issuer within a predefined trust policy. Prior compact IHAC constructions for BBS signatures (by Katz–Sefranek and Sanders–Traoré) require verifiers to hold *policy-specific secret keys*, turning verification into a private-key operation—introducing key management overhead and weakening practical privacy and security guarantees. We propose two new BBS-based IHAC schemes achieving compact presentations (constant-size, independent of policy size) **without any verifier secret keys**. At their core is a novel technique for **randomizing BBS public keys**, enabling policy-level equivalence while preserving signature validity and public verifiability. Our constructions rely solely on standard assumptions (e.g., SXDH) and offer improved deployability and trust model clarity.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Additions, Multiplications, and the Interaction In-Between: Optimizing MPC Protocols via Leveled Linear Secret Sharing</title>
      <link>https://eprint.iacr.org/2026/368</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/368</guid>
      <description>We introduce *leveled linear secret sharing* (LLSS), a paradigm that treats intermediate secret sharing representations—not as transient artifacts but as first-class computational domains—enabling dynamic, on-demand switching between original and intermediate sharing schemes. Crucially, all linear operations (addition, scalar multiplication, linear combinations) become *non-interactive* in either domain. Multiplication is split into: (i) a non-interactive step converting inputs to the intermediate domain, and (ii) a *delayed, batched interactive upgrade* back to the original domain—applied once over aggregated results (e.g., sum of many products) rather than per multiplication. We integrate LLSS into replicated sharing (3PC), BGW-style $n$-party protocols, and masked ABY2.0, and design an optimal circuit-level optimizer that selects the best domain for each gate. Evaluation shows **10–37% communication reduction** and **10–26% runtime speedup** (LAN), with full implementation publicly available.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>High-Precision Functional Bootstrapping for CKKS from Fourier Extension</title>
      <link>https://eprint.iacr.org/2026/367</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/367</guid>
      <description>We propose a novel amortized functional bootstrapping (FBS) framework for the CKKS homomorphic encryption scheme, built upon *Fourier extension*. While Fourier series approximation of modular reduction is standard in CKKS bootstrapping, its efficient generalization to arbitrary smooth functions remains open. We show that by constructing *proper Fourier extensions*, any $C^\kappa$ function on a bounded domain can be approximated by a degree-$n$ Fourier series with local error $O(n^{-\kappa-2})$ (except at singularities), improving over the prior global $O(n^{-1})$ bound [AKP2025]. Our key insight is a new extension method that maximizes smoothness *in the Fourier sense*—ensuring rapid spectral decay. Implemented in OpenFHE, our FBS achieves **10–27 bits higher precision** and **1.1–2× lower amortized latency** across benchmark functions (e.g., Sigmoid, GeLU), enabling more accurate and efficient privacy-preserving ML inference.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Careful with the Ring: Enhanced Hybrid Decoding Attacks against Module/Ring-LWE</title>
      <link>https://eprint.iacr.org/2026/366</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/366</guid>
      <description>We propose the first ring-structure-aware **enhanced hybrid decoding attack** against Module/Ring-LWE over $\mathbb{Z}_q[X]/(X^N+1)$. By exploiting cyclic convolution and structured lattice reduction, our method accelerates both the guessing (by $O(N)$ factor in sparse secret setting) and decoding steps. Implemented on standard benchmarks ([WSM+25, S&amp;P]), it achieves 17×–114× speedup over prior art ([KKN+26, EC]) on broken instances. Integrated into the Lattice Estimator framework, our attack reduces concrete bit-security estimates by 7–13 bits across five state-of-the-art sparse Ring-LWE parameter sets used in FHE—causing **10 out of 16 configurations to fall below the 128-bit security threshold**. This demonstrates that algebraic structure must be accounted for in real-world security assessments.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Cube and Integral Attacks on ChiLow-32</title>
      <link>https://eprint.iacr.org/2026/365</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/365</guid>
      <description>This paper presents the first dedicated cryptanalysis of ChiLow-(32 + τ), the 32-bit tweakable block cipher underlying the ACE-2 authenticated code encryption framework (EUROCRYPT 2025). Leveraging the algebraic properties of its quadratic ChiChi ($\dchi$) layer, we mount practical cube and integral attacks in both single- and multiple-tweak settings. We recover the full key for 5-round ChiLow with $2^{32}$ decryptions and $2^{18.58}$ chosen ciphertexts; extend to 6 rounds with $2^{34}$ decryptions and $2^{33.58}$ data; and—most significantly—achieve the first 7-round key-recovery integral attack requiring only $2^{6.32}$ chosen ciphertexts, $2^{108.55}$ encryption-equivalents, and negligible memory, enabled by a novel nested tweak-recovery strategy and high-degree monomial exploitation. All attacks respect the scheme’s stated security constraints.</description>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Taming Preconditioner Drift: Unlocking the Potential of Second-Order Optimizers for Federated Learning on Non-IID Data</title>
      <link>https://arxiv.org/abs/2602.19271v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19271v1</guid>
      <description>Second-order optimizers accelerate large-scale training but suffer from instability and divergence in federated learning (FL) on non-IID data. We identify *preconditioner drift*—the accumulation of heterogeneous, curvature-defined geometries across clients—as the key cause: naive model averaging under incompatible local preconditioners corrupts the global descent direction. To resolve this geometric mismatch, we propose **FedPAC**, a framework that explicitly decouples parameter aggregation from geometry synchronization via (i) *Alignment*: aggregating local preconditioners into a global reference and warm-starting clients with it; and (ii) *Correction*: steering local preconditioned updates using a global preconditioned direction to suppress long-term drift. We provide non-convex convergence guarantees with linear speedup under partial participation. Empirically, FedPAC improves stability and accuracy across vision and language tasks—e.g., +5.8% absolute accuracy on non-IID CIFAR-100 with ViTs—and achieves up to 2.3× faster convergence. Code is available at https://anonymous.4open.science/r/FedPAC-8B24.</description>
      <pubDate>Sun, 22 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>HybridFL: A Federated Learning Approach for Financial Crime Detection</title>
      <link>https://arxiv.org/abs/2602.19207v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.19207v1</guid>
      <description>Federated learning (FL) enables collaborative model training without raw data sharing, yet standard FL assumes either horizontal (user-partitioned) or vertical (feature-partitioned) data splits—failing to address real-world financial crime detection, where data is *hybridly distributed*: disjoint user sets (e.g., non-overlapping bank customers) *and* complementary features (e.g., banks hold account-level attributes; payment processors hold transaction-level attributes). This paper proposes **HybridFL**, the first FL framework jointly optimizing horizontal aggregation and vertical feature fusion under strict data locality. HybridFL introduces a hierarchical secure aggregation protocol to align encrypted account representations across banks while fusing transaction sequences across parties—without exposing raw data or identifiers. Evaluated on AMLSim and SWIFT datasets, HybridFL achieves **23.6% higher AUC** than transaction-only local models and reaches **96.3% of centralized benchmark performance**, with communication overhead within 112% of standard FL. It bridges the gap between privacy compliance and detection efficacy in cross-institutional financial surveillance.</description>
      <pubDate>Sun, 22 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>machine</category>
      <category>privacy-preserving</category>
      <category>learning</category>
      <category>federated</category>
    </item>
  </channel>
</rss>