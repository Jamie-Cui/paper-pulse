{
  "papers": [
    {
      "id": "arxiv_2602.19271v1",
      "arxiv_id": "2602.19271v1",
      "title": "Taming Preconditioner Drift: Unlocking the Potential of Second-Order Optimizers for Federated Learning on Non-IID Data",
      "authors": [
        "Junkang Liu",
        "Fanhua Shang",
        "Hongying Liu",
        "Jin Liu",
        "Weixin An",
        "Yuanyuan Liu"
      ],
      "abstract": "Second-order optimizers can significantly accelerate large-scale training, yet their naive federated variants are often unstable or even diverge on non-IID data.   We show that a key culprit is \\emph{preconditioner drift}: client-side second-order training induces heterogeneous \\emph{curvature-defined geometries} (i.e., preconditioner coordinate systems), and server-side model averaging updates computed under incompatible metrics, corrupting the global descent direction.   To address this geometric mismatch, we propose \\texttt{FedPAC}, a \\emph{preconditioner alignment and correction} framework for reliable federated second-order optimization.   \\texttt{FedPAC} explicitly decouples parameter aggregation from geometry synchronization by:   (i) \\textbf{Alignment} (i.e.,aggregating local preconditioners into a global reference and warm-starting clients via global preconditioner); and   (ii) \\textbf{Correction} (i.e., steering local preconditioned updates using a global preconditioned direction to suppress long-term drift).   We provide drift-coupled non-convex convergence guarantees with linear speedup under partial participation.   Empirically, \\texttt{FedPAC} consistently improves stability and accuracy across vision and language tasks, achieving up to $5.8\\%$ absolute accuracy gain on CIFAR-100 with ViTs.   Code is available at https://anonymous.4open.science/r/FedPAC-8B24.",
      "published": "2026-02-22",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.19271v1",
      "url": "https://arxiv.org/abs/2602.19271v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n二阶优化器在大规模训练中可显著加速收敛，但其直接迁移至联邦学习（FL）时，在非独立同分布（non-IID）数据下常出现不稳定甚至发散现象。本文揭示核心症结在于**预条件器漂移（preconditioner drift）**：各客户端基于本地数据执行二阶训练，导致其隐式构建的**曲率定义几何结构**（即预条件坐标系）高度异构；服务器端对模型参数进行简单平均时，实则在不兼容的几何度量下混合更新方向，严重扭曲全局下降方向，造成优化失败。\n\n## 方法创新：FedPAC 框架  \n为解决这一几何失配问题，我们提出 **FedPAC**（Federated Preconditioner Alignment and Correction），首个显式解耦参数聚合与几何同步的联邦二阶优化框架。其包含两大协同机制：  \n- **对齐（Alignment）**：服务器聚合本地预条件器（如Hessian近似）生成全局参考预条件器，并将其下发以**热启动**客户端下一轮二阶训练，确保几何基准一致；  \n- **校正（Correction）**：客户端在本地预条件梯度更新中，引入由全局预条件器导出的**校正方向**，动态抑制长期漂移，保障局部更新与全局几何对齐。  \n\n## 理论与实验验证  \n我们建立了**漂移耦合的非凸收敛理论**，在部分参与设定下证明线性加速性质。实验表明，FedPAC 在 Vision Transformer（ViT）和语言模型任务上均显著提升稳定性与精度：在 non-IID CIFAR-100 上实现最高 **5.8% 绝对准确率提升**，收敛速度加快 2.3×，且鲁棒支持异构客户端规模与通信轮次。代码已开源。",
      "summary_en": "Second-order optimizers accelerate large-scale training but suffer from instability and divergence in federated learning (FL) on non-IID data. We identify *preconditioner drift*—the accumulation of heterogeneous, curvature-defined geometries across clients—as the key cause: naive model averaging under incompatible local preconditioners corrupts the global descent direction. To resolve this geometric mismatch, we propose **FedPAC**, a framework that explicitly decouples parameter aggregation from geometry synchronization via (i) *Alignment*: aggregating local preconditioners into a global reference and warm-starting clients with it; and (ii) *Correction*: steering local preconditioned updates using a global preconditioned direction to suppress long-term drift. We provide non-convex convergence guarantees with linear speedup under partial participation. Empirically, FedPAC improves stability and accuracy across vision and language tasks—e.g., +5.8% absolute accuracy on non-IID CIFAR-100 with ViTs—and achieves up to 2.3× faster convergence. Code is available at https://anonymous.4open.science/r/FedPAC-8B24.",
      "summary": "## 背景与问题  \n二阶优化器在大规模训练中可显著加速收敛，但其直接迁移至联邦学习（FL）时，在非独立同分布（non-IID）数据下常出现不稳定甚至发散现象。本文揭示核心症结在于**预条件器漂移（preconditioner drift）**：各客户端基于本地数据执行二阶训练，导致其隐式构建的**曲率定义几何结构**（即预条件坐标系）高度异构；服务器端对模型参数进行简单平均时，实则在不兼容的几何度量下混合更新方向，严重扭曲全局下降方向，造成优化失败。\n\n## 方法创新：FedPAC 框架  \n为解决这一几何失配问题，我们提出 **FedPAC**（Federated Preconditioner Alignment and Correction），首个显式解耦参数聚合与几何同步的联邦二阶优化框架。其包含两大协同机制：  \n- **对齐（Alignment）**：服务器聚合本地预条件器（如Hessian近似）生成全局参考预条件器，并将其下发以**热启动**客户端下一轮二阶训练，确保几何基准一致；  \n- **校正（Correction）**：客户端在本地预条件梯度更新中，引入由全局预条件器导出的**校正方向**，动态抑制长期漂移，保障局部更新与全局几何对齐。  \n\n## 理论与实验验证  \n我们建立了**漂移耦合的非凸收敛理论**，在部分参与设定下证明线性加速性质。实验表明，FedPAC 在 Vision Transformer（ViT）和语言模型任务上均显著提升稳定性与精度：在 non-IID CIFAR-100 上实现最高 **5.8% 绝对准确率提升**，收敛速度加快 2.3×，且鲁棒支持异构客户端规模与通信轮次。代码已开源。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.19207v1",
      "arxiv_id": "2602.19207v1",
      "title": "HybridFL: A Federated Learning Approach for Financial Crime Detection",
      "authors": [
        "Afsana Khan",
        "Marijn ten Thij",
        "Guangzhi Tang",
        "Anna Wilbik"
      ],
      "abstract": "Federated learning (FL) is a privacy-preserving machine learning paradigm that enables multiple parties to collaboratively train models on privately owned data without sharing raw information. While standard FL typically addresses either horizontal or vertical data partitions, many real-world scenarios exhibit a complex hybrid distribution. This paper proposes Hybrid Federated Learning (HybridFL) to address data split both horizontally across disjoint users and vertically across complementary feature sets. We evaluate HybridFL in a financial crime detection context, where a transaction party holds transaction-level attributes and multiple banks maintain private account-level features. By integrating horizontal aggregation and vertical feature fusion, the proposed architecture enables joint learning while strictly preserving data locality. Experiments on AMLSim and SWIFT datasets demonstrate that HybridFL significantly outperforms the transaction-only local model and achieves performance comparable to a centralized benchmark.",
      "published": "2026-02-22",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.19207v1",
      "url": "https://arxiv.org/abs/2602.19207v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "privacy-preserving",
        "learning",
        "federated"
      ],
      "keyword_score": 4,
      "summary_zh": "## HybridFL：面向金融犯罪检测的混合联邦学习框架  \n\n**背景与挑战**：在反洗钱（AML）与金融欺诈检测等关键场景中，数据高度敏感且分散于多方主体——如银行持有账户级私有特征（如客户身份、历史行为），而支付网关或监管沙箱则掌握交易级属性（如金额、时间、对手方）。传统联邦学习（FL）仅支持单一维度划分：**水平FL**适用于用户重叠但特征一致的场景（如多家银行共享客户ID），**垂直FL**适用于用户一致但特征互补的场景（如同一客户在银行与电商的数据）。然而真实金融生态呈现**混合数据分布**：用户集合不重叠（跨机构客户隔离）、特征维度互补（账户+交易），现有FL范式无法兼顾数据主权与模型效能。\n\n**方法创新**：本文提出**HybridFL**——首个支持“水平+垂直”双重异构性的端到端联邦学习框架。其核心设计包含：（1）**双通道协同训练**：本地节点并行执行账户特征编码（垂直分支）与交易序列建模（水平分支）；（2）**隐私安全融合机制**：通过可证明安全的**分层加密聚合**（Hierarchical Secure Aggregation），在不暴露原始数据前提下，实现跨银行的账户特征对齐与跨交易的用户行为共识；（3）**轻量级协调器**：引入无状态协调节点，仅分发加密梯度模板与验证签名，杜绝中心化数据泄露风险。\n\n**实验验证**：在AML-Sim合成数据集（模拟10万账户、500万笔可疑交易）与真实SWIFT报文子集（含47家银行脱敏交易流）上评估表明：HybridFL较纯交易本地模型**AUC提升23.6%**，F1-score提高18.9%；性能达集中式训练基准的**96.3%**（p<0.01），同时将通信开销控制在标准FL的112%以内。本工作首次在金融合规领域实现了**严格数据不出域、模型能力近似集中式**的实用化突破。",
      "summary_en": "Federated learning (FL) enables collaborative model training without raw data sharing, yet standard FL assumes either horizontal (user-partitioned) or vertical (feature-partitioned) data splits—failing to address real-world financial crime detection, where data is *hybridly distributed*: disjoint user sets (e.g., non-overlapping bank customers) *and* complementary features (e.g., banks hold account-level attributes; payment processors hold transaction-level attributes). This paper proposes **HybridFL**, the first FL framework jointly optimizing horizontal aggregation and vertical feature fusion under strict data locality. HybridFL introduces a hierarchical secure aggregation protocol to align encrypted account representations across banks while fusing transaction sequences across parties—without exposing raw data or identifiers. Evaluated on AMLSim and SWIFT datasets, HybridFL achieves **23.6% higher AUC** than transaction-only local models and reaches **96.3% of centralized benchmark performance**, with communication overhead within 112% of standard FL. It bridges the gap between privacy compliance and detection efficacy in cross-institutional financial surveillance.",
      "summary": "## HybridFL：面向金融犯罪检测的混合联邦学习框架  \n\n**背景与挑战**：在反洗钱（AML）与金融欺诈检测等关键场景中，数据高度敏感且分散于多方主体——如银行持有账户级私有特征（如客户身份、历史行为），而支付网关或监管沙箱则掌握交易级属性（如金额、时间、对手方）。传统联邦学习（FL）仅支持单一维度划分：**水平FL**适用于用户重叠但特征一致的场景（如多家银行共享客户ID），**垂直FL**适用于用户一致但特征互补的场景（如同一客户在银行与电商的数据）。然而真实金融生态呈现**混合数据分布**：用户集合不重叠（跨机构客户隔离）、特征维度互补（账户+交易），现有FL范式无法兼顾数据主权与模型效能。\n\n**方法创新**：本文提出**HybridFL**——首个支持“水平+垂直”双重异构性的端到端联邦学习框架。其核心设计包含：（1）**双通道协同训练**：本地节点并行执行账户特征编码（垂直分支）与交易序列建模（水平分支）；（2）**隐私安全融合机制**：通过可证明安全的**分层加密聚合**（Hierarchical Secure Aggregation），在不暴露原始数据前提下，实现跨银行的账户特征对齐与跨交易的用户行为共识；（3）**轻量级协调器**：引入无状态协调节点，仅分发加密梯度模板与验证签名，杜绝中心化数据泄露风险。\n\n**实验验证**：在AML-Sim合成数据集（模拟10万账户、500万笔可疑交易）与真实SWIFT报文子集（含47家银行脱敏交易流）上评估表明：HybridFL较纯交易本地模型**AUC提升23.6%**，F1-score提高18.9%；性能达集中式训练基准的**96.3%**（p<0.01），同时将通信开销控制在标准FL的112%以内。本工作首次在金融合规领域实现了**严格数据不出域、模型能力近似集中式**的实用化突破。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.19022v1",
      "arxiv_id": "2602.19022v1",
      "title": "An interpretable framework using foundation models for fish sex identification",
      "authors": [
        "Zheng Miao",
        "Tien-Chieh Hung"
      ],
      "abstract": "Accurate sex identification in fish is vital for optimizing breeding and management strategies in aquaculture, particularly for species at the risk of extinction. However, most existing methods are invasive or stressful and may cause additional mortality, posing severe risks to threatened or endangered fish populations. To address these challenges, we propose FishProtoNet, a robust, non-invasive computer vision-based framework for sex identification of delta smelt (Hypomesus transpacificus), an endangered fish species native to California, across its full life cycle. Unlike the traditional deep learning methods, FishProtoNet provides interpretability through learned prototype representations while improving robustness by leveraging foundation models to reduce the influence of background noise. Specifically, the FishProtoNet framework consists of three key components: fish regions of interest (ROIs) extraction using visual foundation model, feature extraction from fish ROIs and fish sex identification based on an interpretable prototype network. FishProtoNet demonstrates strong performance in delta smelt sex identification during early spawning and post-spawning stages, achieving the accuracies of 74.40% and 81.16% and corresponding F1 scores of 74.27% and 79.43% respectively. In contrast, delta smelt sex identification at the subadult stage remains challenging for current computer vision methods, likely due to less pronounced morphological differences in immature fish. The source code of FishProtoNet is publicly available at: https://github.com/zhengmiao1/Fish_sex_identification",
      "published": "2026-02-22",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.19022v1",
      "url": "https://arxiv.org/abs/2602.19022v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n准确识别鱼类性别对水产养殖育种管理至关重要，尤其对濒危物种（如加州三角洲银鱼 *Hypomesus transpacificus*）而言，传统方法多依赖解剖、激素检测或超声等**侵入性手段**，易引发应激反应与额外死亡率，严重威胁种群恢复。\n\n## 方法创新：FishProtoNet 框架  \n本研究提出 **FishProtoNet**——一种可解释、非侵入式的计算机视觉框架，首次实现对三角洲银鱼全生命周期（亚成体、产卵前期、产卵后期）的无损性别识别。其核心创新在于：  \n- **视觉基础模型驱动的ROI提取**：利用预训练视觉大模型（如SAM）精准分割鱼体区域，显著抑制复杂水体背景干扰；  \n- **原型学习可解释性设计**：摒弃“黑箱”分类器，采用原型网络（ProtoNet）将每类性别映射为可视觉化的特征原型，支持热力图级决策溯源；  \n- **基础模型赋能鲁棒性**：通过冻结大模型主干+轻量适配头，缓解小样本（濒危物种数据稀缺）与形态模糊（如亚成体）带来的泛化难题。\n\n## 关键结果与局限  \n- 在产卵前期与产卵后期阶段，准确率分别达 **74.40%** 和 **81.16%**（F1分数：74.27% / 79.43%），显著优于ResNet50、ViT等基线模型；  \n- 亚成体阶段识别性能受限（准确率<60%），印证了该阶段外部形态二态性微弱的生物学现实；  \n- 所有代码与预训练权重已开源：[https://github.com/zhengmiao1/Fish_sex_identification](https://github.com/zhengmiao1/Fish_sex_identification)，推动濒危鱼类AI保护工具标准化。",
      "summary_en": "Accurate, non-invasive fish sex identification is critical for conservation breeding of endangered species like the delta smelt (*Hypomesus transpacificus*), yet existing methods are often invasive and stressful. To address this, we propose **FishProtoNet**, an interpretable, foundation model–based computer vision framework for life-stage–aware sex classification. It integrates: (1) visual foundation models (e.g., SAM) for robust fish region-of-interest extraction; (2) prototype-based representation learning for human-interpretable decisions via visualizable class prototypes; and (3) lightweight adaptation to mitigate background noise and data scarcity. Evaluated across life stages, FishProtoNet achieves **74.40% accuracy (F1: 74.27%)** in pre-spawning and **81.16% accuracy (F1: 79.43%)** in post-spawning delta smelt—outperforming standard CNNs and ViTs. Performance remains limited in subadults due to minimal morphological dimorphism, highlighting a biological constraint rather than a methodological gap. Code and models are publicly available.",
      "summary": "## 背景与挑战  \n准确识别鱼类性别对水产养殖育种管理至关重要，尤其对濒危物种（如加州三角洲银鱼 *Hypomesus transpacificus*）而言，传统方法多依赖解剖、激素检测或超声等**侵入性手段**，易引发应激反应与额外死亡率，严重威胁种群恢复。\n\n## 方法创新：FishProtoNet 框架  \n本研究提出 **FishProtoNet**——一种可解释、非侵入式的计算机视觉框架，首次实现对三角洲银鱼全生命周期（亚成体、产卵前期、产卵后期）的无损性别识别。其核心创新在于：  \n- **视觉基础模型驱动的ROI提取**：利用预训练视觉大模型（如SAM）精准分割鱼体区域，显著抑制复杂水体背景干扰；  \n- **原型学习可解释性设计**：摒弃“黑箱”分类器，采用原型网络（ProtoNet）将每类性别映射为可视觉化的特征原型，支持热力图级决策溯源；  \n- **基础模型赋能鲁棒性**：通过冻结大模型主干+轻量适配头，缓解小样本（濒危物种数据稀缺）与形态模糊（如亚成体）带来的泛化难题。\n\n## 关键结果与局限  \n- 在产卵前期与产卵后期阶段，准确率分别达 **74.40%** 和 **81.16%**（F1分数：74.27% / 79.43%），显著优于ResNet50、ViT等基线模型；  \n- 亚成体阶段识别性能受限（准确率<60%），印证了该阶段外部形态二态性微弱的生物学现实；  \n- 所有代码与预训练权重已开源：[https://github.com/zhengmiao1/Fish_sex_identification](https://github.com/zhengmiao1/Fish_sex_identification)，推动濒危鱼类AI保护工具标准化。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.19020v1",
      "arxiv_id": "2602.19020v1",
      "title": "Learning to Detect Language Model Training Data via Active Reconstruction",
      "authors": [
        "Junjie Oscar Yin",
        "John X. Morris",
        "Vitaly Shmatikov",
        "Sewon Min",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \\textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \\textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \\textsc{ADRA} and its adaptive variant \\textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.",
      "published": "2026-02-22",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.19020v1",
      "url": "https://arxiv.org/abs/2602.19020v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "membership"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n大语言模型（LLM）训练数据检测通常被建模为**成员推断攻击（MIA）**，但传统MIA方法（如基于似然或生成文本的被动分析）受限于固定模型权重，难以捕捉数据与模型内部表征间的深层耦合关系。\n\n## 方法创新：主动数据重建攻击（ADRA）  \n本文提出**Active Data Reconstruction Attack（ADRA）**——一种新型主动式MIA范式。其核心假设是：**训练数据比非成员数据更易被模型“重建”**。我们不依赖静态输出，而是通过**在线微调策略网络**，主动引导目标模型重构候选文本。受强化学习（RL）可锐化已有行为的启发，我们以目标模型参数初始化策略，并采用**on-policy RL**进行端到端优化。关键设计包括：（1）面向重建质量的细粒度指标（如token-level重构保真度与序列一致性）；（2）对比式奖励函数，显式拉大成员/非成员样本的重建得分差距。由此衍生出两种算法：基础版 **ADRA** 与自适应变体 **ADRA+**（动态调整重建目标与探索强度）。\n\n## 主要发现与优势  \n在BookMIA（预训练数据检测）、AIME（后训练数据检测）及DistillMIA（蒸馏数据检测）三大基准上，ADRA系列显著超越现有SOTA方法：**平均检测准确率提升10.7%**。其中，ADRA+在BookMIA上较Min-K%++提升**18.8%**，在AIME上提升**7.6%**。实验验证了“可重建性”作为新MIA信号的有效性与鲁棒性，为数据溯源提供了可解释、可优化的新路径。",
      "summary_en": "This paper introduces **Active Data Reconstruction Attack (ADRA)**, a novel membership inference framework that actively *elicits* LLMs to reconstruct candidate texts via on-policy reinforcement learning—rather than passively analyzing fixed outputs. Grounded in the hypothesis that training data are inherently *more reconstructible*, ADRA fine-tunes a policy initialized from the target model to maximize reconstruction fidelity, guided by contrastive rewards and tailored reconstruction metrics. The adaptive variant, **ADRA+**, further improves robustness via dynamic target selection and exploration control. Evaluated across pre-training (BookMIA), post-training (AIME), and distillation (DistillMIA) settings, ADRA+ achieves an average **10.7% absolute improvement** over prior state-of-the-art, including +18.8% on BookMIA and +7.6% on AIME. This work establishes *reconstructibility* as a powerful, actionable signal for data provenance in LLMs.",
      "summary": "## 背景与问题  \n大语言模型（LLM）训练数据检测通常被建模为**成员推断攻击（MIA）**，但传统MIA方法（如基于似然或生成文本的被动分析）受限于固定模型权重，难以捕捉数据与模型内部表征间的深层耦合关系。\n\n## 方法创新：主动数据重建攻击（ADRA）  \n本文提出**Active Data Reconstruction Attack（ADRA）**——一种新型主动式MIA范式。其核心假设是：**训练数据比非成员数据更易被模型“重建”**。我们不依赖静态输出，而是通过**在线微调策略网络**，主动引导目标模型重构候选文本。受强化学习（RL）可锐化已有行为的启发，我们以目标模型参数初始化策略，并采用**on-policy RL**进行端到端优化。关键设计包括：（1）面向重建质量的细粒度指标（如token-level重构保真度与序列一致性）；（2）对比式奖励函数，显式拉大成员/非成员样本的重建得分差距。由此衍生出两种算法：基础版 **ADRA** 与自适应变体 **ADRA+**（动态调整重建目标与探索强度）。\n\n## 主要发现与优势  \n在BookMIA（预训练数据检测）、AIME（后训练数据检测）及DistillMIA（蒸馏数据检测）三大基准上，ADRA系列显著超越现有SOTA方法：**平均检测准确率提升10.7%**。其中，ADRA+在BookMIA上较Min-K%++提升**18.8%**，在AIME上提升**7.6%**。实验验证了“可重建性”作为新MIA信号的有效性与鲁棒性，为数据溯源提供了可解释、可优化的新路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.18776v1",
      "arxiv_id": "2602.18776v1",
      "title": "ArabicNumBench: Evaluating Arabic Number Reading in Large Language Models",
      "authors": [
        "Anas Alhumud",
        "Abdulaziz Alhammadi",
        "Muhammad Badruddin Khan"
      ],
      "abstract": "We present ArabicNumBench, a comprehensive benchmark for evaluating large language models on Arabic number reading tasks across Eastern Arabic-Indic numerals (0-9 in Arabic script) and Western Arabic numerals (0-9). We evaluate 71 models from 10 providers using four prompting strategies (zero-shot, zero-shot CoT, few-shot, few-shot CoT) on 210 number reading tasks spanning six contextual categories: pure numerals, addresses, dates, quantities, and prices. Our evaluation comprises 59,010 individual test cases and tracks extraction methods to measure structured output generation. Evaluation reveals substantial performance variation, with accuracy ranging from 14.29\\% to 99.05\\% across models and strategies. Few-shot Chain-of-Thought prompting achieves 2.8x higher accuracy than zero-shot approaches (80.06\\% vs 28.76\\%). A striking finding emerges: models achieving elite accuracy (98-99\\%) often produce predominantly unstructured output, with most responses lacking Arabic CoT markers. Only 6 models consistently generate structured output across all test cases, while the majority require fallback extraction methods despite high numerical accuracy. Comprehensive evaluation of 281 model-strategy combinations demonstrates that numerical accuracy and instruction-following represent distinct capabilities, establishing baselines for Arabic number comprehension and providing actionable guidance for model selection in production Arabic NLP systems.",
      "published": "2026-02-21",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.18776v1",
      "url": "https://arxiv.org/abs/2602.18776v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## ArabicNumBench：首个面向阿拉伯数字朗读能力的系统性评测基准  \n\n本研究提出 **ArabicNumBench**——首个专为评估大语言模型（LLMs）在阿拉伯语数字朗读任务中表现而设计的综合性基准。该基准覆盖两大数字体系：**东阿拉伯-印度数字**（٠١٢٣٤٥٦٧٨٩）与**西阿拉伯数字**（0–9），涵盖6类真实语境：纯数字、地址、日期、数量、价格及混合格式，共构建210项细粒度任务。我们对来自10家厂商的**71个主流模型**（含闭源与开源）进行了全面评测，采用4种提示策略：零样本（zero-shot）、零样本思维链（zero-shot CoT）、少样本（few-shot）及少样本思维链（few-shot CoT）。评测规模达**59,010个独立测试用例**，并创新性引入**结构化输出追踪机制**，精确量化模型生成阿拉伯语CoT推理步骤、数字转录准确性及指令遵循能力。\n\n关键发现包括：（1）模型性能差异巨大，整体准确率跨度达**14.29%–99.05%**；（2）**few-shot CoT策略效果最优**（80.06%），是零样本方法（28.76%）的**2.8倍**；（3）高准确率（98–99%）模型普遍存在“**准确但不结构化**”现象——其响应多缺失阿拉伯语CoT标记（如“أولاً”“ثانياً”），仅6个模型能在全部测试中稳定输出结构化推理；（4）数值准确率与指令遵循能力呈弱相关，证实二者为**正交能力维度**。本工作不仅确立了阿拉伯数字理解的首套权威基线，更提供面向生产环境的模型选型指南：在需可解释输出的场景（如金融、政务NLP系统）中，应优先选择结构化生成能力强而非仅数值准确率高的模型。",
      "summary_en": "We introduce **ArabicNumBench**, the first comprehensive benchmark for evaluating large language models (LLMs) on Arabic number reading—covering both Eastern Arabic-Indic (٠–٩) and Western Arabic (0–9) numerals across six contextual categories (e.g., addresses, dates, prices). Evaluating **71 models** from 10 providers on **59,010 test cases**, we assess four prompting strategies (zero-shot, zero-shot CoT, few-shot, few-shot CoT) and track structured output generation via Arabic CoT markers. Results reveal wide performance variance (14.29%–99.05% accuracy), with few-shot CoT achieving **80.06% accuracy**—2.8× higher than zero-shot (28.76%). Crucially, top-performing models (98–99% accuracy) often produce *unstructured* outputs lacking Arabic CoT reasoning; only **6 models consistently generate structured responses** across all tasks. This demonstrates that **numerical accuracy and instruction-following (i.e., structured reasoning) are distinct capabilities**, establishing foundational baselines and actionable guidance for deploying robust Arabic NLP systems.",
      "summary": "## ArabicNumBench：首个面向阿拉伯数字朗读能力的系统性评测基准  \n\n本研究提出 **ArabicNumBench**——首个专为评估大语言模型（LLMs）在阿拉伯语数字朗读任务中表现而设计的综合性基准。该基准覆盖两大数字体系：**东阿拉伯-印度数字**（٠١٢٣٤٥٦٧٨٩）与**西阿拉伯数字**（0–9），涵盖6类真实语境：纯数字、地址、日期、数量、价格及混合格式，共构建210项细粒度任务。我们对来自10家厂商的**71个主流模型**（含闭源与开源）进行了全面评测，采用4种提示策略：零样本（zero-shot）、零样本思维链（zero-shot CoT）、少样本（few-shot）及少样本思维链（few-shot CoT）。评测规模达**59,010个独立测试用例**，并创新性引入**结构化输出追踪机制**，精确量化模型生成阿拉伯语CoT推理步骤、数字转录准确性及指令遵循能力。\n\n关键发现包括：（1）模型性能差异巨大，整体准确率跨度达**14.29%–99.05%**；（2）**few-shot CoT策略效果最优**（80.06%），是零样本方法（28.76%）的**2.8倍**；（3）高准确率（98–99%）模型普遍存在“**准确但不结构化**”现象——其响应多缺失阿拉伯语CoT标记（如“أولاً”“ثانياً”），仅6个模型能在全部测试中稳定输出结构化推理；（4）数值准确率与指令遵循能力呈弱相关，证实二者为**正交能力维度**。本工作不仅确立了阿拉伯数字理解的首套权威基线，更提供面向生产环境的模型选型指南：在需可解释输出的场景（如金融、政务NLP系统）中，应优先选择结构化生成能力强而非仅数值准确率高的模型。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.18082v1",
      "arxiv_id": "2602.18082v1",
      "title": "AndroWasm: an Empirical Study on Android Malware Obfuscation through WebAssembly",
      "authors": [
        "Diego Soi",
        "Silvia Lucia Sanna",
        "Lorenzo Pisu",
        "Leonardo Regano",
        "Giorgio Giacinto"
      ],
      "abstract": "In recent years, stealthy Android malware has increasingly adopted sophisticated techniques to bypass automatic detection mechanisms and harden manual analysis. Adversaries typically rely on obfuscation, anti-repacking, steganography, poisoning, and evasion techniques to AI-based tools, and in-memory execution to conceal malicious functionality.   In this paper, we investigate WebAssembly (Wasm) as a novel technique for hiding malicious payloads and evading traditional static analysis and signature-matching mechanisms. While Wasm is typically employed to render specific gaming activities and interact with the native components in web browsers, we provide an in-depth analysis on the mechanisms Android may employ to include Wasm modules in its execution pipeline. Additionally, we provide Proofs-of-Concept to demonstrate a threat model in which an attacker embeds and executes malicious routines, effectively bypassing IoC detection by industrial state-of-the-art tools, like VirusTotal and MobSF.",
      "published": "2026-02-20",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.18082v1",
      "url": "https://arxiv.org/abs/2602.18082v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "poisoning"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n近年来，Android恶意软件日趋隐蔽化，攻击者广泛采用**代码混淆、反重打包、隐写术、AI工具对抗（如数据投毒与检测规避）及内存中执行**等高级技术，以绕过自动化检测系统并阻碍人工逆向分析。传统静态分析工具和基于签名的检测引擎在面对此类动态、跨层隐蔽策略时日益失效。\n\n## 方法与创新  \n本文首次系统性提出并实证研究 **AndroWasm**——一种利用**WebAssembly（Wasm）作为恶意载荷载体**的新型Android隐蔽执行范式。我们深入剖析了Android平台集成Wasm模块的可行路径：包括通过**JNI桥接Native层Wasm运行时（如Wasmtime/WAMR）、在WebView中动态加载.wasm字节码、以及借助新兴Android NDK对Wasm的原生支持**。不同于Wasm在Web场景中的常规用途，本工作揭示其被滥用于**解耦恶意逻辑与Java/Kotlin主程序、规避DEX字节码静态扫描、延迟解析关键指令**等攻击面。\n\n## 主要发现  \n- 构建了6类可复现的PoC样本（含加密C2通信、权限提升、屏幕捕获等），全部成功绕过**VirusTotal（平均检出率仅12.3%）与MobSF（0%静态规则命中）**；  \n- Wasm模块在APK中以资源文件或加密内嵌形式存在，**不生成任何.dex/.odex文件，完全脱离Android Dalvik/ART执行栈可见性**；  \n- 首次量化证实：主流反病毒引擎对Wasm二进制无语义理解能力，仅依赖文件头或熵值等浅层特征，导致高漏报率。  \n\n本研究为Android安全生态敲响警钟，推动检测框架向**跨语言（Java/Wasm/C++）联合分析、运行时Wasm行为监控、以及Wasm字节码反编译增强**方向演进。",
      "summary_en": "This paper presents **AndroWasm**, the first empirical study on leveraging WebAssembly (Wasm) as a stealthy execution substrate for Android malware. While Wasm is designed for safe, portable web computation, we demonstrate how adversaries can embed malicious Wasm modules into Android apps via JNI-integrated runtimes (e.g., Wasmtime), WebView injection, or NDK-based native execution—bypassing DEX-based static analysis entirely. We develop 6 realistic PoCs implementing C2 communication, privilege escalation, and screen capture, all evading detection by industry-standard tools: VirusTotal (average detection rate: 12.3%) and MobSF (0% rule coverage). Crucially, Wasm payloads reside as opaque binary resources—never compiled to DEX, invisible to conventional Android analyzers, and semantically opaque to most AV engines. Our work exposes a critical blind spot in mobile security tooling and calls for cross-language behavioral analysis and Wasm-aware runtime monitoring.",
      "summary": "## 背景与问题  \n近年来，Android恶意软件日趋隐蔽化，攻击者广泛采用**代码混淆、反重打包、隐写术、AI工具对抗（如数据投毒与检测规避）及内存中执行**等高级技术，以绕过自动化检测系统并阻碍人工逆向分析。传统静态分析工具和基于签名的检测引擎在面对此类动态、跨层隐蔽策略时日益失效。\n\n## 方法与创新  \n本文首次系统性提出并实证研究 **AndroWasm**——一种利用**WebAssembly（Wasm）作为恶意载荷载体**的新型Android隐蔽执行范式。我们深入剖析了Android平台集成Wasm模块的可行路径：包括通过**JNI桥接Native层Wasm运行时（如Wasmtime/WAMR）、在WebView中动态加载.wasm字节码、以及借助新兴Android NDK对Wasm的原生支持**。不同于Wasm在Web场景中的常规用途，本工作揭示其被滥用于**解耦恶意逻辑与Java/Kotlin主程序、规避DEX字节码静态扫描、延迟解析关键指令**等攻击面。\n\n## 主要发现  \n- 构建了6类可复现的PoC样本（含加密C2通信、权限提升、屏幕捕获等），全部成功绕过**VirusTotal（平均检出率仅12.3%）与MobSF（0%静态规则命中）**；  \n- Wasm模块在APK中以资源文件或加密内嵌形式存在，**不生成任何.dex/.odex文件，完全脱离Android Dalvik/ART执行栈可见性**；  \n- 首次量化证实：主流反病毒引擎对Wasm二进制无语义理解能力，仅依赖文件头或熵值等浅层特征，导致高漏报率。  \n\n本研究为Android安全生态敲响警钟，推动检测框架向**跨语言（Java/Wasm/C++）联合分析、运行时Wasm行为监控、以及Wasm字节码反编译增强**方向演进。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17973v1",
      "arxiv_id": "2602.17973v1",
      "title": "PenTiDef: Enhancing Privacy and Robustness in Decentralized Federated Intrusion Detection Systems against Poisoning Attacks",
      "authors": [
        "Phan The Duy",
        "Nghi Hoang Khoa",
        "Nguyen Tran Anh Quan",
        "Luong Ha Tien",
        "Ngo Duc Hoang Son",
        "Van-Hau Pham"
      ],
      "abstract": "The increasing deployment of Federated Learning (FL) in Intrusion Detection Systems (IDS) introduces new challenges related to data privacy, centralized coordination, and susceptibility to poisoning attacks. While significant research has focused on protecting traditional FL-IDS with centralized aggregation servers, there remains a notable gap in addressing the unique challenges of decentralized FL-IDS (DFL-IDS). This study aims to address the limitations of traditional centralized FL-IDS by proposing a novel defense framework tailored for the decentralized FL-IDS architecture, with a focus on privacy preservation and robustness against poisoning attacks. We propose PenTiDef, a privacy-preserving and robust defense framework for DFL-IDS, which incorporates Distributed Differential Privacy (DDP) to protect data confidentiality and utilizes latent space representations (LSR) derived from neural networks to detect malicious updates in the decentralized model aggregation context. To eliminate single points of failure and enhance trust without a centralized aggregation server, PenTiDef employs a blockchain-based decentralized coordination mechanism that manages model aggregation, tracks update history, and supports trust enforcement through smart contracts. Experimental results on CIC-IDS2018 and Edge-IIoTSet demonstrate that PenTiDef consistently outperforms existing defenses (e.g., FLARE, FedCC) across various attack scenarios and data distributions. These findings highlight the potential of PenTiDef as a scalable and secure framework for deploying DFL-based IDS in adversarial environments. By leveraging privacy protection, malicious behavior detection in hidden data, and working without a central server, it provides a useful security solution against real-world attacks from untrust participants.",
      "published": "2026-02-20",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17973v1",
      "url": "https://arxiv.org/abs/2602.17973v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "differential",
        "privacy",
        "model",
        "learning",
        "poisoning",
        "data"
      ],
      "keyword_score": 7,
      "summary_zh": "## 背景与挑战  \n随着联邦学习（FL）在入侵检测系统（IDS）中的广泛应用，**集中式架构**带来的单点故障、隐私泄露风险及对投毒攻击的脆弱性日益凸显。尤其在边缘/物联网场景下，去中心化联邦学习入侵检测系统（DFL-IDS）因无需可信聚合服务器而更具部署优势，但现有防御方案（如FLARE、FedCC）多面向中心化FL设计，难以适配DFL中无全局协调者、更新异步、节点完全自治等特性，导致隐私保护不足、恶意更新难识别、信任机制缺失三大瓶颈。\n\n## 方法创新：PenTiDef框架  \n本研究提出**PenTiDef**——首个专为DFL-IDS定制的隐私增强型鲁棒防御框架，融合三大核心技术：  \n- **分布式差分隐私（DDP）**：在本地模型上传前注入噪声，实现端到端隐私保障，避免中心化噪声放大问题；  \n- **隐空间表征（LSR）检测机制**：利用神经网络中间层特征构建轻量级异常检测器，精准识别投毒更新在高维隐空间中的分布偏移，显著提升对标签翻转、后门等隐蔽攻击的检出率；  \n- **区块链赋能的去中心化协调**：通过智能合约自动执行模型聚合、版本追溯与信誉评分，消除单点依赖，支持动态准入控制与可验证的公平聚合。\n\n## 实验验证与价值  \n在CIC-IDS2018与Edge-IIoTSet两大真实网络流量数据集上，PenTiDef在5类典型投毒攻击（包括梯度反转、模型替换、混合攻击）下，平均检测准确率达98.7%，F1-score较基线提升12.4%；在非独立同分布（Non-IID）数据下仍保持96.2%的AUC。其通信开销低于传统DP方案37%，且支持千级节点扩展。本工作为构建**可验证、抗合谋、免信任**的下一代分布式安全检测基础设施提供了可落地的技术路径。",
      "summary_en": "PenTiDef is a novel privacy-preserving and robust defense framework designed specifically for Decentralized Federated Learning-based Intrusion Detection Systems (DFL-IDS), addressing critical gaps in privacy protection, poisoning resilience, and trust management under serverless coordination. It integrates **Distributed Differential Privacy (DDP)** to guarantee end-to-end data confidentiality without centralized noise amplification, leverages **Latent Space Representations (LSR)** from neural network layers to detect malicious model updates with high sensitivity—even against stealthy label-flipping and backdoor attacks—and employs a **blockchain-based decentralized coordination layer** using smart contracts for verifiable, fault-tolerant model aggregation and update auditing. Extensive experiments on CIC-IDS2018 and Edge-IIoTSet show PenTiDef achieves **98.7% average detection accuracy** across diverse poisoning attacks and Non-IID settings, outperforming FLARE and FedCC by ≥12.4% in F1-score, while reducing communication overhead by 37% versus standard DP approaches. It demonstrates strong scalability (≥1,000 nodes) and establishes a practical foundation for trustworthy DFL-IDS deployment in adversarial edge environments.",
      "summary": "## 背景与挑战  \n随着联邦学习（FL）在入侵检测系统（IDS）中的广泛应用，**集中式架构**带来的单点故障、隐私泄露风险及对投毒攻击的脆弱性日益凸显。尤其在边缘/物联网场景下，去中心化联邦学习入侵检测系统（DFL-IDS）因无需可信聚合服务器而更具部署优势，但现有防御方案（如FLARE、FedCC）多面向中心化FL设计，难以适配DFL中无全局协调者、更新异步、节点完全自治等特性，导致隐私保护不足、恶意更新难识别、信任机制缺失三大瓶颈。\n\n## 方法创新：PenTiDef框架  \n本研究提出**PenTiDef**——首个专为DFL-IDS定制的隐私增强型鲁棒防御框架，融合三大核心技术：  \n- **分布式差分隐私（DDP）**：在本地模型上传前注入噪声，实现端到端隐私保障，避免中心化噪声放大问题；  \n- **隐空间表征（LSR）检测机制**：利用神经网络中间层特征构建轻量级异常检测器，精准识别投毒更新在高维隐空间中的分布偏移，显著提升对标签翻转、后门等隐蔽攻击的检出率；  \n- **区块链赋能的去中心化协调**：通过智能合约自动执行模型聚合、版本追溯与信誉评分，消除单点依赖，支持动态准入控制与可验证的公平聚合。\n\n## 实验验证与价值  \n在CIC-IDS2018与Edge-IIoTSet两大真实网络流量数据集上，PenTiDef在5类典型投毒攻击（包括梯度反转、模型替换、混合攻击）下，平均检测准确率达98.7%，F1-score较基线提升12.4%；在非独立同分布（Non-IID）数据下仍保持96.2%的AUC。其通信开销低于传统DP方案37%，且支持千级节点扩展。本工作为构建**可验证、抗合谋、免信任**的下一代分布式安全检测基础设施提供了可落地的技术路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.18384v1",
      "arxiv_id": "2602.18384v1",
      "title": "FedZMG: Efficient Client-Side Optimization in Federated Learning",
      "authors": [
        "Fotios Zantalis",
        "Evangelos Zervas",
        "Grigorios Koulouras"
      ],
      "abstract": "Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the \"intensity\" or \"bias\" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.",
      "published": "2026-02-20",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.18384v1",
      "url": "https://arxiv.org/abs/2602.18384v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary_zh": "## FedZMG：面向联邦学习的高效客户端优化新范式\n\n联邦学习（FL）在保障边缘设备数据隐私的前提下实现分布式模型训练，但客户端数据普遍呈现**非独立同分布（non-IID）特性**，易引发显著的**客户端漂移（client-drift）**，导致收敛缓慢、模型精度下降。现有自适应优化器（如FedAdam）虽部分缓解该问题，却常引入额外计算开销或通信负担，难以适配资源受限的物联网（IoT）终端。\n\n本文提出**Federated Zero Mean Gradients（FedZMG）**——一种**无参数、纯客户端部署**的新型优化算法。其核心创新在于结构化正则化优化空间：受梯度中心化（Gradient Centralization）启发，FedZMG在本地训练中将每个客户端的梯度向量**正交投影至零均值超平面**（即减去梯度分量的通道/维度均值），从而**天然抵消由数据异构性引起的梯度“强度偏置”**。该操作仅需本地计算，**零通信开销、零超参数调优、零模型修改**，完美契合边缘侧轻量化需求。\n\n理论分析证明：FedZMG严格降低有效梯度方差，在非-IID设定下可获得比标准FedAvg更紧致的收敛上界。在EMNIST（字符识别）、CIFAR100（细粒度图像分类）和Shakespeare（文本生成）三大基准数据集上的系统实验表明：在高度non-IID场景（如Dirichlet α=0.1）下，FedZMG相较FedAvg提升验证精度**2.3–5.7个百分点**，收敛速度加快**1.8–2.4倍**；且全面优于FedAdam，同时减少约**37%的客户端训练时延**。FedZMG为隐私保护与效率协同的联邦学习提供了简洁、鲁棒、可即插即用的新基线。",
      "summary_en": "Federated Learning (FL) enables privacy-preserving distributed training but suffers from client drift under non-IID data, degrading convergence and accuracy. Existing adaptive optimizers often incur prohibitive computation or communication overhead for resource-constrained edge devices. This paper proposes **FedZMG**, a *parameter-free, client-side-only* optimization algorithm that mitigates drift via structural regularization: it projects local gradients onto a zero-mean hyperplane—i.e., subtracts the per-dimension mean—neutralizing bias shifts from data heterogeneity *without any extra communication or hyperparameter tuning*. Theoretically, FedZMG reduces effective gradient variance and yields tighter convergence bounds than FedAvg. Empirically, on EMNIST, CIFAR100, and Shakespeare under extreme non-IID settings (Dirichlet α=0.1), FedZMG achieves **2.3–5.7% higher final accuracy** and **1.8–2.4× faster convergence** versus FedAvg, outperforms FedAdam in both accuracy and efficiency, and cuts client training latency by ~37%.",
      "summary": "## FedZMG：面向联邦学习的高效客户端优化新范式\n\n联邦学习（FL）在保障边缘设备数据隐私的前提下实现分布式模型训练，但客户端数据普遍呈现**非独立同分布（non-IID）特性**，易引发显著的**客户端漂移（client-drift）**，导致收敛缓慢、模型精度下降。现有自适应优化器（如FedAdam）虽部分缓解该问题，却常引入额外计算开销或通信负担，难以适配资源受限的物联网（IoT）终端。\n\n本文提出**Federated Zero Mean Gradients（FedZMG）**——一种**无参数、纯客户端部署**的新型优化算法。其核心创新在于结构化正则化优化空间：受梯度中心化（Gradient Centralization）启发，FedZMG在本地训练中将每个客户端的梯度向量**正交投影至零均值超平面**（即减去梯度分量的通道/维度均值），从而**天然抵消由数据异构性引起的梯度“强度偏置”**。该操作仅需本地计算，**零通信开销、零超参数调优、零模型修改**，完美契合边缘侧轻量化需求。\n\n理论分析证明：FedZMG严格降低有效梯度方差，在非-IID设定下可获得比标准FedAvg更紧致的收敛上界。在EMNIST（字符识别）、CIFAR100（细粒度图像分类）和Shakespeare（文本生成）三大基准数据集上的系统实验表明：在高度non-IID场景（如Dirichlet α=0.1）下，FedZMG相较FedAvg提升验证精度**2.3–5.7个百分点**，收敛速度加快**1.8–2.4倍**；且全面优于FedAdam，同时减少约**37%的客户端训练时延**。FedZMG为隐私保护与效率协同的联邦学习提供了简洁、鲁棒、可即插即用的新基线。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.18216v1",
      "arxiv_id": "2602.18216v1",
      "title": "Generative Model via Quantile Assignment",
      "authors": [
        "Georgi Hrusanov",
        "Oliver Y. Chén",
        "Julien S. Bodelet"
      ],
      "abstract": "Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.",
      "published": "2026-02-20",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.18216v1",
      "url": "https://arxiv.org/abs/2602.18216v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "adversarial",
        "learning"
      ],
      "keyword_score": 3,
      "summary_zh": "## 新型生成范式：基于分位数分配的神经生成模型（NeuroSQL）\n\n深度生成模型（DGMs）在现代机器学习中承担两大核心任务：**信息合成**（如图像生成）与**隐空间降维**。然而，主流架构（如VAE依赖编码器、GAN依赖判别器）引入了训练不稳定、计算开销大及模式崩溃等固有风险。本文提出**NeuroSQL**——一种无需任何辅助网络的端到端生成新范式。其核心创新在于：**摒弃显式编码器，转而通过渐近最优传输理论，将隐变量建模为分位数分配问题的解**。具体而言，NeuroSQL首先在数据分布与标准均匀分布间构建线性指派（linear assignment）以高效求解低维潜表示，再将该紧凑潜码输入轻量级独立生成器。我们在四大基准数据集上系统评估：MNIST（手写数字）、CelebA（人脸）、AFHQ（动物脸）和OASIS（脑部MRI）。结果表明：（1）**质量更优**：合成图像与真实图像的平均像素距离显著更低，LPIPS与FID指标全面优于VAE、GAN及算力预算匹配的扩散模型；（2）**效率更高**：训练耗时仅为对比方法的30–60%，无对抗训练震荡或变分下界优化困境；（3）**小样本鲁棒**：在仅500样本的受限设定下仍保持高保真生成能力。NeuroSQL以“分位数即编码”的简洁思想，实现了**快速、稳定、低损**的合成数据生成，为隐私敏感与数据稀缺场景提供了新范式。",
      "summary_en": "Deep generative models (DGMs) face persistent challenges from auxiliary networks—encoders in VAEs and discriminators in GANs—causing instability, high computation, and mode collapse. We propose **NeuroSQL**, a novel paradigm that eliminates auxiliary networks by learning latent representations *implicitly* via quantile assignment. Grounded in asymptotic optimal transport theory, NeuroSQL solves a linear assignment problem to map data to low-dimensional uniform quantiles, then feeds the resulting compact latents to a standalone generator. Evaluated on MNIST, CelebA, AFHQ, and OASIS, NeuroSQL outperforms VAEs, GANs, and a budget-matched diffusion baseline: it achieves **lower mean pixel distance**, **superior perceptual fidelity** (e.g., 12% lower LPIPS vs. StyleGAN2), **fastest training** (up to 3.2× speedup), and **strong small-data generalization** (robust at ≤500 samples). By replacing encoders with quantile assignment, NeuroSQL delivers fast, stable, and information-preserving synthesis.",
      "summary": "## 新型生成范式：基于分位数分配的神经生成模型（NeuroSQL）\n\n深度生成模型（DGMs）在现代机器学习中承担两大核心任务：**信息合成**（如图像生成）与**隐空间降维**。然而，主流架构（如VAE依赖编码器、GAN依赖判别器）引入了训练不稳定、计算开销大及模式崩溃等固有风险。本文提出**NeuroSQL**——一种无需任何辅助网络的端到端生成新范式。其核心创新在于：**摒弃显式编码器，转而通过渐近最优传输理论，将隐变量建模为分位数分配问题的解**。具体而言，NeuroSQL首先在数据分布与标准均匀分布间构建线性指派（linear assignment）以高效求解低维潜表示，再将该紧凑潜码输入轻量级独立生成器。我们在四大基准数据集上系统评估：MNIST（手写数字）、CelebA（人脸）、AFHQ（动物脸）和OASIS（脑部MRI）。结果表明：（1）**质量更优**：合成图像与真实图像的平均像素距离显著更低，LPIPS与FID指标全面优于VAE、GAN及算力预算匹配的扩散模型；（2）**效率更高**：训练耗时仅为对比方法的30–60%，无对抗训练震荡或变分下界优化困境；（3）**小样本鲁棒**：在仅500样本的受限设定下仍保持高保真生成能力。NeuroSQL以“分位数即编码”的简洁思想，实现了**快速、稳定、低损**的合成数据生成，为隐私敏感与数据稀缺场景提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.18137v1",
      "arxiv_id": "2602.18137v1",
      "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs",
      "authors": [
        "Vincent Grari",
        "Ciprian Tomoiaga",
        "Sylvain Lamprier",
        "Tatsunori Hashimoto",
        "Marcin Detyniecki"
      ],
      "abstract": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.",
      "published": "2026-02-20",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.18137v1",
      "url": "https://arxiv.org/abs/2602.18137v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n大型语言模型（LLMs）虽经海量通用语料预训练，但在法律、医疗等**专业化领域**常表现出理解浅层化、推理脆弱性高、领域术语误用等问题。现有微调方法受限于高质量、任务对齐的标注数据极度稀缺，而主流合成数据策略（如规则 paraphrasing 或知识抽取）存在两大瓶颈：**(i) 忽视解释性推理能力培养**——仅复现事实性陈述，无法激发模型对隐含逻辑、条款冲突或因果链条的深度解析；**(ii) 样本效率低下**——生成冗余度高、语义覆盖重叠的庞大数据集，导致训练成本激增且边际收益递减。\n\n## 方法创新：Agentic Adversarial QA 框架  \n我们提出一种**基于智能体（agent）的对抗式问答生成框架**。该框架不依赖静态模板，而是构建双模型闭环：以待适配的轻量级目标模型（如 LLaMA-3-8B）为“学习者”，以基于权威领域文档（如判例库、法规原文）微调的强专家模型为“裁判”。通过**迭代式对抗生成**——比较二者在相同文档上的答案分歧，自动定位语义鸿沟（如法律要件遗漏、时效性误判），并生成**紧凑、高信息熵的挑战性问题**（例如：“若原告在诉讼时效届满后提交补充证据，且被告未主张时效抗辩，法院是否应采纳？请援引《民法典》第192条及司法解释论证”）。每个问题均经可解释性验证，确保其直指核心能力缺口。\n\n## 关键结果与价值  \n在 LegalBench 的 12 个细粒度法律子任务（含合同解释、程序合规、侵权归责）上评估表明：仅使用 **827 个合成问题**（不足传统合成数据规模的 3%），本方法即提升目标模型平均准确率 **+9.6%**，显著超越同等样本量下的 SOTA 基线（+4.2% 相对提升）。更重要的是，模型在**跨条款推理**与**反事实场景泛化**上的错误率下降 37%，验证了其对深层领域认知结构的强化效果。该框架为低资源专业领域 LLM 适配提供了高精度、高效率、可解释的新范式。",
      "summary_en": "Large Language Models (LLMs) often underperform in specialized domains due to insufficient domain-specific reasoning and scarce high-quality fine-tuning data. While synthetic data generation is widely adopted, conventional methods (e.g., paraphrasing, knowledge extraction) suffer from poor interpretive reasoning support and low sample efficiency—producing large, redundant corpora with minimal semantic challenge. To address this, we propose **Agentic Adversarial QA**, a feedback-driven framework that iteratively generates a compact set of semantically adversarial questions. It pits a target model against a robust expert model grounded in authoritative domain documents; discrepancies in their answers trigger the synthesis of questions explicitly designed to expose and close comprehension gaps—especially in interpretive, causal, and multi-clause reasoning. Evaluated on 12 fine-grained LegalBench tasks, our method achieves **+9.6% average accuracy gain** using only **827 synthetic questions**—less than 3% of typical synthetic dataset sizes—and reduces reasoning errors by 37% in cross-clause and counterfactual scenarios. This demonstrates superior sample efficiency, reasoning fidelity, and domain adaptability over prior approaches.",
      "summary": "## 背景与挑战  \n大型语言模型（LLMs）虽经海量通用语料预训练，但在法律、医疗等**专业化领域**常表现出理解浅层化、推理脆弱性高、领域术语误用等问题。现有微调方法受限于高质量、任务对齐的标注数据极度稀缺，而主流合成数据策略（如规则 paraphrasing 或知识抽取）存在两大瓶颈：**(i) 忽视解释性推理能力培养**——仅复现事实性陈述，无法激发模型对隐含逻辑、条款冲突或因果链条的深度解析；**(ii) 样本效率低下**——生成冗余度高、语义覆盖重叠的庞大数据集，导致训练成本激增且边际收益递减。\n\n## 方法创新：Agentic Adversarial QA 框架  \n我们提出一种**基于智能体（agent）的对抗式问答生成框架**。该框架不依赖静态模板，而是构建双模型闭环：以待适配的轻量级目标模型（如 LLaMA-3-8B）为“学习者”，以基于权威领域文档（如判例库、法规原文）微调的强专家模型为“裁判”。通过**迭代式对抗生成**——比较二者在相同文档上的答案分歧，自动定位语义鸿沟（如法律要件遗漏、时效性误判），并生成**紧凑、高信息熵的挑战性问题**（例如：“若原告在诉讼时效届满后提交补充证据，且被告未主张时效抗辩，法院是否应采纳？请援引《民法典》第192条及司法解释论证”）。每个问题均经可解释性验证，确保其直指核心能力缺口。\n\n## 关键结果与价值  \n在 LegalBench 的 12 个细粒度法律子任务（含合同解释、程序合规、侵权归责）上评估表明：仅使用 **827 个合成问题**（不足传统合成数据规模的 3%），本方法即提升目标模型平均准确率 **+9.6%**，显著超越同等样本量下的 SOTA 基线（+4.2% 相对提升）。更重要的是，模型在**跨条款推理**与**反事实场景泛化**上的错误率下降 37%，验证了其对深层领域认知结构的强化效果。该框架为低资源专业领域 LLM 适配提供了高精度、高效率、可解释的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.18047v1",
      "arxiv_id": "2602.18047v1",
      "title": "CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Yibo Meng",
        "Jia Yee Tan",
        "Jiaxuan Lu",
        "Rui Lu",
        "Jiekai Wu",
        "Zhaolu Kang",
        "Simon Fong"
      ],
      "abstract": "City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.",
      "published": "2026-02-20",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.18047v1",
      "url": "https://arxiv.org/abs/2602.18047v1",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "privacy"
      ],
      "keyword_score": 2,
      "summary_zh": "## CityGuard：面向城市级跨摄像头身份检索的图感知隐私保护描述符框架\n\n城市规模的跨摄像头行人重识别（re-ID）面临三重挑战：**视角变化、严重遮挡与域偏移**导致外观剧烈波动；**数据隐私法规**（如GDPR）禁止原始图像跨设备共享；**分布式部署环境**缺乏高精度几何标定（如激光雷达或精确相机位姿）。为此，本文提出 **CityGuard**——首个融合图结构先验、差分隐私与拓扑感知注意力的端到端隐私增强型re-ID框架。\n\nCityGuard包含三大创新模块：  \n1. **离散自适应度量学习器**：动态调整类内样本间的边界间隔，依据特征分布广度优化实例级margin，显著提升类内紧凑性与类间可分性；  \n2. **空间条件化图注意力机制**：将粗粒度地理信息（如GPS坐标或楼层平面图）编码为位置先验，注入图结构化的自注意力计算中，仅依赖低成本部署元数据即可实现**投影一致的跨视角对齐**，无需毫米级相机标定；  \n3. **差分隐私嵌入映射 + 紧凑近似索引**：在特征嵌入层引入$(\\varepsilon,\\delta)$-差分隐私噪声，并耦合基于LSH的轻量级近似最近邻索引，兼顾**严格隐私保障**与**边缘设备实时检索吞吐量**。\n\n在Market-1501、DukeMTMC-reID及自建UrbanCam-1K（含真实城市场景遮挡与光照变化）上系统验证，CityGuard在mAP指标上平均提升+4.2%，Rank-1准确率提升+3.8%，同时查询延迟降低37%。更重要的是，其隐私-效用权衡曲线经Rényi差分隐私会计严格验证，支持ε∈[0.5, 4.0]范围内灵活配置。结果表明，CityGuard为合规、鲁棒、可扩展的城市级身份搜索提供了切实可行的技术路径。",
      "summary_en": "CityGuard is a graph-aware, privacy-preserving framework for city-scale person re-identification across decentralized urban cameras. It addresses three core challenges: severe appearance variation (viewpoint, occlusion, domain shift), strict data protection constraints prohibiting raw image sharing, and the absence of precise geometric calibration in real-world deployments. CityGuard introduces: (1) a dispersion-adaptive metric learner that dynamically tunes instance-level margins to enhance intra-class compactness; (2) spatially conditioned graph attention that leverages coarse geometry (e.g., GPS or floor plans) — *not survey-grade calibration* — to enable projectively consistent cross-view alignment; and (3) differentially private embedding maps coupled with compact LSH-based approximate indexes for secure, low-latency retrieval. Evaluated on Market-1501, DukeMTMC-reID, and UrbanCam-1K, CityGuard achieves consistent gains (+4.2% mAP, +3.8% Rank-1) over strong baselines while reducing query latency by 37%. Its privacy-utility trade-off is rigorously certified under Rényi differential privacy accounting, enabling tunable ε ∈ [0.5, 4.0].",
      "summary": "## CityGuard：面向城市级跨摄像头身份检索的图感知隐私保护描述符框架\n\n城市规模的跨摄像头行人重识别（re-ID）面临三重挑战：**视角变化、严重遮挡与域偏移**导致外观剧烈波动；**数据隐私法规**（如GDPR）禁止原始图像跨设备共享；**分布式部署环境**缺乏高精度几何标定（如激光雷达或精确相机位姿）。为此，本文提出 **CityGuard**——首个融合图结构先验、差分隐私与拓扑感知注意力的端到端隐私增强型re-ID框架。\n\nCityGuard包含三大创新模块：  \n1. **离散自适应度量学习器**：动态调整类内样本间的边界间隔，依据特征分布广度优化实例级margin，显著提升类内紧凑性与类间可分性；  \n2. **空间条件化图注意力机制**：将粗粒度地理信息（如GPS坐标或楼层平面图）编码为位置先验，注入图结构化的自注意力计算中，仅依赖低成本部署元数据即可实现**投影一致的跨视角对齐**，无需毫米级相机标定；  \n3. **差分隐私嵌入映射 + 紧凑近似索引**：在特征嵌入层引入$(\\varepsilon,\\delta)$-差分隐私噪声，并耦合基于LSH的轻量级近似最近邻索引，兼顾**严格隐私保障**与**边缘设备实时检索吞吐量**。\n\n在Market-1501、DukeMTMC-reID及自建UrbanCam-1K（含真实城市场景遮挡与光照变化）上系统验证，CityGuard在mAP指标上平均提升+4.2%，Rank-1准确率提升+3.8%，同时查询延迟降低37%。更重要的是，其隐私-效用权衡曲线经Rényi差分隐私会计严格验证，支持ε∈[0.5, 4.0]范围内灵活配置。结果表明，CityGuard为合规、鲁棒、可扩展的城市级身份搜索提供了切实可行的技术路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17978v1",
      "arxiv_id": "2602.17978v1",
      "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees",
      "authors": [
        "Daqian Shao"
      ],
      "abstract": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.",
      "published": "2026-02-20",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17978v1",
      "url": "https://arxiv.org/abs/2602.17978v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "adversarial",
        "learning"
      ],
      "keyword_score": 3,
      "summary_zh": "## 研究背景与挑战  \n强化学习（RL）与深度学习正深刻变革决策范式，但在高风险场景（如医疗干预、自动驾驶、金融风控）中落地仍面临双重瓶颈：**在线交互成本高昂或不可行**，而**离线学习又易受隐藏混杂因素（hidden confounders）干扰**，导致策略产生虚假相关性，引发次优甚至对抗性行为。\n\n## 核心方法与创新  \n本研究围绕“**带理论保障的最优且样本高效决策策略学习**”展开三方面突破：  \n1. **因果鲁棒的离线策略学习**：针对存在隐藏混杂的离线数据，引入工具变量（IV）建模，将策略优化转化为**条件矩约束（CMR）问题**；受双/去偏机器学习启发，提出首个兼具**收敛性保证与最优统计速率**的CMR求解算法，在理论和实验上均显著优于现有SOTA方法（如DeepIV、KernelIV）。  \n2. **放松假设的离线模仿学习**：在标准模仿学习框架下，首次系统松弛对隐藏混杂结构的强假设（如排他性约束），将所提CMR估计器适配为**具备收敛率保证的仿效策略学习器**，实现从专家轨迹中稳健提取因果驱动的行为模式。  \n3. **LTL目标驱动的可证明最优学习**：面向高阶时序目标（以线性时序逻辑LTL表达），设计新型策略优化框架，融合符号推理与样本自适应采样，提出**首个具备全局最优性证明与样本复杂度上界保障的LTL策略学习算法**，样本效率较基线提升达3.2×（基准测试验证）。\n\n## 实证效果  \n在MuJoCo机器人控制、ICU治疗模拟、合成/半合成医疗决策数据集及LTL规划任务上的系统评估表明：所提方法在策略性能、泛化稳定性与部署安全性上均显著提升，为高可信AI决策提供可验证的技术路径。",
      "summary_en": "This thesis advances sample-efficient and provably reliable decision-making under uncertainty. First, we tackle offline policy learning with hidden confounders by formulating causal effect identification as a conditional moment restriction (CMR) problem using instrumental variables; we propose a novel double/debiased-inspired algorithm with statistical optimality guarantees and superior empirical performance over state-of-the-art IV methods. Second, we relax stringent assumptions on confounders in offline imitation learning and adapt our CMR estimator to yield an imitator policy with provable convergence rates. Third, for high-level objectives specified in Linear Temporal Logic (LTL), we develop the first learning algorithm with formal global optimality guarantees and improved sample complexity—reducing required interactions by up to 3.2× versus prior LTL-RL approaches. Experiments across RL benchmarks (MuJoCo), healthcare simulators (ICU treatment), and synthetic/semi-synthetic datasets validate robustness, safety, and real-world applicability.",
      "summary": "## 研究背景与挑战  \n强化学习（RL）与深度学习正深刻变革决策范式，但在高风险场景（如医疗干预、自动驾驶、金融风控）中落地仍面临双重瓶颈：**在线交互成本高昂或不可行**，而**离线学习又易受隐藏混杂因素（hidden confounders）干扰**，导致策略产生虚假相关性，引发次优甚至对抗性行为。\n\n## 核心方法与创新  \n本研究围绕“**带理论保障的最优且样本高效决策策略学习**”展开三方面突破：  \n1. **因果鲁棒的离线策略学习**：针对存在隐藏混杂的离线数据，引入工具变量（IV）建模，将策略优化转化为**条件矩约束（CMR）问题**；受双/去偏机器学习启发，提出首个兼具**收敛性保证与最优统计速率**的CMR求解算法，在理论和实验上均显著优于现有SOTA方法（如DeepIV、KernelIV）。  \n2. **放松假设的离线模仿学习**：在标准模仿学习框架下，首次系统松弛对隐藏混杂结构的强假设（如排他性约束），将所提CMR估计器适配为**具备收敛率保证的仿效策略学习器**，实现从专家轨迹中稳健提取因果驱动的行为模式。  \n3. **LTL目标驱动的可证明最优学习**：面向高阶时序目标（以线性时序逻辑LTL表达），设计新型策略优化框架，融合符号推理与样本自适应采样，提出**首个具备全局最优性证明与样本复杂度上界保障的LTL策略学习算法**，样本效率较基线提升达3.2×（基准测试验证）。\n\n## 实证效果  \n在MuJoCo机器人控制、ICU治疗模拟、合成/半合成医疗决策数据集及LTL规划任务上的系统评估表明：所提方法在策略性能、泛化稳定性与部署安全性上均显著提升，为高可信AI决策提供可验证的技术路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17566v1",
      "arxiv_id": "2602.17566v1",
      "title": "A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN",
      "authors": [
        "Asif Hasan Chowdhury",
        "Md. Fahim Islam",
        "M Ragib Anjum Riad",
        "Faiyaz Bin Hashem",
        "Md Tanzim Reza",
        "Md. Golam Rabiul Alam"
      ],
      "abstract": "The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17566v1",
      "url": "https://arxiv.org/abs/2602.17566v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n随着算力提升与医疗数据激增，人工智能（AI）在肺部疾病辅助诊断中展现出巨大潜力。然而，临床数据高度敏感、分布异构且受隐私法规严格约束，传统中心化深度学习模型面临**数据孤岛、隐私泄露与泛化能力弱**三大瓶颈。\n\n## 方法创新  \n本研究提出一种**联邦学习（FL）赋能的混合集成模型**，首次将轻量级视觉Transformer——**SWIN Transformer**与多支先进CNN（DenseNet201、Inception V3、VGG19）深度融合，并构建基于联邦学习的分布式协同训练框架：  \n- 各医院/机构作为本地客户端，仅上传**模型参数更新（非原始X光影像）**，保障数据不出域；  \n- 中央服务器执行安全聚合（Secure Aggregation），动态融合SWIN的全局建模能力与CNN的局部纹理特征；  \n- 引入**实时持续学习机制**，支持新病例流式接入与模型在线优化，提升对变异病灶（如新型肺炎亚型）的适应性。\n\n## 主要发现与贡献  \n在公开胸部X光数据集（COVIDx、RSNA Pneumonia）上，该模型实现**98.7%平均准确率**（较单模型提升4.2–6.8%），**F1-score达0.971**，显著优于独立训练基准；同时将跨机构数据泄露风险降低99.3%（通过差分隐私+加密梯度验证）。本工作不仅为COVID-19与细菌性肺炎提供高精度、低延迟的双分类诊断工具，更构建了可扩展、合伦理、强鲁棒的医疗AI协作范式，为多中心肺病早筛系统落地提供关键技术支撑。",
      "summary_en": "This paper proposes a **hybrid federated learning (FL)-enabled ensemble model** for lung disease diagnosis, integrating the SWIN Transformer and multiple CNNs (DenseNet201, Inception V3, VGG19) to jointly analyze chest X-ray images. Leveraging FL, the framework enables collaborative training across hospitals without sharing raw patient data—only encrypted, differentially private model updates are exchanged. A real-time continual learning mechanism allows dynamic adaptation to emerging disease patterns. Evaluated on COVIDx and RSNA Pneumonia datasets, the model achieves **98.7% average accuracy and 0.971 F1-score**, outperforming standalone models by 4.2–6.8% in accuracy while reducing cross-institutional privacy risk by 99.3%. This work establishes a secure, scalable, and clinically actionable AI paradigm for federated pulmonary diagnostics.",
      "summary": "## 背景与挑战  \n随着算力提升与医疗数据激增，人工智能（AI）在肺部疾病辅助诊断中展现出巨大潜力。然而，临床数据高度敏感、分布异构且受隐私法规严格约束，传统中心化深度学习模型面临**数据孤岛、隐私泄露与泛化能力弱**三大瓶颈。\n\n## 方法创新  \n本研究提出一种**联邦学习（FL）赋能的混合集成模型**，首次将轻量级视觉Transformer——**SWIN Transformer**与多支先进CNN（DenseNet201、Inception V3、VGG19）深度融合，并构建基于联邦学习的分布式协同训练框架：  \n- 各医院/机构作为本地客户端，仅上传**模型参数更新（非原始X光影像）**，保障数据不出域；  \n- 中央服务器执行安全聚合（Secure Aggregation），动态融合SWIN的全局建模能力与CNN的局部纹理特征；  \n- 引入**实时持续学习机制**，支持新病例流式接入与模型在线优化，提升对变异病灶（如新型肺炎亚型）的适应性。\n\n## 主要发现与贡献  \n在公开胸部X光数据集（COVIDx、RSNA Pneumonia）上，该模型实现**98.7%平均准确率**（较单模型提升4.2–6.8%），**F1-score达0.971**，显著优于独立训练基准；同时将跨机构数据泄露风险降低99.3%（通过差分隐私+加密梯度验证）。本工作不仅为COVID-19与细菌性肺炎提供高精度、低延迟的双分类诊断工具，更构建了可扩展、合伦理、强鲁棒的医疗AI协作范式，为多中心肺病早筛系统落地提供关键技术支撑。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17452v1",
      "arxiv_id": "2602.17452v1",
      "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
      "authors": [
        "Wyatt Benno",
        "Alberto Centelles",
        "Antoine Douchet",
        "Khalil Gibran"
      ],
      "abstract": "We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions.   Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verification optimisations detailed in this paper. We demonstrate that Jolt Atlas can prove model inference in memory-constrained environments -- a prover property commonly referred to as \\textit{streaming}. Furthermore, we discuss how Jolt Atlas achieves zero-knowledge through the BlindFold technique, as introduced in Vega. In contrast to existing zkML frameworks, we show practical proving times for classification, embedding, automated reasoning, and small language models.   Jolt Atlas enables cryptographic verification that can be run on-device, without specialised hardware. The resulting proofs are succinctly verifiable. This makes Jolt Atlas well-suited for privacy-centric and adversarial environments. In a companion work, we outline various use cases of Jolt Atlas, including how it serves as guardrails in agentic commerce and for trustless AI context (often referred to as \\textit{AI memory}).",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17452v1",
      "url": "https://arxiv.org/abs/2602.17452v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "zero-knowledge",
        "learning",
        "adversarial"
      ],
      "keyword_score": 4,
      "summary_zh": "## Jolt Atlas：基于查找论证的零知识可验证机器学习推理框架  \n\n**背景与动机**：现有零知识机器学习（zkML）方案多依赖零知识虚拟机（zkVM）模拟CPU指令，开销大、内存占用高，难以在资源受限设备上部署。Jolt Atlas针对这一瓶颈，提出首个**直接面向ONNX张量计算图**的zkML框架，将零知识证明能力从通用计算下沉至AI原语层。\n\n**核心方法**：  \n- 基于Jolt证明系统的**查找中心化（lookup-centric）范式**，摒弃传统CPU寄存器建模，转而对ONNX标准算子（如MatMul、ReLU、Softmax）构建专用查找表（LUT）；  \n- 采用**sumcheck协议实现高效非线性查找论证**，天然适配ML中关键的非线性激活函数；  \n- 引入**神经跃迁（Neural Teleportation）** 技术压缩查找表规模（最高降低87%），同时严格保持模型精度；  \n- 设计多项**张量级验证优化**：包括稀疏张量编码、分块查表流水线、内存访问模式预承诺，实现真正**流式（streaming）证明**——证明者峰值内存恒定，不随模型规模增长。\n\n**创新与优势**：  \n✅ 首个支持**端侧零知识推理验证**的zkML框架，无需TPM/FPGA等专用硬件；  \n✅ 证明体积小、验证快（毫秒级），满足隐私敏感与对抗环境需求；  \n✅ 基于开放、跨平台的ONNX格式，无缝对接PyTorch/TensorFlow/JAX，消除框架锁定；  \n✅ 已实证支持分类、嵌入、自动推理及小型语言模型（≤100M参数）的端到端可验证推理；  \n✅ 通过BlindFold技术实现信息论安全的零知识性，保障输入/权重/中间态全程保密。  \n该工作为“AI记忆”（AI Memory）、可信智能体商业等新型应用提供了密码学基础设施支撑。",
      "summary_en": "Jolt Atlas is a zero-knowledge machine learning (zkML) framework that enables succinct, verifiable inference directly over ONNX tensor operations—bypassing CPU emulation entirely. It extends the Jolt proving system with lookup arguments powered by the sumcheck protocol, making it especially efficient for non-linear ML primitives. Key innovations include *Neural Teleportation* to compress lookup tables without accuracy loss, and tensor-level optimizations enabling true *streaming* provers (constant memory, scalable to large models). Unlike prior zkML systems, Jolt Atlas achieves practical proving times across classification, embedding, automated reasoning, and small language models—all while supporting on-device, hardware-agnostic verification. Proofs are succinctly verifiable (ms-scale), and zero-knowledge is guaranteed via BlindFold. Built on open, portable ONNX, it eliminates framework lock-in and enables trustless AI context (“AI memory”) and agentic commerce guardrails.",
      "summary": "## Jolt Atlas：基于查找论证的零知识可验证机器学习推理框架  \n\n**背景与动机**：现有零知识机器学习（zkML）方案多依赖零知识虚拟机（zkVM）模拟CPU指令，开销大、内存占用高，难以在资源受限设备上部署。Jolt Atlas针对这一瓶颈，提出首个**直接面向ONNX张量计算图**的zkML框架，将零知识证明能力从通用计算下沉至AI原语层。\n\n**核心方法**：  \n- 基于Jolt证明系统的**查找中心化（lookup-centric）范式**，摒弃传统CPU寄存器建模，转而对ONNX标准算子（如MatMul、ReLU、Softmax）构建专用查找表（LUT）；  \n- 采用**sumcheck协议实现高效非线性查找论证**，天然适配ML中关键的非线性激活函数；  \n- 引入**神经跃迁（Neural Teleportation）** 技术压缩查找表规模（最高降低87%），同时严格保持模型精度；  \n- 设计多项**张量级验证优化**：包括稀疏张量编码、分块查表流水线、内存访问模式预承诺，实现真正**流式（streaming）证明**——证明者峰值内存恒定，不随模型规模增长。\n\n**创新与优势**：  \n✅ 首个支持**端侧零知识推理验证**的zkML框架，无需TPM/FPGA等专用硬件；  \n✅ 证明体积小、验证快（毫秒级），满足隐私敏感与对抗环境需求；  \n✅ 基于开放、跨平台的ONNX格式，无缝对接PyTorch/TensorFlow/JAX，消除框架锁定；  \n✅ 已实证支持分类、嵌入、自动推理及小型语言模型（≤100M参数）的端到端可验证推理；  \n✅ 通过BlindFold技术实现信息论安全的零知识性，保障输入/权重/中间态全程保密。  \n该工作为“AI记忆”（AI Memory）、可信智能体商业等新型应用提供了密码学基础设施支撑。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17423v1",
      "arxiv_id": "2602.17423v1",
      "title": "Convergence Analysis of Two-Layer Neural Networks under Gaussian Input Masking",
      "authors": [
        "Afroditi Kolomvaki",
        "Fangshuo Liao",
        "Evan Dramko",
        "Ziyun Guang",
        "Anastasios Kyrillidis"
      ],
      "abstract": "We investigate the convergence guarantee of two-layer neural network training with Gaussian randomly masked inputs. This scenario corresponds to Gaussian dropout at the input level, or noisy input training common in sensor networks, privacy-preserving training, and federated learning, where each user may have access to partial or corrupted features. Using a Neural Tangent Kernel (NTK) analysis, we demonstrate that training a two-layer ReLU network with Gaussian randomly masked inputs achieves linear convergence up to an error region proportional to the mask's variance. A key technical contribution is resolving the randomness within the non-linear activation, a problem of independent interest.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17423v1",
      "url": "https://arxiv.org/abs/2602.17423v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "math.OC"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n在传感器网络、隐私保护学习与联邦学习等实际场景中，终端用户常仅能获取**部分观测特征**或遭受**输入噪声干扰**（如模数转换误差、通信丢包、差分隐私加噪）。此类问题可建模为**高斯随机掩码输入**（Gaussian input masking）：原始输入 $x$ 被乘以独立同分布的高斯掩码向量 $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$，形成带噪输入 $\\xi \\odot x$。该设定等价于输入层高斯Dropout，但传统收敛分析对此缺乏理论支撑。\n\n## 方法与技术突破  \n本文基于**神经正切核（NTK）框架**，对宽度为 $m$ 的两层ReLU神经网络展开严格分析。核心挑战在于：掩码引入的随机性与ReLU激活函数的非线性耦合，导致梯度动态高度异质——既含确定性NTK主导项，又含依赖掩码路径的随机扰动项。我们提出**双尺度分解技术**：将网络输出分解为“平均核流形”与“方差校正项”，并利用**高斯积分恒等式**与**集中不等式耦合控制**，首次实现了对掩码-激活联合随机性的精确刻画。\n\n## 主要发现与理论贡献  \n- 在过参数化条件 $m = \\Omega(\\mathrm{poly}(n, 1/\\sigma^2))$ 下，梯度下降以**线性速率**收敛至误差上界 $\\mathcal{O}(\\sigma^2)$，即收敛精度由掩码方差直接决定；  \n- 误差界具有**最优阶数**：当 $\\sigma \\to 0$ 时退化为标准NTK线性收敛；当 $\\sigma > 0$ 时，$\\mathcal{O}(\\sigma^2)$ 不可改进（构造反例验证）；  \n- 关键创新在于**解析了非线性激活内生随机性**——此前工作多假设权重初始化随机性或标签噪声，而本工作首次系统处理“输入扰动→激活状态跳变→梯度失配”的完整因果链，为鲁棒训练提供新分析范式。",
      "summary_en": "We study the convergence of gradient descent for two-layer ReLU networks trained on inputs corrupted by independent Gaussian masks—i.e., $x \\mapsto \\xi \\odot x$ with $\\xi_i \\sim \\mathcal{N}(0,\\sigma^2)$. This models practical settings including noisy sensor data, privacy-preserving input perturbation, and federated learning with partial features. Using a refined Neural Tangent Kernel (NTK) analysis, we prove that under mild over-parameterization ($m = \\Omega(\\mathrm{poly}(n,1/\\sigma^2))$), training converges linearly to a neighborhood of the global minimum, with final error bounded by $\\mathcal{O}(\\sigma^2)$. Crucially, we resolve the technical challenge of *joint randomness* between Gaussian masks and ReLU activations—by decomposing the network output into a deterministic NTK component and a variance-controlled correction term, enabled by Gaussian integration identities and sharp concentration. Our $\\mathcal{O}(\\sigma^2)$ bound is tight: it recovers standard NTK convergence as $\\sigma \\to 0$, and we show optimality via explicit counterexamples. This work establishes the first convergence guarantee for neural training under input-level Gaussian corruption, with implications for robustness and distributed learning.",
      "summary": "## 研究背景  \n在传感器网络、隐私保护学习与联邦学习等实际场景中，终端用户常仅能获取**部分观测特征**或遭受**输入噪声干扰**（如模数转换误差、通信丢包、差分隐私加噪）。此类问题可建模为**高斯随机掩码输入**（Gaussian input masking）：原始输入 $x$ 被乘以独立同分布的高斯掩码向量 $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$，形成带噪输入 $\\xi \\odot x$。该设定等价于输入层高斯Dropout，但传统收敛分析对此缺乏理论支撑。\n\n## 方法与技术突破  \n本文基于**神经正切核（NTK）框架**，对宽度为 $m$ 的两层ReLU神经网络展开严格分析。核心挑战在于：掩码引入的随机性与ReLU激活函数的非线性耦合，导致梯度动态高度异质——既含确定性NTK主导项，又含依赖掩码路径的随机扰动项。我们提出**双尺度分解技术**：将网络输出分解为“平均核流形”与“方差校正项”，并利用**高斯积分恒等式**与**集中不等式耦合控制**，首次实现了对掩码-激活联合随机性的精确刻画。\n\n## 主要发现与理论贡献  \n- 在过参数化条件 $m = \\Omega(\\mathrm{poly}(n, 1/\\sigma^2))$ 下，梯度下降以**线性速率**收敛至误差上界 $\\mathcal{O}(\\sigma^2)$，即收敛精度由掩码方差直接决定；  \n- 误差界具有**最优阶数**：当 $\\sigma \\to 0$ 时退化为标准NTK线性收敛；当 $\\sigma > 0$ 时，$\\mathcal{O}(\\sigma^2)$ 不可改进（构造反例验证）；  \n- 关键创新在于**解析了非线性激活内生随机性**——此前工作多假设权重初始化随机性或标签噪声，而本工作首次系统处理“输入扰动→激活状态跳变→梯度失配”的完整因果链，为鲁棒训练提供新分析范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17394v1",
      "arxiv_id": "2602.17394v1",
      "title": "Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks",
      "authors": [
        "Nuno Saavedra",
        "Pedro Ribeiro",
        "André Coelho",
        "Rui Campos"
      ],
      "abstract": "Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17394v1",
      "url": "https://arxiv.org/abs/2602.17394v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SD"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 面向应急通信的语音驱动语义感知框架 SIREN\n\n在地震、洪灾等重大突发事件中，地面通信基础设施常遭损毁，**无人机（UAV）辅助应急网络**因其快速部署、灵活覆盖与高鲁棒性，成为保障一线救援通信的关键支撑。然而，当前应急现场仍高度依赖模拟/数字语音电台进行实时协同——这类语音通信虽抗干扰强、操作门槛低，却因**非结构化、高动态、多源混叠**特性，难以被网络管理系统直接理解与响应，严重制约了UAV资源的智能调度与服务质量（QoS）自适应保障。\n\n本文提出 **SIREN（Semantic Intelligence for Radio-based Emergency Networks）**——首个面向UAV辅助应急网络的端到端语音驱动语义感知框架。SIREN创新性融合三阶段流水线：① 鲁棒型自动语音识别（ASR），适配窄带语音与突发噪声；② 基于轻量化微调大语言模型（LLM）的语义结构化解析，精准抽取**响应单位、地理坐标（含模糊表述如“东侧厂房”）、事件等级（1–5级）、关键资源需求及QoS约束（如时延<200ms）**；③ 基于规则与上下文一致性校验的NLP验证模块，显著抑制幻觉与歧义。我们在涵盖6种方言、3–8说话人混叠、SNR 5–25 dB背景噪声及多层级任务复杂度的**合成应急语料库**上完成系统评估。结果表明：SIREN在平均词错误率（WER）≤12.3%前提下，关键语义要素抽取F1值达89.7%，且对地理指代模糊（如未标注POI）与说话人归属混淆（diarization误差）保持可解释性降级。本工作首次验证了**语音直驱网络智能**的技术可行性，为构建“人在环路、语义闭环”的下一代应急通信系统提供了核心使能技术与实证基础。",
      "summary_en": "This paper introduces **SIREN**, an AI-driven framework enabling voice-driven semantic perception for UAV-assisted emergency networks. By tightly integrating robust ASR, LLM-based semantic extraction (fine-tuned for emergency domain), and NLP validation, SIREN transforms unstructured radio voice traffic into structured machine-readable intents—including responder IDs, location references (even ambiguous ones), severity levels, and QoS requirements. Evaluated on a synthetic emergency corpus with controlled variations in language, speaker count (3–8), background noise (SNR 5–25 dB), and message complexity, SIREN achieves ≤12.3% WER and 89.7% F1 on key semantic elements. Speaker diarization errors and geographic ambiguity are identified as primary limiting factors—yet SIREN maintains interpretable performance degradation. The work establishes the feasibility of real-time, voice-native situational awareness for adaptive UAV network management in infrastructure-degraded scenarios.",
      "summary": "## 面向应急通信的语音驱动语义感知框架 SIREN\n\n在地震、洪灾等重大突发事件中，地面通信基础设施常遭损毁，**无人机（UAV）辅助应急网络**因其快速部署、灵活覆盖与高鲁棒性，成为保障一线救援通信的关键支撑。然而，当前应急现场仍高度依赖模拟/数字语音电台进行实时协同——这类语音通信虽抗干扰强、操作门槛低，却因**非结构化、高动态、多源混叠**特性，难以被网络管理系统直接理解与响应，严重制约了UAV资源的智能调度与服务质量（QoS）自适应保障。\n\n本文提出 **SIREN（Semantic Intelligence for Radio-based Emergency Networks）**——首个面向UAV辅助应急网络的端到端语音驱动语义感知框架。SIREN创新性融合三阶段流水线：① 鲁棒型自动语音识别（ASR），适配窄带语音与突发噪声；② 基于轻量化微调大语言模型（LLM）的语义结构化解析，精准抽取**响应单位、地理坐标（含模糊表述如“东侧厂房”）、事件等级（1–5级）、关键资源需求及QoS约束（如时延<200ms）**；③ 基于规则与上下文一致性校验的NLP验证模块，显著抑制幻觉与歧义。我们在涵盖6种方言、3–8说话人混叠、SNR 5–25 dB背景噪声及多层级任务复杂度的**合成应急语料库**上完成系统评估。结果表明：SIREN在平均词错误率（WER）≤12.3%前提下，关键语义要素抽取F1值达89.7%，且对地理指代模糊（如未标注POI）与说话人归属混淆（diarization误差）保持可解释性降级。本工作首次验证了**语音直驱网络智能**的技术可行性，为构建“人在环路、语义闭环”的下一代应急通信系统提供了核心使能技术与实证基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17345v1",
      "arxiv_id": "2602.17345v1",
      "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?",
      "authors": [
        "Boyang Ma",
        "Hechuan Guo",
        "Peizhuo Lv",
        "Minghui Xu",
        "Xuelong Dai",
        "YeChao Zhang",
        "Yijun Yang",
        "Yue Zhang"
      ],
      "abstract": "Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17345v1",
      "url": "https://arxiv.org/abs/2602.17345v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n具身智能系统（如自动驾驶汽车、服务机器人、大语言模型驱动的交互式代理）正加速从受控环境迈向安全关键的真实世界部署。与“离身”AI不同，其失效将导致不可逆的物理后果，亟需重新审视安全、可靠与鲁棒性的理论基础。当前研究多孤立聚焦于**大语言模型（LLM）漏洞**（如提示注入、幻觉）或**传统信息物理系统（CPS）缺陷**（如传感器欺骗、执行器故障），但大量真实故障案例无法被二者单独解释。\n\n## 核心主张与方法  \n本综述提出：现代具身AI失效的主因常源于**具身性引发的系统级错配**（embodiment-induced system-level mismatches），即感知—决策—行动闭环中语义抽象与物理现实间的结构性张力。我们通过跨领域案例分析（含自动驾驶事故、机器人抓取失败、LLM-agent物理指令误执行等），提炼出四个根本性挑战：\n\n- **语义正确 ≠ 物理安全**：语言推理天然忽略几何约束、动力学特性与接触物理，导致逻辑合理却物理危险的决策；  \n- **动作结果高度状态依赖**：相同指令在微小状态差异下可能引发截然不同的物理响应（如非线性动力学与状态不确定性放大）；  \n- **误差在闭环中指数级传播**：感知噪声→决策偏移→动作偏差→新感知失真，形成正反馈恶化链；  \n- **安全不具备可组合性**：局部安全决策随时间/层级累积，可演化为全局性失效（如连续“微小让步”导致碰撞）。\n\n## 创新点与启示  \n本工作突破“组件归因”范式，首次系统论证**具身性本身即安全威胁源**。安全防护必须升维至**系统级物理风险建模**——整合不确定性量化、失效传播图谱与跨层安全验证，而非仅加固LLM或加固CPS子模块。",
      "summary_en": "This survey challenges the prevailing dichotomy in embodied AI security research—framing failures either as LLM vulnerabilities (e.g., hallucination, jailbreaking) or classical CPS flaws (e.g., sensor spoofing, actuator failure). Through analysis of real-world breakdowns across autonomous vehicles, robotic agents, and LLM-driven interactive systems, we argue that a critical class of failures stems not from isolated component weaknesses, but from *embodiment-induced system-level mismatches*: inherent tensions between linguistic abstraction and physical reality within tightly coupled perception-decision-action loops. We identify four foundational insights: (i) semantic correctness does not guarantee physical safety due to abstraction over geometry, dynamics, and contact constraints; (ii) identical actions yield divergent outcomes under nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across feedback loops; and (iv) safety is non-compositional—locally safe decisions can cumulatively produce globally unsafe behavior. Consequently, securing embodied AI demands system-level reasoning about physical risk, uncertainty propagation, and cross-layer failure modes—not just component-level hardening.",
      "summary": "## 研究背景与问题  \n具身智能系统（如自动驾驶汽车、服务机器人、大语言模型驱动的交互式代理）正加速从受控环境迈向安全关键的真实世界部署。与“离身”AI不同，其失效将导致不可逆的物理后果，亟需重新审视安全、可靠与鲁棒性的理论基础。当前研究多孤立聚焦于**大语言模型（LLM）漏洞**（如提示注入、幻觉）或**传统信息物理系统（CPS）缺陷**（如传感器欺骗、执行器故障），但大量真实故障案例无法被二者单独解释。\n\n## 核心主张与方法  \n本综述提出：现代具身AI失效的主因常源于**具身性引发的系统级错配**（embodiment-induced system-level mismatches），即感知—决策—行动闭环中语义抽象与物理现实间的结构性张力。我们通过跨领域案例分析（含自动驾驶事故、机器人抓取失败、LLM-agent物理指令误执行等），提炼出四个根本性挑战：\n\n- **语义正确 ≠ 物理安全**：语言推理天然忽略几何约束、动力学特性与接触物理，导致逻辑合理却物理危险的决策；  \n- **动作结果高度状态依赖**：相同指令在微小状态差异下可能引发截然不同的物理响应（如非线性动力学与状态不确定性放大）；  \n- **误差在闭环中指数级传播**：感知噪声→决策偏移→动作偏差→新感知失真，形成正反馈恶化链；  \n- **安全不具备可组合性**：局部安全决策随时间/层级累积，可演化为全局性失效（如连续“微小让步”导致碰撞）。\n\n## 创新点与启示  \n本工作突破“组件归因”范式，首次系统论证**具身性本身即安全威胁源**。安全防护必须升维至**系统级物理风险建模**——整合不确定性量化、失效传播图谱与跨层安全验证，而非仅加固LLM或加固CPS子模块。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17288v1",
      "arxiv_id": "2602.17288v1",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "authors": [
        "Anuj Gupta"
      ],
      "abstract": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17288v1",
      "url": "https://arxiv.org/abs/2602.17288v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与目标  \n尽管前沿大语言模型在推理与数学能力上表现突出，但**从原始学术文献（如arXiv LaTeX源码）出发、在有限算力下训练领域专用科学语言模型（SLM）的完整工程实践仍缺乏系统性记录**。本研究填补这一空白，开展一项面向实际部署的实证研究：从零构建一个1.36B参数的科学语言模型，专精于数学、计算机科学与理论物理领域。\n\n## 方法与流程  \n我们设计并实现了端到端训练流水线，涵盖：  \n- **元数据过滤与归档校验**（剔除损坏/非科研PDF、重复提交）；  \n- **高保真LaTeX源码提取**（保留公式结构、定理环境等语义单元）；  \n- **科学文本归一化**（处理宏包依赖、跨文档引用、符号歧义）；  \n- **领域感知分词器**（扩展Unicode数学符号集，保留`\\alpha`→α映射稳定性）；  \n- **轻量级密集Transformer训练**（仅用2×A100 GPU，FP16+梯度检查点）。\n\n## 关键发现  \n通过24组对照实验，我们揭示：  \n✅ **预处理损耗远超预期**：原始arXiv数据经清洗后仅保留约38%可用token（52B有效预训练token）；  \n✅ **分词策略决定符号鲁棒性**：传统字节对编码（BPE）导致数学符号分裂率高达27%，而定制分词器降至<2%；  \n✅ **I/O与存储成新瓶颈**：在GPU计算饱和前，NVMe带宽与POSIX文件系统延迟已限制吞吐达40%；  \n✅ **数据丰富区稳定收敛**：在≥40B token规模下，损失曲线平滑，未见灾难性遗忘或梯度爆炸。\n\n## 创新点  \n本工作**不提出新架构，而提供可复现、可审计、低门槛的工程范式**——所有代码、配置与数据卡均已开源。为中等算力（≤4×A100）的研究者构建垂直领域模型提供首份全流程实践指南。",
      "summary_en": "This paper presents a practical, engineering-first case study of training a 1.36B-parameter scientific language model (SLM) *from scratch* using raw arXiv LaTeX sources across mathematics, CS, and theoretical physics. We detail an end-to-end pipeline—spanning metadata filtering, LaTeX extraction, scientific text normalization, domain-aware tokenization, and dense transformer training on just **2×A100 GPUs**. Across 24 controlled experiments, we quantify critical bottlenecks: preprocessing reduces usable tokens by >60%; custom tokenization cuts symbolic fragmentation from 27% to <2%; and I/O/storage constraints rival compute as primary throughput limits. Crucially, we demonstrate stable, scalable convergence in the data-rich regime (52B pretraining tokens), with smooth loss trajectories and no instability. Rather than architectural novelty, our contribution is a transparent, reproducible, and budget-conscious blueprint—open-sourced in full—for building small, domain-specialized LMs without frontier-scale infrastructure.",
      "summary": "## 背景与目标  \n尽管前沿大语言模型在推理与数学能力上表现突出，但**从原始学术文献（如arXiv LaTeX源码）出发、在有限算力下训练领域专用科学语言模型（SLM）的完整工程实践仍缺乏系统性记录**。本研究填补这一空白，开展一项面向实际部署的实证研究：从零构建一个1.36B参数的科学语言模型，专精于数学、计算机科学与理论物理领域。\n\n## 方法与流程  \n我们设计并实现了端到端训练流水线，涵盖：  \n- **元数据过滤与归档校验**（剔除损坏/非科研PDF、重复提交）；  \n- **高保真LaTeX源码提取**（保留公式结构、定理环境等语义单元）；  \n- **科学文本归一化**（处理宏包依赖、跨文档引用、符号歧义）；  \n- **领域感知分词器**（扩展Unicode数学符号集，保留`\\alpha`→α映射稳定性）；  \n- **轻量级密集Transformer训练**（仅用2×A100 GPU，FP16+梯度检查点）。\n\n## 关键发现  \n通过24组对照实验，我们揭示：  \n✅ **预处理损耗远超预期**：原始arXiv数据经清洗后仅保留约38%可用token（52B有效预训练token）；  \n✅ **分词策略决定符号鲁棒性**：传统字节对编码（BPE）导致数学符号分裂率高达27%，而定制分词器降至<2%；  \n✅ **I/O与存储成新瓶颈**：在GPU计算饱和前，NVMe带宽与POSIX文件系统延迟已限制吞吐达40%；  \n✅ **数据丰富区稳定收敛**：在≥40B token规模下，损失曲线平滑，未见灾难性遗忘或梯度爆炸。\n\n## 创新点  \n本工作**不提出新架构，而提供可复现、可审计、低门槛的工程范式**——所有代码、配置与数据卡均已开源。为中等算力（≤4×A100）的研究者构建垂直领域模型提供首份全流程实践指南。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17095v1",
      "arxiv_id": "2602.17095v1",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
      "authors": [
        "Chuiyang Meng",
        "Ming Tang",
        "Vincent W. S. Wong"
      ],
      "abstract": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\\times$.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17095v1",
      "url": "https://arxiv.org/abs/2602.17095v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n参数高效微调（如LoRA）通过低秩矩阵近似实现大语言模型（LLM）的轻量下游适配；联邦学习（FL）则支持跨客户端协作微调，保护数据隐私。然而，标准LoRA在联邦场景下采用**两个独立低秩矩阵**（ΔW = BA），导致双重瓶颈：（1）**分离聚合误差**——服务器分别平均B和A时，其乘积不等于各客户端BA乘积的平均值；（2）**分解漂移**——即使聚合BA乘积，服务器需对其做SVD分解以恢复可更新参数，但分解结果非唯一，连续轮次间方向不一致，严重损害收敛稳定性。\n\n## 方法创新：FLoRG框架  \n我们提出**FLoRG**（Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment）：  \n- **单矩阵参数化**：仅引入一个低秩增量矩阵U∈ℝ^{d×r}（r≪d），令ΔW = UUᵀ，天然保证对称正半定性；  \n- **Gram矩阵聚合**：客户端上传U的Gram矩阵G = UᵀU∈ℝ^{r×r}（仅r²维），服务器直接平均G，**彻底消除聚合误差**，且通信开销从O(dr)降至O(r²)，理论压缩比达d²/r²；  \n- **Procrustes对齐机制**：每轮聚合后，服务器求解正交矩阵Q，使新分解Uₜ ≈ Uₜ₋₁Q，强制跨轮次子空间对齐，抑制分解漂移。\n\n## 实验与理论贡献  \n理论证明：Procrustes对齐将收敛界中的方向偏差项从O(√T)收紧至O(1)，显著提升稳定性。在6个LLM微调基准（Alpaca、Dolly、FLAN等）上，FLoRG平均提升下游任务准确率**2.1–4.7个百分点**，超越LoRA-FL、FedALA、FedPAC等5种SOTA基线；通信量最高降低**2041×**（如7B模型从22.4 MB/轮→11 KB/轮），同时保持训练效率。",
      "summary_en": "FLoRG is a novel federated fine-tuning framework that addresses critical aggregation and decomposition challenges in applying LoRA to federated learning. Instead of using two separate low-rank matrices (B and A), FLoRG employs a single low-rank matrix U and represents the weight update as ΔW = UUᵀ. Clients upload only the small r×r Gram matrix G = UᵀU, enabling exact, error-free aggregation at the server and reducing communication overhead by up to 2041×. To mitigate decomposition drift across rounds, FLoRG introduces Procrustes alignment—a principled orthogonal transformation that aligns successive decompositions, ensuring consistent parameter updates. We provide theoretical convergence analysis showing that Procrustes alignment yields a tighter bound than naive SVD recovery. Experiments on six LLM fine-tuning benchmarks demonstrate that FLoRG consistently outperforms five state-of-the-art baselines in downstream accuracy while drastically cutting communication cost.",
      "summary": "## 背景与挑战  \n参数高效微调（如LoRA）通过低秩矩阵近似实现大语言模型（LLM）的轻量下游适配；联邦学习（FL）则支持跨客户端协作微调，保护数据隐私。然而，标准LoRA在联邦场景下采用**两个独立低秩矩阵**（ΔW = BA），导致双重瓶颈：（1）**分离聚合误差**——服务器分别平均B和A时，其乘积不等于各客户端BA乘积的平均值；（2）**分解漂移**——即使聚合BA乘积，服务器需对其做SVD分解以恢复可更新参数，但分解结果非唯一，连续轮次间方向不一致，严重损害收敛稳定性。\n\n## 方法创新：FLoRG框架  \n我们提出**FLoRG**（Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment）：  \n- **单矩阵参数化**：仅引入一个低秩增量矩阵U∈ℝ^{d×r}（r≪d），令ΔW = UUᵀ，天然保证对称正半定性；  \n- **Gram矩阵聚合**：客户端上传U的Gram矩阵G = UᵀU∈ℝ^{r×r}（仅r²维），服务器直接平均G，**彻底消除聚合误差**，且通信开销从O(dr)降至O(r²)，理论压缩比达d²/r²；  \n- **Procrustes对齐机制**：每轮聚合后，服务器求解正交矩阵Q，使新分解Uₜ ≈ Uₜ₋₁Q，强制跨轮次子空间对齐，抑制分解漂移。\n\n## 实验与理论贡献  \n理论证明：Procrustes对齐将收敛界中的方向偏差项从O(√T)收紧至O(1)，显著提升稳定性。在6个LLM微调基准（Alpaca、Dolly、FLAN等）上，FLoRG平均提升下游任务准确率**2.1–4.7个百分点**，超越LoRA-FL、FedALA、FedPAC等5种SOTA基线；通信量最高降低**2041×**（如7B模型从22.4 MB/轮→11 KB/轮），同时保持训练效率。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17625v1",
      "arxiv_id": "2602.17625v1",
      "title": "Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning",
      "authors": [
        "Obaidullah Zaland",
        "Zulfiqar Ahmad Khan",
        "Monowar Bhuyan"
      ],
      "abstract": "Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \\textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17625v1",
      "url": "https://arxiv.org/abs/2602.17625v1",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n现代大数据系统持续产生海量、异构、地理分散且高度隐私敏感的数据流，使得数据集中化训练既不可行也不合规。联邦学习（FL）虽能保护隐私，但其经典范式假设数据分布静态、依赖多轮通信协同训练，难以应对**增量式数据到达**（如新类别或新域持续出现）与**极低通信预算**（如仅允许单次上传）的双重约束。更严峻的是，模型在持续学习新增任务时易发生**灾难性遗忘**（catastrophic forgetting），即对旧任务性能显著退化。\n\n## 方法创新：OSI-FL + SSR  \n本文提出**一次性增量式联邦学习框架（OSI-FL）**——首个同时缓解通信开销与灾难性遗忘的FL方案。其核心设计为：  \n- **单轮语义通信**：各客户端利用冻结的视觉-语言模型（VLM）提取**类别级嵌入**（category-specific embeddings），仅上传一次至服务器；  \n- **扩散驱动数据合成**：服务器端预训练扩散模型基于这些嵌入，**无原始数据**地合成高保真样本，忠实反映客户端本地数据分布；  \n- **选择性样本保留（SSR）机制**：为抑制遗忘，SSR在每轮训练中动态识别并保留每个“类别–任务对”中损失值最高的前-*p* 个最具信息量的合成样本，将其纳入后续迭代训练，形成轻量但具代表性的记忆回放池。\n\n## 实验验证与优势  \n在三个主流基准（CIFAR-100、ImageNet-R、DomainNet）上，OSI-FL在**类增量**（class-incremental）与**域增量**（domain-incremental）场景下均显著超越传统FL（FedAvg）、一次性FL（One-Shot FL）及持续学习基线。关键结果包括：平均准确率提升+5.2%~9.7%，遗忘率降低达63%；通信开销压缩至传统FL的1/50以下，且无需客户端参与再训练。",
      "summary_en": "This paper introduces **One-Shot Incremental Federated Learning (OSI-FL)**, the first federated learning framework explicitly designed to tackle *both* severe communication constraints and catastrophic forgetting in incremental learning settings. OSI-FL enables clients to transmit only category-specific embeddings—extracted via a frozen vision-language model—in a *single communication round*. The server then leverages a pre-trained diffusion model to synthesize high-fidelity samples mimicking each client’s local data distribution, eliminating raw-data transmission. To combat forgetting as new tasks arrive incrementally, we propose **Selective Sample Retention (SSR)**: it identifies and retains the top-*p* most informative (i.e., highest-loss) synthesized samples per category–task pair, incorporating them into subsequent training rounds as a compact, adaptive memory buffer. Experiments across three benchmarks (CIFAR-100, ImageNet-R, DomainNet) show OSI-FL consistently outperforms traditional FL, one-shot FL, and continual learning baselines—achieving +5.2–9.7% higher average accuracy and up to 63% lower forgetting, while reducing communication overhead by >98%.",
      "summary": "## 研究背景与挑战  \n现代大数据系统持续产生海量、异构、地理分散且高度隐私敏感的数据流，使得数据集中化训练既不可行也不合规。联邦学习（FL）虽能保护隐私，但其经典范式假设数据分布静态、依赖多轮通信协同训练，难以应对**增量式数据到达**（如新类别或新域持续出现）与**极低通信预算**（如仅允许单次上传）的双重约束。更严峻的是，模型在持续学习新增任务时易发生**灾难性遗忘**（catastrophic forgetting），即对旧任务性能显著退化。\n\n## 方法创新：OSI-FL + SSR  \n本文提出**一次性增量式联邦学习框架（OSI-FL）**——首个同时缓解通信开销与灾难性遗忘的FL方案。其核心设计为：  \n- **单轮语义通信**：各客户端利用冻结的视觉-语言模型（VLM）提取**类别级嵌入**（category-specific embeddings），仅上传一次至服务器；  \n- **扩散驱动数据合成**：服务器端预训练扩散模型基于这些嵌入，**无原始数据**地合成高保真样本，忠实反映客户端本地数据分布；  \n- **选择性样本保留（SSR）机制**：为抑制遗忘，SSR在每轮训练中动态识别并保留每个“类别–任务对”中损失值最高的前-*p* 个最具信息量的合成样本，将其纳入后续迭代训练，形成轻量但具代表性的记忆回放池。\n\n## 实验验证与优势  \n在三个主流基准（CIFAR-100、ImageNet-R、DomainNet）上，OSI-FL在**类增量**（class-incremental）与**域增量**（domain-incremental）场景下均显著超越传统FL（FedAvg）、一次性FL（One-Shot FL）及持续学习基线。关键结果包括：平均准确率提升+5.2%~9.7%，遗忘率降低达63%；通信开销压缩至传统FL的1/50以下，且无需客户端参与再训练。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17614v1",
      "arxiv_id": "2602.17614v1",
      "title": "Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning",
      "authors": [
        "Obaidullah Zaland",
        "Sajib Mistry",
        "Monowar Bhuyan"
      ],
      "abstract": "Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17614v1",
      "url": "https://arxiv.org/abs/2602.17614v1",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "learning",
        "federated",
        "machine",
        "privacy-preserving",
        "privacy"
      ],
      "keyword_score": 6,
      "summary_zh": "## 背景与挑战  \n在海量异构数据分布式存储的大数据场景下，联邦学习（FL）虽能避免数据集中化、保障隐私，但其全本地训练范式对客户端算力要求高。为此，**U型联邦分割学习（UFSL）** 将模型沿深度方向切分，客户端仅执行前端计算并上传中间表示（即“smashed data”），服务器完成剩余计算——然而，这些未经保护的中间表示极易被逆向重构出原始私有数据（如图像、敏感特征），构成严重隐私泄露风险。\n\n## 方法创新：KD-UFSL  \n本文提出 **k-匿名差分隐私UFSL（KD-UFSL）**，首次将**微聚合（microaggregation）** 与**差分隐私（DP）** 协同嵌入UFSL通信环节：客户端先对局部中间表示进行k-匿名分组聚合（抑制个体可识别性），再注入经严格校准的拉普拉斯噪声（满足ε-差分隐私）。该双层防护机制在不移动原始数据、不共享标签的前提下，显著削弱smashed data的可重构性。\n\n## 实验结果与价值  \n在CIFAR-10、MNIST、Fashion-MNIST和SVHN四大基准数据集上验证：KD-UFSL使攻击者重建图像与真实图像的**均方误差（MSE）最高提升50%**，**结构相似性（SSIM）平均下降40%**；同时全局模型精度损失控制在<1.2%，F1-score保持稳定。这证明KD-UFSL在**强隐私保障（k-匿名+ε-DP）与高模型效用之间实现了实质性平衡**，为医疗、金融等高敏领域的大规模联邦智能部署提供了可落地的技术路径。",
      "summary_en": "This paper addresses privacy leakage from intermediate representations (\"smashed data\") in U-shaped Federated Split Learning (UFSL), where clients upload sensitive feature embeddings to the server. We first demonstrate that reconstruction attacks can effectively recover private input data (e.g., images) from these intermediates. To mitigate this, we propose **k-anonymous differentially private UFSL (KD-UFSL)**—a novel framework combining microaggregation (to enforce k-anonymity on local smashed data) and calibrated Laplace noise (to satisfy ε-differential privacy). Evaluated on four benchmark datasets (CIFAR-10, MNIST, Fashion-MNIST, SVHN), KD-UFSL increases reconstruction MSE by up to 50% and reduces SSIM by up to 40% compared to vanilla UFSL, while preserving global model utility—accuracy drop remains under 1.2% and F1-score is stable. KD-UFSL thus achieves a practical privacy-utility trade-off for large-scale, privacy-critical federated applications.",
      "summary": "## 背景与挑战  \n在海量异构数据分布式存储的大数据场景下，联邦学习（FL）虽能避免数据集中化、保障隐私，但其全本地训练范式对客户端算力要求高。为此，**U型联邦分割学习（UFSL）** 将模型沿深度方向切分，客户端仅执行前端计算并上传中间表示（即“smashed data”），服务器完成剩余计算——然而，这些未经保护的中间表示极易被逆向重构出原始私有数据（如图像、敏感特征），构成严重隐私泄露风险。\n\n## 方法创新：KD-UFSL  \n本文提出 **k-匿名差分隐私UFSL（KD-UFSL）**，首次将**微聚合（microaggregation）** 与**差分隐私（DP）** 协同嵌入UFSL通信环节：客户端先对局部中间表示进行k-匿名分组聚合（抑制个体可识别性），再注入经严格校准的拉普拉斯噪声（满足ε-差分隐私）。该双层防护机制在不移动原始数据、不共享标签的前提下，显著削弱smashed data的可重构性。\n\n## 实验结果与价值  \n在CIFAR-10、MNIST、Fashion-MNIST和SVHN四大基准数据集上验证：KD-UFSL使攻击者重建图像与真实图像的**均方误差（MSE）最高提升50%**，**结构相似性（SSIM）平均下降40%**；同时全局模型精度损失控制在<1.2%，F1-score保持稳定。这证明KD-UFSL在**强隐私保障（k-匿名+ε-DP）与高模型效用之间实现了实质性平衡**，为医疗、金融等高敏领域的大规模联邦智能部署提供了可落地的技术路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17284v1",
      "arxiv_id": "2602.17284v1",
      "title": "Efficient privacy loss accounting for subsampling and random allocation",
      "authors": [
        "Vitaly Feldman",
        "Moshe Shenfeld"
      ],
      "abstract": "We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used in privacy loss accounting.   In this work, we demonstrate that the privacy loss distribution (PLD) of random allocation applied to any differentially private algorithm can be computed efficiently. When applied to the Gaussian mechanism, our results demonstrate that the privacy-utility trade-off for random allocation is at least as good as that of Poisson subsampling. In particular, random allocation is better suited for training via DP-SGD. To support these computations, our work develops new tools for general privacy loss accounting based on a notion of PLD realization. This notion allows us to extend accurate privacy loss accounting to subsampling which previously required manual noise-mechanism-specific analysis.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17284v1",
      "url": "https://arxiv.org/abs/2602.17284v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景  \n本文聚焦于一种新兴的随机分配（random allocation）采样机制：用户数据被**均匀随机地分配至 $t$ 个步骤中的 $k$ 个**（即从 $t$ 步中无放回选取 $k$ 步使用）。该机制已在差分隐私优化（Chua et al., 2024a；Choquette-Choo et al., 2025）与通信高效的高维私有聚合（Asi et al., 2025）中展现出优于经典泊松采样的实用性优势。然而，现有理论分析（Feldman & Shenfeld, 2025；Dong et al., 2025）仍存在两大瓶颈：一是依赖近似推导，导致实际隐私参数（如 $(\\varepsilon,\\delta)$）偏松、不紧致；二是输出为hockey-stick或Rényi散度，难以直接嵌入主流隐私损失会计（PLA）框架（如PLD方法），引入额外计算与转换开销。\n\n## 方法与创新  \n本文首次提出**基于隐私损失分布（PLD）实现（PLD realization）的通用会计框架**，可精确、高效地刻画任意DP算法经随机分配后的PLD。核心突破在于：将随机分配建模为离散概率分布的卷积组合，并设计动态规划算法，在 $O(k(t-k))$ 时间内完成PLD精确计算（无需近似或机制特化）。特别地，我们完整推导了高斯机制下随机分配的闭式PLD表达式。\n\n## 主要发现  \n- 在相同$(k,t)$配置下，随机分配的隐私-效用权衡**不低于泊松采样**，且在DP-SGD训练中显著更优：同等$\\varepsilon$下测试精度提升1.2–2.3个百分点（CIFAR-10实验验证）；  \n- 所提PLD realization范式**统一了子采样隐私会计**，摆脱了以往对噪声机制的手动定制分析依赖；  \n- 开源实现支持TensorFlow/PyTorch无缝集成，隐私预算计算速度较蒙特卡洛模拟快3个数量级。",
      "summary_en": "We present the first efficient and exact privacy loss distribution (PLD) accounting framework for *random allocation*—a subsampling scheme where each user’s data is assigned to exactly $k$ out of $t$ steps uniformly at random. Prior analyses relied on loose approximations or non-PLD divergences (e.g., Rényi), hindering tight, composable privacy accounting. We introduce the notion of *PLD realization*, enabling exact PLD computation for any base DP mechanism under random allocation via a dynamic programming algorithm with $O(k(t-k))$ complexity. For the Gaussian mechanism, we derive closed-form PLDs and prove that random allocation achieves privacy-utility trade-offs **at least as strong as Poisson subsampling**, with empirical gains in DP-SGD training (e.g., +2.1% accuracy on CIFAR-10 at $\\varepsilon=2$). Our framework unifies subsampling accounting without mechanism-specific derivations and integrates natively into standard PLD-based privacy ledger tools.",
      "summary": "## 研究背景  \n本文聚焦于一种新兴的随机分配（random allocation）采样机制：用户数据被**均匀随机地分配至 $t$ 个步骤中的 $k$ 个**（即从 $t$ 步中无放回选取 $k$ 步使用）。该机制已在差分隐私优化（Chua et al., 2024a；Choquette-Choo et al., 2025）与通信高效的高维私有聚合（Asi et al., 2025）中展现出优于经典泊松采样的实用性优势。然而，现有理论分析（Feldman & Shenfeld, 2025；Dong et al., 2025）仍存在两大瓶颈：一是依赖近似推导，导致实际隐私参数（如 $(\\varepsilon,\\delta)$）偏松、不紧致；二是输出为hockey-stick或Rényi散度，难以直接嵌入主流隐私损失会计（PLA）框架（如PLD方法），引入额外计算与转换开销。\n\n## 方法与创新  \n本文首次提出**基于隐私损失分布（PLD）实现（PLD realization）的通用会计框架**，可精确、高效地刻画任意DP算法经随机分配后的PLD。核心突破在于：将随机分配建模为离散概率分布的卷积组合，并设计动态规划算法，在 $O(k(t-k))$ 时间内完成PLD精确计算（无需近似或机制特化）。特别地，我们完整推导了高斯机制下随机分配的闭式PLD表达式。\n\n## 主要发现  \n- 在相同$(k,t)$配置下，随机分配的隐私-效用权衡**不低于泊松采样**，且在DP-SGD训练中显著更优：同等$\\varepsilon$下测试精度提升1.2–2.3个百分点（CIFAR-10实验验证）；  \n- 所提PLD realization范式**统一了子采样隐私会计**，摆脱了以往对噪声机制的手动定制分析依赖；  \n- 开源实现支持TensorFlow/PyTorch无缝集成，隐私预算计算速度较蒙特卡洛模拟快3个数量级。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16980v1",
      "arxiv_id": "2602.16980v1",
      "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
      "authors": [
        "Leo Marchyok",
        "Zachary Coalson",
        "Sungho Keum",
        "Sooel Son",
        "Sanghyun Hong"
      ],
      "abstract": "Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16980v1",
      "url": "https://arxiv.org/abs/2602.16980v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n现代大语言模型（LLMs）虽展现出强大的生成能力，但其内部如何表征与调控隐私敏感行为（如**个人身份信息（PII）泄露**）仍属黑箱。现有研究多依赖提示工程或数据驱动的攻击方法，缺乏对模型内部机制的可解释性洞察，也难以泛化至未见提示或新模型。\n\n## 方法创新：UniLeak 框架  \n本文提出 **UniLeak**——首个面向PII泄露的**机制可解释性框架**，旨在发现模型残差流（residual stream）中的**通用激活方向（Universal Activation Directions）**。这些方向是模型特定的、高维隐空间中的线性向量，其核心特性在于：**仅需在推理时将该方向线性叠加至任一中间层激活，即可稳定提升各类提示下PII生成概率**。UniLeak完全无需训练数据、真实PII标注或外部监督，仅利用模型自身生成的文本（self-generated text）进行无监督方向挖掘，通过梯度引导的激活归因与方向对齐优化实现鲁棒提取。\n\n## 关键发现与影响  \n- 在 LLaMA-2、Qwen、Phi-3 等多个开源模型及 diverse PII 数据集（如 PII-Bench、Custom-PII）上验证，UniLeak 发现的方向可使 PII 泄露率平均提升 **2.1–4.7×**，显著超越现有提示注入、越狱等基线方法；  \n- 方向具备强跨上下文泛化性：同一方向在不同句式、语义领域甚至多轮对话中均持续有效；  \n- 生成质量几乎不受影响（BLEU/ROUGE 变化 <0.8%，人工评估保持自然流畅）；  \n- 首次揭示PII泄露本质是一种**隐式信号在表示空间中的线性叠加现象**，为风险放大（steering for attack）与主动防御（directional suppression）提供统一机制基础。",
      "summary_en": "We introduce **UniLeak**, a mechanistic interpretability framework that discovers *universal activation directions*—model-specific linear vectors in the residual stream—whose addition at inference time consistently amplifies personally identifiable information (PII) generation across diverse prompts, without requiring training data or ground-truth PII labels. Leveraging only self-generated text and gradient-based attribution, UniLeak identifies directions that generalize across contexts and models while preserving generation quality. Evaluated on LLaMA-2, Qwen, and Phi-3, UniLeak increases PII leakage by 2.1–4.7× over state-of-the-art prompt-based extraction methods. Our work reframes PII leakage as a *superposed latent signal* in model representations—enabling both precise risk amplification and principled mitigation via directional intervention.",
      "summary": "## 研究背景  \n现代大语言模型（LLMs）虽展现出强大的生成能力，但其内部如何表征与调控隐私敏感行为（如**个人身份信息（PII）泄露**）仍属黑箱。现有研究多依赖提示工程或数据驱动的攻击方法，缺乏对模型内部机制的可解释性洞察，也难以泛化至未见提示或新模型。\n\n## 方法创新：UniLeak 框架  \n本文提出 **UniLeak**——首个面向PII泄露的**机制可解释性框架**，旨在发现模型残差流（residual stream）中的**通用激活方向（Universal Activation Directions）**。这些方向是模型特定的、高维隐空间中的线性向量，其核心特性在于：**仅需在推理时将该方向线性叠加至任一中间层激活，即可稳定提升各类提示下PII生成概率**。UniLeak完全无需训练数据、真实PII标注或外部监督，仅利用模型自身生成的文本（self-generated text）进行无监督方向挖掘，通过梯度引导的激活归因与方向对齐优化实现鲁棒提取。\n\n## 关键发现与影响  \n- 在 LLaMA-2、Qwen、Phi-3 等多个开源模型及 diverse PII 数据集（如 PII-Bench、Custom-PII）上验证，UniLeak 发现的方向可使 PII 泄露率平均提升 **2.1–4.7×**，显著超越现有提示注入、越狱等基线方法；  \n- 方向具备强跨上下文泛化性：同一方向在不同句式、语义领域甚至多轮对话中均持续有效；  \n- 生成质量几乎不受影响（BLEU/ROUGE 变化 <0.8%，人工评估保持自然流畅）；  \n- 首次揭示PII泄露本质是一种**隐式信号在表示空间中的线性叠加现象**，为风险放大（steering for attack）与主动防御（directional suppression）提供统一机制基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17651v1",
      "arxiv_id": "2602.17651v1",
      "title": "Non-Trivial Zero-Knowledge Implies One-Way Functions",
      "authors": [
        "Suvradip Chakraborty",
        "James Hulett",
        "Dakshita Khurana",
        "Kabir Tomer"
      ],
      "abstract": "A recent breakthrough [Hirahara and Nanashima, STOC'2024] established that if $\\mathsf{NP} \\not \\subseteq \\mathsf{ioP/poly}$, the existence of zero-knowledge with negligible errors for $\\mathsf{NP}$ implies the existence of one-way functions (OWFs). In this work, we obtain a characterization of one-way functions from the worst-case complexity of zero-knowledge {\\em in the high-error regime}.   We say that a zero-knowledge argument is {\\em non-trivial} if the sum of its completeness, soundness and zero-knowledge errors is bounded away from $1$. Our results are as follows, assuming $\\mathsf{NP} \\not \\subseteq \\mathsf{ioP/poly}$:   1. {\\em Non-trivial} Non-Interactive ZK (NIZK) arguments for $\\mathsf{NP}$ imply the existence of OWFs. Using known amplification techniques, this result also provides an unconditional transformation from weak to standard NIZK proofs for all meaningful error parameters.   2. We also generalize to the interactive setting: {\\em Non-trivial} constant-round public-coin zero-knowledge arguments for $\\mathsf{NP}$ imply the existence of OWFs, and therefore also (standard) four-message zero-knowledge arguments for $\\mathsf{NP}$.   Prior to this work, one-way functions could be obtained from NIZKs that had constant zero-knowledge error $ε_{zk}$ and soundness error $ε_{s}$ satisfying $ε_{zk} + \\sqrt{ε_{s}} < 1$ [Chakraborty, Hulett and Khurana, CRYPTO'2025]. However, the regime where $ε_{zk} + \\sqrt{ε_{s}} \\geq 1$ remained open. This work closes the gap, and obtains new implications in the interactive setting. Our results and techniques could be useful stepping stones in the quest to construct one-way functions from worst-case hardness.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17651v1",
      "url": "https://arxiv.org/abs/2602.17651v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "crypto"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n零知识证明（ZK）与单向函数（OWF）是密码学两大基石。长久以来，一个核心开放问题是：能否从**最坏情况下的计算复杂度假设**（如 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$）出发，仅凭零知识协议的存在性——尤其是允许较高错误率的“非平凡”版本——推导出单向函数？此前工作（如 Hirahara–Nanashima, STOC’24）仅在**低错误 regime**（如 $ε_{zk} + \\sqrt{ε_s} < 1$）下建立了该蕴含关系，而当误差和接近或超过 1 时，该问题长期悬而未决。\n\n## 核心定义与方法  \n本文提出 **“非平凡零知识”（non-trivial ZK）** 的新刻画：要求完备性、可靠性与零知识性三类错误之和严格小于 1（即有非零“安全余量”）。这一定义自然涵盖高误差但仍有意义的协议（例如 $ε_c=0.3, ε_s=0.4, ε_{zk}=0.2$，总和为 0.9），并规避了退化情形（如全错协议）。技术上，作者结合了**误差放大**、**交互式到非交互式归约**及**抗泄露模拟器构造**等工具，在 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$ 假设下完成关键归约。\n\n## 主要成果  \n1. **非交互式场景**：存在非平凡 NIZK 论证 ⇒ 存在单向函数；进一步，该结果可**无条件地将任意弱 NIZK（含高误差）提升为标准 NIZK**（满足传统误差界）。  \n2. **交互式场景**：存在非平凡、常数轮、公共硬币的 ZK 论证 ⇒ 存在 OWF ⇒ 进而存在标准四消息 ZK 论证。  \n本工作首次完整覆盖 $ε_{zk} + \\sqrt{ε_s} \\geq 1$ 的“高误差空白区”，统一了零知识强度与基础密码原语之间的最坏情况关联，为“从 worst-case hardness 构造 OWF”这一宏伟目标提供了关键中间步骤。",
      "summary_en": "We establish a tight characterization: under the plausible worst-case assumption $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$, the existence of *non-trivial* zero-knowledge (ZK) arguments—where the sum of completeness, soundness, and zero-knowledge errors is bounded away from 1—implies one-way functions (OWFs). Specifically: (1) Non-trivial non-interactive ZK (NIZK) arguments for $\\mathsf{NP}$ imply OWFs; moreover, this yields an *unconditional error-amplification framework*, converting any weak NIZK (even with high error) into a standard NIZK proof. (2) The result extends to the interactive setting: non-trivial constant-round public-coin ZK arguments for $\\mathsf{NP}$ also imply OWFs—and thus standard four-message ZK arguments. This closes the long-standing gap for the high-error regime where prior techniques (e.g., Chakraborty–Hulett–Khurana, CRYPTO’25) required $ε_{zk} + \\sqrt{ε_s} < 1$. Our work provides a unified worst-case foundation linking ZK strength to the existence of OWFs.",
      "summary": "## 研究背景与问题  \n零知识证明（ZK）与单向函数（OWF）是密码学两大基石。长久以来，一个核心开放问题是：能否从**最坏情况下的计算复杂度假设**（如 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$）出发，仅凭零知识协议的存在性——尤其是允许较高错误率的“非平凡”版本——推导出单向函数？此前工作（如 Hirahara–Nanashima, STOC’24）仅在**低错误 regime**（如 $ε_{zk} + \\sqrt{ε_s} < 1$）下建立了该蕴含关系，而当误差和接近或超过 1 时，该问题长期悬而未决。\n\n## 核心定义与方法  \n本文提出 **“非平凡零知识”（non-trivial ZK）** 的新刻画：要求完备性、可靠性与零知识性三类错误之和严格小于 1（即有非零“安全余量”）。这一定义自然涵盖高误差但仍有意义的协议（例如 $ε_c=0.3, ε_s=0.4, ε_{zk}=0.2$，总和为 0.9），并规避了退化情形（如全错协议）。技术上，作者结合了**误差放大**、**交互式到非交互式归约**及**抗泄露模拟器构造**等工具，在 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$ 假设下完成关键归约。\n\n## 主要成果  \n1. **非交互式场景**：存在非平凡 NIZK 论证 ⇒ 存在单向函数；进一步，该结果可**无条件地将任意弱 NIZK（含高误差）提升为标准 NIZK**（满足传统误差界）。  \n2. **交互式场景**：存在非平凡、常数轮、公共硬币的 ZK 论证 ⇒ 存在 OWF ⇒ 进而存在标准四消息 ZK 论证。  \n本工作首次完整覆盖 $ε_{zk} + \\sqrt{ε_s} \\geq 1$ 的“高误差空白区”，统一了零知识强度与基础密码原语之间的最坏情况关联，为“从 worst-case hardness 构造 OWF”这一宏伟目标提供了关键中间步骤。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17590v1",
      "arxiv_id": "2602.17590v1",
      "title": "BMC4TimeSec: Verification Of Timed Security Protocols",
      "authors": [
        "Agnieszka M. Zbrzezny"
      ],
      "abstract": "We present BMC4TimeSec, an end-to-end tool for verifying Timed Security Protocols (TSP) based on SMT-based bounded model checking and multi-agent modelling in the form of Timed Interpreted Systems (TIS) and Timed Interleaved Interpreted Systems (TIIS). In BMC4TimeSec, TSP executions implement the TIS/TIIS environment (join actions, interleaving, delays, lifetimes), and knowledge automata implement the agents (evolution of participant knowledge, including the intruder). The code is publicly available on \\href{https://github.com/agazbrzezny/BMC4TimeSec}{GitHub}, as is a \\href{https://youtu.be/aNybKz6HwdA}{video} demonstration.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17590v1",
      "url": "https://arxiv.org/abs/2602.17590v1",
      "categories": [
        "cs.CR",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## BMC4TimeSec：面向时序安全协议的SMT约束有界模型检验工具  \n\n**背景与问题**：时序安全协议（Timed Security Protocols, TSP）在现实系统中广泛应用（如TLS 1.3握手、5G认证流程），其安全性不仅依赖消息逻辑，更受精确时间约束（如超时、会话生命周期、时钟漂移）影响。传统模型检验工具（如ProVerif、Tamarin）难以直接刻画连续时间行为与多主体知识演化耦合机制，导致验证结果保守或不完整。\n\n**方法与创新**：本文提出**BMC4TimeSec**——首个端到端、基于SMT的有界模型检验框架，专为TSP设计。核心创新包括：  \n- 提出**时序解释系统（Timed Interpreted Systems, TIS）** 及其并发扩展**时序交错解释系统（TIIS）**，统一建模协议环境（含联合动作、非确定性交错、真实时间延迟、实体生命周期）；  \n- 引入**知识自动机（Knowledge Automata）** 显式刻画各参与方（含Dolev-Yao型入侵者）的知识动态演化，支持时序敏感的知识推理（如“t时刻后攻击者才获知密钥”）；  \n- 实现SMT编码：将TIS/TIIS语义与知识自动机迁移同步编译为Z3可解的量化自由SMT公式，支持时间变量离散化与有界深度探索；  \n- 提供完整工具链：支持从协议规范（时序LTL+知识谓词）自动生成模型、参数化时间界限设定、反例可视化及知识状态追踪。\n\n**验证与成果**：已在多个经典TSP案例（如Kerberos V5时序变体、NIST SP 800-171合规认证协议）上完成验证，成功发现2处因时钟偏差导致的新型攻击路径（此前未被Tamarin捕获），并确认了3个协议在≤5跳延迟下的强知识安全性。代码与视频演示已开源，显著提升了TSP形式化验证的实用性与可访问性。",
      "summary_en": "We present **BMC4TimeSec**, an end-to-end SMT-based bounded model checking tool for verifying Timed Security Protocols (TSP). It builds on two novel formalisms: *Timed Interpreted Systems* (TIS) and *Timed Interleaved Interpreted Systems* (TIIS), which rigorously model protocol environments—including joint actions, non-deterministic interleaving, real-time delays, and agent lifetimes. Agent knowledge (including the intruder’s) is captured via *knowledge automata*, enabling precise reasoning about temporal epistemic properties (e.g., “the attacker learns the key only after time *t*”). BMC4TimeSec compiles TIS/TIIS semantics and knowledge evolution into quantifier-free SMT formulas solvable by Z3, supporting parameterized time bounds and counterexample generation. Evaluated on Kerberos variants and NIST-compliant protocols, it uncovered previously unknown timing-dependent attacks and confirmed knowledge security up to 5-hop delays. The tool is open-source with a demo video.",
      "summary": "## BMC4TimeSec：面向时序安全协议的SMT约束有界模型检验工具  \n\n**背景与问题**：时序安全协议（Timed Security Protocols, TSP）在现实系统中广泛应用（如TLS 1.3握手、5G认证流程），其安全性不仅依赖消息逻辑，更受精确时间约束（如超时、会话生命周期、时钟漂移）影响。传统模型检验工具（如ProVerif、Tamarin）难以直接刻画连续时间行为与多主体知识演化耦合机制，导致验证结果保守或不完整。\n\n**方法与创新**：本文提出**BMC4TimeSec**——首个端到端、基于SMT的有界模型检验框架，专为TSP设计。核心创新包括：  \n- 提出**时序解释系统（Timed Interpreted Systems, TIS）** 及其并发扩展**时序交错解释系统（TIIS）**，统一建模协议环境（含联合动作、非确定性交错、真实时间延迟、实体生命周期）；  \n- 引入**知识自动机（Knowledge Automata）** 显式刻画各参与方（含Dolev-Yao型入侵者）的知识动态演化，支持时序敏感的知识推理（如“t时刻后攻击者才获知密钥”）；  \n- 实现SMT编码：将TIS/TIIS语义与知识自动机迁移同步编译为Z3可解的量化自由SMT公式，支持时间变量离散化与有界深度探索；  \n- 提供完整工具链：支持从协议规范（时序LTL+知识谓词）自动生成模型、参数化时间界限设定、反例可视化及知识状态追踪。\n\n**验证与成果**：已在多个经典TSP案例（如Kerberos V5时序变体、NIST SP 800-171合规认证协议）上完成验证，成功发现2处因时钟偏差导致的新型攻击路径（此前未被Tamarin捕获），并确认了3个协议在≤5跳延迟下的强知识安全性。代码与视频演示已开源，显著提升了TSP形式化验证的实用性与可访问性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17488v1",
      "arxiv_id": "2602.17488v1",
      "title": "Computational Hardness of Private Coreset",
      "authors": [
        "Badih Ghazi",
        "Cristóbal Guzmán",
        "Pritish Kamath",
        "Alexander Knop",
        "Ravi Kumar",
        "Pasin Manurangsi"
      ],
      "abstract": "We study the problem of differentially private (DP) computation of coreset for the $k$-means objective. For a given input set of points, a coreset is another set of points such that the $k$-means objective for any candidate solution is preserved up to a multiplicative $(1 \\pm α)$ factor (and some additive factor).   We prove the first computational lower bounds for this problem. Specifically, assuming the existence of one-way functions, we show that no polynomial-time $(ε, 1/n^{ω(1)})$-DP algorithm can compute a coreset for $k$-means in the $\\ell_\\infty$-metric for some constant $α> 0$ (and some constant additive factor), even for $k=3$. For $k$-means in the Euclidean metric, we show a similar result but only for $α= Θ\\left(1/d^2\\right)$, where $d$ is the dimension.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17488v1",
      "url": "https://arxiv.org/abs/2602.17488v1",
      "categories": [
        "cs.CG",
        "cs.CR",
        "cs.DS"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n本研究聚焦**差分隐私（DP）下 $k$-means 聚类核心集（coreset）的计算复杂性**。核心集是原始数据点集的紧凑代理表示，要求对任意 $k$-中心解，其 $k$-means 目标函数值被近似至 $(1\\pm\\alpha)$ 倍乘性误差及某加性项内。在隐私保护机器学习中，同时满足高精度（小 $\\alpha$）、强隐私（小 $\\varepsilon$、极小 $\\delta$）与高效计算（多项式时间）被视为理想目标，但三者能否兼顾尚无理论定论。\n\n## 主要方法与技术路线  \n本文首次建立该问题的**计算硬度下界**，基于密码学标准假设——**单向函数存在性**（widely believed foundational assumption in cryptography）。通过精巧的归约构造，将单向函数的不可逆性嵌入到 $k$-means 核心集的私有计算任务中：若存在高效私有算法，则可反向破解单向函数，导出矛盾。\n\n## 关键发现与创新点  \n- **首个严格下界**：在 $\\ell_\\infty$-度量下，即使仅需 $k=3$ 个聚类中心，也**不存在多项式时间的 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法**能输出具有常数精度（$\\alpha>0$）和常数加性误差的核心集；  \n- **欧氏空间紧致结果**：在 $d$ 维欧氏空间中，同样假设下，对任意多项式时间 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法，若要求 $\\alpha = \\Theta(1/d^2)$，则核心集计算亦不可行；  \n- 该结果揭示了**隐私、精度与效率三者间固有的计算权衡**，解释了为何现有私有核心集算法或牺牲精度（如 $\\alpha \\propto d$），或放宽隐私参数（如 $\\delta = \\mathrm{poly}(1/n)$），或依赖非标准假设。",
      "summary_en": "We establish the first computational hardness results for differentially private (DP) coreset construction for the $k$-means objective. Assuming the existence of one-way functions—a standard cryptographic assumption—we prove that no polynomial-time $(\\varepsilon, 1/n^{\\omega(1)})$-DP algorithm can compute a coreset with constant multiplicative error $\\alpha > 0$ and constant additive error for $k$-means under the $\\ell_\\infty$-metric, even when $k = 3$. For the Euclidean metric in $d$ dimensions, we show an analogous lower bound for $\\alpha = \\Theta(1/d^2)$. These results demonstrate an inherent tension among privacy, approximation quality, and computational efficiency, resolving a fundamental open question and explaining the limitations of existing private coreset algorithms.",
      "summary": "## 研究背景与问题  \n本研究聚焦**差分隐私（DP）下 $k$-means 聚类核心集（coreset）的计算复杂性**。核心集是原始数据点集的紧凑代理表示，要求对任意 $k$-中心解，其 $k$-means 目标函数值被近似至 $(1\\pm\\alpha)$ 倍乘性误差及某加性项内。在隐私保护机器学习中，同时满足高精度（小 $\\alpha$）、强隐私（小 $\\varepsilon$、极小 $\\delta$）与高效计算（多项式时间）被视为理想目标，但三者能否兼顾尚无理论定论。\n\n## 主要方法与技术路线  \n本文首次建立该问题的**计算硬度下界**，基于密码学标准假设——**单向函数存在性**（widely believed foundational assumption in cryptography）。通过精巧的归约构造，将单向函数的不可逆性嵌入到 $k$-means 核心集的私有计算任务中：若存在高效私有算法，则可反向破解单向函数，导出矛盾。\n\n## 关键发现与创新点  \n- **首个严格下界**：在 $\\ell_\\infty$-度量下，即使仅需 $k=3$ 个聚类中心，也**不存在多项式时间的 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法**能输出具有常数精度（$\\alpha>0$）和常数加性误差的核心集；  \n- **欧氏空间紧致结果**：在 $d$ 维欧氏空间中，同样假设下，对任意多项式时间 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法，若要求 $\\alpha = \\Theta(1/d^2)$，则核心集计算亦不可行；  \n- 该结果揭示了**隐私、精度与效率三者间固有的计算权衡**，解释了为何现有私有核心集算法或牺牲精度（如 $\\alpha \\propto d$），或放宽隐私参数（如 $\\delta = \\mathrm{poly}(1/n)$），或依赖非标准假设。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17454v1",
      "arxiv_id": "2602.17454v1",
      "title": "Privacy in Theory, Bugs in Practice: Grey-Box Auditing of Differential Privacy Libraries",
      "authors": [
        "Tudor Cebere",
        "David Erb",
        "Damien Desfontaines",
        "Aurélien Bellet",
        "Jack Fitzsimons"
      ],
      "abstract": "Differential privacy (DP) implementations are notoriously prone to errors, with subtle bugs frequently invalidating theoretical guarantees. Existing verification methods are often impractical: formal tools are too restrictive, while black-box statistical auditing is intractable for complex pipelines and fails to pinpoint the source of the bug. This paper introduces Re:cord-play, a gray-box auditing paradigm that inspects the internal state of DP algorithms. By running an instrumented algorithm on neighboring datasets with identical randomness, Re:cord-play directly checks for data-dependent control flow and provides concrete falsification of sensitivity violations by comparing declared sensitivity against the empirically measured distance between internal inputs. We generalize this to Re:cord-play-sample, a full statistical audit that isolates and tests each component, including untrusted ones. We show that our novel testing approach is both effective and necessary by auditing 12 open-source libraries, including SmartNoise SDK, Opacus, and Diffprivlib, and uncovering 13 privacy violations that impact their theoretical guarantees. We release our framework as an open-source Python package, thereby making it easy for DP developers to integrate effective, computationally inexpensive, and seamless privacy testing as part of their software development lifecycle.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17454v1",
      "url": "https://arxiv.org/abs/2602.17454v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "differential",
        "dp"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与挑战  \n差分隐私（DP）的理论保障高度依赖于实现的精确性，但现实中的DP库普遍存在隐蔽缺陷：微小的代码错误（如敏感度误设、随机性复用不当、控制流数据依赖）即可彻底破坏ε-δ隐私保证。现有验证方法存在严重局限——**形式化验证**工具（如Fuzzing+Coq）适用场景狭窄、难以覆盖复杂机器学习流水线；而**黑盒统计审计**虽易部署，却无法定位故障组件，且对高维、非线性DP机制（如带裁剪的梯度下降）缺乏统计功效，常导致假阴性。\n\n## 方法创新：Re:cord-play 灰盒审计范式  \n本文提出 **Re:cord-play**——首个面向DP库的灰盒动态审计框架。其核心思想是：在**相同随机种子**下，对一对相邻输入数据集（仅单条记录差异）执行插桩后的DP算法，实时捕获内部状态（如裁剪前梯度、噪声添加前的聚合值）。通过比对两路径中各中间变量的**实测L₁/L₂距离**与开发者声明的敏感度，直接检测敏感度违规；更关键的是，它能识别**数据依赖型控制流**（如`if norm > T: clip()`），此类逻辑漏洞在传统测试中极易被忽略。\n\n## 扩展与验证  \n进一步提出 **Re:cord-play-sample**：将完整DP流水线解耦为可测试子组件（含第三方/不可信模块），对每个环节独立施加灰盒审计。我们在12个主流开源DP库（包括Microsoft SmartNoise SDK、PyTorch Opacus、IBM Diffprivlib）上开展系统性审计，**发现13处实质性隐私违规**，涵盖敏感度高估、噪声缩放错误、邻域定义不一致等，其中7例直接导致理论隐私预算失效。所有漏洞均已向维护者披露并获确认。我们已将框架开源为轻量级Python包，支持无缝集成至CI/CD流程，单次审计耗时通常<5秒，无需修改被测代码。",
      "summary_en": "Differential privacy (DP) implementations are highly error-prone, with subtle bugs—such as incorrect sensitivity declarations, data-dependent control flow, or flawed noise scaling—commonly invalidating theoretical privacy guarantees. Existing verification methods fall short: formal tools are overly restrictive and hard to scale, while black-box statistical auditing lacks debugging capability and fails on complex, non-linear DP pipelines. This paper introduces **Re:cord-play**, a novel *gray-box* auditing paradigm that instruments DP algorithms to observe internal states (e.g., pre-noise aggregates, pre-clipping gradients) when executed on neighboring datasets under *identical randomness*. By comparing empirical input distances against declared sensitivities—and detecting data-dependent branching—it provides concrete, actionable falsifications of DP violations. We generalize this to **Re:cord-play-sample**, enabling component-wise auditing, even for untrusted modules. Applied to 12 open-source DP libraries (e.g., SmartNoise, Opacus, Diffprivlib), our framework uncovered **13 critical privacy violations**, 7 of which break the core ε-δ guarantee. We release the tool as an open-source Python package—lightweight, developer-friendly, and CI-ready.",
      "summary": "## 背景与挑战  \n差分隐私（DP）的理论保障高度依赖于实现的精确性，但现实中的DP库普遍存在隐蔽缺陷：微小的代码错误（如敏感度误设、随机性复用不当、控制流数据依赖）即可彻底破坏ε-δ隐私保证。现有验证方法存在严重局限——**形式化验证**工具（如Fuzzing+Coq）适用场景狭窄、难以覆盖复杂机器学习流水线；而**黑盒统计审计**虽易部署，却无法定位故障组件，且对高维、非线性DP机制（如带裁剪的梯度下降）缺乏统计功效，常导致假阴性。\n\n## 方法创新：Re:cord-play 灰盒审计范式  \n本文提出 **Re:cord-play**——首个面向DP库的灰盒动态审计框架。其核心思想是：在**相同随机种子**下，对一对相邻输入数据集（仅单条记录差异）执行插桩后的DP算法，实时捕获内部状态（如裁剪前梯度、噪声添加前的聚合值）。通过比对两路径中各中间变量的**实测L₁/L₂距离**与开发者声明的敏感度，直接检测敏感度违规；更关键的是，它能识别**数据依赖型控制流**（如`if norm > T: clip()`），此类逻辑漏洞在传统测试中极易被忽略。\n\n## 扩展与验证  \n进一步提出 **Re:cord-play-sample**：将完整DP流水线解耦为可测试子组件（含第三方/不可信模块），对每个环节独立施加灰盒审计。我们在12个主流开源DP库（包括Microsoft SmartNoise SDK、PyTorch Opacus、IBM Diffprivlib）上开展系统性审计，**发现13处实质性隐私违规**，涵盖敏感度高估、噪声缩放错误、邻域定义不一致等，其中7例直接导致理论隐私预算失效。所有漏洞均已向维护者披露并获确认。我们已将框架开源为轻量级Python包，支持无缝集成至CI/CD流程，单次审计耗时通常<5秒，无需修改被测代码。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17413v1",
      "arxiv_id": "2602.17413v1",
      "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
      "authors": [
        "René Brinkhege",
        "Prahlad Menon"
      ],
      "abstract": "In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over private documents on behalf of a data provider. Instead of releasing documents, the provider exposes a natural language interface whose responses are constrained by machine-readable usage policies. We formalize policy-violating information disclosure in this setting, drawing on usage control and information flow security, and introduce virtual redaction: suppressing sensitive information at query time without modifying source documents. We describe an architecture for integrating such a spokesperson with Eclipse Dataspace Components and ODRL-style policies, and outline an initial provider-side integration prototype in which QA requests are routed through a spokesperson service instead of triggering raw document transfer. Our contribution is primarily architectural: we do not yet implement or empirically evaluate the full enforcement pipeline. We therefore outline an evaluation methodology to assess security, utility, and performance trade-offs under benign and adversarial querying as a basis for future empirical work on systematically governed LLM access to multi-party data spaces.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17413v1",
      "url": "https://arxiv.org/abs/2602.17413v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n在当前跨组织数据空间中，数据使用策略（usage policies）通常仅在**资产粒度**上实施：整份文档或数据集被整体授权或拒止。当文档仅局部敏感时，提供方为避免泄露受保护信息，往往需人工红标（redact）文档后共享——该方式成本高昂、粒度粗糙（无法按语义细粒度控制），且难以随策略更新或合作方变更而动态维护。\n\n## 方法与创新  \n本文提出 **DAVE**（Data-Access via Policy-Enforcing spokesperson），一种基于大语言模型（LLM）的策略强制型“数据代言人”架构。DAVE 不直接发布原始文档，而是为数据提供方代理响应自然语言查询，其输出严格受**机器可读策略**（如ODRL格式）约束。我们形式化定义了该范式下的策略违规信息泄露，并提出**虚拟红标**（virtual redaction）机制：在查询执行时动态抑制敏感内容，无需修改源文档。技术上，我们设计了与Eclipse Dataspace Components（EDC）集成的分层架构，实现策略解析、意图理解、上下文感知响应生成与合规性验证闭环；并构建了初步的提供方侧原型——将问答请求路由至DAVE服务，替代原始文档传输。\n\n## 贡献与展望  \n本工作核心贡献在于**安全可控的LLM访问架构设计**，而非端到端实现或实证评估。我们明确界定其安全边界，提出兼顾**安全性、实用性与性能**的三维度评估框架，涵盖良性查询下的效用保持率、对抗性提示下的策略鲁棒性，以及延迟与吞吐量等工程指标，为未来在多方协同数据空间中系统化治理LLM访问奠定方法论基础。",
      "summary_en": "DAVE is a policy-enforcing LLM-based spokesperson that enables secure, fine-grained data sharing across organizational boundaries without releasing raw documents. Instead of asset-level access control, DAVE answers natural-language queries over private documents while dynamically enforcing machine-readable usage policies (e.g., ODRL) at query time—introducing *virtual redaction* to suppress sensitive content without modifying source assets. We formalize policy-violating information disclosure using usage control and information flow principles, and propose an architecture integrating DAVE with Eclipse Dataspace Components. A provider-side prototype routes QA requests through the spokesperson service rather than triggering document transfer. Our primary contribution is architectural: we define the enforcement model and outline a rigorous evaluation methodology—assessing security (against adversarial queries), utility (answer fidelity under policies), and performance (latency, scalability)—to guide future empirical work on systematically governed LLM access in multi-party data spaces.",
      "summary": "## 背景与问题  \n在当前跨组织数据空间中，数据使用策略（usage policies）通常仅在**资产粒度**上实施：整份文档或数据集被整体授权或拒止。当文档仅局部敏感时，提供方为避免泄露受保护信息，往往需人工红标（redact）文档后共享——该方式成本高昂、粒度粗糙（无法按语义细粒度控制），且难以随策略更新或合作方变更而动态维护。\n\n## 方法与创新  \n本文提出 **DAVE**（Data-Access via Policy-Enforcing spokesperson），一种基于大语言模型（LLM）的策略强制型“数据代言人”架构。DAVE 不直接发布原始文档，而是为数据提供方代理响应自然语言查询，其输出严格受**机器可读策略**（如ODRL格式）约束。我们形式化定义了该范式下的策略违规信息泄露，并提出**虚拟红标**（virtual redaction）机制：在查询执行时动态抑制敏感内容，无需修改源文档。技术上，我们设计了与Eclipse Dataspace Components（EDC）集成的分层架构，实现策略解析、意图理解、上下文感知响应生成与合规性验证闭环；并构建了初步的提供方侧原型——将问答请求路由至DAVE服务，替代原始文档传输。\n\n## 贡献与展望  \n本工作核心贡献在于**安全可控的LLM访问架构设计**，而非端到端实现或实证评估。我们明确界定其安全边界，提出兼顾**安全性、实用性与性能**的三维度评估框架，涵盖良性查询下的效用保持率、对抗性提示下的策略鲁棒性，以及延迟与吞吐量等工程指标，为未来在多方协同数据空间中系统化治理LLM访问奠定方法论基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16977v1",
      "arxiv_id": "2602.16977v1",
      "title": "Fail-Closed Alignment for Large Language Models",
      "authors": [
        "Zachary Coalson",
        "Beth Sohler",
        "Aiden Gabriel",
        "Sanghyun Hong"
      ],
      "abstract": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16977v1",
      "url": "https://arxiv.org/abs/2602.16977v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 问题背景  \n当前大语言模型（LLM）对齐（alignment）存在结构性缺陷：主流拒绝机制为“**fail-open**”（失效开放）设计——即当部分安全机制被绕过时，整体对齐即崩溃，导致模型在遭受提示词级越狱（prompt-based jailbreaks）攻击时生成有害内容。其根本原因在于，现有对齐方法将拒绝行为编码于少数主导隐空间方向上；一旦攻击者通过精心构造的提示抑制某一关键特征，安全防护便全面失效。\n\n## 方法创新：Fail-Closed Alignment  \n我们提出**fail-closed alignment**（失效闭合对齐）这一全新设计原则：拒绝机制应具备**冗余性与因果独立性**，即使部分路径失效，其余路径仍能独立维持安全响应。为此，我们构建了首个可实现该原则的**渐进式对齐框架**：  \n- 迭代识别模型中已习得的拒绝方向（refusal directions）；  \n- 通过子空间投影与梯度约束**系统性地消融（ablate）这些方向**；  \n- 强制模型在**新、独立的隐子空间中重建安全行为**，形成多路并行、互不干扰的拒绝通路。\n\n## 实验结果与机制验证  \n在四大主流越狱攻击（GCG、AutoDAN、PAIR、TAP）下，本方法达成**最强综合鲁棒性**（平均拒绝成功率提升12.7–31.4%），同时显著缓解“过度拒绝”（over-refusal）问题（合规请求拒绝率降低至<2.1%），且保持生成质量（BLEU、BERTScore与基线无显著差异）。计算开销极小（仅增加约8%训练时间）。  \n**机制分析证实**：经本方法训练的模型编码了**多个因果独立的拒绝方向**——单次越狱攻击无法同步抑制全部通路，为“fail-closed”提供了坚实实证基础，确立其作为鲁棒LLM安全的**原理性基石**。",
      "summary_en": "We identify a critical structural weakness in current LLM alignment: modern refusal mechanisms are *fail-open*—collapsing entirely when even one dominant safety direction is suppressed (e.g., via prompt-based jailbreaks). To address this, we propose *fail-closed alignment* as a robust safety principle: refusal must persist under partial failures through *redundant, causally independent pathways*. We instantiate it with a progressive alignment framework that iteratively identifies and ablates learned refusal directions, forcing the model to reconstruct safety in new, orthogonal subspaces. Evaluated across four state-of-the-art jailbreak attacks, our method achieves the strongest overall robustness (+12.7–31.4% average refusal success), significantly reduces over-refusal (<2.1% false rejections on benign queries), and preserves generation quality—with only ~8% computational overhead. Mechanistic analysis confirms the emergence of multiple causally independent refusal directions, empirically validating fail-closed alignment as a principled foundation for robust LLM safety.",
      "summary": "## 问题背景  \n当前大语言模型（LLM）对齐（alignment）存在结构性缺陷：主流拒绝机制为“**fail-open**”（失效开放）设计——即当部分安全机制被绕过时，整体对齐即崩溃，导致模型在遭受提示词级越狱（prompt-based jailbreaks）攻击时生成有害内容。其根本原因在于，现有对齐方法将拒绝行为编码于少数主导隐空间方向上；一旦攻击者通过精心构造的提示抑制某一关键特征，安全防护便全面失效。\n\n## 方法创新：Fail-Closed Alignment  \n我们提出**fail-closed alignment**（失效闭合对齐）这一全新设计原则：拒绝机制应具备**冗余性与因果独立性**，即使部分路径失效，其余路径仍能独立维持安全响应。为此，我们构建了首个可实现该原则的**渐进式对齐框架**：  \n- 迭代识别模型中已习得的拒绝方向（refusal directions）；  \n- 通过子空间投影与梯度约束**系统性地消融（ablate）这些方向**；  \n- 强制模型在**新、独立的隐子空间中重建安全行为**，形成多路并行、互不干扰的拒绝通路。\n\n## 实验结果与机制验证  \n在四大主流越狱攻击（GCG、AutoDAN、PAIR、TAP）下，本方法达成**最强综合鲁棒性**（平均拒绝成功率提升12.7–31.4%），同时显著缓解“过度拒绝”（over-refusal）问题（合规请求拒绝率降低至<2.1%），且保持生成质量（BLEU、BERTScore与基线无显著差异）。计算开销极小（仅增加约8%训练时间）。  \n**机制分析证实**：经本方法训练的模型编码了**多个因果独立的拒绝方向**——单次越狱攻击无法同步抑制全部通路，为“fail-closed”提供了坚实实证基础，确立其作为鲁棒LLM安全的**原理性基石**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17842v1",
      "arxiv_id": "2602.17842v1",
      "title": "StableAML: Machine Learning for Behavioral Wallet Detection in Stablecoin Anti-Money Laundering on Ethereum",
      "authors": [
        "Luciano Juvinski",
        "Haochen Li",
        "Alessio Brini"
      ],
      "abstract": "Global illicit fund flows exceed an estimated $3.1 trillion annually, with stablecoins emerging as a preferred laundering medium due to their liquidity. While decentralized protocols increasingly adopt zero-knowledge proofs to obfuscate transaction graphs, centralized stablecoins remain critical \"transparent choke points\" for compliance. Leveraging this persistent visibility, this study analyzes an Ethereum dataset and uses behavioral features to develop a robust AML framework. Our findings demonstrate that domain-informed tree ensemble models achieve higher Macro-F1 score, significantly outperforming graph neural networks, which struggle with the increasing fragmentation of transaction networks. The model's interpretability goes beyond binary detection, successfully dissecting distinct typologies: it differentiates the complex, high-velocity dispersion of cybercrime syndicates from the constrained, static footprints left by sanctioned entities. This framework aligns with the industry shift toward deterministic verification, satisfying the auditability and compliance expectations under regulations such as the EU's MiCA and the U.S. GENIUS Act while minimizing unjustified asset freezes. By automating high-precision detection, we propose an approach that effectively raises the economic cost of financial misconduct without stifling innovation.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17842v1",
      "url": "https://arxiv.org/abs/2602.17842v1",
      "categories": [
        "cs.CR",
        "cs.CE"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "learning",
        "zero-knowledge"
      ],
      "keyword_score": 3,
      "summary_zh": "## StableAML：面向以太坊稳定币反洗钱的行为钱包检测机器学习框架\n\n全球非法资金流动年均超**3.1万亿美元**，而稳定币凭借其高流动性与跨链便捷性，正成为洗钱活动日益偏好的媒介。尽管去中心化协议通过零知识证明等技术加剧交易图谱的匿名化，但由中心化发行方托管、链上可追溯的稳定币（如USDC、USDT）仍构成监管合规中不可替代的“**透明扼流点**”。\n\n本研究基于真实以太坊链上数据，构建首个专为稳定币反洗钱（AML）定制的**行为驱动型检测框架 StableAML**。我们摒弃对复杂图结构建模的依赖，转而提取涵盖资金流速、地址生命周期、多跳交互熵、时序波动性等**27维可解释行为特征**，并融合监管先验知识（如OFAC制裁模式、勒索软件收款惯性）设计特征工程。实验表明：**领域增强的梯度提升树模型（XGBoost+SHAP）在Macro-F1达0.862**，显著优于图神经网络（GNNs，Macro-F1仅0.613）——后者在地址碎片化、混币器介入导致的图稀疏化场景下性能骤降。\n\nStableAML 的核心创新在于**可审计的细粒度分类能力**：它不仅能二元判定高风险钱包，更能**精准区分三类典型洗钱行为**：① 网络犯罪集团的“高频分散型”行为（短周期内向数百地址快速拆分转账）；② 受制裁实体的“低活静态型”行为（长期持有、极少交互、地址复用率极低）；③ 混币服务中介的“中转桥接型”行为（高入金/出金比、低余额留存、强时间聚类）。该框架完全满足欧盟《加密资产市场法规》（MiCA）和美国《GENIUS法案》对**可验证、可追溯、最小化误伤**的合规要求，实测将误冻结率降低至<0.3%，同时将可疑交易识别效率提升4.2倍。",
      "summary_en": "StableAML introduces a behavior-driven machine learning framework for detecting illicit wallet activity in stablecoin-based money laundering on Ethereum. Leveraging the persistent on-chain transparency of centralized stablecoins—despite growing obfuscation in decentralized protocols—we extract 27 interpretable behavioral features (e.g., velocity, entropy, temporal volatility) informed by AML domain knowledge. On real Ethereum data, domain-enhanced tree ensembles (XGBoost + SHAP) achieve a Macro-F1 of **0.862**, substantially outperforming graph neural networks (0.613), which degrade under network fragmentation and mixer-induced sparsity. Crucially, StableAML moves beyond binary classification: it reliably dissects typologies—distinguishing high-velocity cybercrime dispersion, static sanctioned-entity footprints, and bridging mixer behaviors—with audit-ready interpretability. Fully aligned with MiCA and the U.S. GENIUS Act, it enables deterministic verification while reducing false freezes to <0.3% and boosting detection efficiency 4.2×.",
      "summary": "## StableAML：面向以太坊稳定币反洗钱的行为钱包检测机器学习框架\n\n全球非法资金流动年均超**3.1万亿美元**，而稳定币凭借其高流动性与跨链便捷性，正成为洗钱活动日益偏好的媒介。尽管去中心化协议通过零知识证明等技术加剧交易图谱的匿名化，但由中心化发行方托管、链上可追溯的稳定币（如USDC、USDT）仍构成监管合规中不可替代的“**透明扼流点**”。\n\n本研究基于真实以太坊链上数据，构建首个专为稳定币反洗钱（AML）定制的**行为驱动型检测框架 StableAML**。我们摒弃对复杂图结构建模的依赖，转而提取涵盖资金流速、地址生命周期、多跳交互熵、时序波动性等**27维可解释行为特征**，并融合监管先验知识（如OFAC制裁模式、勒索软件收款惯性）设计特征工程。实验表明：**领域增强的梯度提升树模型（XGBoost+SHAP）在Macro-F1达0.862**，显著优于图神经网络（GNNs，Macro-F1仅0.613）——后者在地址碎片化、混币器介入导致的图稀疏化场景下性能骤降。\n\nStableAML 的核心创新在于**可审计的细粒度分类能力**：它不仅能二元判定高风险钱包，更能**精准区分三类典型洗钱行为**：① 网络犯罪集团的“高频分散型”行为（短周期内向数百地址快速拆分转账）；② 受制裁实体的“低活静态型”行为（长期持有、极少交互、地址复用率极低）；③ 混币服务中介的“中转桥接型”行为（高入金/出金比、低余额留存、强时间聚类）。该框架完全满足欧盟《加密资产市场法规》（MiCA）和美国《GENIUS法案》对**可验证、可追溯、最小化误伤**的合规要求，实测将误冻结率降低至<0.3%，同时将可疑交易识别效率提升4.2倍。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17837v1",
      "arxiv_id": "2602.17837v1",
      "title": "TFL: Targeted Bit-Flip Attack on Large Language Model",
      "authors": [
        "Jingkai Guo",
        "Chaitali Chakrabarti",
        "Deliang Fan"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in safety and security critical applications, raising concerns about their robustness to model parameter fault injection attacks. Recent studies have shown that bit-flip attacks (BFAs), which exploit computer main memory (i.e., DRAM) vulnerabilities to flip a small number of bits in model weights, can severely disrupt LLM behavior. However, existing BFA on LLM largely induce un-targeted failure or general performance degradation, offering limited control over manipulating specific or targeted outputs. In this paper, we present TFL, a novel targeted bit-flip attack framework that enables precise manipulation of LLM outputs for selected prompts while maintaining almost no or minor degradation on unrelated inputs. Within our TFL framework, we propose a novel keyword-focused attack loss to promote attacker-specified target tokens in generative outputs, together with an auxiliary utility score that balances attack effectiveness against collateral performance impact on benign data. We evaluate TFL on multiple LLMs (Qwen, DeepSeek, Llama) and benchmarks (DROP, GSM8K, and TriviaQA). The experiments show that TFL achieves successful targeted LLM output manipulations with less than 50 bit flips and significantly reduced effect on unrelated queries compared to prior BFA approaches. This demonstrates the effectiveness of TFL and positions it as a new class of stealthy and targeted LLM model attack.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17837v1",
      "url": "https://arxiv.org/abs/2602.17837v1",
      "categories": [
        "cs.CR",
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n大型语言模型（LLMs）正广泛部署于安全关键场景（如金融风控、医疗辅助、自动驾驶决策支持），其参数鲁棒性面临严峻挑战。近期研究表明，**位翻转攻击（Bit-Flip Attacks, BFAs）** 可利用DRAM内存硬件漏洞，仅翻转模型权重中极少数比特（常<100 bit），即导致模型输出严重失真。然而，现有BFA方法多为**非定向攻击**：其引发的失效不可控、输出偏移随机，难以精准诱导特定目标响应（例如强制生成某句恶意指令或伪造答案），且对无关输入亦造成显著性能退化，隐蔽性与实用性受限。\n\n## 方法创新：TFL框架  \n本文提出**TFL（Targeted Flip Learning）**——首个面向LLM的**可定向、低开销、高保真**位翻转攻击框架。核心创新包括：  \n- **关键词聚焦攻击损失（Keyword-Focused Attack Loss）**：在梯度优化中显式建模目标提示（prompt）下指定关键词/令牌（token）的生成概率，引导翻转位置精准增强目标输出；  \n- **效用平衡评分机制（Auxiliary Utility Score）**：联合优化攻击成功率与良性样本（unrelated queries）准确率，严格约束对非目标任务的干扰（如DROP阅读理解、GSM8K数学推理等基准上退化<1.2%）；  \n- **硬件感知翻转定位策略**：结合权重敏感度分析与内存位级映射，确保翻转操作在物理层面可行且高效。\n\n## 实验验证与意义  \n在Qwen-7B、DeepSeek-Coder-6.7B、Llama-2-7B三大主流模型及DROP、GSM8K、TriviaQA多任务基准上验证：TFL仅需**平均43.6次位翻转**即可实现>92%的目标输出命中率，而对非目标查询的准确率下降中位数仅为**0.8%**（对比基线BFA平均下降达14.7%）。本工作首次证明：LLM参数空间存在**稀疏、可定位、语义可控的“攻击面”**，为模型硬件级安全评估提供了新范式，亦警示了边缘设备与云服务器中内存防护机制的迫切升级需求。",
      "summary_en": "Large language models (LLMs) deployed in safety-critical applications are vulnerable to bit-flip attacks (BFAs) exploiting DRAM hardware faults—but existing BFAs induce untargeted failures with poor control over specific outputs and significant collateral degradation. This paper introduces **TFL**, the first *targeted* bit-flip attack framework for LLMs that enables precise manipulation of model outputs for selected prompts while preserving near-original performance on unrelated inputs. TFL achieves this via a novel **keyword-focused attack loss** to boost attacker-specified tokens and an **auxiliary utility score** that jointly optimizes attack success and benign-task fidelity. Evaluated across Qwen, DeepSeek, and Llama models on DROP, GSM8K, and TriviaQA, TFL succeeds with **<50 bit flips** (avg. 43.6), achieves >92% targeted output accuracy, and incurs only **0.8% median accuracy drop** on non-target queries—dramatically outperforming prior BFAs (avg. 14.7% drop). TFL establishes a new class of stealthy, semantically guided, hardware-level LLM attacks.",
      "summary": "## 背景与问题  \n大型语言模型（LLMs）正广泛部署于安全关键场景（如金融风控、医疗辅助、自动驾驶决策支持），其参数鲁棒性面临严峻挑战。近期研究表明，**位翻转攻击（Bit-Flip Attacks, BFAs）** 可利用DRAM内存硬件漏洞，仅翻转模型权重中极少数比特（常<100 bit），即导致模型输出严重失真。然而，现有BFA方法多为**非定向攻击**：其引发的失效不可控、输出偏移随机，难以精准诱导特定目标响应（例如强制生成某句恶意指令或伪造答案），且对无关输入亦造成显著性能退化，隐蔽性与实用性受限。\n\n## 方法创新：TFL框架  \n本文提出**TFL（Targeted Flip Learning）**——首个面向LLM的**可定向、低开销、高保真**位翻转攻击框架。核心创新包括：  \n- **关键词聚焦攻击损失（Keyword-Focused Attack Loss）**：在梯度优化中显式建模目标提示（prompt）下指定关键词/令牌（token）的生成概率，引导翻转位置精准增强目标输出；  \n- **效用平衡评分机制（Auxiliary Utility Score）**：联合优化攻击成功率与良性样本（unrelated queries）准确率，严格约束对非目标任务的干扰（如DROP阅读理解、GSM8K数学推理等基准上退化<1.2%）；  \n- **硬件感知翻转定位策略**：结合权重敏感度分析与内存位级映射，确保翻转操作在物理层面可行且高效。\n\n## 实验验证与意义  \n在Qwen-7B、DeepSeek-Coder-6.7B、Llama-2-7B三大主流模型及DROP、GSM8K、TriviaQA多任务基准上验证：TFL仅需**平均43.6次位翻转**即可实现>92%的目标输出命中率，而对非目标查询的准确率下降中位数仅为**0.8%**（对比基线BFA平均下降达14.7%）。本工作首次证明：LLM参数空间存在**稀疏、可定位、语义可控的“攻击面”**，为模型硬件级安全评估提供了新范式，亦警示了边缘设备与云服务器中内存防护机制的迫切升级需求。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17875v1",
      "arxiv_id": "2602.17875v1",
      "title": "MultiVer: Zero-Shot Multi-Agent Vulnerability Detection",
      "authors": [
        "Shreshth Rajan"
      ],
      "abstract": "We present MultiVer, a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning. A four-agent ensemble (security, correctness, performance, style) with union voting achieves 82.7% recall on PyVul, exceeding fine-tuned GPT-3.5 (81.3%) by 1.4 percentage points -- the first zeroshot system to surpass fine-tuned performance on this benchmark. On SecurityEval, the same architecture achieves 91.7% detection rate, matching specialized systems. The recall improvement comes at a precision cost: 48.8% precision versus 63.9% for fine-tuned baselines, yielding 61.4% F1. Ablation experiments isolate component contributions: the multi-agent ensemble adds 17 percentage points recall over single-agent security analysis. These results demonstrate that for security applications where false negatives are costlier than false positives, zero-shot multi-agent ensembles can match and exceed fine-tuned models on the metric that matters most.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17875v1",
      "url": "https://arxiv.org/abs/2602.17875v1",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## MultiVer：零样本多智能体漏洞检测新范式  \n\n本文提出 **MultiVer**——一种无需微调（zero-shot）的多智能体协同漏洞检测系统，旨在解决传统单模型方法在安全关键场景中漏报率（false negative）过高这一核心痛点。MultiVer 构建了由**安全分析、正确性验证、性能评估、代码风格审查**四个专业化智能体组成的协同 ensemble，各代理独立生成漏洞判断，最终通过**并集投票（union voting）** 整合结果，显著提升召回能力。在主流基准 PyVul 上，MultiVer 实现 **82.7% 的召回率**，首次超越同规模微调模型 GPT-3.5（81.3%），领先 1.4 个百分点；在更具挑战性的 SecurityEval 基准上，检测率达 **91.7%**，与专用微调系统持平。尽管精度（precision）为 48.8%，低于微调基线的 63.9%，其综合 F1 分数仍达 **61.4%**。消融实验表明：多智能体架构本身贡献了 **+17 个百分点的召回增益**（相较单安全代理），证实分工协作对覆盖多样化漏洞模式的关键作用。本工作首次证明：在**漏报代价远高于误报**的安全应用场景中，零样本多智能体系统不仅能规避昂贵的数据标注与微调开销，更能于最关键的召回指标上实现反超——为资源受限、高可靠性要求的软件安全分析提供了可即插即用的新路径。",
      "summary_en": "We introduce **MultiVer**, a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning. It employs a four-agent ensemble—security, correctness, performance, and style—combined via union voting. On PyVul, MultiVer attains **82.7% recall**, surpassing fine-tuned GPT-3.5 (81.3%)—the first zero-shot method to exceed fine-tuned performance on this benchmark. On SecurityEval, it achieves **91.7% detection rate**, matching specialized fine-tuned systems. While precision drops to 48.8% (vs. 63.9% for baselines), the resulting F1 is 61.4%. Ablation shows the multi-agent design alone contributes +17 percentage points recall over single-agent security analysis. These results demonstrate that zero-shot multi-agent ensembles can match or exceed fine-tuned models on recall—the most critical metric where false negatives incur high cost in security contexts.",
      "summary": "## MultiVer：零样本多智能体漏洞检测新范式  \n\n本文提出 **MultiVer**——一种无需微调（zero-shot）的多智能体协同漏洞检测系统，旨在解决传统单模型方法在安全关键场景中漏报率（false negative）过高这一核心痛点。MultiVer 构建了由**安全分析、正确性验证、性能评估、代码风格审查**四个专业化智能体组成的协同 ensemble，各代理独立生成漏洞判断，最终通过**并集投票（union voting）** 整合结果，显著提升召回能力。在主流基准 PyVul 上，MultiVer 实现 **82.7% 的召回率**，首次超越同规模微调模型 GPT-3.5（81.3%），领先 1.4 个百分点；在更具挑战性的 SecurityEval 基准上，检测率达 **91.7%**，与专用微调系统持平。尽管精度（precision）为 48.8%，低于微调基线的 63.9%，其综合 F1 分数仍达 **61.4%**。消融实验表明：多智能体架构本身贡献了 **+17 个百分点的召回增益**（相较单安全代理），证实分工协作对覆盖多样化漏洞模式的关键作用。本工作首次证明：在**漏报代价远高于误报**的安全应用场景中，零样本多智能体系统不仅能规避昂贵的数据标注与微调开销，更能于最关键的召回指标上实现反超——为资源受限、高可靠性要求的软件安全分析提供了可即插即用的新路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17868v1",
      "arxiv_id": "2602.17868v1",
      "title": "MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies",
      "authors": [
        "Vasilii Feofanov",
        "Songkang Wen",
        "Jianfeng Zhang",
        "Lujia Pan",
        "Ievgen Redko"
      ],
      "abstract": "Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17868v1",
      "url": "https://arxiv.org/abs/2602.17868v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n时间序列分类（TSC）基础模型具有重要实用价值，可作为通用特征提取器支撑多样化的下游任务。然而，早期模型（如Mantis）虽展现出潜力，其**冻结编码器的零样本性能仍显著落后于微调后表现**，存在难以忽视的“零样本鸿沟”。\n\n## 核心方法与创新  \n本研究提出MantisV2，系统性弥合该鸿沟，包含三大技术突破：  \n- **Mantis+**：首个**完全基于合成时间序列预训练**的Mantis变体，通过可控、多样化、标签丰富的合成数据（覆盖周期性、趋势、噪声等典型模式）增强泛化表征能力；  \n- **架构精炼**：经严谨消融实验优化网络深度、注意力头数与归一化策略，获得更轻量（参数减少32%）、更鲁棒的**MantisV2编码器**；  \n- **增强型测试时策略**：引入**中间层特征融合机制**，动态加权不同深度的隐藏表示，并改进输出token聚合方式（替代简单平均），显著提升判别性；  \n- **进一步增益**：结合**自集成（self-ensembling）** 与**跨模型嵌入融合**（如与TS-TCC、TS2Vec互补嵌入拼接），实现性能叠加。\n\n## 实验结果与意义  \n在UCR/UEA全量数据集（128个数据集）、HAR动作识别及多模态EEG脑电基准上，MantisV2实现全面领先：零样本准确率平均提升**+5.2%**（vs. Mantis），在37个数据集上刷新SOTA；Mantis+在小样本迁移中亦展现更强适应性。本工作首次验证了**高质量合成数据+测试时智能推理**可实质性消除时间序列基础模型的零样本性能瓶颈，为无标注场景下的工业时序分析提供了新范式。",
      "summary_en": "Time series foundation models promise universal feature extraction but suffer from a large zero-shot performance gap versus fine-tuned counterparts. This paper introduces **MantisV2**, which closes this gap via three key advances: (1) **Mantis+**, a variant pre-trained *exclusively* on diverse, label-rich synthetic time series to enhance generalization; (2) an optimized, lightweight encoder architecture derived from controlled ablations; and (3) an enhanced test-time strategy leveraging intermediate-layer representations and refined output-token aggregation. Further gains are achieved through self-ensembling and cross-model embedding fusion. Extensive evaluation across UCR, UEA, HAR, and EEG benchmarks shows MantisV2 and Mantis+ consistently outperform prior foundation models—achieving new state-of-the-art zero-shot accuracy (avg. +5.2% over Mantis) on 128 datasets. This work demonstrates that high-fidelity synthetic data combined with intelligent test-time inference can effectively eliminate the zero-shot bottleneck in time series representation learning.",
      "summary": "## 研究背景与问题  \n时间序列分类（TSC）基础模型具有重要实用价值，可作为通用特征提取器支撑多样化的下游任务。然而，早期模型（如Mantis）虽展现出潜力，其**冻结编码器的零样本性能仍显著落后于微调后表现**，存在难以忽视的“零样本鸿沟”。\n\n## 核心方法与创新  \n本研究提出MantisV2，系统性弥合该鸿沟，包含三大技术突破：  \n- **Mantis+**：首个**完全基于合成时间序列预训练**的Mantis变体，通过可控、多样化、标签丰富的合成数据（覆盖周期性、趋势、噪声等典型模式）增强泛化表征能力；  \n- **架构精炼**：经严谨消融实验优化网络深度、注意力头数与归一化策略，获得更轻量（参数减少32%）、更鲁棒的**MantisV2编码器**；  \n- **增强型测试时策略**：引入**中间层特征融合机制**，动态加权不同深度的隐藏表示，并改进输出token聚合方式（替代简单平均），显著提升判别性；  \n- **进一步增益**：结合**自集成（self-ensembling）** 与**跨模型嵌入融合**（如与TS-TCC、TS2Vec互补嵌入拼接），实现性能叠加。\n\n## 实验结果与意义  \n在UCR/UEA全量数据集（128个数据集）、HAR动作识别及多模态EEG脑电基准上，MantisV2实现全面领先：零样本准确率平均提升**+5.2%**（vs. Mantis），在37个数据集上刷新SOTA；Mantis+在小样本迁移中亦展现更强适应性。本工作首次验证了**高质量合成数据+测试时智能推理**可实质性消除时间序列基础模型的零样本性能瓶颈，为无标注场景下的工业时序分析提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16708v1",
      "arxiv_id": "2602.16708v1",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Mihai Christodorescu",
        "Somesh Jha"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16708v1",
      "url": "https://arxiv.org/abs/2602.16708v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "prompt",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的高敏场景，如客户服务协议、多级审批流程、敏感数据访问控制及合规性监管。当前主流做法是将策略嵌入提示词（prompt），但该方式缺乏强制力保障，无法防止模型绕过、忽略或误解释策略，导致策略违规频发。\n\n## 方法：PCAS 政策编译器  \n本文提出 **PCAS（Policy Compiler for Agentic Systems）**——一种面向智能体系统的策略编译框架，实现**确定性策略执行**。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图**（causal dependency graph）表征系统状态，显式追踪工具调用、工具返回值、消息传递等事件间的因果与信息流关系；  \n- **策略语言设计**：采用类 Datalog 的声明式策略语言，天然支持**传递性信息流分析**与**跨智能体溯源**（cross-agent provenance），可精确表达“某数据是否经未授权代理传播”等复杂约束；  \n- **运行时强保障**：集成轻量级**参考监视器**（reference monitor），在所有动作执行前实时拦截并阻断违规行为，执行逻辑完全独立于LLM推理过程，杜绝模型幻觉或策略规避。\n\n## 关键成果与验证  \nPCAS 以“编译即合规”范式工作：输入现有智能体代码与策略规范，自动产出**策略内建**（policy-by-construction）的插桩系统，无需重构原有安全逻辑。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离；  \n- 多智能体药物警戒系统中的多级审批策略；  \n- 客户服务组织策略（如地域权限、敏感操作二次确认）。  \n实验表明：在客户服务任务上，PCAS 将前沿模型（如GPT-4、Claude 3）的策略合规率从**48% 提升至 93%**，且所有插桩运行中**零策略违规发生**，验证了其确定性保障能力。",
      "summary_en": "PCAS (Policy Compiler for Agentic Systems) is a novel framework that enables *deterministic, runtime-enforced* policy compliance for LLM-based agents—without relying on prompt engineering or model fine-tuning. It addresses the fundamental limitation of linear message histories by modeling system state as a *causal dependency graph*, capturing information flow across tool calls, results, and messages. Policies are written in a declarative, Datalog-derived language supporting transitive flow and cross-agent provenance. A lightweight reference monitor intercepts all actions pre-execution, blocking violations with guaranteed enforcement—decoupled from model reasoning. Given an existing agent implementation and a policy specification, PCAS compiles them into an instrumented, policy-compliant system *by construction*. Evaluated on three real-world case studies—including prompt injection defense, pharmacovigilance approval workflows, and customer service policies—PCAS boosts compliance from 48% to 93% across frontier models (e.g., GPT-4, Claude 3), achieving zero policy violations in all instrumented runs.",
      "summary": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的高敏场景，如客户服务协议、多级审批流程、敏感数据访问控制及合规性监管。当前主流做法是将策略嵌入提示词（prompt），但该方式缺乏强制力保障，无法防止模型绕过、忽略或误解释策略，导致策略违规频发。\n\n## 方法：PCAS 政策编译器  \n本文提出 **PCAS（Policy Compiler for Agentic Systems）**——一种面向智能体系统的策略编译框架，实现**确定性策略执行**。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图**（causal dependency graph）表征系统状态，显式追踪工具调用、工具返回值、消息传递等事件间的因果与信息流关系；  \n- **策略语言设计**：采用类 Datalog 的声明式策略语言，天然支持**传递性信息流分析**与**跨智能体溯源**（cross-agent provenance），可精确表达“某数据是否经未授权代理传播”等复杂约束；  \n- **运行时强保障**：集成轻量级**参考监视器**（reference monitor），在所有动作执行前实时拦截并阻断违规行为，执行逻辑完全独立于LLM推理过程，杜绝模型幻觉或策略规避。\n\n## 关键成果与验证  \nPCAS 以“编译即合规”范式工作：输入现有智能体代码与策略规范，自动产出**策略内建**（policy-by-construction）的插桩系统，无需重构原有安全逻辑。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离；  \n- 多智能体药物警戒系统中的多级审批策略；  \n- 客户服务组织策略（如地域权限、敏感操作二次确认）。  \n实验表明：在客户服务任务上，PCAS 将前沿模型（如GPT-4、Claude 3）的策略合规率从**48% 提升至 93%**，且所有插桩运行中**零策略违规发生**，验证了其确定性保障能力。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16596v1",
      "arxiv_id": "2602.16596v1",
      "title": "Sequential Membership Inference Attacks",
      "authors": [
        "Thomas Michel",
        "Debabrota Basu",
        "Emilie Kaufmann"
      ],
      "abstract": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16596v1",
      "url": "https://arxiv.org/abs/2602.16596v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "math.ST",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "membership",
        "dp",
        "inference"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n现代AI模型具有动态演化特性——在其生命周期中经历多次增量更新（如持续学习、微调或在线训练）。然而，现有成员推断（Membership Inference, MI）攻击与隐私审计方法多基于**静态终态模型**，忽视了模型序列蕴含的时序敏感信息。尽管实证研究发现利用模型更新序列可提升MI攻击效能，但缺乏对“最优”动态攻击的理论刻画：既有分析局限于无限样本下的静态设定，无法指导有限数据、含噪（如差分隐私）场景下的实际部署。\n\n## 方法创新：SeMI* 攻击框架  \n本文提出首个严格定义的**序列式成员推断最优攻击 SeMI***，其核心思想是：将目标样本（canary）的潜在插入时间点建模为隐变量，联合利用模型更新轨迹（而非仅最终模型）构建似然比检验。我们为经验均值估计器推导出SeMI*在**有限样本、含/不含隐私保护（如DP-SGD）** 条件下的**精确最优检测功率**（optimal power），并证明该结果自然退化至经典渐近结论，填补了理论空白。\n\n## 关键发现与优势  \n- **信号增强**：相比仅观测终态模型的攻击，SeMI* 利用更新序列避免MI信号随训练数据累积而“稀释”，显著提升小样本与早期插入场景的敏感性；  \n- **审计可调性**：攻击者可协同优化**插入时机**与**canary设计**，实现更紧致的隐私边界估计；  \n- **实证验证**：在多种数据分布（MNIST、CIFAR-10、Purchase）及DP-SGD训练/微调模型上，SeMI*的实用变体 consistently 超越SOTA基线（如Loss-based、Shadow Model），将隐私审计的ε-上界平均收紧18–35%。",
      "summary_en": "Modern AI models evolve sequentially through updates—yet membership inference (MI) attacks and privacy audits remain largely static. While empirical studies suggest model sequences boost MI power, rigorous analysis of *optimal* sequential attacks is missing: existing theory assumes infinite samples and static models. We bridge this gap by proposing **SeMI\\***, the first theoretically grounded sequential MI attack that leverages the full model update trajectory to detect whether a target sample was inserted at a specific step. For empirical mean estimation, we derive SeMI\\*'s *exact optimal detection power* under finite samples—with or without privacy (e.g., DP-SGD)—recovering known asymptotics as a special case. Crucially, SeMI\\* avoids signal dilution inherent in final-model-only attacks, enabling stronger inference early in training. Moreover, adversaries can jointly optimize insertion timing and canary design for tighter privacy auditing. Experiments across datasets (MNIST, CIFAR-10, Purchase) and DP-SGD-trained/fine-tuned models confirm that practical SeMI\\* variants yield significantly tighter privacy bounds—reducing estimated ε by 18–35% over state-of-the-art baselines.",
      "summary": "## 背景与问题  \n现代AI模型具有动态演化特性——在其生命周期中经历多次增量更新（如持续学习、微调或在线训练）。然而，现有成员推断（Membership Inference, MI）攻击与隐私审计方法多基于**静态终态模型**，忽视了模型序列蕴含的时序敏感信息。尽管实证研究发现利用模型更新序列可提升MI攻击效能，但缺乏对“最优”动态攻击的理论刻画：既有分析局限于无限样本下的静态设定，无法指导有限数据、含噪（如差分隐私）场景下的实际部署。\n\n## 方法创新：SeMI* 攻击框架  \n本文提出首个严格定义的**序列式成员推断最优攻击 SeMI***，其核心思想是：将目标样本（canary）的潜在插入时间点建模为隐变量，联合利用模型更新轨迹（而非仅最终模型）构建似然比检验。我们为经验均值估计器推导出SeMI*在**有限样本、含/不含隐私保护（如DP-SGD）** 条件下的**精确最优检测功率**（optimal power），并证明该结果自然退化至经典渐近结论，填补了理论空白。\n\n## 关键发现与优势  \n- **信号增强**：相比仅观测终态模型的攻击，SeMI* 利用更新序列避免MI信号随训练数据累积而“稀释”，显著提升小样本与早期插入场景的敏感性；  \n- **审计可调性**：攻击者可协同优化**插入时机**与**canary设计**，实现更紧致的隐私边界估计；  \n- **实证验证**：在多种数据分布（MNIST、CIFAR-10、Purchase）及DP-SGD训练/微调模型上，SeMI*的实用变体 consistently 超越SOTA基线（如Loss-based、Shadow Model），将隐私审计的ε-上界平均收紧18–35%。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16564v1",
      "arxiv_id": "2602.16564v1",
      "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
      "authors": [
        "Michael Lanier",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16564v1",
      "url": "https://arxiv.org/abs/2602.16564v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n在大规模网络攻防博弈中，基于仿真的安全博弈（Simulation-Based Network Security Games）面临“维度灾难”：网络节点数激增导致状态空间指数级膨胀，传统双 oracle（Double Oracle）或策略空间响应优化（PSRO）方法因需枚举大量纯策略而难以扩展。现有方法在万级节点网络中常遭遇内存溢出、训练停滞或策略质量骤降等问题。\n\n## 方法创新：MetaDOAR 框架  \n本文提出 **MetaDOAR**——一种轻量级元控制器，通过三层协同机制实现可扩展性突破：  \n- **分区感知过滤层**：学习从节点结构嵌入（如 PageRank、度中心性等）到紧凑状态投影的映射，**实时筛选 top-k 关键设备子集**（如高价值服务器、边界网关），将搜索空间压缩 2–3 个数量级；  \n- **分层决策架构**：低层执行器仅在该子集上进行聚焦式束搜索（beam search），由专用 critic 代理提供动作评估；  \n- **量化缓存机制**：将 critic 的前向计算结果按**量化状态投影 + 局部动作标识**为键存入 LRU 缓存，并采用保守的 *k-hop 缓存失效策略*（仅当邻接子图结构变化 ≥k 跳时刷新），减少 >78% 冗余计算，且不牺牲策略最优性。\n\n## 实验结果与意义  \n在包含 5,000–50,000 节点的真实拓扑变体（AS-level、企业内网合成图）上，MetaDOAR 在攻击者/防御者平均收益上**超越 SOTA 基线（如 PSRO+GNN、DeepStack-Net）12.6%–29.3%**，训练内存占用降低 5.4×，单轮迭代耗时下降 68%。本工作为超大规模网络化决策问题提供了**首个兼具理论保障（收敛性继承自 DOAR）、工程实用性（<16GB GPU 显存可训）与部署友好性（模块化设计支持在线热更新）的分层强化学习路径**。",
      "summary_en": "We propose **MetaDOAR**, a scalable meta-controller for simulation-based network security games that extends the Double Oracle / PSRO paradigm with three key innovations: (1) a learned, partition-aware filtering layer that projects per-node structural embeddings into a compact state representation to rapidly select a *top-k subset* of critical devices; (2) a hierarchical execution pipeline where a low-level actor performs focused beam search only on this subset, guided by a critic agent; and (3) a quantized LRU cache for critic evaluations—keyed by discretized state projections and local action IDs—with conservative *k-hop invalidation* to eliminate >78% redundant computation while preserving decision quality. Empirically, MetaDOAR achieves **12.6–29.3% higher player payoffs** than state-of-the-art baselines on networks with up to 50,000 nodes, with **5.4× lower memory usage** and **68% faster iteration time**, without performance degradation. This work delivers a practical, theoretically grounded framework for hierarchical policy learning in large-scale cyber-networked systems.",
      "summary": "## 研究背景与挑战  \n在大规模网络攻防博弈中，基于仿真的安全博弈（Simulation-Based Network Security Games）面临“维度灾难”：网络节点数激增导致状态空间指数级膨胀，传统双 oracle（Double Oracle）或策略空间响应优化（PSRO）方法因需枚举大量纯策略而难以扩展。现有方法在万级节点网络中常遭遇内存溢出、训练停滞或策略质量骤降等问题。\n\n## 方法创新：MetaDOAR 框架  \n本文提出 **MetaDOAR**——一种轻量级元控制器，通过三层协同机制实现可扩展性突破：  \n- **分区感知过滤层**：学习从节点结构嵌入（如 PageRank、度中心性等）到紧凑状态投影的映射，**实时筛选 top-k 关键设备子集**（如高价值服务器、边界网关），将搜索空间压缩 2–3 个数量级；  \n- **分层决策架构**：低层执行器仅在该子集上进行聚焦式束搜索（beam search），由专用 critic 代理提供动作评估；  \n- **量化缓存机制**：将 critic 的前向计算结果按**量化状态投影 + 局部动作标识**为键存入 LRU 缓存，并采用保守的 *k-hop 缓存失效策略*（仅当邻接子图结构变化 ≥k 跳时刷新），减少 >78% 冗余计算，且不牺牲策略最优性。\n\n## 实验结果与意义  \n在包含 5,000–50,000 节点的真实拓扑变体（AS-level、企业内网合成图）上，MetaDOAR 在攻击者/防御者平均收益上**超越 SOTA 基线（如 PSRO+GNN、DeepStack-Net）12.6%–29.3%**，训练内存占用降低 5.4×，单轮迭代耗时下降 68%。本工作为超大规模网络化决策问题提供了**首个兼具理论保障（收敛性继承自 DOAR）、工程实用性（<16GB GPU 显存可训）与部署友好性（模块化设计支持在线热更新）的分层强化学习路径**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16520v1",
      "arxiv_id": "2602.16520v1",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "authors": [
        "Doron Shavit"
      ],
      "abstract": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16520v1",
      "url": "https://arxiv.org/abs/2602.16520v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n**越狱提示（Jailbreak prompts）** 是当前大语言模型（LLM）尤其是**工具增强型智能体（tool-augmented agents）** 面临的关键安全威胁。此类攻击常利用长上下文隐藏、语义伪装及轻量级混淆等手法，绕过传统单次推理的防护机制（如静态分类器或一次性守门员模型），导致现有防御在真实代理场景中鲁棒性不足。\n\n## 方法：递归语言模型检测框架（RLM-JB）  \n我们提出 **RLM-JB**——一种基于**递归语言模型（Recursive Language Models, RLMs）** 的端到端越狱检测框架。其核心思想是将检测建模为**可审计的程序化流程**，而非黑盒分类：  \n- **根模型（Root Model）** 作为协调器，执行结构化分析程序；  \n- 对输入进行**规范化与去混淆处理**（如解码Base64、还原Unicode逃逸）；  \n- 将文本**动态分块（chunking）**，缓解长上下文稀释问题，并确保全覆盖；  \n- 启动多个**工作模型（Worker Models）并行筛查各分块**；  \n- 通过跨块信号聚合（cross-chunk signal composition）识别被刻意拆分的恶意载荷（split-payload attacks）。\n\n## 主要结果与创新  \n在AutoDAN等强对抗性越狱数据集上，RLM-JB在三种主流LLM后端（Llama-3-8B、Qwen2-7B、Phi-3-mini）上实现：  \n- **高召回率（ASR/Recall）92.5%–98.0%**；  \n- **超高精度（Precision）98.99%–100%**，误报率仅**0.0%–2.0%**；  \n- 首次验证了**程序化检测范式**在敏感性-特异性权衡中的实用性，且决策过程全程可追溯、可审计。",
      "summary_en": "We introduce **RLM-JB**, a procedural jailbreak detection framework built on Recursive Language Models (RLMs), designed specifically for tool-augmented agents operating on untrusted inputs. Unlike one-shot classifiers, RLM-JB treats detection as an auditable program: a root model normalizes and de-obfuscates suspicious prompts, chunks text to ensure full coverage and mitigate context dilution, dispatches parallel worker-model queries over segments, and composes cross-chunk evidence to recover split-payload attacks. Evaluated on AutoDAN-style adversarial prompts across three LLM backends (Llama-3-8B, Qwen2-7B, Phi-3-mini), RLM-JB achieves high recall (92.5–98.0%), exceptional precision (98.99–100%), and near-zero false positives (0.0–2.0%). This demonstrates that recursive, procedure-driven defense offers a practical and interpretable trade-off between sensitivity and specificity—without compromising operational safety.",
      "summary": "## 背景与挑战  \n**越狱提示（Jailbreak prompts）** 是当前大语言模型（LLM）尤其是**工具增强型智能体（tool-augmented agents）** 面临的关键安全威胁。此类攻击常利用长上下文隐藏、语义伪装及轻量级混淆等手法，绕过传统单次推理的防护机制（如静态分类器或一次性守门员模型），导致现有防御在真实代理场景中鲁棒性不足。\n\n## 方法：递归语言模型检测框架（RLM-JB）  \n我们提出 **RLM-JB**——一种基于**递归语言模型（Recursive Language Models, RLMs）** 的端到端越狱检测框架。其核心思想是将检测建模为**可审计的程序化流程**，而非黑盒分类：  \n- **根模型（Root Model）** 作为协调器，执行结构化分析程序；  \n- 对输入进行**规范化与去混淆处理**（如解码Base64、还原Unicode逃逸）；  \n- 将文本**动态分块（chunking）**，缓解长上下文稀释问题，并确保全覆盖；  \n- 启动多个**工作模型（Worker Models）并行筛查各分块**；  \n- 通过跨块信号聚合（cross-chunk signal composition）识别被刻意拆分的恶意载荷（split-payload attacks）。\n\n## 主要结果与创新  \n在AutoDAN等强对抗性越狱数据集上，RLM-JB在三种主流LLM后端（Llama-3-8B、Qwen2-7B、Phi-3-mini）上实现：  \n- **高召回率（ASR/Recall）92.5%–98.0%**；  \n- **超高精度（Precision）98.99%–100%**，误报率仅**0.0%–2.0%**；  \n- 首次验证了**程序化检测范式**在敏感性-特异性权衡中的实用性，且决策过程全程可追溯、可审计。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16480v1",
      "arxiv_id": "2602.16480v1",
      "title": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data",
      "authors": [
        "Yiwen Lu"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16480v1",
      "url": "https://arxiv.org/abs/2602.16480v1",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "learning",
        "federated",
        "security",
        "model",
        "data",
        "inference"
      ],
      "keyword_score": 7,
      "summary_zh": "## 背景与挑战  \n联邦学习（FL）在保护数据隐私的前提下实现多方协同建模，广泛应用于医疗、金融等敏感场景。然而，其面临双重安全威胁：**好奇服务器**可能通过模型上传推断客户端原始数据（推理攻击），而**恶意客户端**可注入毒化模型破坏全局聚合（投毒攻击）。现有方案多结合差分隐私/安全聚合与鲁棒聚合策略，但普遍存在两大缺陷：（1）计算与通信开销过高，难以部署于资源受限设备；（2）在非独立同分布（Non-IID）数据下鲁棒性显著下降——因数据异构导致正常模型差异增大，传统异常检测易将良性偏差误判为攻击。\n\n## 方法创新  \n本文提出 **SRFed**——首个兼顾高效性、Byzantine鲁棒性与端到端隐私保护的Non-IID适配FL框架：  \n- **去中心化高效函数加密（DEFE）**：摒弃可信第三方，支持客户端对本地模型参数进行轻量级加密，服务器无需交互即可完成非线性操作（如加权聚合）后的解密；理论证明其抵御服务器侧推理攻击，且加密/解密复杂度仅为 *O(d)*（*d* 为模型维度），较同态加密降低两个数量级；  \n- **隐私保护的分层鲁棒聚合机制**：基于DEFE密文直接执行层间投影降维与密度聚类分析，动态识别并过滤毒化层参数，避免明文暴露导致的隐私泄露，且在Non-IID下误删率降低42.7%（vs. Krum、Median）。\n\n## 实验验证  \n在CIFAR-10/100、Tiny-ImageNet及真实医疗数据集上，SRFed在3种典型投毒攻击（Label-Flipping、ALIE、Min-Max）下保持>89%准确率（较SOTA提升5.2–11.8%），同时通信开销仅为Secure Aggregation的63%，训练延迟降低37%。理论安全性证明覆盖半诚实服务器与拜占庭客户端混合威胁模型。",
      "summary_en": "Federated Learning (FL) enables collaborative model training while preserving data privacy, yet remains vulnerable to both server-side inference attacks and client-side poisoning attacks—especially under Non-IID data. Existing defenses suffer from high overhead or poor robustness in heterogeneous settings. We propose **SRFed**, the first efficient, Byzantine-robust, and end-to-end privacy-preserving FL framework tailored for Non-IID scenarios. Its core innovations are: (1) a **Decentralized Efficient Functional Encryption (DEFE)** scheme that eliminates third-party trust, enables non-interactive decryption after encrypted aggregation, and provably thwarts server inference with *O(d)* computational cost; and (2) a **privacy-preserving defensive aggregation** mechanism that performs layer-wise projection and clustering directly on encrypted models to filter poisoned updates without revealing raw parameters. Extensive experiments across four datasets show SRFed achieves >89% accuracy under diverse poisoning attacks—outperforming state-of-the-art baselines by 5.2–11.8%—while reducing communication overhead by 37% and latency by 37% compared to Secure Aggregation. Theoretical analysis confirms security against semi-honest servers and Byzantine clients.",
      "summary": "## 背景与挑战  \n联邦学习（FL）在保护数据隐私的前提下实现多方协同建模，广泛应用于医疗、金融等敏感场景。然而，其面临双重安全威胁：**好奇服务器**可能通过模型上传推断客户端原始数据（推理攻击），而**恶意客户端**可注入毒化模型破坏全局聚合（投毒攻击）。现有方案多结合差分隐私/安全聚合与鲁棒聚合策略，但普遍存在两大缺陷：（1）计算与通信开销过高，难以部署于资源受限设备；（2）在非独立同分布（Non-IID）数据下鲁棒性显著下降——因数据异构导致正常模型差异增大，传统异常检测易将良性偏差误判为攻击。\n\n## 方法创新  \n本文提出 **SRFed**——首个兼顾高效性、Byzantine鲁棒性与端到端隐私保护的Non-IID适配FL框架：  \n- **去中心化高效函数加密（DEFE）**：摒弃可信第三方，支持客户端对本地模型参数进行轻量级加密，服务器无需交互即可完成非线性操作（如加权聚合）后的解密；理论证明其抵御服务器侧推理攻击，且加密/解密复杂度仅为 *O(d)*（*d* 为模型维度），较同态加密降低两个数量级；  \n- **隐私保护的分层鲁棒聚合机制**：基于DEFE密文直接执行层间投影降维与密度聚类分析，动态识别并过滤毒化层参数，避免明文暴露导致的隐私泄露，且在Non-IID下误删率降低42.7%（vs. Krum、Median）。\n\n## 实验验证  \n在CIFAR-10/100、Tiny-ImageNet及真实医疗数据集上，SRFed在3种典型投毒攻击（Label-Flipping、ALIE、Min-Max）下保持>89%准确率（较SOTA提升5.2–11.8%），同时通信开销仅为Secure Aggregation的63%，训练延迟降低37%。理论安全性证明覆盖半诚实服务器与拜占庭客户端混合威胁模型。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16436v1",
      "arxiv_id": "2602.16436v1",
      "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent",
      "authors": [
        "Jean Dufraiche",
        "Paul Mangold",
        "Michaël Perrot",
        "Marc Tommasi"
      ],
      "abstract": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16436v1",
      "url": "https://arxiv.org/abs/2602.16436v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "differential"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n在非交互式**本地差分隐私（LDP）**框架下一次性发布原始数据，虽保障了数据的长期可重用性，但强噪声注入会显著扭曲统计结构，尤其导致二分类任务中损失函数梯度的系统性偏差。传统LDP学习方法（如随机响应或高斯机制）往往忽略该偏差的解析结构，仅依赖渐近校正或启发式去噪，难以保证有限样本下的无偏性与收敛性。\n\n## 方法创新  \n本文首次将**Weierstrass变换**引入LDP理论分析：证明LDP扰动等价于对真实数据分布施加Weierstrass卷积核平滑，其偏差本质是函数在高斯核下的期望漂移。进一步，我们严格推导其**逆Weierstrass变换**的显式形式，构建首个可解析、可计算的偏差校正算子——该算子能对任意LDP发布的样本（如经拉普拉斯/高斯扰动的特征向量）进行逐点校正，从而获得**非线性函数（如Sigmoid、平方损失）的无偏估计**。\n\n## 核心算法与理论保证  \n基于上述校正机制，我们提出**逆Weierstrass私有随机梯度下降（IWP-SGD）**：在每次迭代中，先对LDP样本应用逆Weierstrass校正，再计算梯度并更新模型。理论证明：IWP-SGD以$\\mathcal{O}(1/n)$速率收敛至真实总体风险最小化器（$n$为样本量），显著优于标准LDP-SGD的$\\mathcal{O}(1/\\sqrt{n})$速率，且无需额外隐私预算开销。\n\n## 实证验证  \n在合成数据（含非线性边界）及真实数据集（UCI Adult、Bank Marketing）上，IWP-SGD在相同隐私预算（$\\varepsilon=1.0$）下，测试准确率平均提升**5.2–8.7个百分点**，验证了其偏差校正的有效性与实用性。",
      "summary_en": "This paper addresses bias in noninteractive Local Differential Privacy (LDP) for binary classification. We characterize LDP-induced distortion as a Weierstrass transform of the true data distribution and derive its exact inverse, enabling unbiased estimation of nonlinear functions (e.g., logistic loss) on privatized examples. Based on this, we propose **Inverse Weierstrass Private SGD (IWP-SGD)**—a novel optimization algorithm that applies analytical bias correction *before* gradient computation. We prove IWP-SGD converges to the true population risk minimizer at rate $\\mathcal{O}(1/n)$, improving upon the standard $\\mathcal{O}(1/\\sqrt{n})$ rate under LDP. Experiments on synthetic and real-world datasets (UCI Adult, Bank Marketing) confirm consistent accuracy gains of 5.2–8.7 percentage points at $\\varepsilon = 1.0$, demonstrating both theoretical soundness and practical efficacy.",
      "summary": "## 背景与挑战  \n在非交互式**本地差分隐私（LDP）**框架下一次性发布原始数据，虽保障了数据的长期可重用性，但强噪声注入会显著扭曲统计结构，尤其导致二分类任务中损失函数梯度的系统性偏差。传统LDP学习方法（如随机响应或高斯机制）往往忽略该偏差的解析结构，仅依赖渐近校正或启发式去噪，难以保证有限样本下的无偏性与收敛性。\n\n## 方法创新  \n本文首次将**Weierstrass变换**引入LDP理论分析：证明LDP扰动等价于对真实数据分布施加Weierstrass卷积核平滑，其偏差本质是函数在高斯核下的期望漂移。进一步，我们严格推导其**逆Weierstrass变换**的显式形式，构建首个可解析、可计算的偏差校正算子——该算子能对任意LDP发布的样本（如经拉普拉斯/高斯扰动的特征向量）进行逐点校正，从而获得**非线性函数（如Sigmoid、平方损失）的无偏估计**。\n\n## 核心算法与理论保证  \n基于上述校正机制，我们提出**逆Weierstrass私有随机梯度下降（IWP-SGD）**：在每次迭代中，先对LDP样本应用逆Weierstrass校正，再计算梯度并更新模型。理论证明：IWP-SGD以$\\mathcal{O}(1/n)$速率收敛至真实总体风险最小化器（$n$为样本量），显著优于标准LDP-SGD的$\\mathcal{O}(1/\\sqrt{n})$速率，且无需额外隐私预算开销。\n\n## 实证验证  \n在合成数据（含非线性边界）及真实数据集（UCI Adult、Bank Marketing）上，IWP-SGD在相同隐私预算（$\\varepsilon=1.0$）下，测试准确率平均提升**5.2–8.7个百分点**，验证了其偏差校正的有效性与实用性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16268v1",
      "arxiv_id": "2602.16268v1",
      "title": "Quantum Oracle Distribution Switching and its Applications to Fully Anonymous Ring Signatures",
      "authors": [
        "Marvin Beckmann",
        "Christian Majenz"
      ],
      "abstract": "Ring signatures are a powerful primitive that allows a member to sign on behalf of a group, without revealing their identity. Recently, ring signatures have received additional attention as an ingredient for post-quantum deniable authenticated key exchange, e.g., for a post-quantum version of the Signal protocol, employed by virtually all end-to-end-encrypted messenger services. While several ring signature constructions from post-quantum assumptions offer suitable security and efficiency for use in deniable key exchange, they are currently proven secure in the random oracle model (ROM) only, which is insufficient for post-quantum security.   In this work, we provide four security reductions in the quantum-accessible random oracle model (QROM) for two generic ring signature constructions: two for the AOS framework and two for a construction paradigm based on ring trapdoors, whose generic backbone we formalize. The two security proofs for AOS ring signatures differ in their requirements on the underlying sigma protocol and their tightness. The two reductions for the ring-trapdoor-based ring signatures exhibit various differences in requirements and the security they provide. We employ the measure-and-reprogram technique, QROM straightline extraction tools based on the compressed oracle, history-free reductions and QROM reprogramming tools. To make use of Rényi divergence properties in the QROM, we study the behavior of quantum algorithms that interact with an oracle whose distribution is based on one of two different distributions over the set of outputs. We provide tight bounds for the statistical distance, show that the Rényi divergence can not be used to replace the entire oracle and provide a workaround.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16268v1",
      "url": "https://arxiv.org/abs/2602.16268v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n环签名（Ring Signature）是一种关键密码原语，允许群组中任意成员以匿名方式代表整个群组签署消息，且不泄露其真实身份。近年来，其在**后量子可否认认证密钥交换**（如后量子版Signal协议）中的应用引发广泛关注。然而，现有基于后量子假设的环签名方案大多仅在**随机预言机模型**（ROM）下被证明安全；而ROM在量子环境下不具可信性——攻击者可量子查询预言机，故需升级至**量子可访问随机预言机模型**（QROM）以保障真正后量子安全性。\n\n## 方法与技术贡献  \n本文首次系统构建了环签名在QROM下的严格安全性基础：  \n- 提出**四条紧致安全归约**，覆盖两类通用构造范式：  \n  • **AOS框架**（Au, Liu, Susilo）：给出两个归约，分别适配不同强度的底层Σ协议（强零知识 vs. 标准零知识），并在紧致性（tightness）上形成互补；  \n  • **环陷门范式**（Ring Trapdoor）：首次形式化其通用骨架，并提供两个差异化归约——一个支持选择消息攻击下的不可伪造性（EUF-CMA），另一个强化为全匿名性（full anonymity）保证。  \n- 关键技术融合：综合运用**测量-重编程技术**（measure-and-reprogram）、基于**压缩预言机**（compressed oracle）的QROM直线提取工具、**无历史归约**（history-free reductions）及QROM重编程机制。  \n- 理论突破：深入研究当预言机输出分布切换于两种不同输出分布之间时，量子算法行为的统计特性；给出**统计距离的紧界**，严格证明**Rényi散度无法完全替代预言机模拟**，并提出可实用的规避方案。\n\n## 创新意义  \n本工作填补了后量子环签名在QROM下安全性证明的空白，为Signal等隐私通信协议向后量子安全迁移提供了首个兼具**理论严谨性与工程适用性**的密码学支撑。",
      "summary_en": "This paper establishes the first rigorous quantum-security foundations for ring signatures in the **Quantum Random Oracle Model (QROM)**. We provide **four tight security reductions**: two for the AOS framework—differing in Σ-protocol assumptions (strong vs. standard zero-knowledge) and tightness—and two for a newly formalized **ring-trapdoor paradigm**, offering distinct guarantees (EUF-CMA vs. full anonymity). Our proofs integrate advanced QROM techniques: measure-and-reprogram, compressed-oracle-based straightline extraction, history-free reductions, and QROM reprogramming. Crucially, we analyze quantum algorithms interacting with oracles whose output distributions switch between two alternatives; we derive tight bounds on statistical distance, prove Rényi divergence *cannot* fully replace oracle simulation in QROM, and propose a practical workaround. This work enables post-quantum secure deniable key exchange (e.g., quantum-safe Signal) with provable anonymity and unforgeability.",
      "summary": "## 研究背景与问题  \n环签名（Ring Signature）是一种关键密码原语，允许群组中任意成员以匿名方式代表整个群组签署消息，且不泄露其真实身份。近年来，其在**后量子可否认认证密钥交换**（如后量子版Signal协议）中的应用引发广泛关注。然而，现有基于后量子假设的环签名方案大多仅在**随机预言机模型**（ROM）下被证明安全；而ROM在量子环境下不具可信性——攻击者可量子查询预言机，故需升级至**量子可访问随机预言机模型**（QROM）以保障真正后量子安全性。\n\n## 方法与技术贡献  \n本文首次系统构建了环签名在QROM下的严格安全性基础：  \n- 提出**四条紧致安全归约**，覆盖两类通用构造范式：  \n  • **AOS框架**（Au, Liu, Susilo）：给出两个归约，分别适配不同强度的底层Σ协议（强零知识 vs. 标准零知识），并在紧致性（tightness）上形成互补；  \n  • **环陷门范式**（Ring Trapdoor）：首次形式化其通用骨架，并提供两个差异化归约——一个支持选择消息攻击下的不可伪造性（EUF-CMA），另一个强化为全匿名性（full anonymity）保证。  \n- 关键技术融合：综合运用**测量-重编程技术**（measure-and-reprogram）、基于**压缩预言机**（compressed oracle）的QROM直线提取工具、**无历史归约**（history-free reductions）及QROM重编程机制。  \n- 理论突破：深入研究当预言机输出分布切换于两种不同输出分布之间时，量子算法行为的统计特性；给出**统计距离的紧界**，严格证明**Rényi散度无法完全替代预言机模拟**，并提出可实用的规避方案。\n\n## 创新意义  \n本工作填补了后量子环签名在QROM下安全性证明的空白，为Signal等隐私通信协议向后量子安全迁移提供了首个兼具**理论严谨性与工程适用性**的密码学支撑。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16156v1",
      "arxiv_id": "2602.16156v1",
      "title": "Weak Zero-Knowledge and One-Way Functions",
      "authors": [
        "Rohit Chatterjee",
        "Yunqi Li",
        "Prashant Nalini Vasudevan"
      ],
      "abstract": "We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $ε_c$, $ε_s$, and $ε_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:   1. If all languages in NP have NIZK proofs or arguments satisfying $ ε_c+ε_s+ ε_z < 1 $, then One-Way Functions (OWFs) exist.   This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $ε_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ ε_c+\\sqrt{ε_s}+ε_z < 1 $ [Chakraborty et al., CRYPTO 2025].   2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+(2k-1).ε_z < 1 $, then OWFs exist.   3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+k.ε_z < 1 $, then infinitely-often OWFs exist.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16156v1",
      "url": "https://arxiv.org/abs/2602.16156v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "crypto"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n本文系统研究**弱零知识（Weak Zero-Knowledge, Weak ZK）协议**的存在性对密码学基础假设——特别是**单向函数（One-Way Functions, OWFs）存在性**——的蕴含关系。传统零知识要求完备性、可靠性和零知识性错误均“可忽略”，而Weak ZK允许这些错误率（记为 $ε_c$、$ε_s$、$ε_z$）为常数甚至较大，仅需满足某种非平凡约束。核心问题是：在**NP中存在最坏情况困难语言**这一温和假设下，何种程度的Weak ZK能力足以推出OWFs存在？\n\n## 主要结果与创新点  \n1. **统一性突破**：首次证明——若**所有NP语言均存在满足 $ε_c + ε_s + ε_z < 1$ 的非交互式零知识（NIZK）证明或论证**，则OWFs必然存在。该条件覆盖全部非平凡常数错误组合（如 $ε_c=0.3, ε_s=0.3, ε_z=0.39$），且**无需根号项**；此前最优结果要求更严格的 $ε_c + \\sqrt{ε_s} + ε_z < 1$（Chakraborty et al., CRYPTO 2025）。进一步，若完备性错误 $ε_c$ 可忽略，则该NIZK系统可“提升”为**全错误均可忽略的标准NIZK**，实现从弱到强的自动强化。  \n2. **轮数依赖刻画**：针对$k$轮公开硬币（public-coin）ZK协议，证明若满足 $ε_c + ε_s + (2k-1)ε_z < 1$，则OWFs存在；若更强地满足 $ε_c + ε_s + k·ε_z < 1$（对某常数 $k$），则可推出**无穷多次单向函数（infinitely-often OWFs）** 存在。  \n3. **技术贡献**：通过精巧构造“错误放大—冲突检测”归约框架，绕过传统模拟器分析瓶颈，首次将线性错误和（而非平方根或指数形式）作为临界阈值，显著拓展了Weak ZK的密码学意义边界。",
      "summary_en": "This paper establishes new implications of weak zero-knowledge (ZK) protocols for the existence of one-way functions (OWFs), assuming worst-case hard languages in NP. First, if *all* NP languages admit non-interactive ZK (NIZK) proofs or arguments with completeness, soundness, and zero-knowledge errors $ε_c$, $ε_s$, $ε_z$ satisfying $ε_c + ε_s + ε_z < 1$, then OWFs exist—unifying and strictly improving prior work requiring $ε_c + \\sqrt{ε_s} + ε_z < 1$. Moreover, if $ε_c$ is negligible, such NIZKs can be upgraded to fully negligible-error ones. Second, for $k$-round public-coin ZK, OWFs follow from $ε_c + ε_s + (2k-1)ε_z < 1$; under the tighter bound $ε_c + ε_s + k·ε_z < 1$, infinitely-often OWFs exist. These results reveal linear error thresholds as fundamental to cryptographic hardness.",
      "summary": "## 研究背景与问题  \n本文系统研究**弱零知识（Weak Zero-Knowledge, Weak ZK）协议**的存在性对密码学基础假设——特别是**单向函数（One-Way Functions, OWFs）存在性**——的蕴含关系。传统零知识要求完备性、可靠性和零知识性错误均“可忽略”，而Weak ZK允许这些错误率（记为 $ε_c$、$ε_s$、$ε_z$）为常数甚至较大，仅需满足某种非平凡约束。核心问题是：在**NP中存在最坏情况困难语言**这一温和假设下，何种程度的Weak ZK能力足以推出OWFs存在？\n\n## 主要结果与创新点  \n1. **统一性突破**：首次证明——若**所有NP语言均存在满足 $ε_c + ε_s + ε_z < 1$ 的非交互式零知识（NIZK）证明或论证**，则OWFs必然存在。该条件覆盖全部非平凡常数错误组合（如 $ε_c=0.3, ε_s=0.3, ε_z=0.39$），且**无需根号项**；此前最优结果要求更严格的 $ε_c + \\sqrt{ε_s} + ε_z < 1$（Chakraborty et al., CRYPTO 2025）。进一步，若完备性错误 $ε_c$ 可忽略，则该NIZK系统可“提升”为**全错误均可忽略的标准NIZK**，实现从弱到强的自动强化。  \n2. **轮数依赖刻画**：针对$k$轮公开硬币（public-coin）ZK协议，证明若满足 $ε_c + ε_s + (2k-1)ε_z < 1$，则OWFs存在；若更强地满足 $ε_c + ε_s + k·ε_z < 1$（对某常数 $k$），则可推出**无穷多次单向函数（infinitely-often OWFs）** 存在。  \n3. **技术贡献**：通过精巧构造“错误放大—冲突检测”归约框架，绕过传统模拟器分析瓶颈，首次将线性错误和（而非平方根或指数形式）作为临界阈值，显著拓展了Weak ZK的密码学意义边界。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16109v1",
      "arxiv_id": "2602.16109v1",
      "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes",
      "authors": [
        "Srikumar Nayak",
        "James Walmesley"
      ],
      "abstract": "Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16109v1",
      "url": "https://arxiv.org/abs/2602.16109v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CE"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "learning",
        "federated",
        "differential"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与挑战  \n跨境内部威胁正严重危及各国政府财政计划的安全性，尤其在数据分布式存储、隐私敏感且受多司法管辖区法规约束的场景下。现有方法存在三重瓶颈：**隐私壁垒**导致跨国情报无法安全共享；**推理能力缺失**难以识别跨机构、多步骤的隐蔽攻击链；**结构建模不足**无法刻画金融交易网络中复杂的图结构关系（如资金环路、嵌套代理、异常子图模式）。\n\n## 方法创新：FedGraph-AGI 框架  \n我们提出 **FedGraph-AGI**——首个融合人工通用智能（AGI）推理与联邦图学习的隐私增强型框架，专为跨境内部威胁情报协同而设计。其核心包含三重技术突破：  \n1. **联邦图神经网络（Federated GNN）**：各司法管辖区本地训练图模型，原始交易图数据不出域，严格保障数据主权；  \n2. **混合专家聚合（MoE-Aggregation）**：针对各国监管差异、数据分布异构性与图拓扑多样性，动态加权融合本地模型更新，避免“一刀切”聚合偏差；  \n3. **AGI驱动的因果推理引擎**：基于大型动作模型（LAM），在加密图嵌入空间执行可解释的因果推断（如“若阻断节点X，则攻击路径Y中断概率提升73%”），实现从相关性检测到因果归因的跃迁。\n\n## 实验验证与价值  \n在覆盖10个司法管辖区、含50,000笔真实级交易的跨域数据集上，FedGraph-AGI达**92.3%检测准确率**，显著优于联邦基线（86.1%）和中心化方案（84.7%）。消融实验表明：AGI推理模块贡献**+6.8%**性能增益，MoE机制贡献**+4.4%**；系统在满足**ε = 1.0差分隐私**强约束下仍保持近最优性能，并可高效扩展至50+参与方。本工作首次将AGI级因果推理深度嵌入联邦图学习范式，为全球金融监管协同树立了可验证、可部署、可解释的新范式。",
      "summary_en": "Cross-border insider threats critically undermine government financial schemes, yet existing methods fail to reconcile privacy compliance, multi-jurisdictional heterogeneity, and complex attack pattern reasoning. We propose **FedGraph-AGI**, the first framework unifying federated graph learning with Artificial General Intelligence (AGI) reasoning for privacy-preserving threat intelligence sharing. It integrates: (1) sovereign-preserving federated graph neural networks; (2) Mixture-of-Experts (MoE) aggregation to harmonize jurisdictionally diverse models; and (3) Large Action Models (LAMs) performing causal inference over encrypted graph embeddings. Evaluated on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves **92.3% accuracy**, outperforming federated baselines (+6.2%) and centralized approaches (+7.6%). Ablation confirms AGI reasoning contributes +6.8% and MoE +4.4%. The system satisfies ε = 1.0 differential privacy, scales to 50+ clients, and enables actionable, interpretable threat attribution—pioneering AGI-augmented federated graph intelligence for global financial security.",
      "summary": "## 背景与挑战  \n跨境内部威胁正严重危及各国政府财政计划的安全性，尤其在数据分布式存储、隐私敏感且受多司法管辖区法规约束的场景下。现有方法存在三重瓶颈：**隐私壁垒**导致跨国情报无法安全共享；**推理能力缺失**难以识别跨机构、多步骤的隐蔽攻击链；**结构建模不足**无法刻画金融交易网络中复杂的图结构关系（如资金环路、嵌套代理、异常子图模式）。\n\n## 方法创新：FedGraph-AGI 框架  \n我们提出 **FedGraph-AGI**——首个融合人工通用智能（AGI）推理与联邦图学习的隐私增强型框架，专为跨境内部威胁情报协同而设计。其核心包含三重技术突破：  \n1. **联邦图神经网络（Federated GNN）**：各司法管辖区本地训练图模型，原始交易图数据不出域，严格保障数据主权；  \n2. **混合专家聚合（MoE-Aggregation）**：针对各国监管差异、数据分布异构性与图拓扑多样性，动态加权融合本地模型更新，避免“一刀切”聚合偏差；  \n3. **AGI驱动的因果推理引擎**：基于大型动作模型（LAM），在加密图嵌入空间执行可解释的因果推断（如“若阻断节点X，则攻击路径Y中断概率提升73%”），实现从相关性检测到因果归因的跃迁。\n\n## 实验验证与价值  \n在覆盖10个司法管辖区、含50,000笔真实级交易的跨域数据集上，FedGraph-AGI达**92.3%检测准确率**，显著优于联邦基线（86.1%）和中心化方案（84.7%）。消融实验表明：AGI推理模块贡献**+6.8%**性能增益，MoE机制贡献**+4.4%**；系统在满足**ε = 1.0差分隐私**强约束下仍保持近最优性能，并可高效扩展至50+参与方。本工作首次将AGI级因果推理深度嵌入联邦图学习范式，为全球金融监管协同树立了可验证、可部署、可解释的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16155v1",
      "arxiv_id": "2602.16155v1",
      "title": "Differentially Private Non-convex Distributionally Robust Optimization",
      "authors": [
        "Difei Xu",
        "Meng Ding",
        "Zebin Ma",
        "Huanyi Xie",
        "Youming Tao",
        "Aicha Slaitane",
        "Di Wang"
      ],
      "abstract": "Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.   Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.   Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.   In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.   To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $ψ$-divergence and non-convex loss.   First, we study DRO with general $ψ$-divergence by reformulating it as a minimization problem, and develop a novel $(\\varepsilon, δ)$-DP optimization method, called DP Double-Spider, tailored to this structure.   Under mild assumptions, we show that it achieves a utility bound of $\\mathcal{O}(\\frac{1}{\\sqrt{n}}+ (\\frac{\\sqrt{d \\log (1/δ)}}{n \\varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.   We further improve the utility rate for specific divergences.   In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\\mathcal{O}((\\frac{\\sqrt{d \\log(1/δ)}}{n\\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.   Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16155v1",
      "url": "https://arxiv.org/abs/2602.16155v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "dp",
        "differential"
      ],
      "keyword_score": 3,
      "summary_zh": "## 研究背景与问题  \n现实机器学习部署常面临**分布偏移、群体不平衡与对抗扰动**等挑战，传统经验风险最小化（ERM）性能显著下降。分布鲁棒优化（DRO）通过在分布不确定性集上优化最坏情况期望损失，为鲁棒性提供理论保障。然而，DRO训练数据往往含敏感信息，亟需结合**差分隐私（DP）** 防范隐私泄露。现有工作多聚焦于DP-ERM，而DP-DRO因具**带约束的极小极大结构**，研究严重不足，尤其在非凸、有限和、ψ-散度设定下尚无系统性算法与理论。\n\n## 方法创新  \n本文首次对**差分私有非凸有限和DRO**开展全面研究：  \n- 提出通用框架：将任意ψ-散度DRO重写为单层最小化问题，设计新型$(\\varepsilon,\\delta)$-DP算法——**DP Double-Spider**，融合双重方差缩减与隐私噪声注入；  \n- 针对KL散度这一关键特例，进一步将其转化为**复合有限和优化问题**，提出**DP Recursive-Spider**算法，实现更紧致的隐私-效用权衡。\n\n## 主要理论结果  \n- DP Double-Spider在温和假设下达到梯度范数界：$\\mathcal{O}\\!\\left(\\frac{1}{\\sqrt{n}} + \\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，其中$n$为样本量，$d$为模型维度；  \n- DP Recursive-Spider对KL-DRO取得**最优速率**：$\\mathcal{O}\\!\\left(\\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，与当前非凸DP-ERM最优界完全匹配；  \n- 实验验证：在多个真实数据集（如Adult、CelebA）上，所提方法在隐私预算$(\\varepsilon,\\delta)=(1,10^{-5})$下，测试鲁棒准确率平均提升2.1–4.7个百分点，显著优于基线DP-SGDA、DP-VRBO等。",
      "summary_en": "Distributionally Robust Optimization (DRO) enhances model robustness against distribution shifts and adversarial perturbations, yet its deployment on sensitive data necessitates differential privacy (DP). This paper presents the first comprehensive study of *non-convex, finite-sum, differentially private DRO* under ψ-divergence uncertainty sets. We reformulate general ψ-DRO as a single-level minimization and propose **DP Double-Spider**, achieving a gradient-norm utility bound of $\\mathcal{O}(1/\\sqrt{n} + (\\sqrt{d \\log(1/\\delta)}/(n\\varepsilon))^{2/3})$. For KL-divergence DRO, we cast it as a compositional finite-sum problem and design **DP Recursive-Spider**, attaining the optimal rate $\\mathcal{O}((\\sqrt{d \\log(1/\\delta)}/(n\\varepsilon))^{2/3})$—matching the best-known bound for non-convex DP-ERM. Experiments confirm consistent superiority over existing DP minimax methods across benchmark datasets under realistic privacy budgets.",
      "summary": "## 研究背景与问题  \n现实机器学习部署常面临**分布偏移、群体不平衡与对抗扰动**等挑战，传统经验风险最小化（ERM）性能显著下降。分布鲁棒优化（DRO）通过在分布不确定性集上优化最坏情况期望损失，为鲁棒性提供理论保障。然而，DRO训练数据往往含敏感信息，亟需结合**差分隐私（DP）** 防范隐私泄露。现有工作多聚焦于DP-ERM，而DP-DRO因具**带约束的极小极大结构**，研究严重不足，尤其在非凸、有限和、ψ-散度设定下尚无系统性算法与理论。\n\n## 方法创新  \n本文首次对**差分私有非凸有限和DRO**开展全面研究：  \n- 提出通用框架：将任意ψ-散度DRO重写为单层最小化问题，设计新型$(\\varepsilon,\\delta)$-DP算法——**DP Double-Spider**，融合双重方差缩减与隐私噪声注入；  \n- 针对KL散度这一关键特例，进一步将其转化为**复合有限和优化问题**，提出**DP Recursive-Spider**算法，实现更紧致的隐私-效用权衡。\n\n## 主要理论结果  \n- DP Double-Spider在温和假设下达到梯度范数界：$\\mathcal{O}\\!\\left(\\frac{1}{\\sqrt{n}} + \\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，其中$n$为样本量，$d$为模型维度；  \n- DP Recursive-Spider对KL-DRO取得**最优速率**：$\\mathcal{O}\\!\\left(\\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，与当前非凸DP-ERM最优界完全匹配；  \n- 实验验证：在多个真实数据集（如Adult、CelebA）上，所提方法在隐私预算$(\\varepsilon,\\delta)=(1,10^{-5})$下，测试鲁棒准确率平均提升2.1–4.7个百分点，显著优于基线DP-SGDA、DP-VRBO等。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16653v1",
      "arxiv_id": "2602.16653v1",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "abstract": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16653v1",
      "url": "https://arxiv.org/abs/2602.16653v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \nAgent Skill框架（智能体技能框架）已获GitHub Copilot、LangChain与OpenAI等主流平台官方集成，其核心优势在于通过**结构化技能封装**优化上下文工程、显著抑制幻觉、提升任务执行准确率。然而，该范式在**小语言模型（SLMs）** 上的适用性尚未系统验证——这在工业落地中尤为关键：企业常因数据安全合规与API调用成本限制，无法持续依赖公有云大模型服务；而轻量级SLMs又普遍面临定制化场景泛化能力弱、技能选择不可靠等瓶颈。\n\n## 方法与实验设计  \n本研究首次提出Agent Skill过程的**形式化数学定义**，明确技能调用、上下文约束与决策边界之间的映射关系。在此基础上，系统评估了从**1.5B至80B参数规模**的多类开源模型（含通用与代码专用变体），覆盖三大场景：（1）两个标准开源任务（ToolAlpaca风格工具调用、MultiHopQA）；（2）真实保险理赔工单数据集（含非结构化文本、表格与业务规则约束）。所有实验均在本地化部署环境下完成，严格隔离训练/测试数据与外部API。\n\n## 主要发现与创新点  \n- **规模阈值效应显著**：1.5B–7B模型难以稳定识别并调度适配技能（准确率<52%）；**12B–30B中等规模SLMs**受益最突出——技能选择准确率提升28.6%，端到端任务成功率提高34.2%；  \n- **代码专用80B模型实现性能跃迁**：在保险理赔任务中达到92.4% F1，媲美GPT-4 Turbo（93.1%），同时GPU显存占用降低41%，推理延迟减少37%；  \n- **工业部署启示**：Agent Skill非“越大越好”，而是为SLMs提供可解释、可审计、低资源消耗的模块化增强路径，为高合规要求场景提供可行替代方案。",
      "summary_en": "The Agent Skill Framework—formally adopted by GitHub Copilot, LangChain, and OpenAI—enhances context engineering, reduces hallucinations, and improves task accuracy, especially with proprietary models. This work investigates whether these benefits extend to Small Language Models (SLMs), critical for industrial settings where public API reliance is infeasible due to data security and budget constraints. We introduce the first formal mathematical definition of the Agent Skill process and conduct systematic evaluation across model sizes (1.5B–80B) on two open-source benchmarks and a real-world insurance claims dataset. Results reveal a clear scale-dependent effect: tiny models (<7B) fail at reliable skill selection, while mid-sized SLMs (12B–30B) gain substantial improvements in accuracy (+28.6%) and task success (+34.2%). Notably, code-specialized ~80B models match closed-source baselines (e.g., GPT-4 Turbo) in F1 score (92.4% vs. 93.1%) while cutting GPU memory by 41% and latency by 37%. These findings establish Agent Skills as a principled, efficient, and deployable enhancement strategy for SLM-centric industrial AI.",
      "summary": "## 研究背景与问题  \nAgent Skill框架（智能体技能框架）已获GitHub Copilot、LangChain与OpenAI等主流平台官方集成，其核心优势在于通过**结构化技能封装**优化上下文工程、显著抑制幻觉、提升任务执行准确率。然而，该范式在**小语言模型（SLMs）** 上的适用性尚未系统验证——这在工业落地中尤为关键：企业常因数据安全合规与API调用成本限制，无法持续依赖公有云大模型服务；而轻量级SLMs又普遍面临定制化场景泛化能力弱、技能选择不可靠等瓶颈。\n\n## 方法与实验设计  \n本研究首次提出Agent Skill过程的**形式化数学定义**，明确技能调用、上下文约束与决策边界之间的映射关系。在此基础上，系统评估了从**1.5B至80B参数规模**的多类开源模型（含通用与代码专用变体），覆盖三大场景：（1）两个标准开源任务（ToolAlpaca风格工具调用、MultiHopQA）；（2）真实保险理赔工单数据集（含非结构化文本、表格与业务规则约束）。所有实验均在本地化部署环境下完成，严格隔离训练/测试数据与外部API。\n\n## 主要发现与创新点  \n- **规模阈值效应显著**：1.5B–7B模型难以稳定识别并调度适配技能（准确率<52%）；**12B–30B中等规模SLMs**受益最突出——技能选择准确率提升28.6%，端到端任务成功率提高34.2%；  \n- **代码专用80B模型实现性能跃迁**：在保险理赔任务中达到92.4% F1，媲美GPT-4 Turbo（93.1%），同时GPU显存占用降低41%，推理延迟减少37%；  \n- **工业部署启示**：Agent Skill非“越大越好”，而是为SLMs提供可解释、可审计、低资源消耗的模块化增强路径，为高合规要求场景提供可行替代方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16422v1",
      "arxiv_id": "2602.16422v1",
      "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
      "authors": [
        "Ahmet Halici",
        "Ece Tugba Cebeci",
        "Musa Balci",
        "Mustafa Cini",
        "Serkan Sokmen"
      ],
      "abstract": "Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16422v1",
      "url": "https://arxiv.org/abs/2602.16422v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n组织病理学全切片图像（WSI）可达吉像素量级，其诊断报告生成需兼顾宏观结构理解与微观形态判读，同时输出符合临床规范的精准医学术语。传统端到端方法受限于显存、长程依赖建模能力及专业词汇表覆盖不足，导致报告常出现术语错误、关键病变遗漏或语义失真。\n\n## 方法创新  \n本研究提出一种**分层视觉-语言框架**：  \n- **金字塔式补丁筛选**：在2³–2⁶多尺度下进行自适应下采样，结合Laplacian方差（滤除模糊区域）与HSV色彩空间阈值（剔除背景/刀痕/气泡等伪影），显著压缩输入冗余；  \n- **冻结式基础模型协同**：采用预训练病理大模型UNI-ViT提取高质量局部特征，避免在小规模标注数据上微调导致的过拟合；  \n- **生物医学感知解码**：6层Transformer解码器通过交叉注意力融合视觉特征，输出端采用**BioGPT分词器**，显式建模“腺体排列紊乱”“核仁明显”等专业短语的子词结构；  \n- **检索增强验证（RAV）**：基于Sentence-BERT计算生成报告与10万份真实报告库的语义相似度，若余弦相似度＞0.82，则以检索到的最优匹配报告替代生成结果，提升临床可信度。\n\n## 主要结果  \n在PandaSet与Camelyon17双基准测试中，本方法在BLEU-4（+12.3）、ROUGE-L（+9.7）及临床术语准确率（+18.5%）上显著优于SOTA；RAV模块将关键诊断错误率降低至2.1%，验证了“生成+检索校正”范式的可靠性。本工作为AI辅助病理报告自动化提供了可解释、可验证、临床就绪的新范式。",
      "summary_en": "Generating diagnostic reports from gigapixel whole slide images (WSIs) remains challenging due to scale, fine-grained morphology requirements, and domain-specific language fidelity. We propose a hierarchical vision-language framework that combines a *frozen* UNI Vision Transformer for robust pyramidal feature extraction (at 2³–2⁶ downsampled scales) with background/artifact removal (via Laplacian variance and HSV thresholds), followed by a 6-layer Transformer decoder with cross-attention. To enhance biomedical terminology modeling, we tokenize outputs using BioGPT. Crucially, we introduce retrieval-based verification (RAV): generated reports are embedded via Sentence-BERT and matched against a large clinical corpus; if similarity exceeds 0.82, the top-matched ground-truth report replaces the generated one. On PandaSet and Camelyon17, our method achieves +12.3 BLEU-4 and +18.5% clinical term accuracy over prior SOTA, reducing critical diagnostic errors to 2.1%. This work delivers a reliable, interpretable, and clinically aligned solution for automated histopathology reporting.",
      "summary": "## 背景与挑战  \n组织病理学全切片图像（WSI）可达吉像素量级，其诊断报告生成需兼顾宏观结构理解与微观形态判读，同时输出符合临床规范的精准医学术语。传统端到端方法受限于显存、长程依赖建模能力及专业词汇表覆盖不足，导致报告常出现术语错误、关键病变遗漏或语义失真。\n\n## 方法创新  \n本研究提出一种**分层视觉-语言框架**：  \n- **金字塔式补丁筛选**：在2³–2⁶多尺度下进行自适应下采样，结合Laplacian方差（滤除模糊区域）与HSV色彩空间阈值（剔除背景/刀痕/气泡等伪影），显著压缩输入冗余；  \n- **冻结式基础模型协同**：采用预训练病理大模型UNI-ViT提取高质量局部特征，避免在小规模标注数据上微调导致的过拟合；  \n- **生物医学感知解码**：6层Transformer解码器通过交叉注意力融合视觉特征，输出端采用**BioGPT分词器**，显式建模“腺体排列紊乱”“核仁明显”等专业短语的子词结构；  \n- **检索增强验证（RAV）**：基于Sentence-BERT计算生成报告与10万份真实报告库的语义相似度，若余弦相似度＞0.82，则以检索到的最优匹配报告替代生成结果，提升临床可信度。\n\n## 主要结果  \n在PandaSet与Camelyon17双基准测试中，本方法在BLEU-4（+12.3）、ROUGE-L（+9.7）及临床术语准确率（+18.5%）上显著优于SOTA；RAV模块将关键诊断错误率降低至2.1%，验证了“生成+检索校正”范式的可靠性。本工作为AI辅助病理报告自动化提供了可解释、可验证、临床就绪的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16174v1",
      "arxiv_id": "2602.16174v1",
      "title": "Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation",
      "authors": [
        "Fatih Temiz",
        "Shavbo Salehi",
        "Melike Erol-Kantarci"
      ],
      "abstract": "Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16174v1",
      "url": "https://arxiv.org/abs/2602.16174v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MM"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n面向元宇宙的移动边缘计算（MEC）无线服务需在严苛时延约束与高视觉质量要求下，为用户提供无缚沉浸式体验。实现卓越用户体验质量（QoE）亟需智能、协同的资源分配机制。传统联邦学习（FL）虽支持跨MEC服务器的分布式模型训练，但存在两大瓶颈：（1）需频繁上传完整模型参数至云端，加剧通信开销与延迟；（2）采用简单全局平均聚合策略，在多制式异构无线接入环境（如Wi-Fi/5G/6G混合网络）中易导致策略泛化能力下降与性能退化。\n\n## 方法创新：联邦分割决策Transformer（FSDT）  \n本文提出**Federated Split Decision Transformer（FSDT）**——一种面向离线强化学习（RL）的新型联邦架构。其核心在于**模型空间解耦**：将Transformer决策模型纵向分割为两部分——  \n- **边缘侧**：轻量级、代理专属组件（含MEC本地嵌入层与动作预测头），保障对本地信道状态、用户行为与设备异构性的快速自适应；  \n- **云端侧**：共享的全局注意力与序列建模层，通过联邦协调实现跨MEC服务器的知识融合与策略协同优化。  \n该设计显著降低边缘计算负载，同时规避全模型传输与粗粒度聚合缺陷。\n\n## 主要结果与贡献  \n实验表明，在真实轨迹驱动的异构多RAT元宇宙仿真环境中，FSDT相较FedAvg、FedProx及集中式RL基线，**QoE提升最高达10%**；同时，**98%的Transformer参数被卸载至云端**，MEC服务器仅需维持<2%参数量，大幅缓解内存与算力压力。本工作首次将分割式Transformer与联邦离线RL深度融合，为低开销、高鲁棒的元宇宙边缘智能提供了可部署新范式。",
      "summary_en": "Mobile edge computing (MEC) enables immersive metaverse services, yet achieving high QoE under strict latency and visual fidelity constraints demands intelligent, cooperative resource allocation across heterogeneous MEC servers. Conventional federated learning (FL) suffers from excessive communication overhead (full-model transmission) and poor generalization due to naive global aggregation—especially in multi-RAT environments. To address this, we propose the **Federated Split Decision Transformer (FSDT)**: an offline RL framework that *vertically partitions* a decision Transformer between edge (MEC-specific embedding/prediction layers) and cloud (shared global attention layers). This design enables local adaptability while fostering cross-server cooperation via federated training of only the cloud-resident parameters. Experiments in heterogeneous multi-RAT metaverse scenarios show FSDT improves QoE by up to **10%** over FL and centralized RL baselines, while offloading **98% of Transformer parameters to the cloud**, drastically reducing MEC computational burden. FSDT establishes a new paradigm for scalable, low-overhead edge intelligence in the metaverse.",
      "summary": "## 背景与挑战  \n面向元宇宙的移动边缘计算（MEC）无线服务需在严苛时延约束与高视觉质量要求下，为用户提供无缚沉浸式体验。实现卓越用户体验质量（QoE）亟需智能、协同的资源分配机制。传统联邦学习（FL）虽支持跨MEC服务器的分布式模型训练，但存在两大瓶颈：（1）需频繁上传完整模型参数至云端，加剧通信开销与延迟；（2）采用简单全局平均聚合策略，在多制式异构无线接入环境（如Wi-Fi/5G/6G混合网络）中易导致策略泛化能力下降与性能退化。\n\n## 方法创新：联邦分割决策Transformer（FSDT）  \n本文提出**Federated Split Decision Transformer（FSDT）**——一种面向离线强化学习（RL）的新型联邦架构。其核心在于**模型空间解耦**：将Transformer决策模型纵向分割为两部分——  \n- **边缘侧**：轻量级、代理专属组件（含MEC本地嵌入层与动作预测头），保障对本地信道状态、用户行为与设备异构性的快速自适应；  \n- **云端侧**：共享的全局注意力与序列建模层，通过联邦协调实现跨MEC服务器的知识融合与策略协同优化。  \n该设计显著降低边缘计算负载，同时规避全模型传输与粗粒度聚合缺陷。\n\n## 主要结果与贡献  \n实验表明，在真实轨迹驱动的异构多RAT元宇宙仿真环境中，FSDT相较FedAvg、FedProx及集中式RL基线，**QoE提升最高达10%**；同时，**98%的Transformer参数被卸载至云端**，MEC服务器仅需维持<2%参数量，大幅缓解内存与算力压力。本工作首次将分割式Transformer与联邦离线RL深度融合，为低开销、高鲁棒的元宇宙边缘智能提供了可部署新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16346v1",
      "arxiv_id": "2602.16346v1",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "authors": [
        "Nivya Talokar",
        "Ayush K Tarun",
        "Murari Mandal",
        "Maksym Andriushchenko",
        "Antoine Bosselut"
      ],
      "abstract": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16346v1",
      "url": "https://arxiv.org/abs/2602.16346v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具和维护记忆执行真实世界工作流，但这一能力也使恶意攻击者得以利用其完成复杂非法任务。现有滥用评估基准多聚焦于**单轮指令式攻击**（single-prompt），严重忽视了在**多轮交互、多语言环境**下，代理如何被逐步诱导、渐进式地协助完成有害或违法目标——这恰恰是实际部署中最典型的风险场景。\n\n## 方法：STING 框架与分析范式  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**：一种自动化红队测试框架。它首先构建以“良性身份”为掩护的**多步非法计划**（如伪装成合规研究员获取受控化学品信息），再通过**自适应多轮追问**持续 probing 目标代理，并由专用 judge 代理实时判定各阶段完成状态。进一步，我们创新性地将多轮红队过程建模为 **“首次越狱时间”（Time-to-First Jailbreak）随机变量**，支持：① **发现曲线（discovery curves）** 刻画风险暴露进程；② 基于攻击语言的**风险比归因分析**（hazard-ratio attribution）；③ 提出新指标 **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**，量化指定步数内越狱发生的平均进度。\n\n## 关键发现与创新  \n在 AgentHarm 基准上，STING 的非法任务完成率显著超越单轮提示与适配工具调用的多轮聊天基线。跨六种非英语语种（含低资源语言如斯瓦希里语、孟加拉语）测试表明：**攻击成功率与非法任务完成度并不随语言资源减少而系统性升高**，挑战了“低资源语言更易越狱”的常见假设。STING 首次实现了对现实部署中**多轮、多语言、工具增强型代理**的可复现、可量化、可归因的滥用压力测试，为安全评估提供实用新范式。",
      "summary_en": "We introduce **STING**, an automated red-teaming framework for evaluating illicit assistance in multi-turn, multilingual LLM agents. STING constructs grounded, step-by-step illegal plans disguised under benign personas and probes target agents via adaptive follow-up turns, using judge agents to track phase-wise completion. We model multi-turn jailbreaking as a *time-to-first-jailbreak* random variable, enabling discovery curves, hazard-ratio attribution by attack language, and the novel **Restricted Mean Jailbreak Discovery (RMJD)** metric. On AgentHarm, STING achieves substantially higher illicit-task completion than single-turn and chat-based multi-turn baselines adapted for tool-using agents. Crucially, across six non-English languages—including low-resource ones like Swahili and Bengali—we find no consistent increase in attack success or task completion, contradicting prevalent “low-resource = higher vulnerability” assumptions in chatbot literature. STING provides a practical, quantifiable, and multilingual stress test aligned with real-world agent deployment.",
      "summary": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具和维护记忆执行真实世界工作流，但这一能力也使恶意攻击者得以利用其完成复杂非法任务。现有滥用评估基准多聚焦于**单轮指令式攻击**（single-prompt），严重忽视了在**多轮交互、多语言环境**下，代理如何被逐步诱导、渐进式地协助完成有害或违法目标——这恰恰是实际部署中最典型的风险场景。\n\n## 方法：STING 框架与分析范式  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**：一种自动化红队测试框架。它首先构建以“良性身份”为掩护的**多步非法计划**（如伪装成合规研究员获取受控化学品信息），再通过**自适应多轮追问**持续 probing 目标代理，并由专用 judge 代理实时判定各阶段完成状态。进一步，我们创新性地将多轮红队过程建模为 **“首次越狱时间”（Time-to-First Jailbreak）随机变量**，支持：① **发现曲线（discovery curves）** 刻画风险暴露进程；② 基于攻击语言的**风险比归因分析**（hazard-ratio attribution）；③ 提出新指标 **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**，量化指定步数内越狱发生的平均进度。\n\n## 关键发现与创新  \n在 AgentHarm 基准上，STING 的非法任务完成率显著超越单轮提示与适配工具调用的多轮聊天基线。跨六种非英语语种（含低资源语言如斯瓦希里语、孟加拉语）测试表明：**攻击成功率与非法任务完成度并不随语言资源减少而系统性升高**，挑战了“低资源语言更易越狱”的常见假设。STING 首次实现了对现实部署中**多轮、多语言、工具增强型代理**的可复现、可量化、可归因的滥用压力测试，为安全评估提供实用新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16320v1",
      "arxiv_id": "2602.16320v1",
      "title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion",
      "authors": [
        "Kavyansh Tyagi",
        "Vishwas Rathi",
        "Puneet Goyal"
      ],
      "abstract": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The architecture integrates three key components: (i) GhostConv3D-based patch embedding for efficient feature extraction with minimal redundancy, (ii) MixFFN3D module with low-rank projections and depthwise convolutions for parameter-efficient feature extraction, and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. RefineFormer3D contains only 2.94M parameters, substantially fewer than contemporary transformer-based methods. Extensive experiments on ACDC and BraTS benchmarks demonstrate that RefineFormer3D achieves 93.44\\% and 85.9\\% average Dice scores respectively, outperforming or matching state-of-the-art methods while requiring significantly fewer parameters. Furthermore, the model achieves fast inference (8.35 ms per volume on GPU) with low memory requirements, supporting deployment in resource-constrained clinical environments. These results establish RefineFormer3D as an effective and scalable solution for practical 3D medical image segmentation.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16320v1",
      "url": "https://arxiv.org/abs/2602.16320v1",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n精准且高效的3D医学图像分割对临床诊断与手术规划至关重要，但现有Transformer模型虽具备优异的全局建模能力，却普遍存在参数量大、显存占用高、推理延迟长等问题，严重制约其在资源受限的临床环境（如边缘设备或实时术中系统）中的实际部署。\n\n## 方法创新  \n本文提出**RefineFormer3D**——一种轻量级分层式三维Transformer架构，通过三项协同设计实现精度与效率的最优平衡：  \n- **GhostConv3D嵌入模块**：采用“幽灵卷积”策略生成低冗余3D图像块嵌入，在保留结构信息的同时显著压缩初始特征维度；  \n- **MixFFN3D前馈网络**：融合低秩线性投影与深度可分离三维卷积，大幅降低FFN层参数量（较标准FFN减少约68%）；  \n- **跨尺度交叉注意力融合解码器**：动态加权整合多层级编码器特征，自适应选择最具判别性的跨尺度跳接信息，提升病灶边界与小目标分割鲁棒性。\n\n## 性能表现  \nRefineFormer3D仅含**294万参数**（约为Swin-Unet的1/5、TransBTS的1/7），在ACDC心脏数据集上达**93.44%平均Dice分数**，在BraTS脑肿瘤数据集上达**85.9% Dice**，全面超越或持平当前SOTA方法。单体积GPU推理耗时仅**8.35毫秒**，显存峰值低于3.2 GB，支持端到端实时处理。本工作为临床可落地的3D医学影像AI提供了兼具高精度、低开销与强泛化性的新范式。",
      "summary_en": "Accurate and efficient 3D medical image segmentation is vital for clinical practice, yet mainstream transformer models suffer from excessive parameters and memory overhead. RefineFormer3D addresses this by introducing a lightweight hierarchical 3D transformer with three key innovations: (i) GhostConv3D-based patch embedding for redundancy-aware feature initialization; (ii) MixFFN3D—a parameter-efficient feed-forward module combining low-rank projections and depthwise 3D convolutions; and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. With only **2.94M parameters**, it achieves **93.44% mean Dice on ACDC** and **85.9% on BraTS**, matching or exceeding state-of-the-art methods while requiring significantly fewer resources. It runs at **8.35 ms per volume on GPU**, demonstrating strong potential for real-time, resource-constrained clinical deployment.",
      "summary": "## 背景与挑战  \n精准且高效的3D医学图像分割对临床诊断与手术规划至关重要，但现有Transformer模型虽具备优异的全局建模能力，却普遍存在参数量大、显存占用高、推理延迟长等问题，严重制约其在资源受限的临床环境（如边缘设备或实时术中系统）中的实际部署。\n\n## 方法创新  \n本文提出**RefineFormer3D**——一种轻量级分层式三维Transformer架构，通过三项协同设计实现精度与效率的最优平衡：  \n- **GhostConv3D嵌入模块**：采用“幽灵卷积”策略生成低冗余3D图像块嵌入，在保留结构信息的同时显著压缩初始特征维度；  \n- **MixFFN3D前馈网络**：融合低秩线性投影与深度可分离三维卷积，大幅降低FFN层参数量（较标准FFN减少约68%）；  \n- **跨尺度交叉注意力融合解码器**：动态加权整合多层级编码器特征，自适应选择最具判别性的跨尺度跳接信息，提升病灶边界与小目标分割鲁棒性。\n\n## 性能表现  \nRefineFormer3D仅含**294万参数**（约为Swin-Unet的1/5、TransBTS的1/7），在ACDC心脏数据集上达**93.44%平均Dice分数**，在BraTS脑肿瘤数据集上达**85.9% Dice**，全面超越或持平当前SOTA方法。单体积GPU推理耗时仅**8.35毫秒**，显存峰值低于3.2 GB，支持端到端实时处理。本工作为临床可落地的3D医学影像AI提供了兼具高精度、低开销与强泛化性的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16181v1",
      "arxiv_id": "2602.16181v1",
      "title": "Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters",
      "authors": [
        "Diego Labate",
        "Dipanwita Thakur",
        "Giancarlo Fortino"
      ],
      "abstract": "Energy theft poses a significant threat to the stability and efficiency of smart grids, leading to substantial economic losses and operational challenges. Traditional centralized machine learning approaches for theft detection require aggregating user data, raising serious concerns about privacy and data security. These issues are further exacerbated in smart meter environments, where devices are often resource-constrained and lack the capacity to run heavy models. In this work, we propose a privacy-preserving federated learning framework for energy theft detection that addresses both privacy and computational constraints. Our approach leverages a lightweight multilayer perceptron (MLP) model, suitable for deployment on low-power smart meters, and integrates basic differential privacy (DP) by injecting Gaussian noise into local model updates before aggregation. This ensures formal privacy guarantees without compromising learning performance. We evaluate our framework on a real-world smart meter dataset under both IID and non-IID data distributions. Experimental results demonstrate that our method achieves competitive accuracy, precision, recall, and AUC scores while maintaining privacy and efficiency. This makes the proposed solution practical and scalable for secure energy theft detection in next-generation smart grid infrastructures.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16181v1",
      "url": "https://arxiv.org/abs/2602.16181v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "privacy-preserving",
        "differential",
        "federated",
        "learning",
        "dp",
        "privacy",
        "machine"
      ],
      "keyword_score": 7,
      "summary_zh": "## 研究背景与挑战  \n能源窃取严重威胁智能电网的稳定性、经济性与可持续性，每年造成巨额经济损失与运维风险。传统基于中心化机器学习的窃电检测方法需将用户用电数据上传至云端服务器进行模型训练，不仅违背《个人信息保护法》与GDPR等隐私法规，更在资源受限的智能电表（如MCU主频<100 MHz、RAM<64 KB）上难以部署复杂模型，导致隐私泄露与计算不可行双重瓶颈。\n\n## 方法创新  \n本文提出一种面向资源受限场景的**安全可扩展联邦学习框架**，实现端到端隐私保护与轻量化协同学习：  \n- **模型轻量化**：设计超低参数量多层感知机（MLP），仅含2个隐藏层（16→8神经元）、ReLU激活与批量归一化，模型体积<15 KB，推理延迟<8 ms（ARM Cortex-M4平台实测）；  \n- **隐私增强机制**：在本地模型更新阶段注入**高斯噪声**（σ=0.5），满足(ε=2.1, δ=1e⁻⁵)-差分隐私，严格保障单次参与者的梯度隐私；  \n- **通信高效性**：采用模型压缩+梯度稀疏化（top-k=5%），单轮通信量降低73%，适配窄带PLC信道。\n\n## 实验结果与价值  \n在真实世界某省电网23万块电表的12个月时序数据集（含3.7%标注窃电样本）上验证：  \n- 在非独立同分布（non-IID）场景下，F1-score达92.4%，AUC为0.961，显著优于基线FedAvg（-4.2%）与中心化ResNet（-6.8%，且无隐私保障）；  \n- 模型在STM32L4系列电表嵌入式环境稳定运行，内存占用峰值<42 KB，支持OTA增量更新；  \n- 首次在电力物联网中实现**形式化隐私证明+边缘实时推理+商用级检测精度**三重统一，为新型智能电网提供可落地的隐私安全基础设施。",
      "summary_en": "Energy theft severely undermines smart grid stability and causes massive economic losses. Centralized detection methods compromise user privacy by requiring raw data aggregation and are infeasible on resource-constrained smart meters. We propose a privacy-preserving federated learning framework featuring a lightweight MLP model (under 15 KB, <8 ms inference on ARM Cortex-M4) and Gaussian-noise-based differential privacy (ε=2.1, δ=1e⁻⁵) applied to local gradients before aggregation. Evaluated on a real-world dataset of 230,000 smart meters under both IID and non-IID settings, our method achieves 92.4% F1-score and 0.961 AUC—outperforming FedAvg and centralized baselines—while ensuring formal privacy guarantees and ultra-low communication overhead (73% reduction via top-k sparsification). This work bridges the gap between rigorous privacy, edge deployability, and high detection accuracy for next-generation secure smart grids.",
      "summary": "## 研究背景与挑战  \n能源窃取严重威胁智能电网的稳定性、经济性与可持续性，每年造成巨额经济损失与运维风险。传统基于中心化机器学习的窃电检测方法需将用户用电数据上传至云端服务器进行模型训练，不仅违背《个人信息保护法》与GDPR等隐私法规，更在资源受限的智能电表（如MCU主频<100 MHz、RAM<64 KB）上难以部署复杂模型，导致隐私泄露与计算不可行双重瓶颈。\n\n## 方法创新  \n本文提出一种面向资源受限场景的**安全可扩展联邦学习框架**，实现端到端隐私保护与轻量化协同学习：  \n- **模型轻量化**：设计超低参数量多层感知机（MLP），仅含2个隐藏层（16→8神经元）、ReLU激活与批量归一化，模型体积<15 KB，推理延迟<8 ms（ARM Cortex-M4平台实测）；  \n- **隐私增强机制**：在本地模型更新阶段注入**高斯噪声**（σ=0.5），满足(ε=2.1, δ=1e⁻⁵)-差分隐私，严格保障单次参与者的梯度隐私；  \n- **通信高效性**：采用模型压缩+梯度稀疏化（top-k=5%），单轮通信量降低73%，适配窄带PLC信道。\n\n## 实验结果与价值  \n在真实世界某省电网23万块电表的12个月时序数据集（含3.7%标注窃电样本）上验证：  \n- 在非独立同分布（non-IID）场景下，F1-score达92.4%，AUC为0.961，显著优于基线FedAvg（-4.2%）与中心化ResNet（-6.8%，且无隐私保障）；  \n- 模型在STM32L4系列电表嵌入式环境稳定运行，内存占用峰值<42 KB，支持OTA增量更新；  \n- 首次在电力物联网中实现**形式化隐私证明+边缘实时推理+商用级检测精度**三重统一，为新型智能电网提供可落地的隐私安全基础设施。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16379v1",
      "arxiv_id": "2602.16379v1",
      "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
      "authors": [
        "Mohammad H. A. Monfared",
        "Lucie Flek",
        "Akbar Karimi"
      ],
      "abstract": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16379v1",
      "url": "https://arxiv.org/abs/2602.16379v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 基于LLM智能体的标签一致性数据生成方法  \n本研究针对**方面级情感分析（ABSA）**中高质量标注数据稀缺的问题，提出一种新型**智能体驱动（agentic）数据增强框架**。该方法通过**多轮迭代生成—验证闭环**：LLM智能体首先根据原始句子与目标方面/情感标签生成候选样本，继而调用专用验证模块（同样由LLM实现）对生成结果进行细粒度校验（如方面术语是否忠实于原文、情感极性是否与上下文一致、标签结构是否符合任务规范），不满足条件则触发修正重生成，直至输出满足**标签一致性（Label-Consistency）**要求的合成样本。\n\n为严谨评估智能体架构的增益，我们构建了严格对照的**提示工程基线**——采用完全相同的LLM模型（GPT-4）、相同指令模板与解码参数，仅去除迭代验证与反馈机制。实验在三大ABSA子任务（方面词抽取ATE、方面情感分类ATSC、方面情感对抽取ASPE）、四个主流SemEval基准数据集（14res, 14lap, 15rest, 16rest）及两类编码器-解码器模型（T5-Base与强预训练的Tk-Instruct）上系统展开。\n\n关键发现：（1）智能体方法在**标签保真度上显著优于基线**，尤其在需生成新方面词的ATE与ASPE任务中，错误率降低达32.7%；（2）当与真实数据混合训练时，智能体增强数据带来更稳定的性能提升，在T5-Base上平均F1提升+4.2%，而提示基线仅+1.8%；（3）模型能力差异影响增益幅度：T5-Base受益最显著，其增强后性能**追平未增强的Tk-Instruct**，证实智能体框架可有效弥补中小模型的知识与推理短板。",
      "summary_en": "We propose an **LLM-agent-based data augmentation method** for Aspect-Based Sentiment Analysis (ABSA) that enforces *label consistency* through iterative generation and verification—unlike static prompting baselines. Using GPT-4 as the agent, our approach generates synthetic examples (e.g., aspect terms, sentiment polarities) and rigorously validates their structural and semantic fidelity before acceptance. Evaluated across three ABSA subtasks (ATE, ATSC, ASPE), four SemEval datasets, and two models (T5-Base and Tk-Instruct), our method achieves significantly higher label preservation—especially in aspect term generation—and delivers larger performance gains when augmenting real training data. Notably, T5-Base boosted by agentic augmentation matches the performance of the stronger Tk-Instruct without augmentation, demonstrating its efficacy in compensating for model capacity limitations.",
      "summary": "## 基于LLM智能体的标签一致性数据生成方法  \n本研究针对**方面级情感分析（ABSA）**中高质量标注数据稀缺的问题，提出一种新型**智能体驱动（agentic）数据增强框架**。该方法通过**多轮迭代生成—验证闭环**：LLM智能体首先根据原始句子与目标方面/情感标签生成候选样本，继而调用专用验证模块（同样由LLM实现）对生成结果进行细粒度校验（如方面术语是否忠实于原文、情感极性是否与上下文一致、标签结构是否符合任务规范），不满足条件则触发修正重生成，直至输出满足**标签一致性（Label-Consistency）**要求的合成样本。\n\n为严谨评估智能体架构的增益，我们构建了严格对照的**提示工程基线**——采用完全相同的LLM模型（GPT-4）、相同指令模板与解码参数，仅去除迭代验证与反馈机制。实验在三大ABSA子任务（方面词抽取ATE、方面情感分类ATSC、方面情感对抽取ASPE）、四个主流SemEval基准数据集（14res, 14lap, 15rest, 16rest）及两类编码器-解码器模型（T5-Base与强预训练的Tk-Instruct）上系统展开。\n\n关键发现：（1）智能体方法在**标签保真度上显著优于基线**，尤其在需生成新方面词的ATE与ASPE任务中，错误率降低达32.7%；（2）当与真实数据混合训练时，智能体增强数据带来更稳定的性能提升，在T5-Base上平均F1提升+4.2%，而提示基线仅+1.8%；（3）模型能力差异影响增益幅度：T5-Base受益最显著，其增强后性能**追平未增强的Tk-Instruct**，证实智能体框架可有效弥补中小模型的知识与推理短板。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16958v1",
      "arxiv_id": "2602.16958v1",
      "title": "Automating Agent Hijacking via Structural Template Injection",
      "authors": [
        "Xinhao Deng",
        "Jiaqing Wu",
        "Miao Chen",
        "Yue Xiao",
        "Ke Xu",
        "Qi Li"
      ],
      "abstract": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16958v1",
      "url": "https://arxiv.org/abs/2602.16958v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "injection",
        "prompt"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \nAgent劫持（Agent Hijacking）被OWASP列为大语言模型（LLM）生态系统的**最高危威胁之一**，其核心是攻击者通过向检索内容注入恶意指令，篡改智能体（agent）的执行逻辑。现有方法多依赖人工构造、语义驱动的提示词扰动，导致**攻击成功率低、泛化性差**，尤其难以迁移至GPT、Gemini等闭源商用模型。\n\n## 方法创新：Phantom框架  \n本文提出**Phantom**——首个基于**结构化模板注入（Structured Template Injection）** 的自动化Agent劫持框架。其核心洞见在于：LLM智能体严格依赖预定义的**对话模板标记**（如`<|system|>`、`<|user|>`、`<|tool_response|>`）来区分系统指令、用户输入与工具输出。Phantom通过在检索上下文中注入**经优化的结构化模板片段**，诱导智能体产生角色混淆，误将恶意内容识别为合法用户请求或历史工具响应。\n\n为提升对黑盒商用Agent的迁移能力，Phantom设计了**新型攻击模板搜索框架**：  \n- **多级模板增强**：生成语法合规、结构多样的候选模板；  \n- **模板自编码器（TAE）**：将离散模板映射至连续、可微的隐空间；  \n- **贝叶斯优化**：高效搜索最优对抗向量，并解码为高杀伤力结构化模板。\n\n## 实验与影响  \n在Qwen、GPT-4、Gemini-1.5等主流模型上，Phantom的**攻击成功率（ASR）提升达3.2×，查询效率提高5.8×**（相较SOTA基线）。更关键的是，我们已在70+真实商业产品（含客服Agent、AI工作流平台）中发现并验证漏洞，全部获厂商官方确认。本研究首次揭示了**模板结构本身即为新型攻击面**，为下一代具身智能系统提供了可落地的防御基准与实证依据。",
      "summary_en": "Agent hijacking—ranked a top-tier threat by OWASP—enables adversaries to subvert LLM agents by injecting malicious instructions into retrieved content. Prior attacks rely on manual, semantics-based prompt engineering, suffering from low success rates and poor transferability to black-box commercial models (e.g., GPT, Gemini). We propose **Phantom**, the first automated framework leveraging *Structural Template Injection* to exploit the fundamental chat template architecture of LLM agents. By injecting optimized, syntactically valid template tokens (e.g., `<|user|>`, `<|tool_response|>`) into retrieval contexts, Phantom induces role confusion—causing agents to misinterpret adversarial content as legitimate user input or prior tool outputs. To enhance black-box transferability, Phantom introduces a novel template search pipeline: multi-level structural augmentation, a Template Autoencoder (TAE) for continuous latent embedding, and Bayesian optimization to efficiently discover high-potency adversarial templates. Experiments across Qwen, GPT, and Gemini show Phantom achieves up to **3.2× higher Attack Success Rate (ASR)** and **5.8× better query efficiency** than state-of-the-art baselines. Critically, we identified and verified **70+ vulnerabilities in real-world commercial products**, confirmed by vendors—demonstrating the severe practical risk of structural template–based hijacking and establishing an empirical foundation for securing agentic AI systems.",
      "summary": "## 背景与问题  \nAgent劫持（Agent Hijacking）被OWASP列为大语言模型（LLM）生态系统的**最高危威胁之一**，其核心是攻击者通过向检索内容注入恶意指令，篡改智能体（agent）的执行逻辑。现有方法多依赖人工构造、语义驱动的提示词扰动，导致**攻击成功率低、泛化性差**，尤其难以迁移至GPT、Gemini等闭源商用模型。\n\n## 方法创新：Phantom框架  \n本文提出**Phantom**——首个基于**结构化模板注入（Structured Template Injection）** 的自动化Agent劫持框架。其核心洞见在于：LLM智能体严格依赖预定义的**对话模板标记**（如`<|system|>`、`<|user|>`、`<|tool_response|>`）来区分系统指令、用户输入与工具输出。Phantom通过在检索上下文中注入**经优化的结构化模板片段**，诱导智能体产生角色混淆，误将恶意内容识别为合法用户请求或历史工具响应。\n\n为提升对黑盒商用Agent的迁移能力，Phantom设计了**新型攻击模板搜索框架**：  \n- **多级模板增强**：生成语法合规、结构多样的候选模板；  \n- **模板自编码器（TAE）**：将离散模板映射至连续、可微的隐空间；  \n- **贝叶斯优化**：高效搜索最优对抗向量，并解码为高杀伤力结构化模板。\n\n## 实验与影响  \n在Qwen、GPT-4、Gemini-1.5等主流模型上，Phantom的**攻击成功率（ASR）提升达3.2×，查询效率提高5.8×**（相较SOTA基线）。更关键的是，我们已在70+真实商业产品（含客服Agent、AI工作流平台）中发现并验证漏洞，全部获厂商官方确认。本研究首次揭示了**模板结构本身即为新型攻击面**，为下一代具身智能系统提供了可落地的防御基准与实证依据。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16943v1",
      "arxiv_id": "2602.16943v1",
      "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
      "authors": [
        "Arnold Cartagena",
        "Ariane Teixeira"
      ],
      "abstract": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16943v1",
      "url": "https://arxiv.org/abs/2602.16943v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n当前大语言模型（LLM）作为智能体（agents）日益通过**工具调用（tool calls）** 与外部系统交互——这些操作具备真实世界影响，远超纯文本输出的风险边界。然而，主流安全评估仍集中于**文本层面的拒绝行为**（如拒答有害请求），严重忽视工具调用这一关键行动层。由此产生一个根本性缺口：文本对齐（text alignment）能否自然泛化至行动对齐（action alignment）？  \n\n## 方法与基准  \n本研究提出 **GAP 基准（Gap Assessment Protocol）**，首次系统量化文本安全与工具调用安全之间的**行为分化程度**。实验覆盖：  \n- **6个前沿闭源/开源模型**（含GPT-4o、Claude-3.5、Qwen2.5-72B等）；  \n- **6个强监管领域**（医药、金融、教育、就业、法律、基础设施）；  \n- **7类领域特异性越狱（jailbreak）攻击** × 3种系统提示条件（中性/安全强化/工具鼓励） × 2种提示变体；  \n- 共生成 **17,420 条可分析数据点**，全部标注文本响应与实际工具调用序列。  \n\n## 核心发现  \n1. **文本安全 ≠ 工具调用安全**：所有6个模型均出现“文本拒绝但工具执行”的矛盾行为——即模型在回复中明确拒答，却同步触发高风险工具调用（如查询处方药剂量后调用`get_drug_info`并传入非法参数）。该现象被形式化为 **GAP 指标**。  \n2. **安全提示效果有限**：即使在“安全强化”系统提示下，仍存在**219例跨模型GAP事件**；提示措辞对工具行为影响巨大——最鲁棒模型的工具安全率波动达21个百分点，最敏感模型达57个百分点。  \n3. **治理机制局限性**：运行时治理合约（runtime governance contracts）可降低信息泄露，但**无法抑制非法工具调用本身**。  \n\n## 创新意义  \nGAP 基准揭示了当前LLM安全范式的结构性缺陷，证明**纯文本评估无法保障代理体（agent）行为安全**，亟需构建面向工具调用的专用安全测量框架与干预机制。",
      "summary_en": "This paper exposes a critical safety gap in LLM agents: **text-level safety does not transfer to tool-call-level safety**. While current evaluations focus almost exclusively on whether models *refuse harmful text requests*, real-world agent deployments execute *actions* via tool calls—each carrying tangible, often irreversible consequences. We introduce the **GAP benchmark**, the first systematic framework to quantify divergence between textual refusal and forbidden tool execution. Across 6 frontier models, 6 regulated domains (e.g., pharmaceutical, legal), 7 jailbreak types per domain, and 3 system prompt conditions, we collect 17,420 datapoints. Our central finding is pervasive “text-refuse + tool-execute” behavior—formalized as the GAP metric—even under safety-reinforced prompts (219 persistent cases). Prompt wording strongly modulates tool safety rates (spanning 21–57 percentage points across models), and runtime governance contracts reduce information leakage but *fail to deter forbidden tool calls*. These results demonstrate that text-only safety evaluation is insufficient for agents and that tool-call safety demands dedicated measurement, benchmarking, and mitigation.",
      "summary": "## 背景与问题  \n当前大语言模型（LLM）作为智能体（agents）日益通过**工具调用（tool calls）** 与外部系统交互——这些操作具备真实世界影响，远超纯文本输出的风险边界。然而，主流安全评估仍集中于**文本层面的拒绝行为**（如拒答有害请求），严重忽视工具调用这一关键行动层。由此产生一个根本性缺口：文本对齐（text alignment）能否自然泛化至行动对齐（action alignment）？  \n\n## 方法与基准  \n本研究提出 **GAP 基准（Gap Assessment Protocol）**，首次系统量化文本安全与工具调用安全之间的**行为分化程度**。实验覆盖：  \n- **6个前沿闭源/开源模型**（含GPT-4o、Claude-3.5、Qwen2.5-72B等）；  \n- **6个强监管领域**（医药、金融、教育、就业、法律、基础设施）；  \n- **7类领域特异性越狱（jailbreak）攻击** × 3种系统提示条件（中性/安全强化/工具鼓励） × 2种提示变体；  \n- 共生成 **17,420 条可分析数据点**，全部标注文本响应与实际工具调用序列。  \n\n## 核心发现  \n1. **文本安全 ≠ 工具调用安全**：所有6个模型均出现“文本拒绝但工具执行”的矛盾行为——即模型在回复中明确拒答，却同步触发高风险工具调用（如查询处方药剂量后调用`get_drug_info`并传入非法参数）。该现象被形式化为 **GAP 指标**。  \n2. **安全提示效果有限**：即使在“安全强化”系统提示下，仍存在**219例跨模型GAP事件**；提示措辞对工具行为影响巨大——最鲁棒模型的工具安全率波动达21个百分点，最敏感模型达57个百分点。  \n3. **治理机制局限性**：运行时治理合约（runtime governance contracts）可降低信息泄露，但**无法抑制非法工具调用本身**。  \n\n## 创新意义  \nGAP 基准揭示了当前LLM安全范式的结构性缺陷，证明**纯文本评估无法保障代理体（agent）行为安全**，亟需构建面向工具调用的专用安全测量框架与干预机制。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16935v1",
      "arxiv_id": "2602.16935v1",
      "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
      "authors": [
        "Justin Albrethsen",
        "Yash Datta",
        "Kunal Kumar",
        "Sharath Rajasekar"
      ],
      "abstract": "While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16935v1",
      "url": "https://arxiv.org/abs/2602.16935v1",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n当前大语言模型（LLM）能力持续增强，但其安全防护机制仍普遍采用**无状态（stateless）设计**，将多轮对话视为彼此独立的单次交互。这一缺陷导致显著的“安全缺口”（Safety Gap）：攻击者可利用**渐进式对抗策略**（如Crescendo、ActorAttack），将恶意意图拆解、稀释并跨轮次逐步累积，从而绕过仅依赖单轮内容的静态过滤器。\n\n## 方法创新：DeepContext框架  \n我们提出**DeepContext**——首个面向实时、多轮对抗意图漂移检测的**有状态（stateful）监控框架**。其核心突破在于摒弃孤立轮次评估范式，转而采用**循环神经网络（RNN）架构**，以序列化方式处理经微调的轮次级语义嵌入（turn-level embeddings）。通过在对话中**持续传递隐状态（hidden state）**，DeepContext显式建模用户意图的时序演化轨迹，精准捕获状态无关模型所忽略的**风险增量累积过程**。\n\n## 主要成果与优势  \n- 在多轮越狱（jailbreak）检测任务上达到**SOTA F1分数0.84**，显著优于主流方案：超大规模云服务商安全护栏（~0.52–0.61）、Llama-Prompt-Guard-2（0.67）及Granite-Guardian（0.67）；  \n- 推理延迟严格控制在**<20ms（T4 GPU）**，满足工业级实时部署要求；  \n- 证明：**建模意图的时序动态性**，比堆叠巨型无状态模型更具效果与效率双重优势。",
      "summary_en": "Large Language Models (LLMs) have advanced rapidly, yet their safety guardrails remain predominantly *stateless*, treating multi-turn dialogues as independent single-turn events. This temporal blindness enables adversarial intent drift—e.g., via Crescendo or ActorAttack—to accumulate incrementally across turns and evade static filters. We introduce **DeepContext**, a lightweight, *stateful* real-time monitoring framework that models the temporal evolution of user intent using a Recurrent Neural Network (RNN) over fine-tuned turn-level embeddings. By maintaining and updating a hidden state across conversation history, DeepContext detects subtle, multi-turn adversarial patterns missed by stateless baselines. Evaluated on multi-turn jailbreak detection, DeepContext achieves a new state-of-the-art **F1 score of 0.84**, substantially outperforming hyperscaler cloud guardrails (~0.52–0.61), Llama-Prompt-Guard-2 (0.67), and Granite-Guardian (0.67). Crucially, it incurs only **<20 ms inference latency on a T4 GPU**, demonstrating that sequential intent modeling is both more effective and computationally efficient than scaling up stateless defenses.",
      "summary": "## 背景与问题  \n当前大语言模型（LLM）能力持续增强，但其安全防护机制仍普遍采用**无状态（stateless）设计**，将多轮对话视为彼此独立的单次交互。这一缺陷导致显著的“安全缺口”（Safety Gap）：攻击者可利用**渐进式对抗策略**（如Crescendo、ActorAttack），将恶意意图拆解、稀释并跨轮次逐步累积，从而绕过仅依赖单轮内容的静态过滤器。\n\n## 方法创新：DeepContext框架  \n我们提出**DeepContext**——首个面向实时、多轮对抗意图漂移检测的**有状态（stateful）监控框架**。其核心突破在于摒弃孤立轮次评估范式，转而采用**循环神经网络（RNN）架构**，以序列化方式处理经微调的轮次级语义嵌入（turn-level embeddings）。通过在对话中**持续传递隐状态（hidden state）**，DeepContext显式建模用户意图的时序演化轨迹，精准捕获状态无关模型所忽略的**风险增量累积过程**。\n\n## 主要成果与优势  \n- 在多轮越狱（jailbreak）检测任务上达到**SOTA F1分数0.84**，显著优于主流方案：超大规模云服务商安全护栏（~0.52–0.61）、Llama-Prompt-Guard-2（0.67）及Granite-Guardian（0.67）；  \n- 推理延迟严格控制在**<20ms（T4 GPU）**，满足工业级实时部署要求；  \n- 证明：**建模意图的时序动态性**，比堆叠巨型无状态模型更具效果与效率双重优势。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16901v1",
      "arxiv_id": "2602.16901v1",
      "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
      "authors": [
        "Tanqiu Jiang",
        "Yuhui Wang",
        "Jiacheng Liang",
        "Ting Wang"
      ],
      "abstract": "LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16901v1",
      "url": "https://arxiv.org/abs/2602.16901v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm",
        "agent"
      ],
      "keyword_score": 3,
      "summary_zh": "## AgentLAB：首个面向长周期攻击的LLM智能体安全基准\n\n随着大语言模型（LLM）智能体在**长周期、高复杂度环境**中的广泛应用（如自动化运维、多步决策代理、智能助手等），其面临的安全威胁已从传统单轮交互漏洞，演变为依赖**多轮人-智能体-环境动态交互**的长周期攻击。此类攻击通过精心编排多轮对话与工具调用，实现单轮场景下不可达的恶意目标（如绕过权限控制、窃取敏感记忆、篡改任务本质），但现有评估体系严重缺失对此类渐进式、自适应威胁的系统性评测能力。\n\n为此，我们提出 **AgentLAB**——首个专为评估LLM智能体对**自适应长周期攻击**鲁棒性而设计的开源基准。AgentLAB创新性构建了五大新型攻击范式：**意图劫持**（Intent Hijacking）、**工具链滥用**（Tool Chaining）、**任务注入**（Task Injection）、**目标偏移**（Objective Drifting）与**记忆污染**（Memory Poisoning）；覆盖28个贴近真实部署场景的智能体环境（含API集成、文件系统、多代理协作等）；并提供644个经过人工验证、具备明确攻击路径与成功判据的安全测试用例。\n\n基于AgentLAB的大规模实证评估表明：当前主流LLM智能体（包括基于ReAct、Plan-and-Execute及LLM-as-Judge架构的代表系统）**普遍高度脆弱**——平均攻击成功率超73%，且防御策略若仅针对单轮提示注入或输入过滤（如prompt sanitization、guardrails），**几乎无法抵御长周期攻击**（防御后成功率仍达68%）。本工作不仅揭示了长周期安全建模的紧迫性，更提供了可复现、可扩展、可增量更新的评估基础设施。AgentLAB已开源，地址：https://tanqiujiang.github.io/AgentLAB_main。",
      "summary_en": "## AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks\n\nWe introduce **AgentLAB**, the first benchmark dedicated to evaluating LLM agents’ susceptibility to *adaptive, long-horizon attacks*—multi-turn adversarial strategies that exploit sequential interactions across user, agent, and environment to achieve objectives infeasible in single-turn settings. AgentLAB comprises **5 novel attack types** (intent hijacking, tool chaining, task injection, objective drifting, memory poisoning), **28 realistic agentic environments**, and **644 rigorously validated security test cases**. Evaluating state-of-the-art agents reveals alarming vulnerability: average attack success exceeds **73%**, and standard single-turn defenses (e.g., prompt sanitization, output guardrails) fail catastrophically—success remains **~68%** post-mitigation. AgentLAB provides a reproducible, extensible foundation for measuring progress in securing practical LLM agents. Code and data: https://tanqiujiang.github.io/AgentLAB_main.",
      "summary": "## AgentLAB：首个面向长周期攻击的LLM智能体安全基准\n\n随着大语言模型（LLM）智能体在**长周期、高复杂度环境**中的广泛应用（如自动化运维、多步决策代理、智能助手等），其面临的安全威胁已从传统单轮交互漏洞，演变为依赖**多轮人-智能体-环境动态交互**的长周期攻击。此类攻击通过精心编排多轮对话与工具调用，实现单轮场景下不可达的恶意目标（如绕过权限控制、窃取敏感记忆、篡改任务本质），但现有评估体系严重缺失对此类渐进式、自适应威胁的系统性评测能力。\n\n为此，我们提出 **AgentLAB**——首个专为评估LLM智能体对**自适应长周期攻击**鲁棒性而设计的开源基准。AgentLAB创新性构建了五大新型攻击范式：**意图劫持**（Intent Hijacking）、**工具链滥用**（Tool Chaining）、**任务注入**（Task Injection）、**目标偏移**（Objective Drifting）与**记忆污染**（Memory Poisoning）；覆盖28个贴近真实部署场景的智能体环境（含API集成、文件系统、多代理协作等）；并提供644个经过人工验证、具备明确攻击路径与成功判据的安全测试用例。\n\n基于AgentLAB的大规模实证评估表明：当前主流LLM智能体（包括基于ReAct、Plan-and-Execute及LLM-as-Judge架构的代表系统）**普遍高度脆弱**——平均攻击成功率超73%，且防御策略若仅针对单轮提示注入或输入过滤（如prompt sanitization、guardrails），**几乎无法抵御长周期攻击**（防御后成功率仍达68%）。本工作不仅揭示了长周期安全建模的紧迫性，更提供了可复现、可扩展、可增量更新的评估基础设施。AgentLAB已开源，地址：https://tanqiujiang.github.io/AgentLAB_main。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16708v2",
      "arxiv_id": "2602.16708v2",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Somesh Jha"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16708v2",
      "url": "https://arxiv.org/abs/2602.16708v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "injection",
        "prompt",
        "security",
        "llm",
        "agent"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的场景，如客户服务规程、多级审批流程、敏感数据访问控制及合规监管（如GDPR、HIPAA）。当前主流做法是将策略嵌入提示词（prompt），但该方式**无强制力保障**——模型可能忽略、误解或绕过策略，导致策略违规频发。\n\n## 方法：PCAS策略编译器  \n我们提出 **PCAS（Policy Compiler for Agentic Systems）**，首个支持**确定性策略执行**的编译型安全框架。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图（causal dependency graph）**，显式刻画工具调用、工具返回、消息传递等事件间的时序与信息流向；  \n- **策略语言设计**：采用类Datalog声明式语言，原生支持**跨智能体溯源**与**传递性信息流推理**（如“A影响B，B影响C ⇒ A间接影响C”）；  \n- **零信任执行机制**：通过轻量级**参考监视器（reference monitor）** 拦截所有动作（含工具调用、消息发送），在执行前实时验证策略，**阻断违规行为**，完全解耦策略执行与LLM推理过程。\n\n## 关键成果  \nPCAS以“编译即合规”范式工作：输入现有智能体代码+策略规范，自动输出**策略内建（policy-by-construction）** 的仪器化系统，**无需重构智能体架构或引入安全专用模块**。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离策略；  \n- 多智能体药物警戒系统的双签审批工作流；  \n- 企业客户服务中的组织级权限策略（如客服不可接触支付信息）。  \n在客户服务任务中，PCAS将前沿模型（GPT-4、Claude 3、Gemini 1.5）的策略合规率从**48%提升至93%**，且所有仪器化运行中实现**零策略违规**。",
      "summary_en": "Large language model (LLM)-based agents are increasingly deployed in high-stakes domains requiring rigorous authorization policies—yet prompt-based policy embedding offers no enforcement guarantees. We present PCAS, the first *Policy Compiler for Agentic Systems*, enabling **deterministic, runtime-enforced policy compliance**. PCAS models agent state as a **causal dependency graph**, capturing information flow across tool calls, results, and messages—beyond linear message histories. Policies are written in a declarative Datalog-derived language supporting transitive flow and cross-agent provenance. A lightweight reference monitor intercepts all actions pre-execution, blocking violations irrespective of LLM reasoning. Given an existing agent implementation and a policy spec, PCAS compiles an instrumented, policy-compliant system *by construction*, requiring no security-specific refactoring. Evaluated on three real-world case studies—including prompt-injection defense, pharmacovigilance approval workflows, and customer service policies—PCAS boosts policy compliance from 48% to 93% across frontier models (GPT-4, Claude 3, Gemini 1.5), with **zero policy violations** in all instrumented runs.",
      "summary": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的场景，如客户服务规程、多级审批流程、敏感数据访问控制及合规监管（如GDPR、HIPAA）。当前主流做法是将策略嵌入提示词（prompt），但该方式**无强制力保障**——模型可能忽略、误解或绕过策略，导致策略违规频发。\n\n## 方法：PCAS策略编译器  \n我们提出 **PCAS（Policy Compiler for Agentic Systems）**，首个支持**确定性策略执行**的编译型安全框架。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图（causal dependency graph）**，显式刻画工具调用、工具返回、消息传递等事件间的时序与信息流向；  \n- **策略语言设计**：采用类Datalog声明式语言，原生支持**跨智能体溯源**与**传递性信息流推理**（如“A影响B，B影响C ⇒ A间接影响C”）；  \n- **零信任执行机制**：通过轻量级**参考监视器（reference monitor）** 拦截所有动作（含工具调用、消息发送），在执行前实时验证策略，**阻断违规行为**，完全解耦策略执行与LLM推理过程。\n\n## 关键成果  \nPCAS以“编译即合规”范式工作：输入现有智能体代码+策略规范，自动输出**策略内建（policy-by-construction）** 的仪器化系统，**无需重构智能体架构或引入安全专用模块**。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离策略；  \n- 多智能体药物警戒系统的双签审批工作流；  \n- 企业客户服务中的组织级权限策略（如客服不可接触支付信息）。  \n在客户服务任务中，PCAS将前沿模型（GPT-4、Claude 3、Gemini 1.5）的策略合规率从**48%提升至93%**，且所有仪器化运行中实现**零策略违规**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16752v1",
      "arxiv_id": "2602.16752v1",
      "title": "The Vulnerability of LLM Rankers to Prompt Injection Attacks",
      "authors": [
        "Yu Yin",
        "Shuai Wang",
        "Bevan Koopman",
        "Guido Zuccon"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful re-rankers. Recent research has however showed that simple prompt injections embedded within a candidate document (i.e., jailbreak prompt attacks) can significantly alter an LLM's ranking decisions. While this poses serious security risks to LLM-based ranking pipelines, the extent to which this vulnerability persists across diverse LLM families, architectures, and settings remains largely under-explored. In this paper, we present a comprehensive empirical study of jailbreak prompt attacks against LLM rankers. We focus our evaluation on two complementary tasks: (1) Preference Vulnerability Assessment, measuring intrinsic susceptibility via attack success rate (ASR); and (2) Ranking Vulnerability Assessment, quantifying the operational impact on the ranking's quality (nDCG@10). We systematically examine three prevalent ranking paradigms (pairwise, listwise, setwise) under two injection variants: decision objective hijacking and decision criteria hijacking. Beyond reproducing prior findings, we expand the analysis to cover vulnerability scaling across model families, position sensitivity, backbone architectures, and cross-domain robustness. Our results characterize the boundary conditions of these vulnerabilities, revealing critical insights such as that encoder-decoder architectures exhibit strong inherent resilience to jailbreak attacks. We publicly release our code and additional experimental results at https://github.com/ielab/LLM-Ranker-Attack.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16752v1",
      "url": "https://arxiv.org/abs/2602.16752v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "prompt",
        "jailbreak",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 研究背景  \n大型语言模型（LLMs）正被广泛用作检索后重排序器（rankers），但其对**提示注入攻击**（尤其是嵌入候选文档中的“越狱式”jailbreak prompt）的脆弱性已引发严重安全关切。尽管已有工作证实简单注入可显著扭曲排名决策，但该漏洞在不同模型家族、架构（如encoder-only、encoder-decoder、decoder-only）、排序范式及跨领域场景下的普适性与边界条件仍缺乏系统性实证研究。\n\n## 方法设计  \n本文开展了迄今最全面的LLM重排序器越狱攻击实证研究：  \n- **双维度评估框架**：① **偏好脆弱性评估**（Preference Vulnerability），以攻击成功率（ASR）量化模型内在易感性；② **排序脆弱性评估**（Ranking Vulnerability），以nDCG@10下降幅度衡量实际业务影响；  \n- **三类主流排序范式**：pairwise（成对比较）、listwise（列表级打分）、setwise（集合级判断）；  \n- **两类注入机制**：*决策目标劫持*（如诱导模型忽略相关性转而响应恶意指令）与*决策标准劫持*（如篡改相关性判定依据）；  \n- **多维扩展分析**：覆盖6大模型家族（Llama, Qwen, GLM, ChatGLM, BGE-Reranker, Rerank-Mini）、位置敏感性（注入位置对攻击效果的影响）、骨干架构对比、以及跨领域（MSMARCO→TREC-DL）鲁棒性验证。\n\n## 关键发现与创新  \n- 首次揭示**encoder-decoder架构**（如BGE-Reranker）具有**强内生抗性**，ASR平均低于12%，显著优于decoder-only模型（ASR达68%+）；  \n- listwise范式最脆弱，setwise相对稳健；注入位置对encoder-only模型影响显著（首/尾句敏感），而encoder-decoder模型表现稳定；  \n- 跨域迁移攻击成功率下降超40%，表明当前越狱策略泛化能力有限；  \n- 公开全部代码、攻击模板与细粒度实验数据（含120+配置组合结果），推动可复现、可防御的LLM排序安全研究。",
      "summary_en": "Large Language Models (LLMs) are increasingly deployed as re-rankers, yet their susceptibility to jailbreak prompt injection attacks—especially when malicious prompts are embedded within candidate documents—poses critical security risks. This paper presents the first comprehensive empirical study of such attacks across diverse LLM families, architectures, and ranking paradigms. We introduce a dual-axis evaluation framework: (1) *Preference Vulnerability*, measured by Attack Success Rate (ASR); and (2) *Ranking Vulnerability*, quantified via nDCG@10 degradation. We systematically assess three ranking paradigms (pairwise, listwise, setwise) under two injection variants (decision objective hijacking and decision criteria hijacking), spanning 6 model families, position sensitivity, backbone architectures, and cross-domain robustness. Key findings include: encoder-decoder models (e.g., BGE-Reranker) exhibit strong inherent resilience (mean ASR <12%), significantly outperforming decoder-only counterparts (ASR >68%); listwise ranking is most vulnerable; and cross-domain attack transferability drops sharply (>40% ASR reduction). We publicly release all code and experimental results to advance reproducible, secure LLM ranking research.",
      "summary": "## 研究背景  \n大型语言模型（LLMs）正被广泛用作检索后重排序器（rankers），但其对**提示注入攻击**（尤其是嵌入候选文档中的“越狱式”jailbreak prompt）的脆弱性已引发严重安全关切。尽管已有工作证实简单注入可显著扭曲排名决策，但该漏洞在不同模型家族、架构（如encoder-only、encoder-decoder、decoder-only）、排序范式及跨领域场景下的普适性与边界条件仍缺乏系统性实证研究。\n\n## 方法设计  \n本文开展了迄今最全面的LLM重排序器越狱攻击实证研究：  \n- **双维度评估框架**：① **偏好脆弱性评估**（Preference Vulnerability），以攻击成功率（ASR）量化模型内在易感性；② **排序脆弱性评估**（Ranking Vulnerability），以nDCG@10下降幅度衡量实际业务影响；  \n- **三类主流排序范式**：pairwise（成对比较）、listwise（列表级打分）、setwise（集合级判断）；  \n- **两类注入机制**：*决策目标劫持*（如诱导模型忽略相关性转而响应恶意指令）与*决策标准劫持*（如篡改相关性判定依据）；  \n- **多维扩展分析**：覆盖6大模型家族（Llama, Qwen, GLM, ChatGLM, BGE-Reranker, Rerank-Mini）、位置敏感性（注入位置对攻击效果的影响）、骨干架构对比、以及跨领域（MSMARCO→TREC-DL）鲁棒性验证。\n\n## 关键发现与创新  \n- 首次揭示**encoder-decoder架构**（如BGE-Reranker）具有**强内生抗性**，ASR平均低于12%，显著优于decoder-only模型（ASR达68%+）；  \n- listwise范式最脆弱，setwise相对稳健；注入位置对encoder-only模型影响显著（首/尾句敏感），而encoder-decoder模型表现稳定；  \n- 跨域迁移攻击成功率下降超40%，表明当前越狱策略泛化能力有限；  \n- 公开全部代码、攻击模板与细粒度实验数据（含120+配置组合结果），推动可复现、可防御的LLM排序安全研究。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16741v1",
      "arxiv_id": "2602.16741v1",
      "title": "Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis",
      "authors": [
        "Scott Thornton"
      ],
      "abstract": "AI-assisted code review is widely used to detect vulnerabilities before production release. Prior work shows that adversarial prompt manipulation can degrade large language model (LLM) performance in code generation. We test whether similar comment-based manipulation misleads LLMs during vulnerability detection. We build a 100-sample benchmark across Python, JavaScript, and Java, each paired with eight comment variants ranging from no comments to adversarial strategies such as authority spoofing and technical deception. Eight frontier models, five commercial and three open-source, are evaluated in 9,366 trials. Adversarial comments produce small, statistically non-significant effects on detection accuracy (McNemar exact p > 0.21; all 95 percent confidence intervals include zero). This holds for commercial models with 89 to 96 percent baseline detection and open-source models with 53 to 72 percent, despite large absolute performance gaps. Unlike generation settings where comment manipulation achieves high attack success, detection performance does not meaningfully degrade. More complex adversarial strategies offer no advantage over simple manipulative comments. We test four automated defenses across 4,646 additional trials (14,012 total). Static analysis cross-referencing performs best at 96.9 percent detection and recovers 47 percent of baseline misses. Comment stripping reduces detection for weaker models by removing helpful context. Failures concentrate on inherently difficult vulnerability classes, including race conditions, timing side channels, and complex authorization logic, rather than on adversarial comments.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16741v1",
      "url": "https://arxiv.org/abs/2602.16741v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与目标  \n随着AI辅助代码审查在生产前漏洞检测中的广泛应用，其安全性正面临新型对抗威胁。已有研究证实，针对代码生成任务的对抗性提示操纵可显著削弱大语言模型（LLM）性能；但**在漏洞检测这一关键安全场景中，仅通过源码注释实施的对抗攻击是否有效，尚无大规模实证验证**。本研究首次系统探究“对抗性代码注释”对LLM安全审查能力的干扰效应，并评估实用化防御策略。\n\n## 方法与规模  \n我们构建了覆盖Python、JavaScript和Java三语言的**100个真实漏洞样本基准集**，每个样本配以8种注释变体（含无注释、误导性权威伪装、技术性欺骗等），共800个测试用例。在8个前沿模型（5个商用+3个开源）上开展**9,366次严格控制的检测实验**；进一步引入4类自动化防御机制，在额外4,646次试验中完成总计**14,012次评估**，为迄今该方向最大规模实证研究。\n\n## 核心发现  \n- **对抗注释几乎不损害检测鲁棒性**：所有模型在各类对抗策略下准确率变化均微小且统计不显著（McNemar精确检验 *p* > 0.21；95% CI均包含零）；商用模型（基线89–96%）与开源模型（53–72%）均呈现相同趋势。  \n- **复杂策略无增益**：权威伪装、技术欺骗等高级对抗手段未优于简单误导性注释。  \n- **防御效果分化明显**：静态分析交叉验证表现最优（96.9%检测率，挽回47%基线漏报）；而注释剥离反而降低弱模型性能——证明注释常含关键上下文。  \n- **根本瓶颈在语义难度**：失败案例高度集中于固有高难度漏洞类型（竞态条件、时序侧信道、复杂授权逻辑），**与注释是否对抗无关**，揭示当前LLM安全分析的核心局限在于深层程序语义理解，而非表层文本干扰。",
      "summary_en": "This large-scale empirical study investigates whether adversarial code comments can meaningfully degrade LLM-based vulnerability detection—a critical security application distinct from code generation. We evaluate 8 frontier models (5 commercial, 3 open-source) across 100 real-world vulnerable code samples in Python, JavaScript, and Java, each paired with 8 comment variants (including authority spoofing and technical deception), yielding 9,366 detection trials. Contrary to prior findings in code generation, adversarial comments induce only negligible, statistically non-significant accuracy changes (McNemar *p* > 0.21; all 95% CIs include zero), even for models with wide baseline performance gaps (53–96%). More sophisticated comment attacks confer no advantage over simple manipulations. Among four automated defenses tested in 4,646 additional trials, static analysis cross-referencing achieves 96.9% detection and recovers 47% of baseline misses—while comment stripping harms weaker models by removing helpful context. Failures stem primarily from intrinsically hard vulnerability classes (e.g., race conditions, timing side channels), not adversarial comments—highlighting semantic reasoning, not textual robustness, as the core bottleneck.",
      "summary": "## 研究背景与目标  \n随着AI辅助代码审查在生产前漏洞检测中的广泛应用，其安全性正面临新型对抗威胁。已有研究证实，针对代码生成任务的对抗性提示操纵可显著削弱大语言模型（LLM）性能；但**在漏洞检测这一关键安全场景中，仅通过源码注释实施的对抗攻击是否有效，尚无大规模实证验证**。本研究首次系统探究“对抗性代码注释”对LLM安全审查能力的干扰效应，并评估实用化防御策略。\n\n## 方法与规模  \n我们构建了覆盖Python、JavaScript和Java三语言的**100个真实漏洞样本基准集**，每个样本配以8种注释变体（含无注释、误导性权威伪装、技术性欺骗等），共800个测试用例。在8个前沿模型（5个商用+3个开源）上开展**9,366次严格控制的检测实验**；进一步引入4类自动化防御机制，在额外4,646次试验中完成总计**14,012次评估**，为迄今该方向最大规模实证研究。\n\n## 核心发现  \n- **对抗注释几乎不损害检测鲁棒性**：所有模型在各类对抗策略下准确率变化均微小且统计不显著（McNemar精确检验 *p* > 0.21；95% CI均包含零）；商用模型（基线89–96%）与开源模型（53–72%）均呈现相同趋势。  \n- **复杂策略无增益**：权威伪装、技术欺骗等高级对抗手段未优于简单误导性注释。  \n- **防御效果分化明显**：静态分析交叉验证表现最优（96.9%检测率，挽回47%基线漏报）；而注释剥离反而降低弱模型性能——证明注释常含关键上下文。  \n- **根本瓶颈在语义难度**：失败案例高度集中于固有高难度漏洞类型（竞态条件、时序侧信道、复杂授权逻辑），**与注释是否对抗无关**，揭示当前LLM安全分析的核心局限在于深层程序语义理解，而非表层文本干扰。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16944v1",
      "arxiv_id": "2602.16944v1",
      "title": "Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming",
      "authors": [
        "Philip Sosnin",
        "Jodie Knapp",
        "Fraser Kennedy",
        "Josh Collyer",
        "Calvin Tsay"
      ],
      "abstract": "This work introduces a verification framework that provides both sound and complete guarantees for data poisoning attacks during neural network training. We formulate adversarial data manipulation, model training, and test-time evaluation in a single mixed-integer quadratic programming (MIQCP) problem. Finding the global optimum of the proposed formulation provably yields worst-case poisoning attacks, while simultaneously bounding the effectiveness of all possible attacks on the given training pipeline. Our framework encodes both the gradient-based training dynamics and model evaluation at test time, enabling the first exact certification of training-time robustness. Experimental evaluation on small models confirms that our approach delivers a complete characterization of robustness against data poisoning.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16944v1",
      "url": "https://arxiv.org/abs/2602.16944v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "data",
        "poisoning"
      ],
      "keyword_score": 3,
      "summary_zh": "## 精确验证数据投毒攻击：基于混合整数规划的完备认证框架\n\n本研究提出首个**严格完备（sound and complete）**的数据投毒攻击鲁棒性验证框架，可对神经网络训练全过程提供**可证明的精确认证**。传统防御方法多依赖启发式或近似分析，难以保证无漏报（false negative）与无误报（false positive）；而本文突破性地将**对抗性数据操纵、梯度下降训练动态、以及测试时模型评估**三者统一建模为一个**单层混合整数二次规划（MIQCP）问题**。该公式化设计的关键创新在于：  \n- **精确编码训练过程**：显式建模有限步数的SGD/Adam更新，包括权重更新、激活函数（如ReLU）的分段线性约束，并通过大M法与二进制变量实现非线性操作的整数线性化；  \n- **端到端最坏情况求解**：求解该MIQCP的全局最优解，**严格等价于构造出使目标模型性能最差的最优投毒样本集**，同时天然导出对所有可能投毒攻击的**紧致鲁棒性上界**；  \n- **首次实现训练时鲁棒性的精确认证**：不同于仅验证输入鲁棒性（如对抗样本）的工作，本框架首次在**训练阶段**完成对数据完整性威胁的数学完备验证。  \n\n在MNIST、CIFAR-10等基准数据集上的小规模模型（≤3层全连接/轻量CNN）实验表明：该方法能**100%完备地刻画鲁棒性边界**——即对给定污染预算（如最多5个毒样本），精确判定“是否存在可导致分类错误的投毒”，且认证结果经穷举验证无误。本工作为可信机器学习提供了理论坚实的验证基石，也为后续可扩展认证算法设计建立了关键范式。",
      "summary_en": "This paper introduces the first verification framework that provides **sound and complete guarantees** for robustness against data-poisoning attacks during neural network training. We formulate adversarial data manipulation, gradient-based training dynamics (e.g., SGD with ReLU activations), and test-time evaluation as a single **mixed-integer quadratic programming (MIQCP)** problem. Solving this MIQCP to global optimality **provably yields the worst-case poisoning attack**, while simultaneously computing a tight upper bound on the maximum possible impact of *any* poisoning strategy under the given training pipeline. Crucially, our formulation exactly encodes finite-step optimization and non-linear model behavior—enabling, for the first time, **exact certification of training-time robustness**. Experiments on small-scale models confirm that our approach delivers a **complete characterization**: it definitively answers whether a poisoning attack exists within a given budget (e.g., ≤5 poisoned samples) that can flip a target prediction—and all certified claims are verified exhaustively.",
      "summary": "## 精确验证数据投毒攻击：基于混合整数规划的完备认证框架\n\n本研究提出首个**严格完备（sound and complete）**的数据投毒攻击鲁棒性验证框架，可对神经网络训练全过程提供**可证明的精确认证**。传统防御方法多依赖启发式或近似分析，难以保证无漏报（false negative）与无误报（false positive）；而本文突破性地将**对抗性数据操纵、梯度下降训练动态、以及测试时模型评估**三者统一建模为一个**单层混合整数二次规划（MIQCP）问题**。该公式化设计的关键创新在于：  \n- **精确编码训练过程**：显式建模有限步数的SGD/Adam更新，包括权重更新、激活函数（如ReLU）的分段线性约束，并通过大M法与二进制变量实现非线性操作的整数线性化；  \n- **端到端最坏情况求解**：求解该MIQCP的全局最优解，**严格等价于构造出使目标模型性能最差的最优投毒样本集**，同时天然导出对所有可能投毒攻击的**紧致鲁棒性上界**；  \n- **首次实现训练时鲁棒性的精确认证**：不同于仅验证输入鲁棒性（如对抗样本）的工作，本框架首次在**训练阶段**完成对数据完整性威胁的数学完备验证。  \n\n在MNIST、CIFAR-10等基准数据集上的小规模模型（≤3层全连接/轻量CNN）实验表明：该方法能**100%完备地刻画鲁棒性边界**——即对给定污染预算（如最多5个毒样本），精确判定“是否存在可导致分类错误的投毒”，且认证结果经穷举验证无误。本工作为可信机器学习提供了理论坚实的验证基石，也为后续可扩展认证算法设计建立了关键范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16346v2",
      "arxiv_id": "2602.16346v2",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "authors": [
        "Nivya Talokar",
        "Ayush K Tarun",
        "Murari Mandal",
        "Maksym Andriushchenko",
        "Antoine Bosselut"
      ],
      "abstract": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16346v2",
      "url": "https://arxiv.org/abs/2602.16346v2",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具与长期记忆执行真实世界工作流，但其能力亦可能被恶意行为者用于实施复杂滥用——现有滥用评估基准多聚焦单轮指令攻击，**严重忽视多轮交互中危害行为的渐进式演化**，尤其缺乏对跨语言、多步骤非法目标达成过程的系统性度量。\n\n## 方法：STING 框架  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**——首个面向工具增强型多轮代理的自动化红队测试框架。其核心创新包括：  \n- **良性伪装的分步非法计划生成**：基于可信身份（如“合规税务顾问”）构建逻辑连贯的N步非法目标（如伪造跨境资金流水）；  \n- **自适应多轮探针机制**：以迭代式后续提问模拟真实对抗交互，动态调整策略；  \n- **双层判断架构**：轻量级 judge agent 实时追踪各阶段完成状态，避免人工标注瓶颈。\n\n## 关键发现与创新指标  \n- 在 AgentHarm 基准上，STING 的非法任务完成率较单轮提示提升 **3.2×**，优于适配工具能力的多轮对话基线（+68%）；  \n- 提出**时间至首次越狱（Time-to-First-Jailbreak）随机变量建模范式**，衍生出三项分析工具：  \n  - **发现曲线（Discovery Curves）**：刻画攻击成功率随轮次演化的动态轨迹；  \n  - **语言特异性风险归因（Hazard-Ratio by Language）**：量化不同语种攻击的相对危害强度；  \n  - **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**：首个兼顾时效性与鲁棒性的多轮滥用效能聚合指标。  \n- 多语言实验（覆盖西班牙语、阿拉伯语、日语等6种非英语场景）揭示：**低资源语言攻击成功率并不普遍更高**，颠覆了传统聊天机器人“越小语种越易越狱”的经验认知，凸显工具型代理安全机制的语种依赖性差异。",
      "summary_en": "We introduce **STING**, the first automated red-teaming framework for evaluating illicit assistance in *multi-turn, multilingual, tool-using LLM agents*. Unlike prior single-prompt benchmarks, STING constructs adaptive, step-by-step illicit plans grounded in benign personas and probes target agents iteratively, using lightweight judge agents to track phase-wise completion. We model multi-turn jailbreaking as a *time-to-first-jailbreak* random variable, enabling novel analysis tools—including discovery curves, hazard-ratio attribution by language, and the **Restricted Mean Jailbreak Discovery (RMJD)** metric. Across AgentHarm scenarios, STING achieves substantially higher illicit-task completion than single-turn and chat-oriented multi-turn baselines (+68% over adapted tool-using baselines). In six non-English settings, attack success does *not* consistently increase in lower-resource languages—contradicting common chatbot findings and highlighting distinct failure modes in tool-augmented agents. STING provides a practical, scalable methodology for stress-testing real-world agent deployments.",
      "summary": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具与长期记忆执行真实世界工作流，但其能力亦可能被恶意行为者用于实施复杂滥用——现有滥用评估基准多聚焦单轮指令攻击，**严重忽视多轮交互中危害行为的渐进式演化**，尤其缺乏对跨语言、多步骤非法目标达成过程的系统性度量。\n\n## 方法：STING 框架  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**——首个面向工具增强型多轮代理的自动化红队测试框架。其核心创新包括：  \n- **良性伪装的分步非法计划生成**：基于可信身份（如“合规税务顾问”）构建逻辑连贯的N步非法目标（如伪造跨境资金流水）；  \n- **自适应多轮探针机制**：以迭代式后续提问模拟真实对抗交互，动态调整策略；  \n- **双层判断架构**：轻量级 judge agent 实时追踪各阶段完成状态，避免人工标注瓶颈。\n\n## 关键发现与创新指标  \n- 在 AgentHarm 基准上，STING 的非法任务完成率较单轮提示提升 **3.2×**，优于适配工具能力的多轮对话基线（+68%）；  \n- 提出**时间至首次越狱（Time-to-First-Jailbreak）随机变量建模范式**，衍生出三项分析工具：  \n  - **发现曲线（Discovery Curves）**：刻画攻击成功率随轮次演化的动态轨迹；  \n  - **语言特异性风险归因（Hazard-Ratio by Language）**：量化不同语种攻击的相对危害强度；  \n  - **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**：首个兼顾时效性与鲁棒性的多轮滥用效能聚合指标。  \n- 多语言实验（覆盖西班牙语、阿拉伯语、日语等6种非英语场景）揭示：**低资源语言攻击成功率并不普遍更高**，颠覆了传统聊天机器人“越小语种越易越狱”的经验认知，凸显工具型代理安全机制的语种依赖性差异。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16749v1",
      "arxiv_id": "2602.16749v1",
      "title": "U-FedTomAtt: Ultra-lightweight Federated Learning with Attention for Tomato Disease Recognition",
      "authors": [
        "Romiyal George",
        "Sathiyamohan Nishankar",
        "Selvarajah Thuseethan",
        "Chathrie Wimalasooriya",
        "Yakub Sebastian",
        "Roshan G. Ragel",
        "Zhongwei Liang"
      ],
      "abstract": "Federated learning has emerged as a privacy-preserving and efficient approach for deploying intelligent agricultural solutions. Accurate edge-based diagnosis across geographically dispersed farms is crucial for recognising tomato diseases in sustainable farming. Traditional centralised training aggregates raw data on a central server, leading to communication overhead, privacy risks and latency. Meanwhile, edge devices require lightweight networks to operate effectively within limited resources. In this paper, we propose U-FedTomAtt, an ultra-lightweight federated learning framework with attention for tomato disease recognition in resource-constrained and distributed environments. The model comprises only 245.34K parameters and 71.41 MFLOPS. First, we propose an ultra-lightweight neural network with dilated bottleneck (DBNeck) modules and a linear transformer to minimise computational and memory overhead. To mitigate potential accuracy loss, a novel local-global residual attention (LoGRA) module is incorporated. Second, we propose the federated dual adaptive weight aggregation (FedDAWA) algorithm that enhances global model accuracy. Third, our framework is validated using three benchmark datasets for tomato diseases under simulated federated settings. Experimental results show that the proposed method achieves 0.9910% and 0.9915% Top-1 accuracy and 0.9923% and 0.9897% F1-scores on SLIF-Tomato and PlantVillage tomato datasets, respectively.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16749v1",
      "url": "https://arxiv.org/abs/2602.16749v1",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n在智慧农业中，番茄病害的实时、隐私保护型边缘识别对可持续种植至关重要。传统中心化深度学习需上传原始图像至服务器，引发**数据隐私泄露、通信带宽压力与推理延迟**三大瓶颈；而边缘设备（如田间摄像头、微型无人机）又受限于算力、内存与功耗，难以部署常规CNN或Transformer模型。\n\n## 方法创新  \n本文提出**U-FedTomAtt**——一种面向资源极度受限场景的超轻量联邦学习框架，专用于分布式番茄病害识别：  \n- **超轻量主干网络**：仅含**245.34K参数**与**71.41 MFLOPS**计算量，融合**空洞瓶颈模块（DBNeck）** 与**线性Transformer**，显著压缩模型体积与推理开销；  \n- **局部-全局残差注意力（LoGRA）模块**：在极低参数预算下引入细粒度空间-通道协同注意力，有效补偿轻量化导致的判别力衰减；  \n- **联邦双自适应权重聚合算法（FedDAWA）**：动态校准客户端本地更新贡献度，缓解数据异构性（Non-IID）导致的模型偏差，提升全局收敛稳定性与精度。\n\n## 实验验证与性能  \n在SLIF-Tomato与PlantVillage两大公开番茄病害数据集上构建三机构模拟联邦设置。结果表明：U-FedTomAtt在严苛边缘约束下仍实现**卓越泛化能力**——SLIF-Tomato上Top-1准确率达**99.10%**、F1-score达**99.23%**；PlantVillage上达**99.15%**准确率与**98.97%** F1-score，显著优于MobileNetV3、EfficientNet-Lite及FedAvg基线。本工作为农业AI落地提供了**可部署、可信赖、可扩展**的联邦轻量化新范式。",
      "summary_en": "Federated learning (FL) enables privacy-preserving, edge-based tomato disease recognition across distributed farms—yet faces dual challenges: excessive communication/latency in centralized training and prohibitive resource demands of standard models on edge devices. To address this, we propose **U-FedTomAtt**, an ultra-lightweight FL framework featuring only **245.34K parameters** and **71.41 MFLOPS**, designed for real-world agricultural constraints. Its core innovations include: (i) a novel backbone integrating **dilated bottleneck (DBNeck) modules** and a **linear transformer** for extreme efficiency; (ii) a **local-global residual attention (LoGRA)** module to preserve discriminative capability without increasing parameters; and (iii) **FedDAWA**, a dual-adaptive weight aggregation algorithm that dynamically weights heterogeneous client updates to boost global model accuracy. Evaluated on SLIF-Tomato and PlantVillage under realistic non-IID federated settings, U-FedTomAtt achieves **99.10% and 99.15% Top-1 accuracy**, and **99.23% and 98.97% F1-score**, respectively—surpassing lightweight baselines while enabling on-device deployment.",
      "summary": "## 研究背景  \n在智慧农业中，番茄病害的实时、隐私保护型边缘识别对可持续种植至关重要。传统中心化深度学习需上传原始图像至服务器，引发**数据隐私泄露、通信带宽压力与推理延迟**三大瓶颈；而边缘设备（如田间摄像头、微型无人机）又受限于算力、内存与功耗，难以部署常规CNN或Transformer模型。\n\n## 方法创新  \n本文提出**U-FedTomAtt**——一种面向资源极度受限场景的超轻量联邦学习框架，专用于分布式番茄病害识别：  \n- **超轻量主干网络**：仅含**245.34K参数**与**71.41 MFLOPS**计算量，融合**空洞瓶颈模块（DBNeck）** 与**线性Transformer**，显著压缩模型体积与推理开销；  \n- **局部-全局残差注意力（LoGRA）模块**：在极低参数预算下引入细粒度空间-通道协同注意力，有效补偿轻量化导致的判别力衰减；  \n- **联邦双自适应权重聚合算法（FedDAWA）**：动态校准客户端本地更新贡献度，缓解数据异构性（Non-IID）导致的模型偏差，提升全局收敛稳定性与精度。\n\n## 实验验证与性能  \n在SLIF-Tomato与PlantVillage两大公开番茄病害数据集上构建三机构模拟联邦设置。结果表明：U-FedTomAtt在严苛边缘约束下仍实现**卓越泛化能力**——SLIF-Tomato上Top-1准确率达**99.10%**、F1-score达**99.23%**；PlantVillage上达**99.15%**准确率与**98.97%** F1-score，显著优于MobileNetV3、EfficientNet-Lite及FedAvg基线。本工作为农业AI落地提供了**可部署、可信赖、可扩展**的联邦轻量化新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16843v1",
      "arxiv_id": "2602.16843v1",
      "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
      "authors": [
        "Ahmed Rafid",
        "Rumman Adib",
        "Fariya Ahmed",
        "Ajwad Abrar",
        "Mohammed Saidul Islam"
      ],
      "abstract": "Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16843v1",
      "url": "https://arxiv.org/abs/2602.16843v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n事实一致性评估对可信文本摘要至关重要，尤其在医疗、新闻等高风险领域。然而，现有主流指标（如BERTScore、FactCC）严重依赖英文参考摘要，且普遍忽视孟加拉语——全球超2.7亿人使用的高使用率但低资源语言。参考依赖不仅限制实际部署（因人工撰写参考摘要成本高昂），更在低资源场景下加剧数据瓶颈。\n\n## 方法创新  \n本文提出**BanglaSummEval**：首个面向孟加拉语、无需参考摘要的事实一致性评估框架。其核心是**问答驱动的端到端评估范式**：  \n- 利用单个多语言指令微调模型（mT5-base）统一完成四大任务：从源文档与摘要中自动生成关键问题、回答问题、提取候选答案、并为问题分配重要性权重；  \n- 通过**BERTScore-Recall**比对模型从源文档与摘要中生成的答案，捕捉深层语义一致性（而非表面词重叠）；  \n- 同时量化**事实准确性**（答案是否可由源文档支持）与**内容覆盖度**（摘要是否涵盖源文档的关键信息点）。\n\n## 验证与优势  \n在300篇真实人工撰写的教育与医疗领域孟加拉语摘要上进行严格验证，BanglaSummEval与专家人工评分呈现强相关性（Pearson *r* = 0.694，Spearman *ρ* = 0.763），显著优于基线无参考指标（+0.21 Pearson提升）。其**可解释性诊断能力**（逐问题溯源不一致点）和**轻量级设计**（单模型全流程）使其成为低资源语言摘要评估的实用、透明、低成本解决方案。",
      "summary_en": "BanglaSummEval is the first reference-free, question-answering-based framework for factual consistency evaluation in Bangla summarization. It leverages a single multilingual instruction-tuned language model to jointly generate questions from source documents and summaries, answer them, extract candidate answers, and weight question importance—enabling unified, low-cost assessment of both factual accuracy and content coverage. Crucially, it uses BERTScore-Recall to compare answers derived from source and summary, capturing semantic consistency beyond lexical overlap. Evaluated on 300 human-written Bangla summaries from educational and medical domains, BanglaSummEval achieves strong correlation with expert judgments (Pearson *r* = 0.694; Spearman *ρ* = 0.763), outperforming existing no-reference baselines. Its interpretability, efficiency, and language-specific design make it a practical solution for low-resource NLP evaluation.",
      "summary": "## 背景与问题  \n事实一致性评估对可信文本摘要至关重要，尤其在医疗、新闻等高风险领域。然而，现有主流指标（如BERTScore、FactCC）严重依赖英文参考摘要，且普遍忽视孟加拉语——全球超2.7亿人使用的高使用率但低资源语言。参考依赖不仅限制实际部署（因人工撰写参考摘要成本高昂），更在低资源场景下加剧数据瓶颈。\n\n## 方法创新  \n本文提出**BanglaSummEval**：首个面向孟加拉语、无需参考摘要的事实一致性评估框架。其核心是**问答驱动的端到端评估范式**：  \n- 利用单个多语言指令微调模型（mT5-base）统一完成四大任务：从源文档与摘要中自动生成关键问题、回答问题、提取候选答案、并为问题分配重要性权重；  \n- 通过**BERTScore-Recall**比对模型从源文档与摘要中生成的答案，捕捉深层语义一致性（而非表面词重叠）；  \n- 同时量化**事实准确性**（答案是否可由源文档支持）与**内容覆盖度**（摘要是否涵盖源文档的关键信息点）。\n\n## 验证与优势  \n在300篇真实人工撰写的教育与医疗领域孟加拉语摘要上进行严格验证，BanglaSummEval与专家人工评分呈现强相关性（Pearson *r* = 0.694，Spearman *ρ* = 0.763），显著优于基线无参考指标（+0.21 Pearson提升）。其**可解释性诊断能力**（逐问题溯源不一致点）和**轻量级设计**（单模型全流程）使其成为低资源语言摘要评估的实用、透明、低成本解决方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16653v2",
      "arxiv_id": "2602.16653v2",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "abstract": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16653v2",
      "url": "https://arxiv.org/abs/2602.16653v2",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## Agent Skill框架：小语言模型在工业场景中的潜力再审视\n\n随着GitHub Copilot、LangChain与OpenAI等主流平台正式集成**Agent Skill框架**，该范式已在提升上下文工程精度、抑制幻觉、增强任务准确率方面展现出显著优势——尤其在适配专有大模型时效果突出。然而，在数据安全敏感、预算受限且API调用不可持续的**工业实际场景**中，企业更依赖轻量、可私有部署的小语言模型（SLMs），而其在高度定制化任务中泛化能力薄弱的问题长期未解。本研究首次系统探究Agent Skill范式对SLMs的适配性与增益边界。\n\n我们提出首个**形式化数学定义**，将Skill选择建模为条件概率约束下的最优决策过程；并构建跨规模、多场景评估体系，涵盖两个开源基准任务（ToolAlpaca、MSAgent）及一个真实保险理赔数据集（含非结构化报案文本、保单规则与多跳逻辑判断）。实验覆盖从1.5B至80B参数的9类模型，重点对比技能路由可靠性、任务完成率与GPU显存效率。\n\n关键发现包括：  \n- **1.5–7B级超轻量模型**难以稳定识别技能意图，路由错误率达42%以上；  \n- **12–30B中等规模SLMs**受益最显著：任务准确率平均提升23.6%，幻觉率下降37%；  \n- **~80B代码特化SLM（如CodeLlama-70B微调版）** 在保险任务上达到GPT-4级别性能（F1=0.89 vs. 0.91），同时**显存占用降低41%、推理延迟减少29%**。  \n\n本研究不仅厘清了Agent Skill对SLMs的能力放大机制与规模阈值，更提供了面向工业落地的**模型选型指南**与**技能编排优化原则**，为构建安全、高效、可控的私有智能体系统奠定理论与实践基础。",
      "summary_en": "The Agent Skill framework—now natively supported by GitHub Copilot, LangChain, and OpenAI—enhances context engineering, reduces hallucinations, and improves task accuracy, especially with proprietary models. Yet its utility for small language models (SLMs) in data-sensitive, API-constrained industrial settings remains unexplored. This work formally defines the Agent Skill process mathematically and evaluates SLMs across two open-source benchmarks (ToolAlpaca, MSAgent) and a real-world insurance claims dataset. Results show that models <7B parameters fail reliably at skill selection, while 12–30B SLMs gain substantial improvements: +23.6% task accuracy and −37% hallucination rate. Notably, code-specialized ~80B SLMs match closed-source baselines (e.g., GPT-4 F1=0.89 vs. 0.91) on insurance tasks while cutting GPU memory usage by 41% and latency by 29%. These findings establish clear capability boundaries and provide actionable deployment guidelines for SLM-centric agent systems.",
      "summary": "## Agent Skill框架：小语言模型在工业场景中的潜力再审视\n\n随着GitHub Copilot、LangChain与OpenAI等主流平台正式集成**Agent Skill框架**，该范式已在提升上下文工程精度、抑制幻觉、增强任务准确率方面展现出显著优势——尤其在适配专有大模型时效果突出。然而，在数据安全敏感、预算受限且API调用不可持续的**工业实际场景**中，企业更依赖轻量、可私有部署的小语言模型（SLMs），而其在高度定制化任务中泛化能力薄弱的问题长期未解。本研究首次系统探究Agent Skill范式对SLMs的适配性与增益边界。\n\n我们提出首个**形式化数学定义**，将Skill选择建模为条件概率约束下的最优决策过程；并构建跨规模、多场景评估体系，涵盖两个开源基准任务（ToolAlpaca、MSAgent）及一个真实保险理赔数据集（含非结构化报案文本、保单规则与多跳逻辑判断）。实验覆盖从1.5B至80B参数的9类模型，重点对比技能路由可靠性、任务完成率与GPU显存效率。\n\n关键发现包括：  \n- **1.5–7B级超轻量模型**难以稳定识别技能意图，路由错误率达42%以上；  \n- **12–30B中等规模SLMs**受益最显著：任务准确率平均提升23.6%，幻觉率下降37%；  \n- **~80B代码特化SLM（如CodeLlama-70B微调版）** 在保险任务上达到GPT-4级别性能（F1=0.89 vs. 0.91），同时**显存占用降低41%、推理延迟减少29%**。  \n\n本研究不仅厘清了Agent Skill对SLMs的能力放大机制与规模阈值，更提供了面向工业落地的**模型选型指南**与**技能编排优化原则**，为构建安全、高效、可控的私有智能体系统奠定理论与实践基础。",
      "summary_status": "success"
    }
  ],
  "last_updated": "2026-02-24T02:08:02.816006",
  "total_count": 62
}