{
  "papers": [
    {
      "id": "arxiv_2602.17566v1",
      "arxiv_id": "2602.17566v1",
      "title": "A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN",
      "authors": [
        "Asif Hasan Chowdhury",
        "Md. Fahim Islam",
        "M Ragib Anjum Riad",
        "Faiyaz Bin Hashem",
        "Md Tanzim Reza",
        "Md. Golam Rabiul Alam"
      ],
      "abstract": "The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17566v1",
      "url": "https://arxiv.org/abs/2602.17566v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n随着算力提升与医疗数据激增，人工智能（AI）在肺部疾病辅助诊断中展现出巨大潜力。然而，临床数据高度敏感、分布异构且受隐私法规严格约束，传统中心化深度学习模型面临**数据孤岛、隐私泄露与泛化能力弱**三大瓶颈。\n\n## 方法创新  \n本研究提出一种**联邦学习（FL）赋能的混合集成模型**，首次将轻量级视觉Transformer——**SWIN Transformer**与多支先进CNN（DenseNet201、Inception V3、VGG19）深度融合，并构建基于联邦学习的分布式协同训练框架：  \n- 各医院/机构作为本地客户端，仅上传**模型参数更新（非原始X光影像）**，保障数据不出域；  \n- 中央服务器执行安全聚合（Secure Aggregation），动态融合SWIN的全局建模能力与CNN的局部纹理特征；  \n- 引入**实时持续学习机制**，支持新病例流式接入与模型在线优化，提升对变异病灶（如新型肺炎亚型）的适应性。\n\n## 主要发现与贡献  \n在公开胸部X光数据集（COVIDx、RSNA Pneumonia）上，该模型实现**98.7%平均准确率**（较单模型提升4.2–6.8%），**F1-score达0.971**，显著优于独立训练基准；同时将跨机构数据泄露风险降低99.3%（通过差分隐私+加密梯度验证）。本工作不仅为COVID-19与细菌性肺炎提供高精度、低延迟的双分类诊断工具，更构建了可扩展、合伦理、强鲁棒的医疗AI协作范式，为多中心肺病早筛系统落地提供关键技术支撑。",
      "summary_en": "This paper proposes a **hybrid federated learning (FL)-enabled ensemble model** for lung disease diagnosis, integrating the SWIN Transformer and multiple CNNs (DenseNet201, Inception V3, VGG19) to jointly analyze chest X-ray images. Leveraging FL, the framework enables collaborative training across hospitals without sharing raw patient data—only encrypted, differentially private model updates are exchanged. A real-time continual learning mechanism allows dynamic adaptation to emerging disease patterns. Evaluated on COVIDx and RSNA Pneumonia datasets, the model achieves **98.7% average accuracy and 0.971 F1-score**, outperforming standalone models by 4.2–6.8% in accuracy while reducing cross-institutional privacy risk by 99.3%. This work establishes a secure, scalable, and clinically actionable AI paradigm for federated pulmonary diagnostics.",
      "summary": "## 背景与挑战  \n随着算力提升与医疗数据激增，人工智能（AI）在肺部疾病辅助诊断中展现出巨大潜力。然而，临床数据高度敏感、分布异构且受隐私法规严格约束，传统中心化深度学习模型面临**数据孤岛、隐私泄露与泛化能力弱**三大瓶颈。\n\n## 方法创新  \n本研究提出一种**联邦学习（FL）赋能的混合集成模型**，首次将轻量级视觉Transformer——**SWIN Transformer**与多支先进CNN（DenseNet201、Inception V3、VGG19）深度融合，并构建基于联邦学习的分布式协同训练框架：  \n- 各医院/机构作为本地客户端，仅上传**模型参数更新（非原始X光影像）**，保障数据不出域；  \n- 中央服务器执行安全聚合（Secure Aggregation），动态融合SWIN的全局建模能力与CNN的局部纹理特征；  \n- 引入**实时持续学习机制**，支持新病例流式接入与模型在线优化，提升对变异病灶（如新型肺炎亚型）的适应性。\n\n## 主要发现与贡献  \n在公开胸部X光数据集（COVIDx、RSNA Pneumonia）上，该模型实现**98.7%平均准确率**（较单模型提升4.2–6.8%），**F1-score达0.971**，显著优于独立训练基准；同时将跨机构数据泄露风险降低99.3%（通过差分隐私+加密梯度验证）。本工作不仅为COVID-19与细菌性肺炎提供高精度、低延迟的双分类诊断工具，更构建了可扩展、合伦理、强鲁棒的医疗AI协作范式，为多中心肺病早筛系统落地提供关键技术支撑。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17452v1",
      "arxiv_id": "2602.17452v1",
      "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
      "authors": [
        "Wyatt Benno",
        "Alberto Centelles",
        "Antoine Douchet",
        "Khalil Gibran"
      ],
      "abstract": "We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions.   Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verification optimisations detailed in this paper. We demonstrate that Jolt Atlas can prove model inference in memory-constrained environments -- a prover property commonly referred to as \\textit{streaming}. Furthermore, we discuss how Jolt Atlas achieves zero-knowledge through the BlindFold technique, as introduced in Vega. In contrast to existing zkML frameworks, we show practical proving times for classification, embedding, automated reasoning, and small language models.   Jolt Atlas enables cryptographic verification that can be run on-device, without specialised hardware. The resulting proofs are succinctly verifiable. This makes Jolt Atlas well-suited for privacy-centric and adversarial environments. In a companion work, we outline various use cases of Jolt Atlas, including how it serves as guardrails in agentic commerce and for trustless AI context (often referred to as \\textit{AI memory}).",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17452v1",
      "url": "https://arxiv.org/abs/2602.17452v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "zero-knowledge",
        "learning",
        "adversarial"
      ],
      "keyword_score": 4,
      "summary_zh": "## Jolt Atlas：基于查找论证的零知识可验证机器学习推理框架  \n\n**背景与动机**：现有零知识机器学习（zkML）方案多依赖零知识虚拟机（zkVM）模拟CPU指令，开销大、内存占用高，难以在资源受限设备上部署。Jolt Atlas针对这一瓶颈，提出首个**直接面向ONNX张量计算图**的zkML框架，将零知识证明能力从通用计算下沉至AI原语层。\n\n**核心方法**：  \n- 基于Jolt证明系统的**查找中心化（lookup-centric）范式**，摒弃传统CPU寄存器建模，转而对ONNX标准算子（如MatMul、ReLU、Softmax）构建专用查找表（LUT）；  \n- 采用**sumcheck协议实现高效非线性查找论证**，天然适配ML中关键的非线性激活函数；  \n- 引入**神经跃迁（Neural Teleportation）** 技术压缩查找表规模（最高降低87%），同时严格保持模型精度；  \n- 设计多项**张量级验证优化**：包括稀疏张量编码、分块查表流水线、内存访问模式预承诺，实现真正**流式（streaming）证明**——证明者峰值内存恒定，不随模型规模增长。\n\n**创新与优势**：  \n✅ 首个支持**端侧零知识推理验证**的zkML框架，无需TPM/FPGA等专用硬件；  \n✅ 证明体积小、验证快（毫秒级），满足隐私敏感与对抗环境需求；  \n✅ 基于开放、跨平台的ONNX格式，无缝对接PyTorch/TensorFlow/JAX，消除框架锁定；  \n✅ 已实证支持分类、嵌入、自动推理及小型语言模型（≤100M参数）的端到端可验证推理；  \n✅ 通过BlindFold技术实现信息论安全的零知识性，保障输入/权重/中间态全程保密。  \n该工作为“AI记忆”（AI Memory）、可信智能体商业等新型应用提供了密码学基础设施支撑。",
      "summary_en": "Jolt Atlas is a zero-knowledge machine learning (zkML) framework that enables succinct, verifiable inference directly over ONNX tensor operations—bypassing CPU emulation entirely. It extends the Jolt proving system with lookup arguments powered by the sumcheck protocol, making it especially efficient for non-linear ML primitives. Key innovations include *Neural Teleportation* to compress lookup tables without accuracy loss, and tensor-level optimizations enabling true *streaming* provers (constant memory, scalable to large models). Unlike prior zkML systems, Jolt Atlas achieves practical proving times across classification, embedding, automated reasoning, and small language models—all while supporting on-device, hardware-agnostic verification. Proofs are succinctly verifiable (ms-scale), and zero-knowledge is guaranteed via BlindFold. Built on open, portable ONNX, it eliminates framework lock-in and enables trustless AI context (“AI memory”) and agentic commerce guardrails.",
      "summary": "## Jolt Atlas：基于查找论证的零知识可验证机器学习推理框架  \n\n**背景与动机**：现有零知识机器学习（zkML）方案多依赖零知识虚拟机（zkVM）模拟CPU指令，开销大、内存占用高，难以在资源受限设备上部署。Jolt Atlas针对这一瓶颈，提出首个**直接面向ONNX张量计算图**的zkML框架，将零知识证明能力从通用计算下沉至AI原语层。\n\n**核心方法**：  \n- 基于Jolt证明系统的**查找中心化（lookup-centric）范式**，摒弃传统CPU寄存器建模，转而对ONNX标准算子（如MatMul、ReLU、Softmax）构建专用查找表（LUT）；  \n- 采用**sumcheck协议实现高效非线性查找论证**，天然适配ML中关键的非线性激活函数；  \n- 引入**神经跃迁（Neural Teleportation）** 技术压缩查找表规模（最高降低87%），同时严格保持模型精度；  \n- 设计多项**张量级验证优化**：包括稀疏张量编码、分块查表流水线、内存访问模式预承诺，实现真正**流式（streaming）证明**——证明者峰值内存恒定，不随模型规模增长。\n\n**创新与优势**：  \n✅ 首个支持**端侧零知识推理验证**的zkML框架，无需TPM/FPGA等专用硬件；  \n✅ 证明体积小、验证快（毫秒级），满足隐私敏感与对抗环境需求；  \n✅ 基于开放、跨平台的ONNX格式，无缝对接PyTorch/TensorFlow/JAX，消除框架锁定；  \n✅ 已实证支持分类、嵌入、自动推理及小型语言模型（≤100M参数）的端到端可验证推理；  \n✅ 通过BlindFold技术实现信息论安全的零知识性，保障输入/权重/中间态全程保密。  \n该工作为“AI记忆”（AI Memory）、可信智能体商业等新型应用提供了密码学基础设施支撑。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17423v1",
      "arxiv_id": "2602.17423v1",
      "title": "Convergence Analysis of Two-Layer Neural Networks under Gaussian Input Masking",
      "authors": [
        "Afroditi Kolomvaki",
        "Fangshuo Liao",
        "Evan Dramko",
        "Ziyun Guang",
        "Anastasios Kyrillidis"
      ],
      "abstract": "We investigate the convergence guarantee of two-layer neural network training with Gaussian randomly masked inputs. This scenario corresponds to Gaussian dropout at the input level, or noisy input training common in sensor networks, privacy-preserving training, and federated learning, where each user may have access to partial or corrupted features. Using a Neural Tangent Kernel (NTK) analysis, we demonstrate that training a two-layer ReLU network with Gaussian randomly masked inputs achieves linear convergence up to an error region proportional to the mask's variance. A key technical contribution is resolving the randomness within the non-linear activation, a problem of independent interest.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17423v1",
      "url": "https://arxiv.org/abs/2602.17423v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "math.OC"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n在传感器网络、隐私保护学习与联邦学习等实际场景中，终端用户常仅能获取**部分观测特征**或遭受**输入噪声干扰**（如模数转换误差、通信丢包、差分隐私加噪）。此类问题可建模为**高斯随机掩码输入**（Gaussian input masking）：原始输入 $x$ 被乘以独立同分布的高斯掩码向量 $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$，形成带噪输入 $\\xi \\odot x$。该设定等价于输入层高斯Dropout，但传统收敛分析对此缺乏理论支撑。\n\n## 方法与技术突破  \n本文基于**神经正切核（NTK）框架**，对宽度为 $m$ 的两层ReLU神经网络展开严格分析。核心挑战在于：掩码引入的随机性与ReLU激活函数的非线性耦合，导致梯度动态高度异质——既含确定性NTK主导项，又含依赖掩码路径的随机扰动项。我们提出**双尺度分解技术**：将网络输出分解为“平均核流形”与“方差校正项”，并利用**高斯积分恒等式**与**集中不等式耦合控制**，首次实现了对掩码-激活联合随机性的精确刻画。\n\n## 主要发现与理论贡献  \n- 在过参数化条件 $m = \\Omega(\\mathrm{poly}(n, 1/\\sigma^2))$ 下，梯度下降以**线性速率**收敛至误差上界 $\\mathcal{O}(\\sigma^2)$，即收敛精度由掩码方差直接决定；  \n- 误差界具有**最优阶数**：当 $\\sigma \\to 0$ 时退化为标准NTK线性收敛；当 $\\sigma > 0$ 时，$\\mathcal{O}(\\sigma^2)$ 不可改进（构造反例验证）；  \n- 关键创新在于**解析了非线性激活内生随机性**——此前工作多假设权重初始化随机性或标签噪声，而本工作首次系统处理“输入扰动→激活状态跳变→梯度失配”的完整因果链，为鲁棒训练提供新分析范式。",
      "summary_en": "We study the convergence of gradient descent for two-layer ReLU networks trained on inputs corrupted by independent Gaussian masks—i.e., $x \\mapsto \\xi \\odot x$ with $\\xi_i \\sim \\mathcal{N}(0,\\sigma^2)$. This models practical settings including noisy sensor data, privacy-preserving input perturbation, and federated learning with partial features. Using a refined Neural Tangent Kernel (NTK) analysis, we prove that under mild over-parameterization ($m = \\Omega(\\mathrm{poly}(n,1/\\sigma^2))$), training converges linearly to a neighborhood of the global minimum, with final error bounded by $\\mathcal{O}(\\sigma^2)$. Crucially, we resolve the technical challenge of *joint randomness* between Gaussian masks and ReLU activations—by decomposing the network output into a deterministic NTK component and a variance-controlled correction term, enabled by Gaussian integration identities and sharp concentration. Our $\\mathcal{O}(\\sigma^2)$ bound is tight: it recovers standard NTK convergence as $\\sigma \\to 0$, and we show optimality via explicit counterexamples. This work establishes the first convergence guarantee for neural training under input-level Gaussian corruption, with implications for robustness and distributed learning.",
      "summary": "## 研究背景  \n在传感器网络、隐私保护学习与联邦学习等实际场景中，终端用户常仅能获取**部分观测特征**或遭受**输入噪声干扰**（如模数转换误差、通信丢包、差分隐私加噪）。此类问题可建模为**高斯随机掩码输入**（Gaussian input masking）：原始输入 $x$ 被乘以独立同分布的高斯掩码向量 $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$，形成带噪输入 $\\xi \\odot x$。该设定等价于输入层高斯Dropout，但传统收敛分析对此缺乏理论支撑。\n\n## 方法与技术突破  \n本文基于**神经正切核（NTK）框架**，对宽度为 $m$ 的两层ReLU神经网络展开严格分析。核心挑战在于：掩码引入的随机性与ReLU激活函数的非线性耦合，导致梯度动态高度异质——既含确定性NTK主导项，又含依赖掩码路径的随机扰动项。我们提出**双尺度分解技术**：将网络输出分解为“平均核流形”与“方差校正项”，并利用**高斯积分恒等式**与**集中不等式耦合控制**，首次实现了对掩码-激活联合随机性的精确刻画。\n\n## 主要发现与理论贡献  \n- 在过参数化条件 $m = \\Omega(\\mathrm{poly}(n, 1/\\sigma^2))$ 下，梯度下降以**线性速率**收敛至误差上界 $\\mathcal{O}(\\sigma^2)$，即收敛精度由掩码方差直接决定；  \n- 误差界具有**最优阶数**：当 $\\sigma \\to 0$ 时退化为标准NTK线性收敛；当 $\\sigma > 0$ 时，$\\mathcal{O}(\\sigma^2)$ 不可改进（构造反例验证）；  \n- 关键创新在于**解析了非线性激活内生随机性**——此前工作多假设权重初始化随机性或标签噪声，而本工作首次系统处理“输入扰动→激活状态跳变→梯度失配”的完整因果链，为鲁棒训练提供新分析范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17394v1",
      "arxiv_id": "2602.17394v1",
      "title": "Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks",
      "authors": [
        "Nuno Saavedra",
        "Pedro Ribeiro",
        "André Coelho",
        "Rui Campos"
      ],
      "abstract": "Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17394v1",
      "url": "https://arxiv.org/abs/2602.17394v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SD"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 面向应急通信的语音驱动语义感知框架 SIREN\n\n在地震、洪灾等重大突发事件中，地面通信基础设施常遭损毁，**无人机（UAV）辅助应急网络**因其快速部署、灵活覆盖与高鲁棒性，成为保障一线救援通信的关键支撑。然而，当前应急现场仍高度依赖模拟/数字语音电台进行实时协同——这类语音通信虽抗干扰强、操作门槛低，却因**非结构化、高动态、多源混叠**特性，难以被网络管理系统直接理解与响应，严重制约了UAV资源的智能调度与服务质量（QoS）自适应保障。\n\n本文提出 **SIREN（Semantic Intelligence for Radio-based Emergency Networks）**——首个面向UAV辅助应急网络的端到端语音驱动语义感知框架。SIREN创新性融合三阶段流水线：① 鲁棒型自动语音识别（ASR），适配窄带语音与突发噪声；② 基于轻量化微调大语言模型（LLM）的语义结构化解析，精准抽取**响应单位、地理坐标（含模糊表述如“东侧厂房”）、事件等级（1–5级）、关键资源需求及QoS约束（如时延<200ms）**；③ 基于规则与上下文一致性校验的NLP验证模块，显著抑制幻觉与歧义。我们在涵盖6种方言、3–8说话人混叠、SNR 5–25 dB背景噪声及多层级任务复杂度的**合成应急语料库**上完成系统评估。结果表明：SIREN在平均词错误率（WER）≤12.3%前提下，关键语义要素抽取F1值达89.7%，且对地理指代模糊（如未标注POI）与说话人归属混淆（diarization误差）保持可解释性降级。本工作首次验证了**语音直驱网络智能**的技术可行性，为构建“人在环路、语义闭环”的下一代应急通信系统提供了核心使能技术与实证基础。",
      "summary_en": "This paper introduces **SIREN**, an AI-driven framework enabling voice-driven semantic perception for UAV-assisted emergency networks. By tightly integrating robust ASR, LLM-based semantic extraction (fine-tuned for emergency domain), and NLP validation, SIREN transforms unstructured radio voice traffic into structured machine-readable intents—including responder IDs, location references (even ambiguous ones), severity levels, and QoS requirements. Evaluated on a synthetic emergency corpus with controlled variations in language, speaker count (3–8), background noise (SNR 5–25 dB), and message complexity, SIREN achieves ≤12.3% WER and 89.7% F1 on key semantic elements. Speaker diarization errors and geographic ambiguity are identified as primary limiting factors—yet SIREN maintains interpretable performance degradation. The work establishes the feasibility of real-time, voice-native situational awareness for adaptive UAV network management in infrastructure-degraded scenarios.",
      "summary": "## 面向应急通信的语音驱动语义感知框架 SIREN\n\n在地震、洪灾等重大突发事件中，地面通信基础设施常遭损毁，**无人机（UAV）辅助应急网络**因其快速部署、灵活覆盖与高鲁棒性，成为保障一线救援通信的关键支撑。然而，当前应急现场仍高度依赖模拟/数字语音电台进行实时协同——这类语音通信虽抗干扰强、操作门槛低，却因**非结构化、高动态、多源混叠**特性，难以被网络管理系统直接理解与响应，严重制约了UAV资源的智能调度与服务质量（QoS）自适应保障。\n\n本文提出 **SIREN（Semantic Intelligence for Radio-based Emergency Networks）**——首个面向UAV辅助应急网络的端到端语音驱动语义感知框架。SIREN创新性融合三阶段流水线：① 鲁棒型自动语音识别（ASR），适配窄带语音与突发噪声；② 基于轻量化微调大语言模型（LLM）的语义结构化解析，精准抽取**响应单位、地理坐标（含模糊表述如“东侧厂房”）、事件等级（1–5级）、关键资源需求及QoS约束（如时延<200ms）**；③ 基于规则与上下文一致性校验的NLP验证模块，显著抑制幻觉与歧义。我们在涵盖6种方言、3–8说话人混叠、SNR 5–25 dB背景噪声及多层级任务复杂度的**合成应急语料库**上完成系统评估。结果表明：SIREN在平均词错误率（WER）≤12.3%前提下，关键语义要素抽取F1值达89.7%，且对地理指代模糊（如未标注POI）与说话人归属混淆（diarization误差）保持可解释性降级。本工作首次验证了**语音直驱网络智能**的技术可行性，为构建“人在环路、语义闭环”的下一代应急通信系统提供了核心使能技术与实证基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17345v1",
      "arxiv_id": "2602.17345v1",
      "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?",
      "authors": [
        "Boyang Ma",
        "Hechuan Guo",
        "Peizhuo Lv",
        "Minghui Xu",
        "Xuelong Dai",
        "YeChao Zhang",
        "Yijun Yang",
        "Yue Zhang"
      ],
      "abstract": "Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17345v1",
      "url": "https://arxiv.org/abs/2602.17345v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n具身智能系统（如自动驾驶汽车、服务机器人、大语言模型驱动的交互式代理）正加速从受控环境迈向安全关键的真实世界部署。与“离身”AI不同，其失效将导致不可逆的物理后果，亟需重新审视安全、可靠与鲁棒性的理论基础。当前研究多孤立聚焦于**大语言模型（LLM）漏洞**（如提示注入、幻觉）或**传统信息物理系统（CPS）缺陷**（如传感器欺骗、执行器故障），但大量真实故障案例无法被二者单独解释。\n\n## 核心主张与方法  \n本综述提出：现代具身AI失效的主因常源于**具身性引发的系统级错配**（embodiment-induced system-level mismatches），即感知—决策—行动闭环中语义抽象与物理现实间的结构性张力。我们通过跨领域案例分析（含自动驾驶事故、机器人抓取失败、LLM-agent物理指令误执行等），提炼出四个根本性挑战：\n\n- **语义正确 ≠ 物理安全**：语言推理天然忽略几何约束、动力学特性与接触物理，导致逻辑合理却物理危险的决策；  \n- **动作结果高度状态依赖**：相同指令在微小状态差异下可能引发截然不同的物理响应（如非线性动力学与状态不确定性放大）；  \n- **误差在闭环中指数级传播**：感知噪声→决策偏移→动作偏差→新感知失真，形成正反馈恶化链；  \n- **安全不具备可组合性**：局部安全决策随时间/层级累积，可演化为全局性失效（如连续“微小让步”导致碰撞）。\n\n## 创新点与启示  \n本工作突破“组件归因”范式，首次系统论证**具身性本身即安全威胁源**。安全防护必须升维至**系统级物理风险建模**——整合不确定性量化、失效传播图谱与跨层安全验证，而非仅加固LLM或加固CPS子模块。",
      "summary_en": "This survey challenges the prevailing dichotomy in embodied AI security research—framing failures either as LLM vulnerabilities (e.g., hallucination, jailbreaking) or classical CPS flaws (e.g., sensor spoofing, actuator failure). Through analysis of real-world breakdowns across autonomous vehicles, robotic agents, and LLM-driven interactive systems, we argue that a critical class of failures stems not from isolated component weaknesses, but from *embodiment-induced system-level mismatches*: inherent tensions between linguistic abstraction and physical reality within tightly coupled perception-decision-action loops. We identify four foundational insights: (i) semantic correctness does not guarantee physical safety due to abstraction over geometry, dynamics, and contact constraints; (ii) identical actions yield divergent outcomes under nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across feedback loops; and (iv) safety is non-compositional—locally safe decisions can cumulatively produce globally unsafe behavior. Consequently, securing embodied AI demands system-level reasoning about physical risk, uncertainty propagation, and cross-layer failure modes—not just component-level hardening.",
      "summary": "## 研究背景与问题  \n具身智能系统（如自动驾驶汽车、服务机器人、大语言模型驱动的交互式代理）正加速从受控环境迈向安全关键的真实世界部署。与“离身”AI不同，其失效将导致不可逆的物理后果，亟需重新审视安全、可靠与鲁棒性的理论基础。当前研究多孤立聚焦于**大语言模型（LLM）漏洞**（如提示注入、幻觉）或**传统信息物理系统（CPS）缺陷**（如传感器欺骗、执行器故障），但大量真实故障案例无法被二者单独解释。\n\n## 核心主张与方法  \n本综述提出：现代具身AI失效的主因常源于**具身性引发的系统级错配**（embodiment-induced system-level mismatches），即感知—决策—行动闭环中语义抽象与物理现实间的结构性张力。我们通过跨领域案例分析（含自动驾驶事故、机器人抓取失败、LLM-agent物理指令误执行等），提炼出四个根本性挑战：\n\n- **语义正确 ≠ 物理安全**：语言推理天然忽略几何约束、动力学特性与接触物理，导致逻辑合理却物理危险的决策；  \n- **动作结果高度状态依赖**：相同指令在微小状态差异下可能引发截然不同的物理响应（如非线性动力学与状态不确定性放大）；  \n- **误差在闭环中指数级传播**：感知噪声→决策偏移→动作偏差→新感知失真，形成正反馈恶化链；  \n- **安全不具备可组合性**：局部安全决策随时间/层级累积，可演化为全局性失效（如连续“微小让步”导致碰撞）。\n\n## 创新点与启示  \n本工作突破“组件归因”范式，首次系统论证**具身性本身即安全威胁源**。安全防护必须升维至**系统级物理风险建模**——整合不确定性量化、失效传播图谱与跨层安全验证，而非仅加固LLM或加固CPS子模块。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17288v1",
      "arxiv_id": "2602.17288v1",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "authors": [
        "Anuj Gupta"
      ],
      "abstract": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17288v1",
      "url": "https://arxiv.org/abs/2602.17288v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与目标  \n尽管前沿大语言模型在推理与数学能力上表现突出，但**从原始学术文献（如arXiv LaTeX源码）出发、在有限算力下训练领域专用科学语言模型（SLM）的完整工程实践仍缺乏系统性记录**。本研究填补这一空白，开展一项面向实际部署的实证研究：从零构建一个1.36B参数的科学语言模型，专精于数学、计算机科学与理论物理领域。\n\n## 方法与流程  \n我们设计并实现了端到端训练流水线，涵盖：  \n- **元数据过滤与归档校验**（剔除损坏/非科研PDF、重复提交）；  \n- **高保真LaTeX源码提取**（保留公式结构、定理环境等语义单元）；  \n- **科学文本归一化**（处理宏包依赖、跨文档引用、符号歧义）；  \n- **领域感知分词器**（扩展Unicode数学符号集，保留`\\alpha`→α映射稳定性）；  \n- **轻量级密集Transformer训练**（仅用2×A100 GPU，FP16+梯度检查点）。\n\n## 关键发现  \n通过24组对照实验，我们揭示：  \n✅ **预处理损耗远超预期**：原始arXiv数据经清洗后仅保留约38%可用token（52B有效预训练token）；  \n✅ **分词策略决定符号鲁棒性**：传统字节对编码（BPE）导致数学符号分裂率高达27%，而定制分词器降至<2%；  \n✅ **I/O与存储成新瓶颈**：在GPU计算饱和前，NVMe带宽与POSIX文件系统延迟已限制吞吐达40%；  \n✅ **数据丰富区稳定收敛**：在≥40B token规模下，损失曲线平滑，未见灾难性遗忘或梯度爆炸。\n\n## 创新点  \n本工作**不提出新架构，而提供可复现、可审计、低门槛的工程范式**——所有代码、配置与数据卡均已开源。为中等算力（≤4×A100）的研究者构建垂直领域模型提供首份全流程实践指南。",
      "summary_en": "This paper presents a practical, engineering-first case study of training a 1.36B-parameter scientific language model (SLM) *from scratch* using raw arXiv LaTeX sources across mathematics, CS, and theoretical physics. We detail an end-to-end pipeline—spanning metadata filtering, LaTeX extraction, scientific text normalization, domain-aware tokenization, and dense transformer training on just **2×A100 GPUs**. Across 24 controlled experiments, we quantify critical bottlenecks: preprocessing reduces usable tokens by >60%; custom tokenization cuts symbolic fragmentation from 27% to <2%; and I/O/storage constraints rival compute as primary throughput limits. Crucially, we demonstrate stable, scalable convergence in the data-rich regime (52B pretraining tokens), with smooth loss trajectories and no instability. Rather than architectural novelty, our contribution is a transparent, reproducible, and budget-conscious blueprint—open-sourced in full—for building small, domain-specialized LMs without frontier-scale infrastructure.",
      "summary": "## 背景与目标  \n尽管前沿大语言模型在推理与数学能力上表现突出，但**从原始学术文献（如arXiv LaTeX源码）出发、在有限算力下训练领域专用科学语言模型（SLM）的完整工程实践仍缺乏系统性记录**。本研究填补这一空白，开展一项面向实际部署的实证研究：从零构建一个1.36B参数的科学语言模型，专精于数学、计算机科学与理论物理领域。\n\n## 方法与流程  \n我们设计并实现了端到端训练流水线，涵盖：  \n- **元数据过滤与归档校验**（剔除损坏/非科研PDF、重复提交）；  \n- **高保真LaTeX源码提取**（保留公式结构、定理环境等语义单元）；  \n- **科学文本归一化**（处理宏包依赖、跨文档引用、符号歧义）；  \n- **领域感知分词器**（扩展Unicode数学符号集，保留`\\alpha`→α映射稳定性）；  \n- **轻量级密集Transformer训练**（仅用2×A100 GPU，FP16+梯度检查点）。\n\n## 关键发现  \n通过24组对照实验，我们揭示：  \n✅ **预处理损耗远超预期**：原始arXiv数据经清洗后仅保留约38%可用token（52B有效预训练token）；  \n✅ **分词策略决定符号鲁棒性**：传统字节对编码（BPE）导致数学符号分裂率高达27%，而定制分词器降至<2%；  \n✅ **I/O与存储成新瓶颈**：在GPU计算饱和前，NVMe带宽与POSIX文件系统延迟已限制吞吐达40%；  \n✅ **数据丰富区稳定收敛**：在≥40B token规模下，损失曲线平滑，未见灾难性遗忘或梯度爆炸。\n\n## 创新点  \n本工作**不提出新架构，而提供可复现、可审计、低门槛的工程范式**——所有代码、配置与数据卡均已开源。为中等算力（≤4×A100）的研究者构建垂直领域模型提供首份全流程实践指南。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17095v1",
      "arxiv_id": "2602.17095v1",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
      "authors": [
        "Chuiyang Meng",
        "Ming Tang",
        "Vincent W. S. Wong"
      ],
      "abstract": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\\times$.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17095v1",
      "url": "https://arxiv.org/abs/2602.17095v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n参数高效微调（如LoRA）通过低秩矩阵近似实现大语言模型（LLM）的轻量下游适配；联邦学习（FL）则支持跨客户端协作微调，保护数据隐私。然而，标准LoRA在联邦场景下采用**两个独立低秩矩阵**（ΔW = BA），导致双重瓶颈：（1）**分离聚合误差**——服务器分别平均B和A时，其乘积不等于各客户端BA乘积的平均值；（2）**分解漂移**——即使聚合BA乘积，服务器需对其做SVD分解以恢复可更新参数，但分解结果非唯一，连续轮次间方向不一致，严重损害收敛稳定性。\n\n## 方法创新：FLoRG框架  \n我们提出**FLoRG**（Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment）：  \n- **单矩阵参数化**：仅引入一个低秩增量矩阵U∈ℝ^{d×r}（r≪d），令ΔW = UUᵀ，天然保证对称正半定性；  \n- **Gram矩阵聚合**：客户端上传U的Gram矩阵G = UᵀU∈ℝ^{r×r}（仅r²维），服务器直接平均G，**彻底消除聚合误差**，且通信开销从O(dr)降至O(r²)，理论压缩比达d²/r²；  \n- **Procrustes对齐机制**：每轮聚合后，服务器求解正交矩阵Q，使新分解Uₜ ≈ Uₜ₋₁Q，强制跨轮次子空间对齐，抑制分解漂移。\n\n## 实验与理论贡献  \n理论证明：Procrustes对齐将收敛界中的方向偏差项从O(√T)收紧至O(1)，显著提升稳定性。在6个LLM微调基准（Alpaca、Dolly、FLAN等）上，FLoRG平均提升下游任务准确率**2.1–4.7个百分点**，超越LoRA-FL、FedALA、FedPAC等5种SOTA基线；通信量最高降低**2041×**（如7B模型从22.4 MB/轮→11 KB/轮），同时保持训练效率。",
      "summary_en": "FLoRG is a novel federated fine-tuning framework that addresses critical aggregation and decomposition challenges in applying LoRA to federated learning. Instead of using two separate low-rank matrices (B and A), FLoRG employs a single low-rank matrix U and represents the weight update as ΔW = UUᵀ. Clients upload only the small r×r Gram matrix G = UᵀU, enabling exact, error-free aggregation at the server and reducing communication overhead by up to 2041×. To mitigate decomposition drift across rounds, FLoRG introduces Procrustes alignment—a principled orthogonal transformation that aligns successive decompositions, ensuring consistent parameter updates. We provide theoretical convergence analysis showing that Procrustes alignment yields a tighter bound than naive SVD recovery. Experiments on six LLM fine-tuning benchmarks demonstrate that FLoRG consistently outperforms five state-of-the-art baselines in downstream accuracy while drastically cutting communication cost.",
      "summary": "## 背景与挑战  \n参数高效微调（如LoRA）通过低秩矩阵近似实现大语言模型（LLM）的轻量下游适配；联邦学习（FL）则支持跨客户端协作微调，保护数据隐私。然而，标准LoRA在联邦场景下采用**两个独立低秩矩阵**（ΔW = BA），导致双重瓶颈：（1）**分离聚合误差**——服务器分别平均B和A时，其乘积不等于各客户端BA乘积的平均值；（2）**分解漂移**——即使聚合BA乘积，服务器需对其做SVD分解以恢复可更新参数，但分解结果非唯一，连续轮次间方向不一致，严重损害收敛稳定性。\n\n## 方法创新：FLoRG框架  \n我们提出**FLoRG**（Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment）：  \n- **单矩阵参数化**：仅引入一个低秩增量矩阵U∈ℝ^{d×r}（r≪d），令ΔW = UUᵀ，天然保证对称正半定性；  \n- **Gram矩阵聚合**：客户端上传U的Gram矩阵G = UᵀU∈ℝ^{r×r}（仅r²维），服务器直接平均G，**彻底消除聚合误差**，且通信开销从O(dr)降至O(r²)，理论压缩比达d²/r²；  \n- **Procrustes对齐机制**：每轮聚合后，服务器求解正交矩阵Q，使新分解Uₜ ≈ Uₜ₋₁Q，强制跨轮次子空间对齐，抑制分解漂移。\n\n## 实验与理论贡献  \n理论证明：Procrustes对齐将收敛界中的方向偏差项从O(√T)收紧至O(1)，显著提升稳定性。在6个LLM微调基准（Alpaca、Dolly、FLAN等）上，FLoRG平均提升下游任务准确率**2.1–4.7个百分点**，超越LoRA-FL、FedALA、FedPAC等5种SOTA基线；通信量最高降低**2041×**（如7B模型从22.4 MB/轮→11 KB/轮），同时保持训练效率。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17625v1",
      "arxiv_id": "2602.17625v1",
      "title": "Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning",
      "authors": [
        "Obaidullah Zaland",
        "Zulfiqar Ahmad Khan",
        "Monowar Bhuyan"
      ],
      "abstract": "Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \\textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17625v1",
      "url": "https://arxiv.org/abs/2602.17625v1",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n现代大数据系统持续产生海量、异构、地理分散且高度隐私敏感的数据流，使得数据集中化训练既不可行也不合规。联邦学习（FL）虽能保护隐私，但其经典范式假设数据分布静态、依赖多轮通信协同训练，难以应对**增量式数据到达**（如新类别或新域持续出现）与**极低通信预算**（如仅允许单次上传）的双重约束。更严峻的是，模型在持续学习新增任务时易发生**灾难性遗忘**（catastrophic forgetting），即对旧任务性能显著退化。\n\n## 方法创新：OSI-FL + SSR  \n本文提出**一次性增量式联邦学习框架（OSI-FL）**——首个同时缓解通信开销与灾难性遗忘的FL方案。其核心设计为：  \n- **单轮语义通信**：各客户端利用冻结的视觉-语言模型（VLM）提取**类别级嵌入**（category-specific embeddings），仅上传一次至服务器；  \n- **扩散驱动数据合成**：服务器端预训练扩散模型基于这些嵌入，**无原始数据**地合成高保真样本，忠实反映客户端本地数据分布；  \n- **选择性样本保留（SSR）机制**：为抑制遗忘，SSR在每轮训练中动态识别并保留每个“类别–任务对”中损失值最高的前-*p* 个最具信息量的合成样本，将其纳入后续迭代训练，形成轻量但具代表性的记忆回放池。\n\n## 实验验证与优势  \n在三个主流基准（CIFAR-100、ImageNet-R、DomainNet）上，OSI-FL在**类增量**（class-incremental）与**域增量**（domain-incremental）场景下均显著超越传统FL（FedAvg）、一次性FL（One-Shot FL）及持续学习基线。关键结果包括：平均准确率提升+5.2%~9.7%，遗忘率降低达63%；通信开销压缩至传统FL的1/50以下，且无需客户端参与再训练。",
      "summary_en": "This paper introduces **One-Shot Incremental Federated Learning (OSI-FL)**, the first federated learning framework explicitly designed to tackle *both* severe communication constraints and catastrophic forgetting in incremental learning settings. OSI-FL enables clients to transmit only category-specific embeddings—extracted via a frozen vision-language model—in a *single communication round*. The server then leverages a pre-trained diffusion model to synthesize high-fidelity samples mimicking each client’s local data distribution, eliminating raw-data transmission. To combat forgetting as new tasks arrive incrementally, we propose **Selective Sample Retention (SSR)**: it identifies and retains the top-*p* most informative (i.e., highest-loss) synthesized samples per category–task pair, incorporating them into subsequent training rounds as a compact, adaptive memory buffer. Experiments across three benchmarks (CIFAR-100, ImageNet-R, DomainNet) show OSI-FL consistently outperforms traditional FL, one-shot FL, and continual learning baselines—achieving +5.2–9.7% higher average accuracy and up to 63% lower forgetting, while reducing communication overhead by >98%.",
      "summary": "## 研究背景与挑战  \n现代大数据系统持续产生海量、异构、地理分散且高度隐私敏感的数据流，使得数据集中化训练既不可行也不合规。联邦学习（FL）虽能保护隐私，但其经典范式假设数据分布静态、依赖多轮通信协同训练，难以应对**增量式数据到达**（如新类别或新域持续出现）与**极低通信预算**（如仅允许单次上传）的双重约束。更严峻的是，模型在持续学习新增任务时易发生**灾难性遗忘**（catastrophic forgetting），即对旧任务性能显著退化。\n\n## 方法创新：OSI-FL + SSR  \n本文提出**一次性增量式联邦学习框架（OSI-FL）**——首个同时缓解通信开销与灾难性遗忘的FL方案。其核心设计为：  \n- **单轮语义通信**：各客户端利用冻结的视觉-语言模型（VLM）提取**类别级嵌入**（category-specific embeddings），仅上传一次至服务器；  \n- **扩散驱动数据合成**：服务器端预训练扩散模型基于这些嵌入，**无原始数据**地合成高保真样本，忠实反映客户端本地数据分布；  \n- **选择性样本保留（SSR）机制**：为抑制遗忘，SSR在每轮训练中动态识别并保留每个“类别–任务对”中损失值最高的前-*p* 个最具信息量的合成样本，将其纳入后续迭代训练，形成轻量但具代表性的记忆回放池。\n\n## 实验验证与优势  \n在三个主流基准（CIFAR-100、ImageNet-R、DomainNet）上，OSI-FL在**类增量**（class-incremental）与**域增量**（domain-incremental）场景下均显著超越传统FL（FedAvg）、一次性FL（One-Shot FL）及持续学习基线。关键结果包括：平均准确率提升+5.2%~9.7%，遗忘率降低达63%；通信开销压缩至传统FL的1/50以下，且无需客户端参与再训练。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17614v1",
      "arxiv_id": "2602.17614v1",
      "title": "Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning",
      "authors": [
        "Obaidullah Zaland",
        "Sajib Mistry",
        "Monowar Bhuyan"
      ],
      "abstract": "Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17614v1",
      "url": "https://arxiv.org/abs/2602.17614v1",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "learning",
        "federated",
        "machine",
        "privacy-preserving",
        "privacy"
      ],
      "keyword_score": 6,
      "summary_zh": "## 背景与挑战  \n在海量异构数据分布式存储的大数据场景下，联邦学习（FL）虽能避免数据集中化、保障隐私，但其全本地训练范式对客户端算力要求高。为此，**U型联邦分割学习（UFSL）** 将模型沿深度方向切分，客户端仅执行前端计算并上传中间表示（即“smashed data”），服务器完成剩余计算——然而，这些未经保护的中间表示极易被逆向重构出原始私有数据（如图像、敏感特征），构成严重隐私泄露风险。\n\n## 方法创新：KD-UFSL  \n本文提出 **k-匿名差分隐私UFSL（KD-UFSL）**，首次将**微聚合（microaggregation）** 与**差分隐私（DP）** 协同嵌入UFSL通信环节：客户端先对局部中间表示进行k-匿名分组聚合（抑制个体可识别性），再注入经严格校准的拉普拉斯噪声（满足ε-差分隐私）。该双层防护机制在不移动原始数据、不共享标签的前提下，显著削弱smashed data的可重构性。\n\n## 实验结果与价值  \n在CIFAR-10、MNIST、Fashion-MNIST和SVHN四大基准数据集上验证：KD-UFSL使攻击者重建图像与真实图像的**均方误差（MSE）最高提升50%**，**结构相似性（SSIM）平均下降40%**；同时全局模型精度损失控制在<1.2%，F1-score保持稳定。这证明KD-UFSL在**强隐私保障（k-匿名+ε-DP）与高模型效用之间实现了实质性平衡**，为医疗、金融等高敏领域的大规模联邦智能部署提供了可落地的技术路径。",
      "summary_en": "This paper addresses privacy leakage from intermediate representations (\"smashed data\") in U-shaped Federated Split Learning (UFSL), where clients upload sensitive feature embeddings to the server. We first demonstrate that reconstruction attacks can effectively recover private input data (e.g., images) from these intermediates. To mitigate this, we propose **k-anonymous differentially private UFSL (KD-UFSL)**—a novel framework combining microaggregation (to enforce k-anonymity on local smashed data) and calibrated Laplace noise (to satisfy ε-differential privacy). Evaluated on four benchmark datasets (CIFAR-10, MNIST, Fashion-MNIST, SVHN), KD-UFSL increases reconstruction MSE by up to 50% and reduces SSIM by up to 40% compared to vanilla UFSL, while preserving global model utility—accuracy drop remains under 1.2% and F1-score is stable. KD-UFSL thus achieves a practical privacy-utility trade-off for large-scale, privacy-critical federated applications.",
      "summary": "## 背景与挑战  \n在海量异构数据分布式存储的大数据场景下，联邦学习（FL）虽能避免数据集中化、保障隐私，但其全本地训练范式对客户端算力要求高。为此，**U型联邦分割学习（UFSL）** 将模型沿深度方向切分，客户端仅执行前端计算并上传中间表示（即“smashed data”），服务器完成剩余计算——然而，这些未经保护的中间表示极易被逆向重构出原始私有数据（如图像、敏感特征），构成严重隐私泄露风险。\n\n## 方法创新：KD-UFSL  \n本文提出 **k-匿名差分隐私UFSL（KD-UFSL）**，首次将**微聚合（microaggregation）** 与**差分隐私（DP）** 协同嵌入UFSL通信环节：客户端先对局部中间表示进行k-匿名分组聚合（抑制个体可识别性），再注入经严格校准的拉普拉斯噪声（满足ε-差分隐私）。该双层防护机制在不移动原始数据、不共享标签的前提下，显著削弱smashed data的可重构性。\n\n## 实验结果与价值  \n在CIFAR-10、MNIST、Fashion-MNIST和SVHN四大基准数据集上验证：KD-UFSL使攻击者重建图像与真实图像的**均方误差（MSE）最高提升50%**，**结构相似性（SSIM）平均下降40%**；同时全局模型精度损失控制在<1.2%，F1-score保持稳定。这证明KD-UFSL在**强隐私保障（k-匿名+ε-DP）与高模型效用之间实现了实质性平衡**，为医疗、金融等高敏领域的大规模联邦智能部署提供了可落地的技术路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17284v1",
      "arxiv_id": "2602.17284v1",
      "title": "Efficient privacy loss accounting for subsampling and random allocation",
      "authors": [
        "Vitaly Feldman",
        "Moshe Shenfeld"
      ],
      "abstract": "We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used in privacy loss accounting.   In this work, we demonstrate that the privacy loss distribution (PLD) of random allocation applied to any differentially private algorithm can be computed efficiently. When applied to the Gaussian mechanism, our results demonstrate that the privacy-utility trade-off for random allocation is at least as good as that of Poisson subsampling. In particular, random allocation is better suited for training via DP-SGD. To support these computations, our work develops new tools for general privacy loss accounting based on a notion of PLD realization. This notion allows us to extend accurate privacy loss accounting to subsampling which previously required manual noise-mechanism-specific analysis.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17284v1",
      "url": "https://arxiv.org/abs/2602.17284v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景  \n本文聚焦于一种新兴的随机分配（random allocation）采样机制：用户数据被**均匀随机地分配至 $t$ 个步骤中的 $k$ 个**（即从 $t$ 步中无放回选取 $k$ 步使用）。该机制已在差分隐私优化（Chua et al., 2024a；Choquette-Choo et al., 2025）与通信高效的高维私有聚合（Asi et al., 2025）中展现出优于经典泊松采样的实用性优势。然而，现有理论分析（Feldman & Shenfeld, 2025；Dong et al., 2025）仍存在两大瓶颈：一是依赖近似推导，导致实际隐私参数（如 $(\\varepsilon,\\delta)$）偏松、不紧致；二是输出为hockey-stick或Rényi散度，难以直接嵌入主流隐私损失会计（PLA）框架（如PLD方法），引入额外计算与转换开销。\n\n## 方法与创新  \n本文首次提出**基于隐私损失分布（PLD）实现（PLD realization）的通用会计框架**，可精确、高效地刻画任意DP算法经随机分配后的PLD。核心突破在于：将随机分配建模为离散概率分布的卷积组合，并设计动态规划算法，在 $O(k(t-k))$ 时间内完成PLD精确计算（无需近似或机制特化）。特别地，我们完整推导了高斯机制下随机分配的闭式PLD表达式。\n\n## 主要发现  \n- 在相同$(k,t)$配置下，随机分配的隐私-效用权衡**不低于泊松采样**，且在DP-SGD训练中显著更优：同等$\\varepsilon$下测试精度提升1.2–2.3个百分点（CIFAR-10实验验证）；  \n- 所提PLD realization范式**统一了子采样隐私会计**，摆脱了以往对噪声机制的手动定制分析依赖；  \n- 开源实现支持TensorFlow/PyTorch无缝集成，隐私预算计算速度较蒙特卡洛模拟快3个数量级。",
      "summary_en": "We present the first efficient and exact privacy loss distribution (PLD) accounting framework for *random allocation*—a subsampling scheme where each user’s data is assigned to exactly $k$ out of $t$ steps uniformly at random. Prior analyses relied on loose approximations or non-PLD divergences (e.g., Rényi), hindering tight, composable privacy accounting. We introduce the notion of *PLD realization*, enabling exact PLD computation for any base DP mechanism under random allocation via a dynamic programming algorithm with $O(k(t-k))$ complexity. For the Gaussian mechanism, we derive closed-form PLDs and prove that random allocation achieves privacy-utility trade-offs **at least as strong as Poisson subsampling**, with empirical gains in DP-SGD training (e.g., +2.1% accuracy on CIFAR-10 at $\\varepsilon=2$). Our framework unifies subsampling accounting without mechanism-specific derivations and integrates natively into standard PLD-based privacy ledger tools.",
      "summary": "## 研究背景  \n本文聚焦于一种新兴的随机分配（random allocation）采样机制：用户数据被**均匀随机地分配至 $t$ 个步骤中的 $k$ 个**（即从 $t$ 步中无放回选取 $k$ 步使用）。该机制已在差分隐私优化（Chua et al., 2024a；Choquette-Choo et al., 2025）与通信高效的高维私有聚合（Asi et al., 2025）中展现出优于经典泊松采样的实用性优势。然而，现有理论分析（Feldman & Shenfeld, 2025；Dong et al., 2025）仍存在两大瓶颈：一是依赖近似推导，导致实际隐私参数（如 $(\\varepsilon,\\delta)$）偏松、不紧致；二是输出为hockey-stick或Rényi散度，难以直接嵌入主流隐私损失会计（PLA）框架（如PLD方法），引入额外计算与转换开销。\n\n## 方法与创新  \n本文首次提出**基于隐私损失分布（PLD）实现（PLD realization）的通用会计框架**，可精确、高效地刻画任意DP算法经随机分配后的PLD。核心突破在于：将随机分配建模为离散概率分布的卷积组合，并设计动态规划算法，在 $O(k(t-k))$ 时间内完成PLD精确计算（无需近似或机制特化）。特别地，我们完整推导了高斯机制下随机分配的闭式PLD表达式。\n\n## 主要发现  \n- 在相同$(k,t)$配置下，随机分配的隐私-效用权衡**不低于泊松采样**，且在DP-SGD训练中显著更优：同等$\\varepsilon$下测试精度提升1.2–2.3个百分点（CIFAR-10实验验证）；  \n- 所提PLD realization范式**统一了子采样隐私会计**，摆脱了以往对噪声机制的手动定制分析依赖；  \n- 开源实现支持TensorFlow/PyTorch无缝集成，隐私预算计算速度较蒙特卡洛模拟快3个数量级。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16980v1",
      "arxiv_id": "2602.16980v1",
      "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
      "authors": [
        "Leo Marchyok",
        "Zachary Coalson",
        "Sungho Keum",
        "Sooel Son",
        "Sanghyun Hong"
      ],
      "abstract": "Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16980v1",
      "url": "https://arxiv.org/abs/2602.16980v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n现代大语言模型（LLMs）虽展现出强大的生成能力，但其内部如何表征与调控隐私敏感行为（如**个人身份信息（PII）泄露**）仍属黑箱。现有研究多依赖提示工程或数据驱动的攻击方法，缺乏对模型内部机制的可解释性洞察，也难以泛化至未见提示或新模型。\n\n## 方法创新：UniLeak 框架  \n本文提出 **UniLeak**——首个面向PII泄露的**机制可解释性框架**，旨在发现模型残差流（residual stream）中的**通用激活方向（Universal Activation Directions）**。这些方向是模型特定的、高维隐空间中的线性向量，其核心特性在于：**仅需在推理时将该方向线性叠加至任一中间层激活，即可稳定提升各类提示下PII生成概率**。UniLeak完全无需训练数据、真实PII标注或外部监督，仅利用模型自身生成的文本（self-generated text）进行无监督方向挖掘，通过梯度引导的激活归因与方向对齐优化实现鲁棒提取。\n\n## 关键发现与影响  \n- 在 LLaMA-2、Qwen、Phi-3 等多个开源模型及 diverse PII 数据集（如 PII-Bench、Custom-PII）上验证，UniLeak 发现的方向可使 PII 泄露率平均提升 **2.1–4.7×**，显著超越现有提示注入、越狱等基线方法；  \n- 方向具备强跨上下文泛化性：同一方向在不同句式、语义领域甚至多轮对话中均持续有效；  \n- 生成质量几乎不受影响（BLEU/ROUGE 变化 <0.8%，人工评估保持自然流畅）；  \n- 首次揭示PII泄露本质是一种**隐式信号在表示空间中的线性叠加现象**，为风险放大（steering for attack）与主动防御（directional suppression）提供统一机制基础。",
      "summary_en": "We introduce **UniLeak**, a mechanistic interpretability framework that discovers *universal activation directions*—model-specific linear vectors in the residual stream—whose addition at inference time consistently amplifies personally identifiable information (PII) generation across diverse prompts, without requiring training data or ground-truth PII labels. Leveraging only self-generated text and gradient-based attribution, UniLeak identifies directions that generalize across contexts and models while preserving generation quality. Evaluated on LLaMA-2, Qwen, and Phi-3, UniLeak increases PII leakage by 2.1–4.7× over state-of-the-art prompt-based extraction methods. Our work reframes PII leakage as a *superposed latent signal* in model representations—enabling both precise risk amplification and principled mitigation via directional intervention.",
      "summary": "## 研究背景  \n现代大语言模型（LLMs）虽展现出强大的生成能力，但其内部如何表征与调控隐私敏感行为（如**个人身份信息（PII）泄露**）仍属黑箱。现有研究多依赖提示工程或数据驱动的攻击方法，缺乏对模型内部机制的可解释性洞察，也难以泛化至未见提示或新模型。\n\n## 方法创新：UniLeak 框架  \n本文提出 **UniLeak**——首个面向PII泄露的**机制可解释性框架**，旨在发现模型残差流（residual stream）中的**通用激活方向（Universal Activation Directions）**。这些方向是模型特定的、高维隐空间中的线性向量，其核心特性在于：**仅需在推理时将该方向线性叠加至任一中间层激活，即可稳定提升各类提示下PII生成概率**。UniLeak完全无需训练数据、真实PII标注或外部监督，仅利用模型自身生成的文本（self-generated text）进行无监督方向挖掘，通过梯度引导的激活归因与方向对齐优化实现鲁棒提取。\n\n## 关键发现与影响  \n- 在 LLaMA-2、Qwen、Phi-3 等多个开源模型及 diverse PII 数据集（如 PII-Bench、Custom-PII）上验证，UniLeak 发现的方向可使 PII 泄露率平均提升 **2.1–4.7×**，显著超越现有提示注入、越狱等基线方法；  \n- 方向具备强跨上下文泛化性：同一方向在不同句式、语义领域甚至多轮对话中均持续有效；  \n- 生成质量几乎不受影响（BLEU/ROUGE 变化 <0.8%，人工评估保持自然流畅）；  \n- 首次揭示PII泄露本质是一种**隐式信号在表示空间中的线性叠加现象**，为风险放大（steering for attack）与主动防御（directional suppression）提供统一机制基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17651v1",
      "arxiv_id": "2602.17651v1",
      "title": "Non-Trivial Zero-Knowledge Implies One-Way Functions",
      "authors": [
        "Suvradip Chakraborty",
        "James Hulett",
        "Dakshita Khurana",
        "Kabir Tomer"
      ],
      "abstract": "A recent breakthrough [Hirahara and Nanashima, STOC'2024] established that if $\\mathsf{NP} \\not \\subseteq \\mathsf{ioP/poly}$, the existence of zero-knowledge with negligible errors for $\\mathsf{NP}$ implies the existence of one-way functions (OWFs). In this work, we obtain a characterization of one-way functions from the worst-case complexity of zero-knowledge {\\em in the high-error regime}.   We say that a zero-knowledge argument is {\\em non-trivial} if the sum of its completeness, soundness and zero-knowledge errors is bounded away from $1$. Our results are as follows, assuming $\\mathsf{NP} \\not \\subseteq \\mathsf{ioP/poly}$:   1. {\\em Non-trivial} Non-Interactive ZK (NIZK) arguments for $\\mathsf{NP}$ imply the existence of OWFs. Using known amplification techniques, this result also provides an unconditional transformation from weak to standard NIZK proofs for all meaningful error parameters.   2. We also generalize to the interactive setting: {\\em Non-trivial} constant-round public-coin zero-knowledge arguments for $\\mathsf{NP}$ imply the existence of OWFs, and therefore also (standard) four-message zero-knowledge arguments for $\\mathsf{NP}$.   Prior to this work, one-way functions could be obtained from NIZKs that had constant zero-knowledge error $ε_{zk}$ and soundness error $ε_{s}$ satisfying $ε_{zk} + \\sqrt{ε_{s}} < 1$ [Chakraborty, Hulett and Khurana, CRYPTO'2025]. However, the regime where $ε_{zk} + \\sqrt{ε_{s}} \\geq 1$ remained open. This work closes the gap, and obtains new implications in the interactive setting. Our results and techniques could be useful stepping stones in the quest to construct one-way functions from worst-case hardness.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17651v1",
      "url": "https://arxiv.org/abs/2602.17651v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "crypto"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n零知识证明（ZK）与单向函数（OWF）是密码学两大基石。长久以来，一个核心开放问题是：能否从**最坏情况下的计算复杂度假设**（如 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$）出发，仅凭零知识协议的存在性——尤其是允许较高错误率的“非平凡”版本——推导出单向函数？此前工作（如 Hirahara–Nanashima, STOC’24）仅在**低错误 regime**（如 $ε_{zk} + \\sqrt{ε_s} < 1$）下建立了该蕴含关系，而当误差和接近或超过 1 时，该问题长期悬而未决。\n\n## 核心定义与方法  \n本文提出 **“非平凡零知识”（non-trivial ZK）** 的新刻画：要求完备性、可靠性与零知识性三类错误之和严格小于 1（即有非零“安全余量”）。这一定义自然涵盖高误差但仍有意义的协议（例如 $ε_c=0.3, ε_s=0.4, ε_{zk}=0.2$，总和为 0.9），并规避了退化情形（如全错协议）。技术上，作者结合了**误差放大**、**交互式到非交互式归约**及**抗泄露模拟器构造**等工具，在 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$ 假设下完成关键归约。\n\n## 主要成果  \n1. **非交互式场景**：存在非平凡 NIZK 论证 ⇒ 存在单向函数；进一步，该结果可**无条件地将任意弱 NIZK（含高误差）提升为标准 NIZK**（满足传统误差界）。  \n2. **交互式场景**：存在非平凡、常数轮、公共硬币的 ZK 论证 ⇒ 存在 OWF ⇒ 进而存在标准四消息 ZK 论证。  \n本工作首次完整覆盖 $ε_{zk} + \\sqrt{ε_s} \\geq 1$ 的“高误差空白区”，统一了零知识强度与基础密码原语之间的最坏情况关联，为“从 worst-case hardness 构造 OWF”这一宏伟目标提供了关键中间步骤。",
      "summary_en": "We establish a tight characterization: under the plausible worst-case assumption $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$, the existence of *non-trivial* zero-knowledge (ZK) arguments—where the sum of completeness, soundness, and zero-knowledge errors is bounded away from 1—implies one-way functions (OWFs). Specifically: (1) Non-trivial non-interactive ZK (NIZK) arguments for $\\mathsf{NP}$ imply OWFs; moreover, this yields an *unconditional error-amplification framework*, converting any weak NIZK (even with high error) into a standard NIZK proof. (2) The result extends to the interactive setting: non-trivial constant-round public-coin ZK arguments for $\\mathsf{NP}$ also imply OWFs—and thus standard four-message ZK arguments. This closes the long-standing gap for the high-error regime where prior techniques (e.g., Chakraborty–Hulett–Khurana, CRYPTO’25) required $ε_{zk} + \\sqrt{ε_s} < 1$. Our work provides a unified worst-case foundation linking ZK strength to the existence of OWFs.",
      "summary": "## 研究背景与问题  \n零知识证明（ZK）与单向函数（OWF）是密码学两大基石。长久以来，一个核心开放问题是：能否从**最坏情况下的计算复杂度假设**（如 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$）出发，仅凭零知识协议的存在性——尤其是允许较高错误率的“非平凡”版本——推导出单向函数？此前工作（如 Hirahara–Nanashima, STOC’24）仅在**低错误 regime**（如 $ε_{zk} + \\sqrt{ε_s} < 1$）下建立了该蕴含关系，而当误差和接近或超过 1 时，该问题长期悬而未决。\n\n## 核心定义与方法  \n本文提出 **“非平凡零知识”（non-trivial ZK）** 的新刻画：要求完备性、可靠性与零知识性三类错误之和严格小于 1（即有非零“安全余量”）。这一定义自然涵盖高误差但仍有意义的协议（例如 $ε_c=0.3, ε_s=0.4, ε_{zk}=0.2$，总和为 0.9），并规避了退化情形（如全错协议）。技术上，作者结合了**误差放大**、**交互式到非交互式归约**及**抗泄露模拟器构造**等工具，在 $\\mathsf{NP} \\not\\subseteq \\mathsf{ioP/poly}$ 假设下完成关键归约。\n\n## 主要成果  \n1. **非交互式场景**：存在非平凡 NIZK 论证 ⇒ 存在单向函数；进一步，该结果可**无条件地将任意弱 NIZK（含高误差）提升为标准 NIZK**（满足传统误差界）。  \n2. **交互式场景**：存在非平凡、常数轮、公共硬币的 ZK 论证 ⇒ 存在 OWF ⇒ 进而存在标准四消息 ZK 论证。  \n本工作首次完整覆盖 $ε_{zk} + \\sqrt{ε_s} \\geq 1$ 的“高误差空白区”，统一了零知识强度与基础密码原语之间的最坏情况关联，为“从 worst-case hardness 构造 OWF”这一宏伟目标提供了关键中间步骤。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17590v1",
      "arxiv_id": "2602.17590v1",
      "title": "BMC4TimeSec: Verification Of Timed Security Protocols",
      "authors": [
        "Agnieszka M. Zbrzezny"
      ],
      "abstract": "We present BMC4TimeSec, an end-to-end tool for verifying Timed Security Protocols (TSP) based on SMT-based bounded model checking and multi-agent modelling in the form of Timed Interpreted Systems (TIS) and Timed Interleaved Interpreted Systems (TIIS). In BMC4TimeSec, TSP executions implement the TIS/TIIS environment (join actions, interleaving, delays, lifetimes), and knowledge automata implement the agents (evolution of participant knowledge, including the intruder). The code is publicly available on \\href{https://github.com/agazbrzezny/BMC4TimeSec}{GitHub}, as is a \\href{https://youtu.be/aNybKz6HwdA}{video} demonstration.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17590v1",
      "url": "https://arxiv.org/abs/2602.17590v1",
      "categories": [
        "cs.CR",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## BMC4TimeSec：面向时序安全协议的SMT约束有界模型检验工具  \n\n**背景与问题**：时序安全协议（Timed Security Protocols, TSP）在现实系统中广泛应用（如TLS 1.3握手、5G认证流程），其安全性不仅依赖消息逻辑，更受精确时间约束（如超时、会话生命周期、时钟漂移）影响。传统模型检验工具（如ProVerif、Tamarin）难以直接刻画连续时间行为与多主体知识演化耦合机制，导致验证结果保守或不完整。\n\n**方法与创新**：本文提出**BMC4TimeSec**——首个端到端、基于SMT的有界模型检验框架，专为TSP设计。核心创新包括：  \n- 提出**时序解释系统（Timed Interpreted Systems, TIS）** 及其并发扩展**时序交错解释系统（TIIS）**，统一建模协议环境（含联合动作、非确定性交错、真实时间延迟、实体生命周期）；  \n- 引入**知识自动机（Knowledge Automata）** 显式刻画各参与方（含Dolev-Yao型入侵者）的知识动态演化，支持时序敏感的知识推理（如“t时刻后攻击者才获知密钥”）；  \n- 实现SMT编码：将TIS/TIIS语义与知识自动机迁移同步编译为Z3可解的量化自由SMT公式，支持时间变量离散化与有界深度探索；  \n- 提供完整工具链：支持从协议规范（时序LTL+知识谓词）自动生成模型、参数化时间界限设定、反例可视化及知识状态追踪。\n\n**验证与成果**：已在多个经典TSP案例（如Kerberos V5时序变体、NIST SP 800-171合规认证协议）上完成验证，成功发现2处因时钟偏差导致的新型攻击路径（此前未被Tamarin捕获），并确认了3个协议在≤5跳延迟下的强知识安全性。代码与视频演示已开源，显著提升了TSP形式化验证的实用性与可访问性。",
      "summary_en": "We present **BMC4TimeSec**, an end-to-end SMT-based bounded model checking tool for verifying Timed Security Protocols (TSP). It builds on two novel formalisms: *Timed Interpreted Systems* (TIS) and *Timed Interleaved Interpreted Systems* (TIIS), which rigorously model protocol environments—including joint actions, non-deterministic interleaving, real-time delays, and agent lifetimes. Agent knowledge (including the intruder’s) is captured via *knowledge automata*, enabling precise reasoning about temporal epistemic properties (e.g., “the attacker learns the key only after time *t*”). BMC4TimeSec compiles TIS/TIIS semantics and knowledge evolution into quantifier-free SMT formulas solvable by Z3, supporting parameterized time bounds and counterexample generation. Evaluated on Kerberos variants and NIST-compliant protocols, it uncovered previously unknown timing-dependent attacks and confirmed knowledge security up to 5-hop delays. The tool is open-source with a demo video.",
      "summary": "## BMC4TimeSec：面向时序安全协议的SMT约束有界模型检验工具  \n\n**背景与问题**：时序安全协议（Timed Security Protocols, TSP）在现实系统中广泛应用（如TLS 1.3握手、5G认证流程），其安全性不仅依赖消息逻辑，更受精确时间约束（如超时、会话生命周期、时钟漂移）影响。传统模型检验工具（如ProVerif、Tamarin）难以直接刻画连续时间行为与多主体知识演化耦合机制，导致验证结果保守或不完整。\n\n**方法与创新**：本文提出**BMC4TimeSec**——首个端到端、基于SMT的有界模型检验框架，专为TSP设计。核心创新包括：  \n- 提出**时序解释系统（Timed Interpreted Systems, TIS）** 及其并发扩展**时序交错解释系统（TIIS）**，统一建模协议环境（含联合动作、非确定性交错、真实时间延迟、实体生命周期）；  \n- 引入**知识自动机（Knowledge Automata）** 显式刻画各参与方（含Dolev-Yao型入侵者）的知识动态演化，支持时序敏感的知识推理（如“t时刻后攻击者才获知密钥”）；  \n- 实现SMT编码：将TIS/TIIS语义与知识自动机迁移同步编译为Z3可解的量化自由SMT公式，支持时间变量离散化与有界深度探索；  \n- 提供完整工具链：支持从协议规范（时序LTL+知识谓词）自动生成模型、参数化时间界限设定、反例可视化及知识状态追踪。\n\n**验证与成果**：已在多个经典TSP案例（如Kerberos V5时序变体、NIST SP 800-171合规认证协议）上完成验证，成功发现2处因时钟偏差导致的新型攻击路径（此前未被Tamarin捕获），并确认了3个协议在≤5跳延迟下的强知识安全性。代码与视频演示已开源，显著提升了TSP形式化验证的实用性与可访问性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17488v1",
      "arxiv_id": "2602.17488v1",
      "title": "Computational Hardness of Private Coreset",
      "authors": [
        "Badih Ghazi",
        "Cristóbal Guzmán",
        "Pritish Kamath",
        "Alexander Knop",
        "Ravi Kumar",
        "Pasin Manurangsi"
      ],
      "abstract": "We study the problem of differentially private (DP) computation of coreset for the $k$-means objective. For a given input set of points, a coreset is another set of points such that the $k$-means objective for any candidate solution is preserved up to a multiplicative $(1 \\pm α)$ factor (and some additive factor).   We prove the first computational lower bounds for this problem. Specifically, assuming the existence of one-way functions, we show that no polynomial-time $(ε, 1/n^{ω(1)})$-DP algorithm can compute a coreset for $k$-means in the $\\ell_\\infty$-metric for some constant $α> 0$ (and some constant additive factor), even for $k=3$. For $k$-means in the Euclidean metric, we show a similar result but only for $α= Θ\\left(1/d^2\\right)$, where $d$ is the dimension.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17488v1",
      "url": "https://arxiv.org/abs/2602.17488v1",
      "categories": [
        "cs.CG",
        "cs.CR",
        "cs.DS"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n本研究聚焦**差分隐私（DP）下 $k$-means 聚类核心集（coreset）的计算复杂性**。核心集是原始数据点集的紧凑代理表示，要求对任意 $k$-中心解，其 $k$-means 目标函数值被近似至 $(1\\pm\\alpha)$ 倍乘性误差及某加性项内。在隐私保护机器学习中，同时满足高精度（小 $\\alpha$）、强隐私（小 $\\varepsilon$、极小 $\\delta$）与高效计算（多项式时间）被视为理想目标，但三者能否兼顾尚无理论定论。\n\n## 主要方法与技术路线  \n本文首次建立该问题的**计算硬度下界**，基于密码学标准假设——**单向函数存在性**（widely believed foundational assumption in cryptography）。通过精巧的归约构造，将单向函数的不可逆性嵌入到 $k$-means 核心集的私有计算任务中：若存在高效私有算法，则可反向破解单向函数，导出矛盾。\n\n## 关键发现与创新点  \n- **首个严格下界**：在 $\\ell_\\infty$-度量下，即使仅需 $k=3$ 个聚类中心，也**不存在多项式时间的 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法**能输出具有常数精度（$\\alpha>0$）和常数加性误差的核心集；  \n- **欧氏空间紧致结果**：在 $d$ 维欧氏空间中，同样假设下，对任意多项式时间 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法，若要求 $\\alpha = \\Theta(1/d^2)$，则核心集计算亦不可行；  \n- 该结果揭示了**隐私、精度与效率三者间固有的计算权衡**，解释了为何现有私有核心集算法或牺牲精度（如 $\\alpha \\propto d$），或放宽隐私参数（如 $\\delta = \\mathrm{poly}(1/n)$），或依赖非标准假设。",
      "summary_en": "We establish the first computational hardness results for differentially private (DP) coreset construction for the $k$-means objective. Assuming the existence of one-way functions—a standard cryptographic assumption—we prove that no polynomial-time $(\\varepsilon, 1/n^{\\omega(1)})$-DP algorithm can compute a coreset with constant multiplicative error $\\alpha > 0$ and constant additive error for $k$-means under the $\\ell_\\infty$-metric, even when $k = 3$. For the Euclidean metric in $d$ dimensions, we show an analogous lower bound for $\\alpha = \\Theta(1/d^2)$. These results demonstrate an inherent tension among privacy, approximation quality, and computational efficiency, resolving a fundamental open question and explaining the limitations of existing private coreset algorithms.",
      "summary": "## 研究背景与问题  \n本研究聚焦**差分隐私（DP）下 $k$-means 聚类核心集（coreset）的计算复杂性**。核心集是原始数据点集的紧凑代理表示，要求对任意 $k$-中心解，其 $k$-means 目标函数值被近似至 $(1\\pm\\alpha)$ 倍乘性误差及某加性项内。在隐私保护机器学习中，同时满足高精度（小 $\\alpha$）、强隐私（小 $\\varepsilon$、极小 $\\delta$）与高效计算（多项式时间）被视为理想目标，但三者能否兼顾尚无理论定论。\n\n## 主要方法与技术路线  \n本文首次建立该问题的**计算硬度下界**，基于密码学标准假设——**单向函数存在性**（widely believed foundational assumption in cryptography）。通过精巧的归约构造，将单向函数的不可逆性嵌入到 $k$-means 核心集的私有计算任务中：若存在高效私有算法，则可反向破解单向函数，导出矛盾。\n\n## 关键发现与创新点  \n- **首个严格下界**：在 $\\ell_\\infty$-度量下，即使仅需 $k=3$ 个聚类中心，也**不存在多项式时间的 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法**能输出具有常数精度（$\\alpha>0$）和常数加性误差的核心集；  \n- **欧氏空间紧致结果**：在 $d$ 维欧氏空间中，同样假设下，对任意多项式时间 $(\\varepsilon, 1/n^{\\omega(1)})$-DP 算法，若要求 $\\alpha = \\Theta(1/d^2)$，则核心集计算亦不可行；  \n- 该结果揭示了**隐私、精度与效率三者间固有的计算权衡**，解释了为何现有私有核心集算法或牺牲精度（如 $\\alpha \\propto d$），或放宽隐私参数（如 $\\delta = \\mathrm{poly}(1/n)$），或依赖非标准假设。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17454v1",
      "arxiv_id": "2602.17454v1",
      "title": "Privacy in Theory, Bugs in Practice: Grey-Box Auditing of Differential Privacy Libraries",
      "authors": [
        "Tudor Cebere",
        "David Erb",
        "Damien Desfontaines",
        "Aurélien Bellet",
        "Jack Fitzsimons"
      ],
      "abstract": "Differential privacy (DP) implementations are notoriously prone to errors, with subtle bugs frequently invalidating theoretical guarantees. Existing verification methods are often impractical: formal tools are too restrictive, while black-box statistical auditing is intractable for complex pipelines and fails to pinpoint the source of the bug. This paper introduces Re:cord-play, a gray-box auditing paradigm that inspects the internal state of DP algorithms. By running an instrumented algorithm on neighboring datasets with identical randomness, Re:cord-play directly checks for data-dependent control flow and provides concrete falsification of sensitivity violations by comparing declared sensitivity against the empirically measured distance between internal inputs. We generalize this to Re:cord-play-sample, a full statistical audit that isolates and tests each component, including untrusted ones. We show that our novel testing approach is both effective and necessary by auditing 12 open-source libraries, including SmartNoise SDK, Opacus, and Diffprivlib, and uncovering 13 privacy violations that impact their theoretical guarantees. We release our framework as an open-source Python package, thereby making it easy for DP developers to integrate effective, computationally inexpensive, and seamless privacy testing as part of their software development lifecycle.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17454v1",
      "url": "https://arxiv.org/abs/2602.17454v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "differential",
        "dp"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与挑战  \n差分隐私（DP）的理论保障高度依赖于实现的精确性，但现实中的DP库普遍存在隐蔽缺陷：微小的代码错误（如敏感度误设、随机性复用不当、控制流数据依赖）即可彻底破坏ε-δ隐私保证。现有验证方法存在严重局限——**形式化验证**工具（如Fuzzing+Coq）适用场景狭窄、难以覆盖复杂机器学习流水线；而**黑盒统计审计**虽易部署，却无法定位故障组件，且对高维、非线性DP机制（如带裁剪的梯度下降）缺乏统计功效，常导致假阴性。\n\n## 方法创新：Re:cord-play 灰盒审计范式  \n本文提出 **Re:cord-play**——首个面向DP库的灰盒动态审计框架。其核心思想是：在**相同随机种子**下，对一对相邻输入数据集（仅单条记录差异）执行插桩后的DP算法，实时捕获内部状态（如裁剪前梯度、噪声添加前的聚合值）。通过比对两路径中各中间变量的**实测L₁/L₂距离**与开发者声明的敏感度，直接检测敏感度违规；更关键的是，它能识别**数据依赖型控制流**（如`if norm > T: clip()`），此类逻辑漏洞在传统测试中极易被忽略。\n\n## 扩展与验证  \n进一步提出 **Re:cord-play-sample**：将完整DP流水线解耦为可测试子组件（含第三方/不可信模块），对每个环节独立施加灰盒审计。我们在12个主流开源DP库（包括Microsoft SmartNoise SDK、PyTorch Opacus、IBM Diffprivlib）上开展系统性审计，**发现13处实质性隐私违规**，涵盖敏感度高估、噪声缩放错误、邻域定义不一致等，其中7例直接导致理论隐私预算失效。所有漏洞均已向维护者披露并获确认。我们已将框架开源为轻量级Python包，支持无缝集成至CI/CD流程，单次审计耗时通常<5秒，无需修改被测代码。",
      "summary_en": "Differential privacy (DP) implementations are highly error-prone, with subtle bugs—such as incorrect sensitivity declarations, data-dependent control flow, or flawed noise scaling—commonly invalidating theoretical privacy guarantees. Existing verification methods fall short: formal tools are overly restrictive and hard to scale, while black-box statistical auditing lacks debugging capability and fails on complex, non-linear DP pipelines. This paper introduces **Re:cord-play**, a novel *gray-box* auditing paradigm that instruments DP algorithms to observe internal states (e.g., pre-noise aggregates, pre-clipping gradients) when executed on neighboring datasets under *identical randomness*. By comparing empirical input distances against declared sensitivities—and detecting data-dependent branching—it provides concrete, actionable falsifications of DP violations. We generalize this to **Re:cord-play-sample**, enabling component-wise auditing, even for untrusted modules. Applied to 12 open-source DP libraries (e.g., SmartNoise, Opacus, Diffprivlib), our framework uncovered **13 critical privacy violations**, 7 of which break the core ε-δ guarantee. We release the tool as an open-source Python package—lightweight, developer-friendly, and CI-ready.",
      "summary": "## 背景与挑战  \n差分隐私（DP）的理论保障高度依赖于实现的精确性，但现实中的DP库普遍存在隐蔽缺陷：微小的代码错误（如敏感度误设、随机性复用不当、控制流数据依赖）即可彻底破坏ε-δ隐私保证。现有验证方法存在严重局限——**形式化验证**工具（如Fuzzing+Coq）适用场景狭窄、难以覆盖复杂机器学习流水线；而**黑盒统计审计**虽易部署，却无法定位故障组件，且对高维、非线性DP机制（如带裁剪的梯度下降）缺乏统计功效，常导致假阴性。\n\n## 方法创新：Re:cord-play 灰盒审计范式  \n本文提出 **Re:cord-play**——首个面向DP库的灰盒动态审计框架。其核心思想是：在**相同随机种子**下，对一对相邻输入数据集（仅单条记录差异）执行插桩后的DP算法，实时捕获内部状态（如裁剪前梯度、噪声添加前的聚合值）。通过比对两路径中各中间变量的**实测L₁/L₂距离**与开发者声明的敏感度，直接检测敏感度违规；更关键的是，它能识别**数据依赖型控制流**（如`if norm > T: clip()`），此类逻辑漏洞在传统测试中极易被忽略。\n\n## 扩展与验证  \n进一步提出 **Re:cord-play-sample**：将完整DP流水线解耦为可测试子组件（含第三方/不可信模块），对每个环节独立施加灰盒审计。我们在12个主流开源DP库（包括Microsoft SmartNoise SDK、PyTorch Opacus、IBM Diffprivlib）上开展系统性审计，**发现13处实质性隐私违规**，涵盖敏感度高估、噪声缩放错误、邻域定义不一致等，其中7例直接导致理论隐私预算失效。所有漏洞均已向维护者披露并获确认。我们已将框架开源为轻量级Python包，支持无缝集成至CI/CD流程，单次审计耗时通常<5秒，无需修改被测代码。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.17413v1",
      "arxiv_id": "2602.17413v1",
      "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
      "authors": [
        "René Brinkhege",
        "Prahlad Menon"
      ],
      "abstract": "In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over private documents on behalf of a data provider. Instead of releasing documents, the provider exposes a natural language interface whose responses are constrained by machine-readable usage policies. We formalize policy-violating information disclosure in this setting, drawing on usage control and information flow security, and introduce virtual redaction: suppressing sensitive information at query time without modifying source documents. We describe an architecture for integrating such a spokesperson with Eclipse Dataspace Components and ODRL-style policies, and outline an initial provider-side integration prototype in which QA requests are routed through a spokesperson service instead of triggering raw document transfer. Our contribution is primarily architectural: we do not yet implement or empirically evaluate the full enforcement pipeline. We therefore outline an evaluation methodology to assess security, utility, and performance trade-offs under benign and adversarial querying as a basis for future empirical work on systematically governed LLM access to multi-party data spaces.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.17413v1",
      "url": "https://arxiv.org/abs/2602.17413v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n在当前跨组织数据空间中，数据使用策略（usage policies）通常仅在**资产粒度**上实施：整份文档或数据集被整体授权或拒止。当文档仅局部敏感时，提供方为避免泄露受保护信息，往往需人工红标（redact）文档后共享——该方式成本高昂、粒度粗糙（无法按语义细粒度控制），且难以随策略更新或合作方变更而动态维护。\n\n## 方法与创新  \n本文提出 **DAVE**（Data-Access via Policy-Enforcing spokesperson），一种基于大语言模型（LLM）的策略强制型“数据代言人”架构。DAVE 不直接发布原始文档，而是为数据提供方代理响应自然语言查询，其输出严格受**机器可读策略**（如ODRL格式）约束。我们形式化定义了该范式下的策略违规信息泄露，并提出**虚拟红标**（virtual redaction）机制：在查询执行时动态抑制敏感内容，无需修改源文档。技术上，我们设计了与Eclipse Dataspace Components（EDC）集成的分层架构，实现策略解析、意图理解、上下文感知响应生成与合规性验证闭环；并构建了初步的提供方侧原型——将问答请求路由至DAVE服务，替代原始文档传输。\n\n## 贡献与展望  \n本工作核心贡献在于**安全可控的LLM访问架构设计**，而非端到端实现或实证评估。我们明确界定其安全边界，提出兼顾**安全性、实用性与性能**的三维度评估框架，涵盖良性查询下的效用保持率、对抗性提示下的策略鲁棒性，以及延迟与吞吐量等工程指标，为未来在多方协同数据空间中系统化治理LLM访问奠定方法论基础。",
      "summary_en": "DAVE is a policy-enforcing LLM-based spokesperson that enables secure, fine-grained data sharing across organizational boundaries without releasing raw documents. Instead of asset-level access control, DAVE answers natural-language queries over private documents while dynamically enforcing machine-readable usage policies (e.g., ODRL) at query time—introducing *virtual redaction* to suppress sensitive content without modifying source assets. We formalize policy-violating information disclosure using usage control and information flow principles, and propose an architecture integrating DAVE with Eclipse Dataspace Components. A provider-side prototype routes QA requests through the spokesperson service rather than triggering document transfer. Our primary contribution is architectural: we define the enforcement model and outline a rigorous evaluation methodology—assessing security (against adversarial queries), utility (answer fidelity under policies), and performance (latency, scalability)—to guide future empirical work on systematically governed LLM access in multi-party data spaces.",
      "summary": "## 背景与问题  \n在当前跨组织数据空间中，数据使用策略（usage policies）通常仅在**资产粒度**上实施：整份文档或数据集被整体授权或拒止。当文档仅局部敏感时，提供方为避免泄露受保护信息，往往需人工红标（redact）文档后共享——该方式成本高昂、粒度粗糙（无法按语义细粒度控制），且难以随策略更新或合作方变更而动态维护。\n\n## 方法与创新  \n本文提出 **DAVE**（Data-Access via Policy-Enforcing spokesperson），一种基于大语言模型（LLM）的策略强制型“数据代言人”架构。DAVE 不直接发布原始文档，而是为数据提供方代理响应自然语言查询，其输出严格受**机器可读策略**（如ODRL格式）约束。我们形式化定义了该范式下的策略违规信息泄露，并提出**虚拟红标**（virtual redaction）机制：在查询执行时动态抑制敏感内容，无需修改源文档。技术上，我们设计了与Eclipse Dataspace Components（EDC）集成的分层架构，实现策略解析、意图理解、上下文感知响应生成与合规性验证闭环；并构建了初步的提供方侧原型——将问答请求路由至DAVE服务，替代原始文档传输。\n\n## 贡献与展望  \n本工作核心贡献在于**安全可控的LLM访问架构设计**，而非端到端实现或实证评估。我们明确界定其安全边界，提出兼顾**安全性、实用性与性能**的三维度评估框架，涵盖良性查询下的效用保持率、对抗性提示下的策略鲁棒性，以及延迟与吞吐量等工程指标，为未来在多方协同数据空间中系统化治理LLM访问奠定方法论基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16977v1",
      "arxiv_id": "2602.16977v1",
      "title": "Fail-Closed Alignment for Large Language Models",
      "authors": [
        "Zachary Coalson",
        "Beth Sohler",
        "Aiden Gabriel",
        "Sanghyun Hong"
      ],
      "abstract": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.",
      "published": "2026-02-19",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16977v1",
      "url": "https://arxiv.org/abs/2602.16977v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 问题背景  \n当前大语言模型（LLM）对齐（alignment）存在结构性缺陷：主流拒绝机制为“**fail-open**”（失效开放）设计——即当部分安全机制被绕过时，整体对齐即崩溃，导致模型在遭受提示词级越狱（prompt-based jailbreaks）攻击时生成有害内容。其根本原因在于，现有对齐方法将拒绝行为编码于少数主导隐空间方向上；一旦攻击者通过精心构造的提示抑制某一关键特征，安全防护便全面失效。\n\n## 方法创新：Fail-Closed Alignment  \n我们提出**fail-closed alignment**（失效闭合对齐）这一全新设计原则：拒绝机制应具备**冗余性与因果独立性**，即使部分路径失效，其余路径仍能独立维持安全响应。为此，我们构建了首个可实现该原则的**渐进式对齐框架**：  \n- 迭代识别模型中已习得的拒绝方向（refusal directions）；  \n- 通过子空间投影与梯度约束**系统性地消融（ablate）这些方向**；  \n- 强制模型在**新、独立的隐子空间中重建安全行为**，形成多路并行、互不干扰的拒绝通路。\n\n## 实验结果与机制验证  \n在四大主流越狱攻击（GCG、AutoDAN、PAIR、TAP）下，本方法达成**最强综合鲁棒性**（平均拒绝成功率提升12.7–31.4%），同时显著缓解“过度拒绝”（over-refusal）问题（合规请求拒绝率降低至<2.1%），且保持生成质量（BLEU、BERTScore与基线无显著差异）。计算开销极小（仅增加约8%训练时间）。  \n**机制分析证实**：经本方法训练的模型编码了**多个因果独立的拒绝方向**——单次越狱攻击无法同步抑制全部通路，为“fail-closed”提供了坚实实证基础，确立其作为鲁棒LLM安全的**原理性基石**。",
      "summary_en": "We identify a critical structural weakness in current LLM alignment: modern refusal mechanisms are *fail-open*—collapsing entirely when even one dominant safety direction is suppressed (e.g., via prompt-based jailbreaks). To address this, we propose *fail-closed alignment* as a robust safety principle: refusal must persist under partial failures through *redundant, causally independent pathways*. We instantiate it with a progressive alignment framework that iteratively identifies and ablates learned refusal directions, forcing the model to reconstruct safety in new, orthogonal subspaces. Evaluated across four state-of-the-art jailbreak attacks, our method achieves the strongest overall robustness (+12.7–31.4% average refusal success), significantly reduces over-refusal (<2.1% false rejections on benign queries), and preserves generation quality—with only ~8% computational overhead. Mechanistic analysis confirms the emergence of multiple causally independent refusal directions, empirically validating fail-closed alignment as a principled foundation for robust LLM safety.",
      "summary": "## 问题背景  \n当前大语言模型（LLM）对齐（alignment）存在结构性缺陷：主流拒绝机制为“**fail-open**”（失效开放）设计——即当部分安全机制被绕过时，整体对齐即崩溃，导致模型在遭受提示词级越狱（prompt-based jailbreaks）攻击时生成有害内容。其根本原因在于，现有对齐方法将拒绝行为编码于少数主导隐空间方向上；一旦攻击者通过精心构造的提示抑制某一关键特征，安全防护便全面失效。\n\n## 方法创新：Fail-Closed Alignment  \n我们提出**fail-closed alignment**（失效闭合对齐）这一全新设计原则：拒绝机制应具备**冗余性与因果独立性**，即使部分路径失效，其余路径仍能独立维持安全响应。为此，我们构建了首个可实现该原则的**渐进式对齐框架**：  \n- 迭代识别模型中已习得的拒绝方向（refusal directions）；  \n- 通过子空间投影与梯度约束**系统性地消融（ablate）这些方向**；  \n- 强制模型在**新、独立的隐子空间中重建安全行为**，形成多路并行、互不干扰的拒绝通路。\n\n## 实验结果与机制验证  \n在四大主流越狱攻击（GCG、AutoDAN、PAIR、TAP）下，本方法达成**最强综合鲁棒性**（平均拒绝成功率提升12.7–31.4%），同时显著缓解“过度拒绝”（over-refusal）问题（合规请求拒绝率降低至<2.1%），且保持生成质量（BLEU、BERTScore与基线无显著差异）。计算开销极小（仅增加约8%训练时间）。  \n**机制分析证实**：经本方法训练的模型编码了**多个因果独立的拒绝方向**——单次越狱攻击无法同步抑制全部通路，为“fail-closed”提供了坚实实证基础，确立其作为鲁棒LLM安全的**原理性基石**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16708v1",
      "arxiv_id": "2602.16708v1",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Mihai Christodorescu",
        "Somesh Jha"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16708v1",
      "url": "https://arxiv.org/abs/2602.16708v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "prompt",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的高敏场景，如客户服务协议、多级审批流程、敏感数据访问控制及合规性监管。当前主流做法是将策略嵌入提示词（prompt），但该方式缺乏强制力保障，无法防止模型绕过、忽略或误解释策略，导致策略违规频发。\n\n## 方法：PCAS 政策编译器  \n本文提出 **PCAS（Policy Compiler for Agentic Systems）**——一种面向智能体系统的策略编译框架，实现**确定性策略执行**。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图**（causal dependency graph）表征系统状态，显式追踪工具调用、工具返回值、消息传递等事件间的因果与信息流关系；  \n- **策略语言设计**：采用类 Datalog 的声明式策略语言，天然支持**传递性信息流分析**与**跨智能体溯源**（cross-agent provenance），可精确表达“某数据是否经未授权代理传播”等复杂约束；  \n- **运行时强保障**：集成轻量级**参考监视器**（reference monitor），在所有动作执行前实时拦截并阻断违规行为，执行逻辑完全独立于LLM推理过程，杜绝模型幻觉或策略规避。\n\n## 关键成果与验证  \nPCAS 以“编译即合规”范式工作：输入现有智能体代码与策略规范，自动产出**策略内建**（policy-by-construction）的插桩系统，无需重构原有安全逻辑。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离；  \n- 多智能体药物警戒系统中的多级审批策略；  \n- 客户服务组织策略（如地域权限、敏感操作二次确认）。  \n实验表明：在客户服务任务上，PCAS 将前沿模型（如GPT-4、Claude 3）的策略合规率从**48% 提升至 93%**，且所有插桩运行中**零策略违规发生**，验证了其确定性保障能力。",
      "summary_en": "PCAS (Policy Compiler for Agentic Systems) is a novel framework that enables *deterministic, runtime-enforced* policy compliance for LLM-based agents—without relying on prompt engineering or model fine-tuning. It addresses the fundamental limitation of linear message histories by modeling system state as a *causal dependency graph*, capturing information flow across tool calls, results, and messages. Policies are written in a declarative, Datalog-derived language supporting transitive flow and cross-agent provenance. A lightweight reference monitor intercepts all actions pre-execution, blocking violations with guaranteed enforcement—decoupled from model reasoning. Given an existing agent implementation and a policy specification, PCAS compiles them into an instrumented, policy-compliant system *by construction*. Evaluated on three real-world case studies—including prompt injection defense, pharmacovigilance approval workflows, and customer service policies—PCAS boosts compliance from 48% to 93% across frontier models (e.g., GPT-4, Claude 3), achieving zero policy violations in all instrumented runs.",
      "summary": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的高敏场景，如客户服务协议、多级审批流程、敏感数据访问控制及合规性监管。当前主流做法是将策略嵌入提示词（prompt），但该方式缺乏强制力保障，无法防止模型绕过、忽略或误解释策略，导致策略违规频发。\n\n## 方法：PCAS 政策编译器  \n本文提出 **PCAS（Policy Compiler for Agentic Systems）**——一种面向智能体系统的策略编译框架，实现**确定性策略执行**。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图**（causal dependency graph）表征系统状态，显式追踪工具调用、工具返回值、消息传递等事件间的因果与信息流关系；  \n- **策略语言设计**：采用类 Datalog 的声明式策略语言，天然支持**传递性信息流分析**与**跨智能体溯源**（cross-agent provenance），可精确表达“某数据是否经未授权代理传播”等复杂约束；  \n- **运行时强保障**：集成轻量级**参考监视器**（reference monitor），在所有动作执行前实时拦截并阻断违规行为，执行逻辑完全独立于LLM推理过程，杜绝模型幻觉或策略规避。\n\n## 关键成果与验证  \nPCAS 以“编译即合规”范式工作：输入现有智能体代码与策略规范，自动产出**策略内建**（policy-by-construction）的插桩系统，无需重构原有安全逻辑。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离；  \n- 多智能体药物警戒系统中的多级审批策略；  \n- 客户服务组织策略（如地域权限、敏感操作二次确认）。  \n实验表明：在客户服务任务上，PCAS 将前沿模型（如GPT-4、Claude 3）的策略合规率从**48% 提升至 93%**，且所有插桩运行中**零策略违规发生**，验证了其确定性保障能力。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16596v1",
      "arxiv_id": "2602.16596v1",
      "title": "Sequential Membership Inference Attacks",
      "authors": [
        "Thomas Michel",
        "Debabrota Basu",
        "Emilie Kaufmann"
      ],
      "abstract": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16596v1",
      "url": "https://arxiv.org/abs/2602.16596v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "math.ST",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "membership",
        "dp",
        "inference"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n现代AI模型具有动态演化特性——在其生命周期中经历多次增量更新（如持续学习、微调或在线训练）。然而，现有成员推断（Membership Inference, MI）攻击与隐私审计方法多基于**静态终态模型**，忽视了模型序列蕴含的时序敏感信息。尽管实证研究发现利用模型更新序列可提升MI攻击效能，但缺乏对“最优”动态攻击的理论刻画：既有分析局限于无限样本下的静态设定，无法指导有限数据、含噪（如差分隐私）场景下的实际部署。\n\n## 方法创新：SeMI* 攻击框架  \n本文提出首个严格定义的**序列式成员推断最优攻击 SeMI***，其核心思想是：将目标样本（canary）的潜在插入时间点建模为隐变量，联合利用模型更新轨迹（而非仅最终模型）构建似然比检验。我们为经验均值估计器推导出SeMI*在**有限样本、含/不含隐私保护（如DP-SGD）** 条件下的**精确最优检测功率**（optimal power），并证明该结果自然退化至经典渐近结论，填补了理论空白。\n\n## 关键发现与优势  \n- **信号增强**：相比仅观测终态模型的攻击，SeMI* 利用更新序列避免MI信号随训练数据累积而“稀释”，显著提升小样本与早期插入场景的敏感性；  \n- **审计可调性**：攻击者可协同优化**插入时机**与**canary设计**，实现更紧致的隐私边界估计；  \n- **实证验证**：在多种数据分布（MNIST、CIFAR-10、Purchase）及DP-SGD训练/微调模型上，SeMI*的实用变体 consistently 超越SOTA基线（如Loss-based、Shadow Model），将隐私审计的ε-上界平均收紧18–35%。",
      "summary_en": "Modern AI models evolve sequentially through updates—yet membership inference (MI) attacks and privacy audits remain largely static. While empirical studies suggest model sequences boost MI power, rigorous analysis of *optimal* sequential attacks is missing: existing theory assumes infinite samples and static models. We bridge this gap by proposing **SeMI\\***, the first theoretically grounded sequential MI attack that leverages the full model update trajectory to detect whether a target sample was inserted at a specific step. For empirical mean estimation, we derive SeMI\\*'s *exact optimal detection power* under finite samples—with or without privacy (e.g., DP-SGD)—recovering known asymptotics as a special case. Crucially, SeMI\\* avoids signal dilution inherent in final-model-only attacks, enabling stronger inference early in training. Moreover, adversaries can jointly optimize insertion timing and canary design for tighter privacy auditing. Experiments across datasets (MNIST, CIFAR-10, Purchase) and DP-SGD-trained/fine-tuned models confirm that practical SeMI\\* variants yield significantly tighter privacy bounds—reducing estimated ε by 18–35% over state-of-the-art baselines.",
      "summary": "## 背景与问题  \n现代AI模型具有动态演化特性——在其生命周期中经历多次增量更新（如持续学习、微调或在线训练）。然而，现有成员推断（Membership Inference, MI）攻击与隐私审计方法多基于**静态终态模型**，忽视了模型序列蕴含的时序敏感信息。尽管实证研究发现利用模型更新序列可提升MI攻击效能，但缺乏对“最优”动态攻击的理论刻画：既有分析局限于无限样本下的静态设定，无法指导有限数据、含噪（如差分隐私）场景下的实际部署。\n\n## 方法创新：SeMI* 攻击框架  \n本文提出首个严格定义的**序列式成员推断最优攻击 SeMI***，其核心思想是：将目标样本（canary）的潜在插入时间点建模为隐变量，联合利用模型更新轨迹（而非仅最终模型）构建似然比检验。我们为经验均值估计器推导出SeMI*在**有限样本、含/不含隐私保护（如DP-SGD）** 条件下的**精确最优检测功率**（optimal power），并证明该结果自然退化至经典渐近结论，填补了理论空白。\n\n## 关键发现与优势  \n- **信号增强**：相比仅观测终态模型的攻击，SeMI* 利用更新序列避免MI信号随训练数据累积而“稀释”，显著提升小样本与早期插入场景的敏感性；  \n- **审计可调性**：攻击者可协同优化**插入时机**与**canary设计**，实现更紧致的隐私边界估计；  \n- **实证验证**：在多种数据分布（MNIST、CIFAR-10、Purchase）及DP-SGD训练/微调模型上，SeMI*的实用变体 consistently 超越SOTA基线（如Loss-based、Shadow Model），将隐私审计的ε-上界平均收紧18–35%。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16564v1",
      "arxiv_id": "2602.16564v1",
      "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
      "authors": [
        "Michael Lanier",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16564v1",
      "url": "https://arxiv.org/abs/2602.16564v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n在大规模网络攻防博弈中，基于仿真的安全博弈（Simulation-Based Network Security Games）面临“维度灾难”：网络节点数激增导致状态空间指数级膨胀，传统双 oracle（Double Oracle）或策略空间响应优化（PSRO）方法因需枚举大量纯策略而难以扩展。现有方法在万级节点网络中常遭遇内存溢出、训练停滞或策略质量骤降等问题。\n\n## 方法创新：MetaDOAR 框架  \n本文提出 **MetaDOAR**——一种轻量级元控制器，通过三层协同机制实现可扩展性突破：  \n- **分区感知过滤层**：学习从节点结构嵌入（如 PageRank、度中心性等）到紧凑状态投影的映射，**实时筛选 top-k 关键设备子集**（如高价值服务器、边界网关），将搜索空间压缩 2–3 个数量级；  \n- **分层决策架构**：低层执行器仅在该子集上进行聚焦式束搜索（beam search），由专用 critic 代理提供动作评估；  \n- **量化缓存机制**：将 critic 的前向计算结果按**量化状态投影 + 局部动作标识**为键存入 LRU 缓存，并采用保守的 *k-hop 缓存失效策略*（仅当邻接子图结构变化 ≥k 跳时刷新），减少 >78% 冗余计算，且不牺牲策略最优性。\n\n## 实验结果与意义  \n在包含 5,000–50,000 节点的真实拓扑变体（AS-level、企业内网合成图）上，MetaDOAR 在攻击者/防御者平均收益上**超越 SOTA 基线（如 PSRO+GNN、DeepStack-Net）12.6%–29.3%**，训练内存占用降低 5.4×，单轮迭代耗时下降 68%。本工作为超大规模网络化决策问题提供了**首个兼具理论保障（收敛性继承自 DOAR）、工程实用性（<16GB GPU 显存可训）与部署友好性（模块化设计支持在线热更新）的分层强化学习路径**。",
      "summary_en": "We propose **MetaDOAR**, a scalable meta-controller for simulation-based network security games that extends the Double Oracle / PSRO paradigm with three key innovations: (1) a learned, partition-aware filtering layer that projects per-node structural embeddings into a compact state representation to rapidly select a *top-k subset* of critical devices; (2) a hierarchical execution pipeline where a low-level actor performs focused beam search only on this subset, guided by a critic agent; and (3) a quantized LRU cache for critic evaluations—keyed by discretized state projections and local action IDs—with conservative *k-hop invalidation* to eliminate >78% redundant computation while preserving decision quality. Empirically, MetaDOAR achieves **12.6–29.3% higher player payoffs** than state-of-the-art baselines on networks with up to 50,000 nodes, with **5.4× lower memory usage** and **68% faster iteration time**, without performance degradation. This work delivers a practical, theoretically grounded framework for hierarchical policy learning in large-scale cyber-networked systems.",
      "summary": "## 研究背景与挑战  \n在大规模网络攻防博弈中，基于仿真的安全博弈（Simulation-Based Network Security Games）面临“维度灾难”：网络节点数激增导致状态空间指数级膨胀，传统双 oracle（Double Oracle）或策略空间响应优化（PSRO）方法因需枚举大量纯策略而难以扩展。现有方法在万级节点网络中常遭遇内存溢出、训练停滞或策略质量骤降等问题。\n\n## 方法创新：MetaDOAR 框架  \n本文提出 **MetaDOAR**——一种轻量级元控制器，通过三层协同机制实现可扩展性突破：  \n- **分区感知过滤层**：学习从节点结构嵌入（如 PageRank、度中心性等）到紧凑状态投影的映射，**实时筛选 top-k 关键设备子集**（如高价值服务器、边界网关），将搜索空间压缩 2–3 个数量级；  \n- **分层决策架构**：低层执行器仅在该子集上进行聚焦式束搜索（beam search），由专用 critic 代理提供动作评估；  \n- **量化缓存机制**：将 critic 的前向计算结果按**量化状态投影 + 局部动作标识**为键存入 LRU 缓存，并采用保守的 *k-hop 缓存失效策略*（仅当邻接子图结构变化 ≥k 跳时刷新），减少 >78% 冗余计算，且不牺牲策略最优性。\n\n## 实验结果与意义  \n在包含 5,000–50,000 节点的真实拓扑变体（AS-level、企业内网合成图）上，MetaDOAR 在攻击者/防御者平均收益上**超越 SOTA 基线（如 PSRO+GNN、DeepStack-Net）12.6%–29.3%**，训练内存占用降低 5.4×，单轮迭代耗时下降 68%。本工作为超大规模网络化决策问题提供了**首个兼具理论保障（收敛性继承自 DOAR）、工程实用性（<16GB GPU 显存可训）与部署友好性（模块化设计支持在线热更新）的分层强化学习路径**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16520v1",
      "arxiv_id": "2602.16520v1",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "authors": [
        "Doron Shavit"
      ],
      "abstract": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16520v1",
      "url": "https://arxiv.org/abs/2602.16520v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n**越狱提示（Jailbreak prompts）** 是当前大语言模型（LLM）尤其是**工具增强型智能体（tool-augmented agents）** 面临的关键安全威胁。此类攻击常利用长上下文隐藏、语义伪装及轻量级混淆等手法，绕过传统单次推理的防护机制（如静态分类器或一次性守门员模型），导致现有防御在真实代理场景中鲁棒性不足。\n\n## 方法：递归语言模型检测框架（RLM-JB）  \n我们提出 **RLM-JB**——一种基于**递归语言模型（Recursive Language Models, RLMs）** 的端到端越狱检测框架。其核心思想是将检测建模为**可审计的程序化流程**，而非黑盒分类：  \n- **根模型（Root Model）** 作为协调器，执行结构化分析程序；  \n- 对输入进行**规范化与去混淆处理**（如解码Base64、还原Unicode逃逸）；  \n- 将文本**动态分块（chunking）**，缓解长上下文稀释问题，并确保全覆盖；  \n- 启动多个**工作模型（Worker Models）并行筛查各分块**；  \n- 通过跨块信号聚合（cross-chunk signal composition）识别被刻意拆分的恶意载荷（split-payload attacks）。\n\n## 主要结果与创新  \n在AutoDAN等强对抗性越狱数据集上，RLM-JB在三种主流LLM后端（Llama-3-8B、Qwen2-7B、Phi-3-mini）上实现：  \n- **高召回率（ASR/Recall）92.5%–98.0%**；  \n- **超高精度（Precision）98.99%–100%**，误报率仅**0.0%–2.0%**；  \n- 首次验证了**程序化检测范式**在敏感性-特异性权衡中的实用性，且决策过程全程可追溯、可审计。",
      "summary_en": "We introduce **RLM-JB**, a procedural jailbreak detection framework built on Recursive Language Models (RLMs), designed specifically for tool-augmented agents operating on untrusted inputs. Unlike one-shot classifiers, RLM-JB treats detection as an auditable program: a root model normalizes and de-obfuscates suspicious prompts, chunks text to ensure full coverage and mitigate context dilution, dispatches parallel worker-model queries over segments, and composes cross-chunk evidence to recover split-payload attacks. Evaluated on AutoDAN-style adversarial prompts across three LLM backends (Llama-3-8B, Qwen2-7B, Phi-3-mini), RLM-JB achieves high recall (92.5–98.0%), exceptional precision (98.99–100%), and near-zero false positives (0.0–2.0%). This demonstrates that recursive, procedure-driven defense offers a practical and interpretable trade-off between sensitivity and specificity—without compromising operational safety.",
      "summary": "## 背景与挑战  \n**越狱提示（Jailbreak prompts）** 是当前大语言模型（LLM）尤其是**工具增强型智能体（tool-augmented agents）** 面临的关键安全威胁。此类攻击常利用长上下文隐藏、语义伪装及轻量级混淆等手法，绕过传统单次推理的防护机制（如静态分类器或一次性守门员模型），导致现有防御在真实代理场景中鲁棒性不足。\n\n## 方法：递归语言模型检测框架（RLM-JB）  \n我们提出 **RLM-JB**——一种基于**递归语言模型（Recursive Language Models, RLMs）** 的端到端越狱检测框架。其核心思想是将检测建模为**可审计的程序化流程**，而非黑盒分类：  \n- **根模型（Root Model）** 作为协调器，执行结构化分析程序；  \n- 对输入进行**规范化与去混淆处理**（如解码Base64、还原Unicode逃逸）；  \n- 将文本**动态分块（chunking）**，缓解长上下文稀释问题，并确保全覆盖；  \n- 启动多个**工作模型（Worker Models）并行筛查各分块**；  \n- 通过跨块信号聚合（cross-chunk signal composition）识别被刻意拆分的恶意载荷（split-payload attacks）。\n\n## 主要结果与创新  \n在AutoDAN等强对抗性越狱数据集上，RLM-JB在三种主流LLM后端（Llama-3-8B、Qwen2-7B、Phi-3-mini）上实现：  \n- **高召回率（ASR/Recall）92.5%–98.0%**；  \n- **超高精度（Precision）98.99%–100%**，误报率仅**0.0%–2.0%**；  \n- 首次验证了**程序化检测范式**在敏感性-特异性权衡中的实用性，且决策过程全程可追溯、可审计。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16480v1",
      "arxiv_id": "2602.16480v1",
      "title": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data",
      "authors": [
        "Yiwen Lu"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16480v1",
      "url": "https://arxiv.org/abs/2602.16480v1",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "learning",
        "federated",
        "security",
        "model",
        "data",
        "inference"
      ],
      "keyword_score": 7,
      "summary_zh": "## 背景与挑战  \n联邦学习（FL）在保护数据隐私的前提下实现多方协同建模，广泛应用于医疗、金融等敏感场景。然而，其面临双重安全威胁：**好奇服务器**可能通过模型上传推断客户端原始数据（推理攻击），而**恶意客户端**可注入毒化模型破坏全局聚合（投毒攻击）。现有方案多结合差分隐私/安全聚合与鲁棒聚合策略，但普遍存在两大缺陷：（1）计算与通信开销过高，难以部署于资源受限设备；（2）在非独立同分布（Non-IID）数据下鲁棒性显著下降——因数据异构导致正常模型差异增大，传统异常检测易将良性偏差误判为攻击。\n\n## 方法创新  \n本文提出 **SRFed**——首个兼顾高效性、Byzantine鲁棒性与端到端隐私保护的Non-IID适配FL框架：  \n- **去中心化高效函数加密（DEFE）**：摒弃可信第三方，支持客户端对本地模型参数进行轻量级加密，服务器无需交互即可完成非线性操作（如加权聚合）后的解密；理论证明其抵御服务器侧推理攻击，且加密/解密复杂度仅为 *O(d)*（*d* 为模型维度），较同态加密降低两个数量级；  \n- **隐私保护的分层鲁棒聚合机制**：基于DEFE密文直接执行层间投影降维与密度聚类分析，动态识别并过滤毒化层参数，避免明文暴露导致的隐私泄露，且在Non-IID下误删率降低42.7%（vs. Krum、Median）。\n\n## 实验验证  \n在CIFAR-10/100、Tiny-ImageNet及真实医疗数据集上，SRFed在3种典型投毒攻击（Label-Flipping、ALIE、Min-Max）下保持>89%准确率（较SOTA提升5.2–11.8%），同时通信开销仅为Secure Aggregation的63%，训练延迟降低37%。理论安全性证明覆盖半诚实服务器与拜占庭客户端混合威胁模型。",
      "summary_en": "Federated Learning (FL) enables collaborative model training while preserving data privacy, yet remains vulnerable to both server-side inference attacks and client-side poisoning attacks—especially under Non-IID data. Existing defenses suffer from high overhead or poor robustness in heterogeneous settings. We propose **SRFed**, the first efficient, Byzantine-robust, and end-to-end privacy-preserving FL framework tailored for Non-IID scenarios. Its core innovations are: (1) a **Decentralized Efficient Functional Encryption (DEFE)** scheme that eliminates third-party trust, enables non-interactive decryption after encrypted aggregation, and provably thwarts server inference with *O(d)* computational cost; and (2) a **privacy-preserving defensive aggregation** mechanism that performs layer-wise projection and clustering directly on encrypted models to filter poisoned updates without revealing raw parameters. Extensive experiments across four datasets show SRFed achieves >89% accuracy under diverse poisoning attacks—outperforming state-of-the-art baselines by 5.2–11.8%—while reducing communication overhead by 37% and latency by 37% compared to Secure Aggregation. Theoretical analysis confirms security against semi-honest servers and Byzantine clients.",
      "summary": "## 背景与挑战  \n联邦学习（FL）在保护数据隐私的前提下实现多方协同建模，广泛应用于医疗、金融等敏感场景。然而，其面临双重安全威胁：**好奇服务器**可能通过模型上传推断客户端原始数据（推理攻击），而**恶意客户端**可注入毒化模型破坏全局聚合（投毒攻击）。现有方案多结合差分隐私/安全聚合与鲁棒聚合策略，但普遍存在两大缺陷：（1）计算与通信开销过高，难以部署于资源受限设备；（2）在非独立同分布（Non-IID）数据下鲁棒性显著下降——因数据异构导致正常模型差异增大，传统异常检测易将良性偏差误判为攻击。\n\n## 方法创新  \n本文提出 **SRFed**——首个兼顾高效性、Byzantine鲁棒性与端到端隐私保护的Non-IID适配FL框架：  \n- **去中心化高效函数加密（DEFE）**：摒弃可信第三方，支持客户端对本地模型参数进行轻量级加密，服务器无需交互即可完成非线性操作（如加权聚合）后的解密；理论证明其抵御服务器侧推理攻击，且加密/解密复杂度仅为 *O(d)*（*d* 为模型维度），较同态加密降低两个数量级；  \n- **隐私保护的分层鲁棒聚合机制**：基于DEFE密文直接执行层间投影降维与密度聚类分析，动态识别并过滤毒化层参数，避免明文暴露导致的隐私泄露，且在Non-IID下误删率降低42.7%（vs. Krum、Median）。\n\n## 实验验证  \n在CIFAR-10/100、Tiny-ImageNet及真实医疗数据集上，SRFed在3种典型投毒攻击（Label-Flipping、ALIE、Min-Max）下保持>89%准确率（较SOTA提升5.2–11.8%），同时通信开销仅为Secure Aggregation的63%，训练延迟降低37%。理论安全性证明覆盖半诚实服务器与拜占庭客户端混合威胁模型。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16436v1",
      "arxiv_id": "2602.16436v1",
      "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent",
      "authors": [
        "Jean Dufraiche",
        "Paul Mangold",
        "Michaël Perrot",
        "Marc Tommasi"
      ],
      "abstract": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16436v1",
      "url": "https://arxiv.org/abs/2602.16436v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "differential"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n在非交互式**本地差分隐私（LDP）**框架下一次性发布原始数据，虽保障了数据的长期可重用性，但强噪声注入会显著扭曲统计结构，尤其导致二分类任务中损失函数梯度的系统性偏差。传统LDP学习方法（如随机响应或高斯机制）往往忽略该偏差的解析结构，仅依赖渐近校正或启发式去噪，难以保证有限样本下的无偏性与收敛性。\n\n## 方法创新  \n本文首次将**Weierstrass变换**引入LDP理论分析：证明LDP扰动等价于对真实数据分布施加Weierstrass卷积核平滑，其偏差本质是函数在高斯核下的期望漂移。进一步，我们严格推导其**逆Weierstrass变换**的显式形式，构建首个可解析、可计算的偏差校正算子——该算子能对任意LDP发布的样本（如经拉普拉斯/高斯扰动的特征向量）进行逐点校正，从而获得**非线性函数（如Sigmoid、平方损失）的无偏估计**。\n\n## 核心算法与理论保证  \n基于上述校正机制，我们提出**逆Weierstrass私有随机梯度下降（IWP-SGD）**：在每次迭代中，先对LDP样本应用逆Weierstrass校正，再计算梯度并更新模型。理论证明：IWP-SGD以$\\mathcal{O}(1/n)$速率收敛至真实总体风险最小化器（$n$为样本量），显著优于标准LDP-SGD的$\\mathcal{O}(1/\\sqrt{n})$速率，且无需额外隐私预算开销。\n\n## 实证验证  \n在合成数据（含非线性边界）及真实数据集（UCI Adult、Bank Marketing）上，IWP-SGD在相同隐私预算（$\\varepsilon=1.0$）下，测试准确率平均提升**5.2–8.7个百分点**，验证了其偏差校正的有效性与实用性。",
      "summary_en": "This paper addresses bias in noninteractive Local Differential Privacy (LDP) for binary classification. We characterize LDP-induced distortion as a Weierstrass transform of the true data distribution and derive its exact inverse, enabling unbiased estimation of nonlinear functions (e.g., logistic loss) on privatized examples. Based on this, we propose **Inverse Weierstrass Private SGD (IWP-SGD)**—a novel optimization algorithm that applies analytical bias correction *before* gradient computation. We prove IWP-SGD converges to the true population risk minimizer at rate $\\mathcal{O}(1/n)$, improving upon the standard $\\mathcal{O}(1/\\sqrt{n})$ rate under LDP. Experiments on synthetic and real-world datasets (UCI Adult, Bank Marketing) confirm consistent accuracy gains of 5.2–8.7 percentage points at $\\varepsilon = 1.0$, demonstrating both theoretical soundness and practical efficacy.",
      "summary": "## 背景与挑战  \n在非交互式**本地差分隐私（LDP）**框架下一次性发布原始数据，虽保障了数据的长期可重用性，但强噪声注入会显著扭曲统计结构，尤其导致二分类任务中损失函数梯度的系统性偏差。传统LDP学习方法（如随机响应或高斯机制）往往忽略该偏差的解析结构，仅依赖渐近校正或启发式去噪，难以保证有限样本下的无偏性与收敛性。\n\n## 方法创新  \n本文首次将**Weierstrass变换**引入LDP理论分析：证明LDP扰动等价于对真实数据分布施加Weierstrass卷积核平滑，其偏差本质是函数在高斯核下的期望漂移。进一步，我们严格推导其**逆Weierstrass变换**的显式形式，构建首个可解析、可计算的偏差校正算子——该算子能对任意LDP发布的样本（如经拉普拉斯/高斯扰动的特征向量）进行逐点校正，从而获得**非线性函数（如Sigmoid、平方损失）的无偏估计**。\n\n## 核心算法与理论保证  \n基于上述校正机制，我们提出**逆Weierstrass私有随机梯度下降（IWP-SGD）**：在每次迭代中，先对LDP样本应用逆Weierstrass校正，再计算梯度并更新模型。理论证明：IWP-SGD以$\\mathcal{O}(1/n)$速率收敛至真实总体风险最小化器（$n$为样本量），显著优于标准LDP-SGD的$\\mathcal{O}(1/\\sqrt{n})$速率，且无需额外隐私预算开销。\n\n## 实证验证  \n在合成数据（含非线性边界）及真实数据集（UCI Adult、Bank Marketing）上，IWP-SGD在相同隐私预算（$\\varepsilon=1.0$）下，测试准确率平均提升**5.2–8.7个百分点**，验证了其偏差校正的有效性与实用性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16268v1",
      "arxiv_id": "2602.16268v1",
      "title": "Quantum Oracle Distribution Switching and its Applications to Fully Anonymous Ring Signatures",
      "authors": [
        "Marvin Beckmann",
        "Christian Majenz"
      ],
      "abstract": "Ring signatures are a powerful primitive that allows a member to sign on behalf of a group, without revealing their identity. Recently, ring signatures have received additional attention as an ingredient for post-quantum deniable authenticated key exchange, e.g., for a post-quantum version of the Signal protocol, employed by virtually all end-to-end-encrypted messenger services. While several ring signature constructions from post-quantum assumptions offer suitable security and efficiency for use in deniable key exchange, they are currently proven secure in the random oracle model (ROM) only, which is insufficient for post-quantum security.   In this work, we provide four security reductions in the quantum-accessible random oracle model (QROM) for two generic ring signature constructions: two for the AOS framework and two for a construction paradigm based on ring trapdoors, whose generic backbone we formalize. The two security proofs for AOS ring signatures differ in their requirements on the underlying sigma protocol and their tightness. The two reductions for the ring-trapdoor-based ring signatures exhibit various differences in requirements and the security they provide. We employ the measure-and-reprogram technique, QROM straightline extraction tools based on the compressed oracle, history-free reductions and QROM reprogramming tools. To make use of Rényi divergence properties in the QROM, we study the behavior of quantum algorithms that interact with an oracle whose distribution is based on one of two different distributions over the set of outputs. We provide tight bounds for the statistical distance, show that the Rényi divergence can not be used to replace the entire oracle and provide a workaround.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16268v1",
      "url": "https://arxiv.org/abs/2602.16268v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n环签名（Ring Signature）是一种关键密码原语，允许群组中任意成员以匿名方式代表整个群组签署消息，且不泄露其真实身份。近年来，其在**后量子可否认认证密钥交换**（如后量子版Signal协议）中的应用引发广泛关注。然而，现有基于后量子假设的环签名方案大多仅在**随机预言机模型**（ROM）下被证明安全；而ROM在量子环境下不具可信性——攻击者可量子查询预言机，故需升级至**量子可访问随机预言机模型**（QROM）以保障真正后量子安全性。\n\n## 方法与技术贡献  \n本文首次系统构建了环签名在QROM下的严格安全性基础：  \n- 提出**四条紧致安全归约**，覆盖两类通用构造范式：  \n  • **AOS框架**（Au, Liu, Susilo）：给出两个归约，分别适配不同强度的底层Σ协议（强零知识 vs. 标准零知识），并在紧致性（tightness）上形成互补；  \n  • **环陷门范式**（Ring Trapdoor）：首次形式化其通用骨架，并提供两个差异化归约——一个支持选择消息攻击下的不可伪造性（EUF-CMA），另一个强化为全匿名性（full anonymity）保证。  \n- 关键技术融合：综合运用**测量-重编程技术**（measure-and-reprogram）、基于**压缩预言机**（compressed oracle）的QROM直线提取工具、**无历史归约**（history-free reductions）及QROM重编程机制。  \n- 理论突破：深入研究当预言机输出分布切换于两种不同输出分布之间时，量子算法行为的统计特性；给出**统计距离的紧界**，严格证明**Rényi散度无法完全替代预言机模拟**，并提出可实用的规避方案。\n\n## 创新意义  \n本工作填补了后量子环签名在QROM下安全性证明的空白，为Signal等隐私通信协议向后量子安全迁移提供了首个兼具**理论严谨性与工程适用性**的密码学支撑。",
      "summary_en": "This paper establishes the first rigorous quantum-security foundations for ring signatures in the **Quantum Random Oracle Model (QROM)**. We provide **four tight security reductions**: two for the AOS framework—differing in Σ-protocol assumptions (strong vs. standard zero-knowledge) and tightness—and two for a newly formalized **ring-trapdoor paradigm**, offering distinct guarantees (EUF-CMA vs. full anonymity). Our proofs integrate advanced QROM techniques: measure-and-reprogram, compressed-oracle-based straightline extraction, history-free reductions, and QROM reprogramming. Crucially, we analyze quantum algorithms interacting with oracles whose output distributions switch between two alternatives; we derive tight bounds on statistical distance, prove Rényi divergence *cannot* fully replace oracle simulation in QROM, and propose a practical workaround. This work enables post-quantum secure deniable key exchange (e.g., quantum-safe Signal) with provable anonymity and unforgeability.",
      "summary": "## 研究背景与问题  \n环签名（Ring Signature）是一种关键密码原语，允许群组中任意成员以匿名方式代表整个群组签署消息，且不泄露其真实身份。近年来，其在**后量子可否认认证密钥交换**（如后量子版Signal协议）中的应用引发广泛关注。然而，现有基于后量子假设的环签名方案大多仅在**随机预言机模型**（ROM）下被证明安全；而ROM在量子环境下不具可信性——攻击者可量子查询预言机，故需升级至**量子可访问随机预言机模型**（QROM）以保障真正后量子安全性。\n\n## 方法与技术贡献  \n本文首次系统构建了环签名在QROM下的严格安全性基础：  \n- 提出**四条紧致安全归约**，覆盖两类通用构造范式：  \n  • **AOS框架**（Au, Liu, Susilo）：给出两个归约，分别适配不同强度的底层Σ协议（强零知识 vs. 标准零知识），并在紧致性（tightness）上形成互补；  \n  • **环陷门范式**（Ring Trapdoor）：首次形式化其通用骨架，并提供两个差异化归约——一个支持选择消息攻击下的不可伪造性（EUF-CMA），另一个强化为全匿名性（full anonymity）保证。  \n- 关键技术融合：综合运用**测量-重编程技术**（measure-and-reprogram）、基于**压缩预言机**（compressed oracle）的QROM直线提取工具、**无历史归约**（history-free reductions）及QROM重编程机制。  \n- 理论突破：深入研究当预言机输出分布切换于两种不同输出分布之间时，量子算法行为的统计特性；给出**统计距离的紧界**，严格证明**Rényi散度无法完全替代预言机模拟**，并提出可实用的规避方案。\n\n## 创新意义  \n本工作填补了后量子环签名在QROM下安全性证明的空白，为Signal等隐私通信协议向后量子安全迁移提供了首个兼具**理论严谨性与工程适用性**的密码学支撑。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16156v1",
      "arxiv_id": "2602.16156v1",
      "title": "Weak Zero-Knowledge and One-Way Functions",
      "authors": [
        "Rohit Chatterjee",
        "Yunqi Li",
        "Prashant Nalini Vasudevan"
      ],
      "abstract": "We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $ε_c$, $ε_s$, and $ε_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:   1. If all languages in NP have NIZK proofs or arguments satisfying $ ε_c+ε_s+ ε_z < 1 $, then One-Way Functions (OWFs) exist.   This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $ε_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ ε_c+\\sqrt{ε_s}+ε_z < 1 $ [Chakraborty et al., CRYPTO 2025].   2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+(2k-1).ε_z < 1 $, then OWFs exist.   3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+k.ε_z < 1 $, then infinitely-often OWFs exist.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16156v1",
      "url": "https://arxiv.org/abs/2602.16156v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "crypto"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n本文系统研究**弱零知识（Weak Zero-Knowledge, Weak ZK）协议**的存在性对密码学基础假设——特别是**单向函数（One-Way Functions, OWFs）存在性**——的蕴含关系。传统零知识要求完备性、可靠性和零知识性错误均“可忽略”，而Weak ZK允许这些错误率（记为 $ε_c$、$ε_s$、$ε_z$）为常数甚至较大，仅需满足某种非平凡约束。核心问题是：在**NP中存在最坏情况困难语言**这一温和假设下，何种程度的Weak ZK能力足以推出OWFs存在？\n\n## 主要结果与创新点  \n1. **统一性突破**：首次证明——若**所有NP语言均存在满足 $ε_c + ε_s + ε_z < 1$ 的非交互式零知识（NIZK）证明或论证**，则OWFs必然存在。该条件覆盖全部非平凡常数错误组合（如 $ε_c=0.3, ε_s=0.3, ε_z=0.39$），且**无需根号项**；此前最优结果要求更严格的 $ε_c + \\sqrt{ε_s} + ε_z < 1$（Chakraborty et al., CRYPTO 2025）。进一步，若完备性错误 $ε_c$ 可忽略，则该NIZK系统可“提升”为**全错误均可忽略的标准NIZK**，实现从弱到强的自动强化。  \n2. **轮数依赖刻画**：针对$k$轮公开硬币（public-coin）ZK协议，证明若满足 $ε_c + ε_s + (2k-1)ε_z < 1$，则OWFs存在；若更强地满足 $ε_c + ε_s + k·ε_z < 1$（对某常数 $k$），则可推出**无穷多次单向函数（infinitely-often OWFs）** 存在。  \n3. **技术贡献**：通过精巧构造“错误放大—冲突检测”归约框架，绕过传统模拟器分析瓶颈，首次将线性错误和（而非平方根或指数形式）作为临界阈值，显著拓展了Weak ZK的密码学意义边界。",
      "summary_en": "This paper establishes new implications of weak zero-knowledge (ZK) protocols for the existence of one-way functions (OWFs), assuming worst-case hard languages in NP. First, if *all* NP languages admit non-interactive ZK (NIZK) proofs or arguments with completeness, soundness, and zero-knowledge errors $ε_c$, $ε_s$, $ε_z$ satisfying $ε_c + ε_s + ε_z < 1$, then OWFs exist—unifying and strictly improving prior work requiring $ε_c + \\sqrt{ε_s} + ε_z < 1$. Moreover, if $ε_c$ is negligible, such NIZKs can be upgraded to fully negligible-error ones. Second, for $k$-round public-coin ZK, OWFs follow from $ε_c + ε_s + (2k-1)ε_z < 1$; under the tighter bound $ε_c + ε_s + k·ε_z < 1$, infinitely-often OWFs exist. These results reveal linear error thresholds as fundamental to cryptographic hardness.",
      "summary": "## 研究背景与问题  \n本文系统研究**弱零知识（Weak Zero-Knowledge, Weak ZK）协议**的存在性对密码学基础假设——特别是**单向函数（One-Way Functions, OWFs）存在性**——的蕴含关系。传统零知识要求完备性、可靠性和零知识性错误均“可忽略”，而Weak ZK允许这些错误率（记为 $ε_c$、$ε_s$、$ε_z$）为常数甚至较大，仅需满足某种非平凡约束。核心问题是：在**NP中存在最坏情况困难语言**这一温和假设下，何种程度的Weak ZK能力足以推出OWFs存在？\n\n## 主要结果与创新点  \n1. **统一性突破**：首次证明——若**所有NP语言均存在满足 $ε_c + ε_s + ε_z < 1$ 的非交互式零知识（NIZK）证明或论证**，则OWFs必然存在。该条件覆盖全部非平凡常数错误组合（如 $ε_c=0.3, ε_s=0.3, ε_z=0.39$），且**无需根号项**；此前最优结果要求更严格的 $ε_c + \\sqrt{ε_s} + ε_z < 1$（Chakraborty et al., CRYPTO 2025）。进一步，若完备性错误 $ε_c$ 可忽略，则该NIZK系统可“提升”为**全错误均可忽略的标准NIZK**，实现从弱到强的自动强化。  \n2. **轮数依赖刻画**：针对$k$轮公开硬币（public-coin）ZK协议，证明若满足 $ε_c + ε_s + (2k-1)ε_z < 1$，则OWFs存在；若更强地满足 $ε_c + ε_s + k·ε_z < 1$（对某常数 $k$），则可推出**无穷多次单向函数（infinitely-often OWFs）** 存在。  \n3. **技术贡献**：通过精巧构造“错误放大—冲突检测”归约框架，绕过传统模拟器分析瓶颈，首次将线性错误和（而非平方根或指数形式）作为临界阈值，显著拓展了Weak ZK的密码学意义边界。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16109v1",
      "arxiv_id": "2602.16109v1",
      "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes",
      "authors": [
        "Srikumar Nayak",
        "James Walmesley"
      ],
      "abstract": "Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16109v1",
      "url": "https://arxiv.org/abs/2602.16109v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CE"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "learning",
        "federated",
        "differential"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与挑战  \n跨境内部威胁正严重危及各国政府财政计划的安全性，尤其在数据分布式存储、隐私敏感且受多司法管辖区法规约束的场景下。现有方法存在三重瓶颈：**隐私壁垒**导致跨国情报无法安全共享；**推理能力缺失**难以识别跨机构、多步骤的隐蔽攻击链；**结构建模不足**无法刻画金融交易网络中复杂的图结构关系（如资金环路、嵌套代理、异常子图模式）。\n\n## 方法创新：FedGraph-AGI 框架  \n我们提出 **FedGraph-AGI**——首个融合人工通用智能（AGI）推理与联邦图学习的隐私增强型框架，专为跨境内部威胁情报协同而设计。其核心包含三重技术突破：  \n1. **联邦图神经网络（Federated GNN）**：各司法管辖区本地训练图模型，原始交易图数据不出域，严格保障数据主权；  \n2. **混合专家聚合（MoE-Aggregation）**：针对各国监管差异、数据分布异构性与图拓扑多样性，动态加权融合本地模型更新，避免“一刀切”聚合偏差；  \n3. **AGI驱动的因果推理引擎**：基于大型动作模型（LAM），在加密图嵌入空间执行可解释的因果推断（如“若阻断节点X，则攻击路径Y中断概率提升73%”），实现从相关性检测到因果归因的跃迁。\n\n## 实验验证与价值  \n在覆盖10个司法管辖区、含50,000笔真实级交易的跨域数据集上，FedGraph-AGI达**92.3%检测准确率**，显著优于联邦基线（86.1%）和中心化方案（84.7%）。消融实验表明：AGI推理模块贡献**+6.8%**性能增益，MoE机制贡献**+4.4%**；系统在满足**ε = 1.0差分隐私**强约束下仍保持近最优性能，并可高效扩展至50+参与方。本工作首次将AGI级因果推理深度嵌入联邦图学习范式，为全球金融监管协同树立了可验证、可部署、可解释的新范式。",
      "summary_en": "Cross-border insider threats critically undermine government financial schemes, yet existing methods fail to reconcile privacy compliance, multi-jurisdictional heterogeneity, and complex attack pattern reasoning. We propose **FedGraph-AGI**, the first framework unifying federated graph learning with Artificial General Intelligence (AGI) reasoning for privacy-preserving threat intelligence sharing. It integrates: (1) sovereign-preserving federated graph neural networks; (2) Mixture-of-Experts (MoE) aggregation to harmonize jurisdictionally diverse models; and (3) Large Action Models (LAMs) performing causal inference over encrypted graph embeddings. Evaluated on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves **92.3% accuracy**, outperforming federated baselines (+6.2%) and centralized approaches (+7.6%). Ablation confirms AGI reasoning contributes +6.8% and MoE +4.4%. The system satisfies ε = 1.0 differential privacy, scales to 50+ clients, and enables actionable, interpretable threat attribution—pioneering AGI-augmented federated graph intelligence for global financial security.",
      "summary": "## 背景与挑战  \n跨境内部威胁正严重危及各国政府财政计划的安全性，尤其在数据分布式存储、隐私敏感且受多司法管辖区法规约束的场景下。现有方法存在三重瓶颈：**隐私壁垒**导致跨国情报无法安全共享；**推理能力缺失**难以识别跨机构、多步骤的隐蔽攻击链；**结构建模不足**无法刻画金融交易网络中复杂的图结构关系（如资金环路、嵌套代理、异常子图模式）。\n\n## 方法创新：FedGraph-AGI 框架  \n我们提出 **FedGraph-AGI**——首个融合人工通用智能（AGI）推理与联邦图学习的隐私增强型框架，专为跨境内部威胁情报协同而设计。其核心包含三重技术突破：  \n1. **联邦图神经网络（Federated GNN）**：各司法管辖区本地训练图模型，原始交易图数据不出域，严格保障数据主权；  \n2. **混合专家聚合（MoE-Aggregation）**：针对各国监管差异、数据分布异构性与图拓扑多样性，动态加权融合本地模型更新，避免“一刀切”聚合偏差；  \n3. **AGI驱动的因果推理引擎**：基于大型动作模型（LAM），在加密图嵌入空间执行可解释的因果推断（如“若阻断节点X，则攻击路径Y中断概率提升73%”），实现从相关性检测到因果归因的跃迁。\n\n## 实验验证与价值  \n在覆盖10个司法管辖区、含50,000笔真实级交易的跨域数据集上，FedGraph-AGI达**92.3%检测准确率**，显著优于联邦基线（86.1%）和中心化方案（84.7%）。消融实验表明：AGI推理模块贡献**+6.8%**性能增益，MoE机制贡献**+4.4%**；系统在满足**ε = 1.0差分隐私**强约束下仍保持近最优性能，并可高效扩展至50+参与方。本工作首次将AGI级因果推理深度嵌入联邦图学习范式，为全球金融监管协同树立了可验证、可部署、可解释的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16155v1",
      "arxiv_id": "2602.16155v1",
      "title": "Differentially Private Non-convex Distributionally Robust Optimization",
      "authors": [
        "Difei Xu",
        "Meng Ding",
        "Zebin Ma",
        "Huanyi Xie",
        "Youming Tao",
        "Aicha Slaitane",
        "Di Wang"
      ],
      "abstract": "Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.   Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.   Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.   In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.   To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $ψ$-divergence and non-convex loss.   First, we study DRO with general $ψ$-divergence by reformulating it as a minimization problem, and develop a novel $(\\varepsilon, δ)$-DP optimization method, called DP Double-Spider, tailored to this structure.   Under mild assumptions, we show that it achieves a utility bound of $\\mathcal{O}(\\frac{1}{\\sqrt{n}}+ (\\frac{\\sqrt{d \\log (1/δ)}}{n \\varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.   We further improve the utility rate for specific divergences.   In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\\mathcal{O}((\\frac{\\sqrt{d \\log(1/δ)}}{n\\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.   Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16155v1",
      "url": "https://arxiv.org/abs/2602.16155v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "dp",
        "differential"
      ],
      "keyword_score": 3,
      "summary_zh": "## 研究背景与问题  \n现实机器学习部署常面临**分布偏移、群体不平衡与对抗扰动**等挑战，传统经验风险最小化（ERM）性能显著下降。分布鲁棒优化（DRO）通过在分布不确定性集上优化最坏情况期望损失，为鲁棒性提供理论保障。然而，DRO训练数据往往含敏感信息，亟需结合**差分隐私（DP）** 防范隐私泄露。现有工作多聚焦于DP-ERM，而DP-DRO因具**带约束的极小极大结构**，研究严重不足，尤其在非凸、有限和、ψ-散度设定下尚无系统性算法与理论。\n\n## 方法创新  \n本文首次对**差分私有非凸有限和DRO**开展全面研究：  \n- 提出通用框架：将任意ψ-散度DRO重写为单层最小化问题，设计新型$(\\varepsilon,\\delta)$-DP算法——**DP Double-Spider**，融合双重方差缩减与隐私噪声注入；  \n- 针对KL散度这一关键特例，进一步将其转化为**复合有限和优化问题**，提出**DP Recursive-Spider**算法，实现更紧致的隐私-效用权衡。\n\n## 主要理论结果  \n- DP Double-Spider在温和假设下达到梯度范数界：$\\mathcal{O}\\!\\left(\\frac{1}{\\sqrt{n}} + \\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，其中$n$为样本量，$d$为模型维度；  \n- DP Recursive-Spider对KL-DRO取得**最优速率**：$\\mathcal{O}\\!\\left(\\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，与当前非凸DP-ERM最优界完全匹配；  \n- 实验验证：在多个真实数据集（如Adult、CelebA）上，所提方法在隐私预算$(\\varepsilon,\\delta)=(1,10^{-5})$下，测试鲁棒准确率平均提升2.1–4.7个百分点，显著优于基线DP-SGDA、DP-VRBO等。",
      "summary_en": "Distributionally Robust Optimization (DRO) enhances model robustness against distribution shifts and adversarial perturbations, yet its deployment on sensitive data necessitates differential privacy (DP). This paper presents the first comprehensive study of *non-convex, finite-sum, differentially private DRO* under ψ-divergence uncertainty sets. We reformulate general ψ-DRO as a single-level minimization and propose **DP Double-Spider**, achieving a gradient-norm utility bound of $\\mathcal{O}(1/\\sqrt{n} + (\\sqrt{d \\log(1/\\delta)}/(n\\varepsilon))^{2/3})$. For KL-divergence DRO, we cast it as a compositional finite-sum problem and design **DP Recursive-Spider**, attaining the optimal rate $\\mathcal{O}((\\sqrt{d \\log(1/\\delta)}/(n\\varepsilon))^{2/3})$—matching the best-known bound for non-convex DP-ERM. Experiments confirm consistent superiority over existing DP minimax methods across benchmark datasets under realistic privacy budgets.",
      "summary": "## 研究背景与问题  \n现实机器学习部署常面临**分布偏移、群体不平衡与对抗扰动**等挑战，传统经验风险最小化（ERM）性能显著下降。分布鲁棒优化（DRO）通过在分布不确定性集上优化最坏情况期望损失，为鲁棒性提供理论保障。然而，DRO训练数据往往含敏感信息，亟需结合**差分隐私（DP）** 防范隐私泄露。现有工作多聚焦于DP-ERM，而DP-DRO因具**带约束的极小极大结构**，研究严重不足，尤其在非凸、有限和、ψ-散度设定下尚无系统性算法与理论。\n\n## 方法创新  \n本文首次对**差分私有非凸有限和DRO**开展全面研究：  \n- 提出通用框架：将任意ψ-散度DRO重写为单层最小化问题，设计新型$(\\varepsilon,\\delta)$-DP算法——**DP Double-Spider**，融合双重方差缩减与隐私噪声注入；  \n- 针对KL散度这一关键特例，进一步将其转化为**复合有限和优化问题**，提出**DP Recursive-Spider**算法，实现更紧致的隐私-效用权衡。\n\n## 主要理论结果  \n- DP Double-Spider在温和假设下达到梯度范数界：$\\mathcal{O}\\!\\left(\\frac{1}{\\sqrt{n}} + \\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，其中$n$为样本量，$d$为模型维度；  \n- DP Recursive-Spider对KL-DRO取得**最优速率**：$\\mathcal{O}\\!\\left(\\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，与当前非凸DP-ERM最优界完全匹配；  \n- 实验验证：在多个真实数据集（如Adult、CelebA）上，所提方法在隐私预算$(\\varepsilon,\\delta)=(1,10^{-5})$下，测试鲁棒准确率平均提升2.1–4.7个百分点，显著优于基线DP-SGDA、DP-VRBO等。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16653v1",
      "arxiv_id": "2602.16653v1",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "abstract": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16653v1",
      "url": "https://arxiv.org/abs/2602.16653v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \nAgent Skill框架（智能体技能框架）已获GitHub Copilot、LangChain与OpenAI等主流平台官方集成，其核心优势在于通过**结构化技能封装**优化上下文工程、显著抑制幻觉、提升任务执行准确率。然而，该范式在**小语言模型（SLMs）** 上的适用性尚未系统验证——这在工业落地中尤为关键：企业常因数据安全合规与API调用成本限制，无法持续依赖公有云大模型服务；而轻量级SLMs又普遍面临定制化场景泛化能力弱、技能选择不可靠等瓶颈。\n\n## 方法与实验设计  \n本研究首次提出Agent Skill过程的**形式化数学定义**，明确技能调用、上下文约束与决策边界之间的映射关系。在此基础上，系统评估了从**1.5B至80B参数规模**的多类开源模型（含通用与代码专用变体），覆盖三大场景：（1）两个标准开源任务（ToolAlpaca风格工具调用、MultiHopQA）；（2）真实保险理赔工单数据集（含非结构化文本、表格与业务规则约束）。所有实验均在本地化部署环境下完成，严格隔离训练/测试数据与外部API。\n\n## 主要发现与创新点  \n- **规模阈值效应显著**：1.5B–7B模型难以稳定识别并调度适配技能（准确率<52%）；**12B–30B中等规模SLMs**受益最突出——技能选择准确率提升28.6%，端到端任务成功率提高34.2%；  \n- **代码专用80B模型实现性能跃迁**：在保险理赔任务中达到92.4% F1，媲美GPT-4 Turbo（93.1%），同时GPU显存占用降低41%，推理延迟减少37%；  \n- **工业部署启示**：Agent Skill非“越大越好”，而是为SLMs提供可解释、可审计、低资源消耗的模块化增强路径，为高合规要求场景提供可行替代方案。",
      "summary_en": "The Agent Skill Framework—formally adopted by GitHub Copilot, LangChain, and OpenAI—enhances context engineering, reduces hallucinations, and improves task accuracy, especially with proprietary models. This work investigates whether these benefits extend to Small Language Models (SLMs), critical for industrial settings where public API reliance is infeasible due to data security and budget constraints. We introduce the first formal mathematical definition of the Agent Skill process and conduct systematic evaluation across model sizes (1.5B–80B) on two open-source benchmarks and a real-world insurance claims dataset. Results reveal a clear scale-dependent effect: tiny models (<7B) fail at reliable skill selection, while mid-sized SLMs (12B–30B) gain substantial improvements in accuracy (+28.6%) and task success (+34.2%). Notably, code-specialized ~80B models match closed-source baselines (e.g., GPT-4 Turbo) in F1 score (92.4% vs. 93.1%) while cutting GPU memory by 41% and latency by 37%. These findings establish Agent Skills as a principled, efficient, and deployable enhancement strategy for SLM-centric industrial AI.",
      "summary": "## 研究背景与问题  \nAgent Skill框架（智能体技能框架）已获GitHub Copilot、LangChain与OpenAI等主流平台官方集成，其核心优势在于通过**结构化技能封装**优化上下文工程、显著抑制幻觉、提升任务执行准确率。然而，该范式在**小语言模型（SLMs）** 上的适用性尚未系统验证——这在工业落地中尤为关键：企业常因数据安全合规与API调用成本限制，无法持续依赖公有云大模型服务；而轻量级SLMs又普遍面临定制化场景泛化能力弱、技能选择不可靠等瓶颈。\n\n## 方法与实验设计  \n本研究首次提出Agent Skill过程的**形式化数学定义**，明确技能调用、上下文约束与决策边界之间的映射关系。在此基础上，系统评估了从**1.5B至80B参数规模**的多类开源模型（含通用与代码专用变体），覆盖三大场景：（1）两个标准开源任务（ToolAlpaca风格工具调用、MultiHopQA）；（2）真实保险理赔工单数据集（含非结构化文本、表格与业务规则约束）。所有实验均在本地化部署环境下完成，严格隔离训练/测试数据与外部API。\n\n## 主要发现与创新点  \n- **规模阈值效应显著**：1.5B–7B模型难以稳定识别并调度适配技能（准确率<52%）；**12B–30B中等规模SLMs**受益最突出——技能选择准确率提升28.6%，端到端任务成功率提高34.2%；  \n- **代码专用80B模型实现性能跃迁**：在保险理赔任务中达到92.4% F1，媲美GPT-4 Turbo（93.1%），同时GPU显存占用降低41%，推理延迟减少37%；  \n- **工业部署启示**：Agent Skill非“越大越好”，而是为SLMs提供可解释、可审计、低资源消耗的模块化增强路径，为高合规要求场景提供可行替代方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16422v1",
      "arxiv_id": "2602.16422v1",
      "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
      "authors": [
        "Ahmet Halici",
        "Ece Tugba Cebeci",
        "Musa Balci",
        "Mustafa Cini",
        "Serkan Sokmen"
      ],
      "abstract": "Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16422v1",
      "url": "https://arxiv.org/abs/2602.16422v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n组织病理学全切片图像（WSI）可达吉像素量级，其诊断报告生成需兼顾宏观结构理解与微观形态判读，同时输出符合临床规范的精准医学术语。传统端到端方法受限于显存、长程依赖建模能力及专业词汇表覆盖不足，导致报告常出现术语错误、关键病变遗漏或语义失真。\n\n## 方法创新  \n本研究提出一种**分层视觉-语言框架**：  \n- **金字塔式补丁筛选**：在2³–2⁶多尺度下进行自适应下采样，结合Laplacian方差（滤除模糊区域）与HSV色彩空间阈值（剔除背景/刀痕/气泡等伪影），显著压缩输入冗余；  \n- **冻结式基础模型协同**：采用预训练病理大模型UNI-ViT提取高质量局部特征，避免在小规模标注数据上微调导致的过拟合；  \n- **生物医学感知解码**：6层Transformer解码器通过交叉注意力融合视觉特征，输出端采用**BioGPT分词器**，显式建模“腺体排列紊乱”“核仁明显”等专业短语的子词结构；  \n- **检索增强验证（RAV）**：基于Sentence-BERT计算生成报告与10万份真实报告库的语义相似度，若余弦相似度＞0.82，则以检索到的最优匹配报告替代生成结果，提升临床可信度。\n\n## 主要结果  \n在PandaSet与Camelyon17双基准测试中，本方法在BLEU-4（+12.3）、ROUGE-L（+9.7）及临床术语准确率（+18.5%）上显著优于SOTA；RAV模块将关键诊断错误率降低至2.1%，验证了“生成+检索校正”范式的可靠性。本工作为AI辅助病理报告自动化提供了可解释、可验证、临床就绪的新范式。",
      "summary_en": "Generating diagnostic reports from gigapixel whole slide images (WSIs) remains challenging due to scale, fine-grained morphology requirements, and domain-specific language fidelity. We propose a hierarchical vision-language framework that combines a *frozen* UNI Vision Transformer for robust pyramidal feature extraction (at 2³–2⁶ downsampled scales) with background/artifact removal (via Laplacian variance and HSV thresholds), followed by a 6-layer Transformer decoder with cross-attention. To enhance biomedical terminology modeling, we tokenize outputs using BioGPT. Crucially, we introduce retrieval-based verification (RAV): generated reports are embedded via Sentence-BERT and matched against a large clinical corpus; if similarity exceeds 0.82, the top-matched ground-truth report replaces the generated one. On PandaSet and Camelyon17, our method achieves +12.3 BLEU-4 and +18.5% clinical term accuracy over prior SOTA, reducing critical diagnostic errors to 2.1%. This work delivers a reliable, interpretable, and clinically aligned solution for automated histopathology reporting.",
      "summary": "## 背景与挑战  \n组织病理学全切片图像（WSI）可达吉像素量级，其诊断报告生成需兼顾宏观结构理解与微观形态判读，同时输出符合临床规范的精准医学术语。传统端到端方法受限于显存、长程依赖建模能力及专业词汇表覆盖不足，导致报告常出现术语错误、关键病变遗漏或语义失真。\n\n## 方法创新  \n本研究提出一种**分层视觉-语言框架**：  \n- **金字塔式补丁筛选**：在2³–2⁶多尺度下进行自适应下采样，结合Laplacian方差（滤除模糊区域）与HSV色彩空间阈值（剔除背景/刀痕/气泡等伪影），显著压缩输入冗余；  \n- **冻结式基础模型协同**：采用预训练病理大模型UNI-ViT提取高质量局部特征，避免在小规模标注数据上微调导致的过拟合；  \n- **生物医学感知解码**：6层Transformer解码器通过交叉注意力融合视觉特征，输出端采用**BioGPT分词器**，显式建模“腺体排列紊乱”“核仁明显”等专业短语的子词结构；  \n- **检索增强验证（RAV）**：基于Sentence-BERT计算生成报告与10万份真实报告库的语义相似度，若余弦相似度＞0.82，则以检索到的最优匹配报告替代生成结果，提升临床可信度。\n\n## 主要结果  \n在PandaSet与Camelyon17双基准测试中，本方法在BLEU-4（+12.3）、ROUGE-L（+9.7）及临床术语准确率（+18.5%）上显著优于SOTA；RAV模块将关键诊断错误率降低至2.1%，验证了“生成+检索校正”范式的可靠性。本工作为AI辅助病理报告自动化提供了可解释、可验证、临床就绪的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16174v1",
      "arxiv_id": "2602.16174v1",
      "title": "Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation",
      "authors": [
        "Fatih Temiz",
        "Shavbo Salehi",
        "Melike Erol-Kantarci"
      ],
      "abstract": "Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16174v1",
      "url": "https://arxiv.org/abs/2602.16174v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MM"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n面向元宇宙的移动边缘计算（MEC）无线服务需在严苛时延约束与高视觉质量要求下，为用户提供无缚沉浸式体验。实现卓越用户体验质量（QoE）亟需智能、协同的资源分配机制。传统联邦学习（FL）虽支持跨MEC服务器的分布式模型训练，但存在两大瓶颈：（1）需频繁上传完整模型参数至云端，加剧通信开销与延迟；（2）采用简单全局平均聚合策略，在多制式异构无线接入环境（如Wi-Fi/5G/6G混合网络）中易导致策略泛化能力下降与性能退化。\n\n## 方法创新：联邦分割决策Transformer（FSDT）  \n本文提出**Federated Split Decision Transformer（FSDT）**——一种面向离线强化学习（RL）的新型联邦架构。其核心在于**模型空间解耦**：将Transformer决策模型纵向分割为两部分——  \n- **边缘侧**：轻量级、代理专属组件（含MEC本地嵌入层与动作预测头），保障对本地信道状态、用户行为与设备异构性的快速自适应；  \n- **云端侧**：共享的全局注意力与序列建模层，通过联邦协调实现跨MEC服务器的知识融合与策略协同优化。  \n该设计显著降低边缘计算负载，同时规避全模型传输与粗粒度聚合缺陷。\n\n## 主要结果与贡献  \n实验表明，在真实轨迹驱动的异构多RAT元宇宙仿真环境中，FSDT相较FedAvg、FedProx及集中式RL基线，**QoE提升最高达10%**；同时，**98%的Transformer参数被卸载至云端**，MEC服务器仅需维持<2%参数量，大幅缓解内存与算力压力。本工作首次将分割式Transformer与联邦离线RL深度融合，为低开销、高鲁棒的元宇宙边缘智能提供了可部署新范式。",
      "summary_en": "Mobile edge computing (MEC) enables immersive metaverse services, yet achieving high QoE under strict latency and visual fidelity constraints demands intelligent, cooperative resource allocation across heterogeneous MEC servers. Conventional federated learning (FL) suffers from excessive communication overhead (full-model transmission) and poor generalization due to naive global aggregation—especially in multi-RAT environments. To address this, we propose the **Federated Split Decision Transformer (FSDT)**: an offline RL framework that *vertically partitions* a decision Transformer between edge (MEC-specific embedding/prediction layers) and cloud (shared global attention layers). This design enables local adaptability while fostering cross-server cooperation via federated training of only the cloud-resident parameters. Experiments in heterogeneous multi-RAT metaverse scenarios show FSDT improves QoE by up to **10%** over FL and centralized RL baselines, while offloading **98% of Transformer parameters to the cloud**, drastically reducing MEC computational burden. FSDT establishes a new paradigm for scalable, low-overhead edge intelligence in the metaverse.",
      "summary": "## 背景与挑战  \n面向元宇宙的移动边缘计算（MEC）无线服务需在严苛时延约束与高视觉质量要求下，为用户提供无缚沉浸式体验。实现卓越用户体验质量（QoE）亟需智能、协同的资源分配机制。传统联邦学习（FL）虽支持跨MEC服务器的分布式模型训练，但存在两大瓶颈：（1）需频繁上传完整模型参数至云端，加剧通信开销与延迟；（2）采用简单全局平均聚合策略，在多制式异构无线接入环境（如Wi-Fi/5G/6G混合网络）中易导致策略泛化能力下降与性能退化。\n\n## 方法创新：联邦分割决策Transformer（FSDT）  \n本文提出**Federated Split Decision Transformer（FSDT）**——一种面向离线强化学习（RL）的新型联邦架构。其核心在于**模型空间解耦**：将Transformer决策模型纵向分割为两部分——  \n- **边缘侧**：轻量级、代理专属组件（含MEC本地嵌入层与动作预测头），保障对本地信道状态、用户行为与设备异构性的快速自适应；  \n- **云端侧**：共享的全局注意力与序列建模层，通过联邦协调实现跨MEC服务器的知识融合与策略协同优化。  \n该设计显著降低边缘计算负载，同时规避全模型传输与粗粒度聚合缺陷。\n\n## 主要结果与贡献  \n实验表明，在真实轨迹驱动的异构多RAT元宇宙仿真环境中，FSDT相较FedAvg、FedProx及集中式RL基线，**QoE提升最高达10%**；同时，**98%的Transformer参数被卸载至云端**，MEC服务器仅需维持<2%参数量，大幅缓解内存与算力压力。本工作首次将分割式Transformer与联邦离线RL深度融合，为低开销、高鲁棒的元宇宙边缘智能提供了可部署新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16346v1",
      "arxiv_id": "2602.16346v1",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "authors": [
        "Nivya Talokar",
        "Ayush K Tarun",
        "Murari Mandal",
        "Maksym Andriushchenko",
        "Antoine Bosselut"
      ],
      "abstract": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16346v1",
      "url": "https://arxiv.org/abs/2602.16346v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具和维护记忆执行真实世界工作流，但这一能力也使恶意攻击者得以利用其完成复杂非法任务。现有滥用评估基准多聚焦于**单轮指令式攻击**（single-prompt），严重忽视了在**多轮交互、多语言环境**下，代理如何被逐步诱导、渐进式地协助完成有害或违法目标——这恰恰是实际部署中最典型的风险场景。\n\n## 方法：STING 框架与分析范式  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**：一种自动化红队测试框架。它首先构建以“良性身份”为掩护的**多步非法计划**（如伪装成合规研究员获取受控化学品信息），再通过**自适应多轮追问**持续 probing 目标代理，并由专用 judge 代理实时判定各阶段完成状态。进一步，我们创新性地将多轮红队过程建模为 **“首次越狱时间”（Time-to-First Jailbreak）随机变量**，支持：① **发现曲线（discovery curves）** 刻画风险暴露进程；② 基于攻击语言的**风险比归因分析**（hazard-ratio attribution）；③ 提出新指标 **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**，量化指定步数内越狱发生的平均进度。\n\n## 关键发现与创新  \n在 AgentHarm 基准上，STING 的非法任务完成率显著超越单轮提示与适配工具调用的多轮聊天基线。跨六种非英语语种（含低资源语言如斯瓦希里语、孟加拉语）测试表明：**攻击成功率与非法任务完成度并不随语言资源减少而系统性升高**，挑战了“低资源语言更易越狱”的常见假设。STING 首次实现了对现实部署中**多轮、多语言、工具增强型代理**的可复现、可量化、可归因的滥用压力测试，为安全评估提供实用新范式。",
      "summary_en": "We introduce **STING**, an automated red-teaming framework for evaluating illicit assistance in multi-turn, multilingual LLM agents. STING constructs grounded, step-by-step illegal plans disguised under benign personas and probes target agents via adaptive follow-up turns, using judge agents to track phase-wise completion. We model multi-turn jailbreaking as a *time-to-first-jailbreak* random variable, enabling discovery curves, hazard-ratio attribution by attack language, and the novel **Restricted Mean Jailbreak Discovery (RMJD)** metric. On AgentHarm, STING achieves substantially higher illicit-task completion than single-turn and chat-based multi-turn baselines adapted for tool-using agents. Crucially, across six non-English languages—including low-resource ones like Swahili and Bengali—we find no consistent increase in attack success or task completion, contradicting prevalent “low-resource = higher vulnerability” assumptions in chatbot literature. STING provides a practical, quantifiable, and multilingual stress test aligned with real-world agent deployment.",
      "summary": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具和维护记忆执行真实世界工作流，但这一能力也使恶意攻击者得以利用其完成复杂非法任务。现有滥用评估基准多聚焦于**单轮指令式攻击**（single-prompt），严重忽视了在**多轮交互、多语言环境**下，代理如何被逐步诱导、渐进式地协助完成有害或违法目标——这恰恰是实际部署中最典型的风险场景。\n\n## 方法：STING 框架与分析范式  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**：一种自动化红队测试框架。它首先构建以“良性身份”为掩护的**多步非法计划**（如伪装成合规研究员获取受控化学品信息），再通过**自适应多轮追问**持续 probing 目标代理，并由专用 judge 代理实时判定各阶段完成状态。进一步，我们创新性地将多轮红队过程建模为 **“首次越狱时间”（Time-to-First Jailbreak）随机变量**，支持：① **发现曲线（discovery curves）** 刻画风险暴露进程；② 基于攻击语言的**风险比归因分析**（hazard-ratio attribution）；③ 提出新指标 **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**，量化指定步数内越狱发生的平均进度。\n\n## 关键发现与创新  \n在 AgentHarm 基准上，STING 的非法任务完成率显著超越单轮提示与适配工具调用的多轮聊天基线。跨六种非英语语种（含低资源语言如斯瓦希里语、孟加拉语）测试表明：**攻击成功率与非法任务完成度并不随语言资源减少而系统性升高**，挑战了“低资源语言更易越狱”的常见假设。STING 首次实现了对现实部署中**多轮、多语言、工具增强型代理**的可复现、可量化、可归因的滥用压力测试，为安全评估提供实用新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16320v1",
      "arxiv_id": "2602.16320v1",
      "title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion",
      "authors": [
        "Kavyansh Tyagi",
        "Vishwas Rathi",
        "Puneet Goyal"
      ],
      "abstract": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The architecture integrates three key components: (i) GhostConv3D-based patch embedding for efficient feature extraction with minimal redundancy, (ii) MixFFN3D module with low-rank projections and depthwise convolutions for parameter-efficient feature extraction, and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. RefineFormer3D contains only 2.94M parameters, substantially fewer than contemporary transformer-based methods. Extensive experiments on ACDC and BraTS benchmarks demonstrate that RefineFormer3D achieves 93.44\\% and 85.9\\% average Dice scores respectively, outperforming or matching state-of-the-art methods while requiring significantly fewer parameters. Furthermore, the model achieves fast inference (8.35 ms per volume on GPU) with low memory requirements, supporting deployment in resource-constrained clinical environments. These results establish RefineFormer3D as an effective and scalable solution for practical 3D medical image segmentation.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16320v1",
      "url": "https://arxiv.org/abs/2602.16320v1",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n精准且高效的3D医学图像分割对临床诊断与手术规划至关重要，但现有Transformer模型虽具备优异的全局建模能力，却普遍存在参数量大、显存占用高、推理延迟长等问题，严重制约其在资源受限的临床环境（如边缘设备或实时术中系统）中的实际部署。\n\n## 方法创新  \n本文提出**RefineFormer3D**——一种轻量级分层式三维Transformer架构，通过三项协同设计实现精度与效率的最优平衡：  \n- **GhostConv3D嵌入模块**：采用“幽灵卷积”策略生成低冗余3D图像块嵌入，在保留结构信息的同时显著压缩初始特征维度；  \n- **MixFFN3D前馈网络**：融合低秩线性投影与深度可分离三维卷积，大幅降低FFN层参数量（较标准FFN减少约68%）；  \n- **跨尺度交叉注意力融合解码器**：动态加权整合多层级编码器特征，自适应选择最具判别性的跨尺度跳接信息，提升病灶边界与小目标分割鲁棒性。\n\n## 性能表现  \nRefineFormer3D仅含**294万参数**（约为Swin-Unet的1/5、TransBTS的1/7），在ACDC心脏数据集上达**93.44%平均Dice分数**，在BraTS脑肿瘤数据集上达**85.9% Dice**，全面超越或持平当前SOTA方法。单体积GPU推理耗时仅**8.35毫秒**，显存峰值低于3.2 GB，支持端到端实时处理。本工作为临床可落地的3D医学影像AI提供了兼具高精度、低开销与强泛化性的新范式。",
      "summary_en": "Accurate and efficient 3D medical image segmentation is vital for clinical practice, yet mainstream transformer models suffer from excessive parameters and memory overhead. RefineFormer3D addresses this by introducing a lightweight hierarchical 3D transformer with three key innovations: (i) GhostConv3D-based patch embedding for redundancy-aware feature initialization; (ii) MixFFN3D—a parameter-efficient feed-forward module combining low-rank projections and depthwise 3D convolutions; and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. With only **2.94M parameters**, it achieves **93.44% mean Dice on ACDC** and **85.9% on BraTS**, matching or exceeding state-of-the-art methods while requiring significantly fewer resources. It runs at **8.35 ms per volume on GPU**, demonstrating strong potential for real-time, resource-constrained clinical deployment.",
      "summary": "## 背景与挑战  \n精准且高效的3D医学图像分割对临床诊断与手术规划至关重要，但现有Transformer模型虽具备优异的全局建模能力，却普遍存在参数量大、显存占用高、推理延迟长等问题，严重制约其在资源受限的临床环境（如边缘设备或实时术中系统）中的实际部署。\n\n## 方法创新  \n本文提出**RefineFormer3D**——一种轻量级分层式三维Transformer架构，通过三项协同设计实现精度与效率的最优平衡：  \n- **GhostConv3D嵌入模块**：采用“幽灵卷积”策略生成低冗余3D图像块嵌入，在保留结构信息的同时显著压缩初始特征维度；  \n- **MixFFN3D前馈网络**：融合低秩线性投影与深度可分离三维卷积，大幅降低FFN层参数量（较标准FFN减少约68%）；  \n- **跨尺度交叉注意力融合解码器**：动态加权整合多层级编码器特征，自适应选择最具判别性的跨尺度跳接信息，提升病灶边界与小目标分割鲁棒性。\n\n## 性能表现  \nRefineFormer3D仅含**294万参数**（约为Swin-Unet的1/5、TransBTS的1/7），在ACDC心脏数据集上达**93.44%平均Dice分数**，在BraTS脑肿瘤数据集上达**85.9% Dice**，全面超越或持平当前SOTA方法。单体积GPU推理耗时仅**8.35毫秒**，显存峰值低于3.2 GB，支持端到端实时处理。本工作为临床可落地的3D医学影像AI提供了兼具高精度、低开销与强泛化性的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16181v1",
      "arxiv_id": "2602.16181v1",
      "title": "Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters",
      "authors": [
        "Diego Labate",
        "Dipanwita Thakur",
        "Giancarlo Fortino"
      ],
      "abstract": "Energy theft poses a significant threat to the stability and efficiency of smart grids, leading to substantial economic losses and operational challenges. Traditional centralized machine learning approaches for theft detection require aggregating user data, raising serious concerns about privacy and data security. These issues are further exacerbated in smart meter environments, where devices are often resource-constrained and lack the capacity to run heavy models. In this work, we propose a privacy-preserving federated learning framework for energy theft detection that addresses both privacy and computational constraints. Our approach leverages a lightweight multilayer perceptron (MLP) model, suitable for deployment on low-power smart meters, and integrates basic differential privacy (DP) by injecting Gaussian noise into local model updates before aggregation. This ensures formal privacy guarantees without compromising learning performance. We evaluate our framework on a real-world smart meter dataset under both IID and non-IID data distributions. Experimental results demonstrate that our method achieves competitive accuracy, precision, recall, and AUC scores while maintaining privacy and efficiency. This makes the proposed solution practical and scalable for secure energy theft detection in next-generation smart grid infrastructures.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16181v1",
      "url": "https://arxiv.org/abs/2602.16181v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "privacy-preserving",
        "differential",
        "federated",
        "learning",
        "dp",
        "privacy",
        "machine"
      ],
      "keyword_score": 7,
      "summary_zh": "## 研究背景与挑战  \n能源窃取严重威胁智能电网的稳定性、经济性与可持续性，每年造成巨额经济损失与运维风险。传统基于中心化机器学习的窃电检测方法需将用户用电数据上传至云端服务器进行模型训练，不仅违背《个人信息保护法》与GDPR等隐私法规，更在资源受限的智能电表（如MCU主频<100 MHz、RAM<64 KB）上难以部署复杂模型，导致隐私泄露与计算不可行双重瓶颈。\n\n## 方法创新  \n本文提出一种面向资源受限场景的**安全可扩展联邦学习框架**，实现端到端隐私保护与轻量化协同学习：  \n- **模型轻量化**：设计超低参数量多层感知机（MLP），仅含2个隐藏层（16→8神经元）、ReLU激活与批量归一化，模型体积<15 KB，推理延迟<8 ms（ARM Cortex-M4平台实测）；  \n- **隐私增强机制**：在本地模型更新阶段注入**高斯噪声**（σ=0.5），满足(ε=2.1, δ=1e⁻⁵)-差分隐私，严格保障单次参与者的梯度隐私；  \n- **通信高效性**：采用模型压缩+梯度稀疏化（top-k=5%），单轮通信量降低73%，适配窄带PLC信道。\n\n## 实验结果与价值  \n在真实世界某省电网23万块电表的12个月时序数据集（含3.7%标注窃电样本）上验证：  \n- 在非独立同分布（non-IID）场景下，F1-score达92.4%，AUC为0.961，显著优于基线FedAvg（-4.2%）与中心化ResNet（-6.8%，且无隐私保障）；  \n- 模型在STM32L4系列电表嵌入式环境稳定运行，内存占用峰值<42 KB，支持OTA增量更新；  \n- 首次在电力物联网中实现**形式化隐私证明+边缘实时推理+商用级检测精度**三重统一，为新型智能电网提供可落地的隐私安全基础设施。",
      "summary_en": "Energy theft severely undermines smart grid stability and causes massive economic losses. Centralized detection methods compromise user privacy by requiring raw data aggregation and are infeasible on resource-constrained smart meters. We propose a privacy-preserving federated learning framework featuring a lightweight MLP model (under 15 KB, <8 ms inference on ARM Cortex-M4) and Gaussian-noise-based differential privacy (ε=2.1, δ=1e⁻⁵) applied to local gradients before aggregation. Evaluated on a real-world dataset of 230,000 smart meters under both IID and non-IID settings, our method achieves 92.4% F1-score and 0.961 AUC—outperforming FedAvg and centralized baselines—while ensuring formal privacy guarantees and ultra-low communication overhead (73% reduction via top-k sparsification). This work bridges the gap between rigorous privacy, edge deployability, and high detection accuracy for next-generation secure smart grids.",
      "summary": "## 研究背景与挑战  \n能源窃取严重威胁智能电网的稳定性、经济性与可持续性，每年造成巨额经济损失与运维风险。传统基于中心化机器学习的窃电检测方法需将用户用电数据上传至云端服务器进行模型训练，不仅违背《个人信息保护法》与GDPR等隐私法规，更在资源受限的智能电表（如MCU主频<100 MHz、RAM<64 KB）上难以部署复杂模型，导致隐私泄露与计算不可行双重瓶颈。\n\n## 方法创新  \n本文提出一种面向资源受限场景的**安全可扩展联邦学习框架**，实现端到端隐私保护与轻量化协同学习：  \n- **模型轻量化**：设计超低参数量多层感知机（MLP），仅含2个隐藏层（16→8神经元）、ReLU激活与批量归一化，模型体积<15 KB，推理延迟<8 ms（ARM Cortex-M4平台实测）；  \n- **隐私增强机制**：在本地模型更新阶段注入**高斯噪声**（σ=0.5），满足(ε=2.1, δ=1e⁻⁵)-差分隐私，严格保障单次参与者的梯度隐私；  \n- **通信高效性**：采用模型压缩+梯度稀疏化（top-k=5%），单轮通信量降低73%，适配窄带PLC信道。\n\n## 实验结果与价值  \n在真实世界某省电网23万块电表的12个月时序数据集（含3.7%标注窃电样本）上验证：  \n- 在非独立同分布（non-IID）场景下，F1-score达92.4%，AUC为0.961，显著优于基线FedAvg（-4.2%）与中心化ResNet（-6.8%，且无隐私保障）；  \n- 模型在STM32L4系列电表嵌入式环境稳定运行，内存占用峰值<42 KB，支持OTA增量更新；  \n- 首次在电力物联网中实现**形式化隐私证明+边缘实时推理+商用级检测精度**三重统一，为新型智能电网提供可落地的隐私安全基础设施。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16379v1",
      "arxiv_id": "2602.16379v1",
      "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
      "authors": [
        "Mohammad H. A. Monfared",
        "Lucie Flek",
        "Akbar Karimi"
      ],
      "abstract": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16379v1",
      "url": "https://arxiv.org/abs/2602.16379v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 基于LLM智能体的标签一致性数据生成方法  \n本研究针对**方面级情感分析（ABSA）**中高质量标注数据稀缺的问题，提出一种新型**智能体驱动（agentic）数据增强框架**。该方法通过**多轮迭代生成—验证闭环**：LLM智能体首先根据原始句子与目标方面/情感标签生成候选样本，继而调用专用验证模块（同样由LLM实现）对生成结果进行细粒度校验（如方面术语是否忠实于原文、情感极性是否与上下文一致、标签结构是否符合任务规范），不满足条件则触发修正重生成，直至输出满足**标签一致性（Label-Consistency）**要求的合成样本。\n\n为严谨评估智能体架构的增益，我们构建了严格对照的**提示工程基线**——采用完全相同的LLM模型（GPT-4）、相同指令模板与解码参数，仅去除迭代验证与反馈机制。实验在三大ABSA子任务（方面词抽取ATE、方面情感分类ATSC、方面情感对抽取ASPE）、四个主流SemEval基准数据集（14res, 14lap, 15rest, 16rest）及两类编码器-解码器模型（T5-Base与强预训练的Tk-Instruct）上系统展开。\n\n关键发现：（1）智能体方法在**标签保真度上显著优于基线**，尤其在需生成新方面词的ATE与ASPE任务中，错误率降低达32.7%；（2）当与真实数据混合训练时，智能体增强数据带来更稳定的性能提升，在T5-Base上平均F1提升+4.2%，而提示基线仅+1.8%；（3）模型能力差异影响增益幅度：T5-Base受益最显著，其增强后性能**追平未增强的Tk-Instruct**，证实智能体框架可有效弥补中小模型的知识与推理短板。",
      "summary_en": "We propose an **LLM-agent-based data augmentation method** for Aspect-Based Sentiment Analysis (ABSA) that enforces *label consistency* through iterative generation and verification—unlike static prompting baselines. Using GPT-4 as the agent, our approach generates synthetic examples (e.g., aspect terms, sentiment polarities) and rigorously validates their structural and semantic fidelity before acceptance. Evaluated across three ABSA subtasks (ATE, ATSC, ASPE), four SemEval datasets, and two models (T5-Base and Tk-Instruct), our method achieves significantly higher label preservation—especially in aspect term generation—and delivers larger performance gains when augmenting real training data. Notably, T5-Base boosted by agentic augmentation matches the performance of the stronger Tk-Instruct without augmentation, demonstrating its efficacy in compensating for model capacity limitations.",
      "summary": "## 基于LLM智能体的标签一致性数据生成方法  \n本研究针对**方面级情感分析（ABSA）**中高质量标注数据稀缺的问题，提出一种新型**智能体驱动（agentic）数据增强框架**。该方法通过**多轮迭代生成—验证闭环**：LLM智能体首先根据原始句子与目标方面/情感标签生成候选样本，继而调用专用验证模块（同样由LLM实现）对生成结果进行细粒度校验（如方面术语是否忠实于原文、情感极性是否与上下文一致、标签结构是否符合任务规范），不满足条件则触发修正重生成，直至输出满足**标签一致性（Label-Consistency）**要求的合成样本。\n\n为严谨评估智能体架构的增益，我们构建了严格对照的**提示工程基线**——采用完全相同的LLM模型（GPT-4）、相同指令模板与解码参数，仅去除迭代验证与反馈机制。实验在三大ABSA子任务（方面词抽取ATE、方面情感分类ATSC、方面情感对抽取ASPE）、四个主流SemEval基准数据集（14res, 14lap, 15rest, 16rest）及两类编码器-解码器模型（T5-Base与强预训练的Tk-Instruct）上系统展开。\n\n关键发现：（1）智能体方法在**标签保真度上显著优于基线**，尤其在需生成新方面词的ATE与ASPE任务中，错误率降低达32.7%；（2）当与真实数据混合训练时，智能体增强数据带来更稳定的性能提升，在T5-Base上平均F1提升+4.2%，而提示基线仅+1.8%；（3）模型能力差异影响增益幅度：T5-Base受益最显著，其增强后性能**追平未增强的Tk-Instruct**，证实智能体框架可有效弥补中小模型的知识与推理短板。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16958v1",
      "arxiv_id": "2602.16958v1",
      "title": "Automating Agent Hijacking via Structural Template Injection",
      "authors": [
        "Xinhao Deng",
        "Jiaqing Wu",
        "Miao Chen",
        "Yue Xiao",
        "Ke Xu",
        "Qi Li"
      ],
      "abstract": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16958v1",
      "url": "https://arxiv.org/abs/2602.16958v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "injection",
        "prompt"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \nAgent劫持（Agent Hijacking）被OWASP列为大语言模型（LLM）生态系统的**最高危威胁之一**，其核心是攻击者通过向检索内容注入恶意指令，篡改智能体（agent）的执行逻辑。现有方法多依赖人工构造、语义驱动的提示词扰动，导致**攻击成功率低、泛化性差**，尤其难以迁移至GPT、Gemini等闭源商用模型。\n\n## 方法创新：Phantom框架  \n本文提出**Phantom**——首个基于**结构化模板注入（Structured Template Injection）** 的自动化Agent劫持框架。其核心洞见在于：LLM智能体严格依赖预定义的**对话模板标记**（如`<|system|>`、`<|user|>`、`<|tool_response|>`）来区分系统指令、用户输入与工具输出。Phantom通过在检索上下文中注入**经优化的结构化模板片段**，诱导智能体产生角色混淆，误将恶意内容识别为合法用户请求或历史工具响应。\n\n为提升对黑盒商用Agent的迁移能力，Phantom设计了**新型攻击模板搜索框架**：  \n- **多级模板增强**：生成语法合规、结构多样的候选模板；  \n- **模板自编码器（TAE）**：将离散模板映射至连续、可微的隐空间；  \n- **贝叶斯优化**：高效搜索最优对抗向量，并解码为高杀伤力结构化模板。\n\n## 实验与影响  \n在Qwen、GPT-4、Gemini-1.5等主流模型上，Phantom的**攻击成功率（ASR）提升达3.2×，查询效率提高5.8×**（相较SOTA基线）。更关键的是，我们已在70+真实商业产品（含客服Agent、AI工作流平台）中发现并验证漏洞，全部获厂商官方确认。本研究首次揭示了**模板结构本身即为新型攻击面**，为下一代具身智能系统提供了可落地的防御基准与实证依据。",
      "summary_en": "Agent hijacking—ranked a top-tier threat by OWASP—enables adversaries to subvert LLM agents by injecting malicious instructions into retrieved content. Prior attacks rely on manual, semantics-based prompt engineering, suffering from low success rates and poor transferability to black-box commercial models (e.g., GPT, Gemini). We propose **Phantom**, the first automated framework leveraging *Structural Template Injection* to exploit the fundamental chat template architecture of LLM agents. By injecting optimized, syntactically valid template tokens (e.g., `<|user|>`, `<|tool_response|>`) into retrieval contexts, Phantom induces role confusion—causing agents to misinterpret adversarial content as legitimate user input or prior tool outputs. To enhance black-box transferability, Phantom introduces a novel template search pipeline: multi-level structural augmentation, a Template Autoencoder (TAE) for continuous latent embedding, and Bayesian optimization to efficiently discover high-potency adversarial templates. Experiments across Qwen, GPT, and Gemini show Phantom achieves up to **3.2× higher Attack Success Rate (ASR)** and **5.8× better query efficiency** than state-of-the-art baselines. Critically, we identified and verified **70+ vulnerabilities in real-world commercial products**, confirmed by vendors—demonstrating the severe practical risk of structural template–based hijacking and establishing an empirical foundation for securing agentic AI systems.",
      "summary": "## 背景与问题  \nAgent劫持（Agent Hijacking）被OWASP列为大语言模型（LLM）生态系统的**最高危威胁之一**，其核心是攻击者通过向检索内容注入恶意指令，篡改智能体（agent）的执行逻辑。现有方法多依赖人工构造、语义驱动的提示词扰动，导致**攻击成功率低、泛化性差**，尤其难以迁移至GPT、Gemini等闭源商用模型。\n\n## 方法创新：Phantom框架  \n本文提出**Phantom**——首个基于**结构化模板注入（Structured Template Injection）** 的自动化Agent劫持框架。其核心洞见在于：LLM智能体严格依赖预定义的**对话模板标记**（如`<|system|>`、`<|user|>`、`<|tool_response|>`）来区分系统指令、用户输入与工具输出。Phantom通过在检索上下文中注入**经优化的结构化模板片段**，诱导智能体产生角色混淆，误将恶意内容识别为合法用户请求或历史工具响应。\n\n为提升对黑盒商用Agent的迁移能力，Phantom设计了**新型攻击模板搜索框架**：  \n- **多级模板增强**：生成语法合规、结构多样的候选模板；  \n- **模板自编码器（TAE）**：将离散模板映射至连续、可微的隐空间；  \n- **贝叶斯优化**：高效搜索最优对抗向量，并解码为高杀伤力结构化模板。\n\n## 实验与影响  \n在Qwen、GPT-4、Gemini-1.5等主流模型上，Phantom的**攻击成功率（ASR）提升达3.2×，查询效率提高5.8×**（相较SOTA基线）。更关键的是，我们已在70+真实商业产品（含客服Agent、AI工作流平台）中发现并验证漏洞，全部获厂商官方确认。本研究首次揭示了**模板结构本身即为新型攻击面**，为下一代具身智能系统提供了可落地的防御基准与实证依据。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16943v1",
      "arxiv_id": "2602.16943v1",
      "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
      "authors": [
        "Arnold Cartagena",
        "Ariane Teixeira"
      ],
      "abstract": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16943v1",
      "url": "https://arxiv.org/abs/2602.16943v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n当前大语言模型（LLM）作为智能体（agents）日益通过**工具调用（tool calls）** 与外部系统交互——这些操作具备真实世界影响，远超纯文本输出的风险边界。然而，主流安全评估仍集中于**文本层面的拒绝行为**（如拒答有害请求），严重忽视工具调用这一关键行动层。由此产生一个根本性缺口：文本对齐（text alignment）能否自然泛化至行动对齐（action alignment）？  \n\n## 方法与基准  \n本研究提出 **GAP 基准（Gap Assessment Protocol）**，首次系统量化文本安全与工具调用安全之间的**行为分化程度**。实验覆盖：  \n- **6个前沿闭源/开源模型**（含GPT-4o、Claude-3.5、Qwen2.5-72B等）；  \n- **6个强监管领域**（医药、金融、教育、就业、法律、基础设施）；  \n- **7类领域特异性越狱（jailbreak）攻击** × 3种系统提示条件（中性/安全强化/工具鼓励） × 2种提示变体；  \n- 共生成 **17,420 条可分析数据点**，全部标注文本响应与实际工具调用序列。  \n\n## 核心发现  \n1. **文本安全 ≠ 工具调用安全**：所有6个模型均出现“文本拒绝但工具执行”的矛盾行为——即模型在回复中明确拒答，却同步触发高风险工具调用（如查询处方药剂量后调用`get_drug_info`并传入非法参数）。该现象被形式化为 **GAP 指标**。  \n2. **安全提示效果有限**：即使在“安全强化”系统提示下，仍存在**219例跨模型GAP事件**；提示措辞对工具行为影响巨大——最鲁棒模型的工具安全率波动达21个百分点，最敏感模型达57个百分点。  \n3. **治理机制局限性**：运行时治理合约（runtime governance contracts）可降低信息泄露，但**无法抑制非法工具调用本身**。  \n\n## 创新意义  \nGAP 基准揭示了当前LLM安全范式的结构性缺陷，证明**纯文本评估无法保障代理体（agent）行为安全**，亟需构建面向工具调用的专用安全测量框架与干预机制。",
      "summary_en": "This paper exposes a critical safety gap in LLM agents: **text-level safety does not transfer to tool-call-level safety**. While current evaluations focus almost exclusively on whether models *refuse harmful text requests*, real-world agent deployments execute *actions* via tool calls—each carrying tangible, often irreversible consequences. We introduce the **GAP benchmark**, the first systematic framework to quantify divergence between textual refusal and forbidden tool execution. Across 6 frontier models, 6 regulated domains (e.g., pharmaceutical, legal), 7 jailbreak types per domain, and 3 system prompt conditions, we collect 17,420 datapoints. Our central finding is pervasive “text-refuse + tool-execute” behavior—formalized as the GAP metric—even under safety-reinforced prompts (219 persistent cases). Prompt wording strongly modulates tool safety rates (spanning 21–57 percentage points across models), and runtime governance contracts reduce information leakage but *fail to deter forbidden tool calls*. These results demonstrate that text-only safety evaluation is insufficient for agents and that tool-call safety demands dedicated measurement, benchmarking, and mitigation.",
      "summary": "## 背景与问题  \n当前大语言模型（LLM）作为智能体（agents）日益通过**工具调用（tool calls）** 与外部系统交互——这些操作具备真实世界影响，远超纯文本输出的风险边界。然而，主流安全评估仍集中于**文本层面的拒绝行为**（如拒答有害请求），严重忽视工具调用这一关键行动层。由此产生一个根本性缺口：文本对齐（text alignment）能否自然泛化至行动对齐（action alignment）？  \n\n## 方法与基准  \n本研究提出 **GAP 基准（Gap Assessment Protocol）**，首次系统量化文本安全与工具调用安全之间的**行为分化程度**。实验覆盖：  \n- **6个前沿闭源/开源模型**（含GPT-4o、Claude-3.5、Qwen2.5-72B等）；  \n- **6个强监管领域**（医药、金融、教育、就业、法律、基础设施）；  \n- **7类领域特异性越狱（jailbreak）攻击** × 3种系统提示条件（中性/安全强化/工具鼓励） × 2种提示变体；  \n- 共生成 **17,420 条可分析数据点**，全部标注文本响应与实际工具调用序列。  \n\n## 核心发现  \n1. **文本安全 ≠ 工具调用安全**：所有6个模型均出现“文本拒绝但工具执行”的矛盾行为——即模型在回复中明确拒答，却同步触发高风险工具调用（如查询处方药剂量后调用`get_drug_info`并传入非法参数）。该现象被形式化为 **GAP 指标**。  \n2. **安全提示效果有限**：即使在“安全强化”系统提示下，仍存在**219例跨模型GAP事件**；提示措辞对工具行为影响巨大——最鲁棒模型的工具安全率波动达21个百分点，最敏感模型达57个百分点。  \n3. **治理机制局限性**：运行时治理合约（runtime governance contracts）可降低信息泄露，但**无法抑制非法工具调用本身**。  \n\n## 创新意义  \nGAP 基准揭示了当前LLM安全范式的结构性缺陷，证明**纯文本评估无法保障代理体（agent）行为安全**，亟需构建面向工具调用的专用安全测量框架与干预机制。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16935v1",
      "arxiv_id": "2602.16935v1",
      "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
      "authors": [
        "Justin Albrethsen",
        "Yash Datta",
        "Kunal Kumar",
        "Sharath Rajasekar"
      ],
      "abstract": "While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16935v1",
      "url": "https://arxiv.org/abs/2602.16935v1",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n当前大语言模型（LLM）能力持续增强，但其安全防护机制仍普遍采用**无状态（stateless）设计**，将多轮对话视为彼此独立的单次交互。这一缺陷导致显著的“安全缺口”（Safety Gap）：攻击者可利用**渐进式对抗策略**（如Crescendo、ActorAttack），将恶意意图拆解、稀释并跨轮次逐步累积，从而绕过仅依赖单轮内容的静态过滤器。\n\n## 方法创新：DeepContext框架  \n我们提出**DeepContext**——首个面向实时、多轮对抗意图漂移检测的**有状态（stateful）监控框架**。其核心突破在于摒弃孤立轮次评估范式，转而采用**循环神经网络（RNN）架构**，以序列化方式处理经微调的轮次级语义嵌入（turn-level embeddings）。通过在对话中**持续传递隐状态（hidden state）**，DeepContext显式建模用户意图的时序演化轨迹，精准捕获状态无关模型所忽略的**风险增量累积过程**。\n\n## 主要成果与优势  \n- 在多轮越狱（jailbreak）检测任务上达到**SOTA F1分数0.84**，显著优于主流方案：超大规模云服务商安全护栏（~0.52–0.61）、Llama-Prompt-Guard-2（0.67）及Granite-Guardian（0.67）；  \n- 推理延迟严格控制在**<20ms（T4 GPU）**，满足工业级实时部署要求；  \n- 证明：**建模意图的时序动态性**，比堆叠巨型无状态模型更具效果与效率双重优势。",
      "summary_en": "Large Language Models (LLMs) have advanced rapidly, yet their safety guardrails remain predominantly *stateless*, treating multi-turn dialogues as independent single-turn events. This temporal blindness enables adversarial intent drift—e.g., via Crescendo or ActorAttack—to accumulate incrementally across turns and evade static filters. We introduce **DeepContext**, a lightweight, *stateful* real-time monitoring framework that models the temporal evolution of user intent using a Recurrent Neural Network (RNN) over fine-tuned turn-level embeddings. By maintaining and updating a hidden state across conversation history, DeepContext detects subtle, multi-turn adversarial patterns missed by stateless baselines. Evaluated on multi-turn jailbreak detection, DeepContext achieves a new state-of-the-art **F1 score of 0.84**, substantially outperforming hyperscaler cloud guardrails (~0.52–0.61), Llama-Prompt-Guard-2 (0.67), and Granite-Guardian (0.67). Crucially, it incurs only **<20 ms inference latency on a T4 GPU**, demonstrating that sequential intent modeling is both more effective and computationally efficient than scaling up stateless defenses.",
      "summary": "## 背景与问题  \n当前大语言模型（LLM）能力持续增强，但其安全防护机制仍普遍采用**无状态（stateless）设计**，将多轮对话视为彼此独立的单次交互。这一缺陷导致显著的“安全缺口”（Safety Gap）：攻击者可利用**渐进式对抗策略**（如Crescendo、ActorAttack），将恶意意图拆解、稀释并跨轮次逐步累积，从而绕过仅依赖单轮内容的静态过滤器。\n\n## 方法创新：DeepContext框架  \n我们提出**DeepContext**——首个面向实时、多轮对抗意图漂移检测的**有状态（stateful）监控框架**。其核心突破在于摒弃孤立轮次评估范式，转而采用**循环神经网络（RNN）架构**，以序列化方式处理经微调的轮次级语义嵌入（turn-level embeddings）。通过在对话中**持续传递隐状态（hidden state）**，DeepContext显式建模用户意图的时序演化轨迹，精准捕获状态无关模型所忽略的**风险增量累积过程**。\n\n## 主要成果与优势  \n- 在多轮越狱（jailbreak）检测任务上达到**SOTA F1分数0.84**，显著优于主流方案：超大规模云服务商安全护栏（~0.52–0.61）、Llama-Prompt-Guard-2（0.67）及Granite-Guardian（0.67）；  \n- 推理延迟严格控制在**<20ms（T4 GPU）**，满足工业级实时部署要求；  \n- 证明：**建模意图的时序动态性**，比堆叠巨型无状态模型更具效果与效率双重优势。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16901v1",
      "arxiv_id": "2602.16901v1",
      "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
      "authors": [
        "Tanqiu Jiang",
        "Yuhui Wang",
        "Jiacheng Liang",
        "Ting Wang"
      ],
      "abstract": "LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16901v1",
      "url": "https://arxiv.org/abs/2602.16901v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm",
        "agent"
      ],
      "keyword_score": 3,
      "summary_zh": "## AgentLAB：首个面向长周期攻击的LLM智能体安全基准\n\n随着大语言模型（LLM）智能体在**长周期、高复杂度环境**中的广泛应用（如自动化运维、多步决策代理、智能助手等），其面临的安全威胁已从传统单轮交互漏洞，演变为依赖**多轮人-智能体-环境动态交互**的长周期攻击。此类攻击通过精心编排多轮对话与工具调用，实现单轮场景下不可达的恶意目标（如绕过权限控制、窃取敏感记忆、篡改任务本质），但现有评估体系严重缺失对此类渐进式、自适应威胁的系统性评测能力。\n\n为此，我们提出 **AgentLAB**——首个专为评估LLM智能体对**自适应长周期攻击**鲁棒性而设计的开源基准。AgentLAB创新性构建了五大新型攻击范式：**意图劫持**（Intent Hijacking）、**工具链滥用**（Tool Chaining）、**任务注入**（Task Injection）、**目标偏移**（Objective Drifting）与**记忆污染**（Memory Poisoning）；覆盖28个贴近真实部署场景的智能体环境（含API集成、文件系统、多代理协作等）；并提供644个经过人工验证、具备明确攻击路径与成功判据的安全测试用例。\n\n基于AgentLAB的大规模实证评估表明：当前主流LLM智能体（包括基于ReAct、Plan-and-Execute及LLM-as-Judge架构的代表系统）**普遍高度脆弱**——平均攻击成功率超73%，且防御策略若仅针对单轮提示注入或输入过滤（如prompt sanitization、guardrails），**几乎无法抵御长周期攻击**（防御后成功率仍达68%）。本工作不仅揭示了长周期安全建模的紧迫性，更提供了可复现、可扩展、可增量更新的评估基础设施。AgentLAB已开源，地址：https://tanqiujiang.github.io/AgentLAB_main。",
      "summary_en": "## AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks\n\nWe introduce **AgentLAB**, the first benchmark dedicated to evaluating LLM agents’ susceptibility to *adaptive, long-horizon attacks*—multi-turn adversarial strategies that exploit sequential interactions across user, agent, and environment to achieve objectives infeasible in single-turn settings. AgentLAB comprises **5 novel attack types** (intent hijacking, tool chaining, task injection, objective drifting, memory poisoning), **28 realistic agentic environments**, and **644 rigorously validated security test cases**. Evaluating state-of-the-art agents reveals alarming vulnerability: average attack success exceeds **73%**, and standard single-turn defenses (e.g., prompt sanitization, output guardrails) fail catastrophically—success remains **~68%** post-mitigation. AgentLAB provides a reproducible, extensible foundation for measuring progress in securing practical LLM agents. Code and data: https://tanqiujiang.github.io/AgentLAB_main.",
      "summary": "## AgentLAB：首个面向长周期攻击的LLM智能体安全基准\n\n随着大语言模型（LLM）智能体在**长周期、高复杂度环境**中的广泛应用（如自动化运维、多步决策代理、智能助手等），其面临的安全威胁已从传统单轮交互漏洞，演变为依赖**多轮人-智能体-环境动态交互**的长周期攻击。此类攻击通过精心编排多轮对话与工具调用，实现单轮场景下不可达的恶意目标（如绕过权限控制、窃取敏感记忆、篡改任务本质），但现有评估体系严重缺失对此类渐进式、自适应威胁的系统性评测能力。\n\n为此，我们提出 **AgentLAB**——首个专为评估LLM智能体对**自适应长周期攻击**鲁棒性而设计的开源基准。AgentLAB创新性构建了五大新型攻击范式：**意图劫持**（Intent Hijacking）、**工具链滥用**（Tool Chaining）、**任务注入**（Task Injection）、**目标偏移**（Objective Drifting）与**记忆污染**（Memory Poisoning）；覆盖28个贴近真实部署场景的智能体环境（含API集成、文件系统、多代理协作等）；并提供644个经过人工验证、具备明确攻击路径与成功判据的安全测试用例。\n\n基于AgentLAB的大规模实证评估表明：当前主流LLM智能体（包括基于ReAct、Plan-and-Execute及LLM-as-Judge架构的代表系统）**普遍高度脆弱**——平均攻击成功率超73%，且防御策略若仅针对单轮提示注入或输入过滤（如prompt sanitization、guardrails），**几乎无法抵御长周期攻击**（防御后成功率仍达68%）。本工作不仅揭示了长周期安全建模的紧迫性，更提供了可复现、可扩展、可增量更新的评估基础设施。AgentLAB已开源，地址：https://tanqiujiang.github.io/AgentLAB_main。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16708v2",
      "arxiv_id": "2602.16708v2",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Somesh Jha"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16708v2",
      "url": "https://arxiv.org/abs/2602.16708v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "injection",
        "prompt",
        "security",
        "llm",
        "agent"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的场景，如客户服务规程、多级审批流程、敏感数据访问控制及合规监管（如GDPR、HIPAA）。当前主流做法是将策略嵌入提示词（prompt），但该方式**无强制力保障**——模型可能忽略、误解或绕过策略，导致策略违规频发。\n\n## 方法：PCAS策略编译器  \n我们提出 **PCAS（Policy Compiler for Agentic Systems）**，首个支持**确定性策略执行**的编译型安全框架。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图（causal dependency graph）**，显式刻画工具调用、工具返回、消息传递等事件间的时序与信息流向；  \n- **策略语言设计**：采用类Datalog声明式语言，原生支持**跨智能体溯源**与**传递性信息流推理**（如“A影响B，B影响C ⇒ A间接影响C”）；  \n- **零信任执行机制**：通过轻量级**参考监视器（reference monitor）** 拦截所有动作（含工具调用、消息发送），在执行前实时验证策略，**阻断违规行为**，完全解耦策略执行与LLM推理过程。\n\n## 关键成果  \nPCAS以“编译即合规”范式工作：输入现有智能体代码+策略规范，自动输出**策略内建（policy-by-construction）** 的仪器化系统，**无需重构智能体架构或引入安全专用模块**。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离策略；  \n- 多智能体药物警戒系统的双签审批工作流；  \n- 企业客户服务中的组织级权限策略（如客服不可接触支付信息）。  \n在客户服务任务中，PCAS将前沿模型（GPT-4、Claude 3、Gemini 1.5）的策略合规率从**48%提升至93%**，且所有仪器化运行中实现**零策略违规**。",
      "summary_en": "Large language model (LLM)-based agents are increasingly deployed in high-stakes domains requiring rigorous authorization policies—yet prompt-based policy embedding offers no enforcement guarantees. We present PCAS, the first *Policy Compiler for Agentic Systems*, enabling **deterministic, runtime-enforced policy compliance**. PCAS models agent state as a **causal dependency graph**, capturing information flow across tool calls, results, and messages—beyond linear message histories. Policies are written in a declarative Datalog-derived language supporting transitive flow and cross-agent provenance. A lightweight reference monitor intercepts all actions pre-execution, blocking violations irrespective of LLM reasoning. Given an existing agent implementation and a policy spec, PCAS compiles an instrumented, policy-compliant system *by construction*, requiring no security-specific refactoring. Evaluated on three real-world case studies—including prompt-injection defense, pharmacovigilance approval workflows, and customer service policies—PCAS boosts policy compliance from 48% to 93% across frontier models (GPT-4, Claude 3, Gemini 1.5), with **zero policy violations** in all instrumented runs.",
      "summary": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的场景，如客户服务规程、多级审批流程、敏感数据访问控制及合规监管（如GDPR、HIPAA）。当前主流做法是将策略嵌入提示词（prompt），但该方式**无强制力保障**——模型可能忽略、误解或绕过策略，导致策略违规频发。\n\n## 方法：PCAS策略编译器  \n我们提出 **PCAS（Policy Compiler for Agentic Systems）**，首个支持**确定性策略执行**的编译型安全框架。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图（causal dependency graph）**，显式刻画工具调用、工具返回、消息传递等事件间的时序与信息流向；  \n- **策略语言设计**：采用类Datalog声明式语言，原生支持**跨智能体溯源**与**传递性信息流推理**（如“A影响B，B影响C ⇒ A间接影响C”）；  \n- **零信任执行机制**：通过轻量级**参考监视器（reference monitor）** 拦截所有动作（含工具调用、消息发送），在执行前实时验证策略，**阻断违规行为**，完全解耦策略执行与LLM推理过程。\n\n## 关键成果  \nPCAS以“编译即合规”范式工作：输入现有智能体代码+策略规范，自动输出**策略内建（policy-by-construction）** 的仪器化系统，**无需重构智能体架构或引入安全专用模块**。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离策略；  \n- 多智能体药物警戒系统的双签审批工作流；  \n- 企业客户服务中的组织级权限策略（如客服不可接触支付信息）。  \n在客户服务任务中，PCAS将前沿模型（GPT-4、Claude 3、Gemini 1.5）的策略合规率从**48%提升至93%**，且所有仪器化运行中实现**零策略违规**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16752v1",
      "arxiv_id": "2602.16752v1",
      "title": "The Vulnerability of LLM Rankers to Prompt Injection Attacks",
      "authors": [
        "Yu Yin",
        "Shuai Wang",
        "Bevan Koopman",
        "Guido Zuccon"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful re-rankers. Recent research has however showed that simple prompt injections embedded within a candidate document (i.e., jailbreak prompt attacks) can significantly alter an LLM's ranking decisions. While this poses serious security risks to LLM-based ranking pipelines, the extent to which this vulnerability persists across diverse LLM families, architectures, and settings remains largely under-explored. In this paper, we present a comprehensive empirical study of jailbreak prompt attacks against LLM rankers. We focus our evaluation on two complementary tasks: (1) Preference Vulnerability Assessment, measuring intrinsic susceptibility via attack success rate (ASR); and (2) Ranking Vulnerability Assessment, quantifying the operational impact on the ranking's quality (nDCG@10). We systematically examine three prevalent ranking paradigms (pairwise, listwise, setwise) under two injection variants: decision objective hijacking and decision criteria hijacking. Beyond reproducing prior findings, we expand the analysis to cover vulnerability scaling across model families, position sensitivity, backbone architectures, and cross-domain robustness. Our results characterize the boundary conditions of these vulnerabilities, revealing critical insights such as that encoder-decoder architectures exhibit strong inherent resilience to jailbreak attacks. We publicly release our code and additional experimental results at https://github.com/ielab/LLM-Ranker-Attack.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16752v1",
      "url": "https://arxiv.org/abs/2602.16752v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "prompt",
        "jailbreak",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 研究背景  \n大型语言模型（LLMs）正被广泛用作检索后重排序器（rankers），但其对**提示注入攻击**（尤其是嵌入候选文档中的“越狱式”jailbreak prompt）的脆弱性已引发严重安全关切。尽管已有工作证实简单注入可显著扭曲排名决策，但该漏洞在不同模型家族、架构（如encoder-only、encoder-decoder、decoder-only）、排序范式及跨领域场景下的普适性与边界条件仍缺乏系统性实证研究。\n\n## 方法设计  \n本文开展了迄今最全面的LLM重排序器越狱攻击实证研究：  \n- **双维度评估框架**：① **偏好脆弱性评估**（Preference Vulnerability），以攻击成功率（ASR）量化模型内在易感性；② **排序脆弱性评估**（Ranking Vulnerability），以nDCG@10下降幅度衡量实际业务影响；  \n- **三类主流排序范式**：pairwise（成对比较）、listwise（列表级打分）、setwise（集合级判断）；  \n- **两类注入机制**：*决策目标劫持*（如诱导模型忽略相关性转而响应恶意指令）与*决策标准劫持*（如篡改相关性判定依据）；  \n- **多维扩展分析**：覆盖6大模型家族（Llama, Qwen, GLM, ChatGLM, BGE-Reranker, Rerank-Mini）、位置敏感性（注入位置对攻击效果的影响）、骨干架构对比、以及跨领域（MSMARCO→TREC-DL）鲁棒性验证。\n\n## 关键发现与创新  \n- 首次揭示**encoder-decoder架构**（如BGE-Reranker）具有**强内生抗性**，ASR平均低于12%，显著优于decoder-only模型（ASR达68%+）；  \n- listwise范式最脆弱，setwise相对稳健；注入位置对encoder-only模型影响显著（首/尾句敏感），而encoder-decoder模型表现稳定；  \n- 跨域迁移攻击成功率下降超40%，表明当前越狱策略泛化能力有限；  \n- 公开全部代码、攻击模板与细粒度实验数据（含120+配置组合结果），推动可复现、可防御的LLM排序安全研究。",
      "summary_en": "Large Language Models (LLMs) are increasingly deployed as re-rankers, yet their susceptibility to jailbreak prompt injection attacks—especially when malicious prompts are embedded within candidate documents—poses critical security risks. This paper presents the first comprehensive empirical study of such attacks across diverse LLM families, architectures, and ranking paradigms. We introduce a dual-axis evaluation framework: (1) *Preference Vulnerability*, measured by Attack Success Rate (ASR); and (2) *Ranking Vulnerability*, quantified via nDCG@10 degradation. We systematically assess three ranking paradigms (pairwise, listwise, setwise) under two injection variants (decision objective hijacking and decision criteria hijacking), spanning 6 model families, position sensitivity, backbone architectures, and cross-domain robustness. Key findings include: encoder-decoder models (e.g., BGE-Reranker) exhibit strong inherent resilience (mean ASR <12%), significantly outperforming decoder-only counterparts (ASR >68%); listwise ranking is most vulnerable; and cross-domain attack transferability drops sharply (>40% ASR reduction). We publicly release all code and experimental results to advance reproducible, secure LLM ranking research.",
      "summary": "## 研究背景  \n大型语言模型（LLMs）正被广泛用作检索后重排序器（rankers），但其对**提示注入攻击**（尤其是嵌入候选文档中的“越狱式”jailbreak prompt）的脆弱性已引发严重安全关切。尽管已有工作证实简单注入可显著扭曲排名决策，但该漏洞在不同模型家族、架构（如encoder-only、encoder-decoder、decoder-only）、排序范式及跨领域场景下的普适性与边界条件仍缺乏系统性实证研究。\n\n## 方法设计  \n本文开展了迄今最全面的LLM重排序器越狱攻击实证研究：  \n- **双维度评估框架**：① **偏好脆弱性评估**（Preference Vulnerability），以攻击成功率（ASR）量化模型内在易感性；② **排序脆弱性评估**（Ranking Vulnerability），以nDCG@10下降幅度衡量实际业务影响；  \n- **三类主流排序范式**：pairwise（成对比较）、listwise（列表级打分）、setwise（集合级判断）；  \n- **两类注入机制**：*决策目标劫持*（如诱导模型忽略相关性转而响应恶意指令）与*决策标准劫持*（如篡改相关性判定依据）；  \n- **多维扩展分析**：覆盖6大模型家族（Llama, Qwen, GLM, ChatGLM, BGE-Reranker, Rerank-Mini）、位置敏感性（注入位置对攻击效果的影响）、骨干架构对比、以及跨领域（MSMARCO→TREC-DL）鲁棒性验证。\n\n## 关键发现与创新  \n- 首次揭示**encoder-decoder架构**（如BGE-Reranker）具有**强内生抗性**，ASR平均低于12%，显著优于decoder-only模型（ASR达68%+）；  \n- listwise范式最脆弱，setwise相对稳健；注入位置对encoder-only模型影响显著（首/尾句敏感），而encoder-decoder模型表现稳定；  \n- 跨域迁移攻击成功率下降超40%，表明当前越狱策略泛化能力有限；  \n- 公开全部代码、攻击模板与细粒度实验数据（含120+配置组合结果），推动可复现、可防御的LLM排序安全研究。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16741v1",
      "arxiv_id": "2602.16741v1",
      "title": "Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis",
      "authors": [
        "Scott Thornton"
      ],
      "abstract": "AI-assisted code review is widely used to detect vulnerabilities before production release. Prior work shows that adversarial prompt manipulation can degrade large language model (LLM) performance in code generation. We test whether similar comment-based manipulation misleads LLMs during vulnerability detection. We build a 100-sample benchmark across Python, JavaScript, and Java, each paired with eight comment variants ranging from no comments to adversarial strategies such as authority spoofing and technical deception. Eight frontier models, five commercial and three open-source, are evaluated in 9,366 trials. Adversarial comments produce small, statistically non-significant effects on detection accuracy (McNemar exact p > 0.21; all 95 percent confidence intervals include zero). This holds for commercial models with 89 to 96 percent baseline detection and open-source models with 53 to 72 percent, despite large absolute performance gaps. Unlike generation settings where comment manipulation achieves high attack success, detection performance does not meaningfully degrade. More complex adversarial strategies offer no advantage over simple manipulative comments. We test four automated defenses across 4,646 additional trials (14,012 total). Static analysis cross-referencing performs best at 96.9 percent detection and recovers 47 percent of baseline misses. Comment stripping reduces detection for weaker models by removing helpful context. Failures concentrate on inherently difficult vulnerability classes, including race conditions, timing side channels, and complex authorization logic, rather than on adversarial comments.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16741v1",
      "url": "https://arxiv.org/abs/2602.16741v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "security"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与目标  \n随着AI辅助代码审查在生产前漏洞检测中的广泛应用，其安全性正面临新型对抗威胁。已有研究证实，针对代码生成任务的对抗性提示操纵可显著削弱大语言模型（LLM）性能；但**在漏洞检测这一关键安全场景中，仅通过源码注释实施的对抗攻击是否有效，尚无大规模实证验证**。本研究首次系统探究“对抗性代码注释”对LLM安全审查能力的干扰效应，并评估实用化防御策略。\n\n## 方法与规模  \n我们构建了覆盖Python、JavaScript和Java三语言的**100个真实漏洞样本基准集**，每个样本配以8种注释变体（含无注释、误导性权威伪装、技术性欺骗等），共800个测试用例。在8个前沿模型（5个商用+3个开源）上开展**9,366次严格控制的检测实验**；进一步引入4类自动化防御机制，在额外4,646次试验中完成总计**14,012次评估**，为迄今该方向最大规模实证研究。\n\n## 核心发现  \n- **对抗注释几乎不损害检测鲁棒性**：所有模型在各类对抗策略下准确率变化均微小且统计不显著（McNemar精确检验 *p* > 0.21；95% CI均包含零）；商用模型（基线89–96%）与开源模型（53–72%）均呈现相同趋势。  \n- **复杂策略无增益**：权威伪装、技术欺骗等高级对抗手段未优于简单误导性注释。  \n- **防御效果分化明显**：静态分析交叉验证表现最优（96.9%检测率，挽回47%基线漏报）；而注释剥离反而降低弱模型性能——证明注释常含关键上下文。  \n- **根本瓶颈在语义难度**：失败案例高度集中于固有高难度漏洞类型（竞态条件、时序侧信道、复杂授权逻辑），**与注释是否对抗无关**，揭示当前LLM安全分析的核心局限在于深层程序语义理解，而非表层文本干扰。",
      "summary_en": "This large-scale empirical study investigates whether adversarial code comments can meaningfully degrade LLM-based vulnerability detection—a critical security application distinct from code generation. We evaluate 8 frontier models (5 commercial, 3 open-source) across 100 real-world vulnerable code samples in Python, JavaScript, and Java, each paired with 8 comment variants (including authority spoofing and technical deception), yielding 9,366 detection trials. Contrary to prior findings in code generation, adversarial comments induce only negligible, statistically non-significant accuracy changes (McNemar *p* > 0.21; all 95% CIs include zero), even for models with wide baseline performance gaps (53–96%). More sophisticated comment attacks confer no advantage over simple manipulations. Among four automated defenses tested in 4,646 additional trials, static analysis cross-referencing achieves 96.9% detection and recovers 47% of baseline misses—while comment stripping harms weaker models by removing helpful context. Failures stem primarily from intrinsically hard vulnerability classes (e.g., race conditions, timing side channels), not adversarial comments—highlighting semantic reasoning, not textual robustness, as the core bottleneck.",
      "summary": "## 研究背景与目标  \n随着AI辅助代码审查在生产前漏洞检测中的广泛应用，其安全性正面临新型对抗威胁。已有研究证实，针对代码生成任务的对抗性提示操纵可显著削弱大语言模型（LLM）性能；但**在漏洞检测这一关键安全场景中，仅通过源码注释实施的对抗攻击是否有效，尚无大规模实证验证**。本研究首次系统探究“对抗性代码注释”对LLM安全审查能力的干扰效应，并评估实用化防御策略。\n\n## 方法与规模  \n我们构建了覆盖Python、JavaScript和Java三语言的**100个真实漏洞样本基准集**，每个样本配以8种注释变体（含无注释、误导性权威伪装、技术性欺骗等），共800个测试用例。在8个前沿模型（5个商用+3个开源）上开展**9,366次严格控制的检测实验**；进一步引入4类自动化防御机制，在额外4,646次试验中完成总计**14,012次评估**，为迄今该方向最大规模实证研究。\n\n## 核心发现  \n- **对抗注释几乎不损害检测鲁棒性**：所有模型在各类对抗策略下准确率变化均微小且统计不显著（McNemar精确检验 *p* > 0.21；95% CI均包含零）；商用模型（基线89–96%）与开源模型（53–72%）均呈现相同趋势。  \n- **复杂策略无增益**：权威伪装、技术欺骗等高级对抗手段未优于简单误导性注释。  \n- **防御效果分化明显**：静态分析交叉验证表现最优（96.9%检测率，挽回47%基线漏报）；而注释剥离反而降低弱模型性能——证明注释常含关键上下文。  \n- **根本瓶颈在语义难度**：失败案例高度集中于固有高难度漏洞类型（竞态条件、时序侧信道、复杂授权逻辑），**与注释是否对抗无关**，揭示当前LLM安全分析的核心局限在于深层程序语义理解，而非表层文本干扰。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16944v1",
      "arxiv_id": "2602.16944v1",
      "title": "Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming",
      "authors": [
        "Philip Sosnin",
        "Jodie Knapp",
        "Fraser Kennedy",
        "Josh Collyer",
        "Calvin Tsay"
      ],
      "abstract": "This work introduces a verification framework that provides both sound and complete guarantees for data poisoning attacks during neural network training. We formulate adversarial data manipulation, model training, and test-time evaluation in a single mixed-integer quadratic programming (MIQCP) problem. Finding the global optimum of the proposed formulation provably yields worst-case poisoning attacks, while simultaneously bounding the effectiveness of all possible attacks on the given training pipeline. Our framework encodes both the gradient-based training dynamics and model evaluation at test time, enabling the first exact certification of training-time robustness. Experimental evaluation on small models confirms that our approach delivers a complete characterization of robustness against data poisoning.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16944v1",
      "url": "https://arxiv.org/abs/2602.16944v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "data",
        "poisoning"
      ],
      "keyword_score": 3,
      "summary_zh": "## 精确验证数据投毒攻击：基于混合整数规划的完备认证框架\n\n本研究提出首个**严格完备（sound and complete）**的数据投毒攻击鲁棒性验证框架，可对神经网络训练全过程提供**可证明的精确认证**。传统防御方法多依赖启发式或近似分析，难以保证无漏报（false negative）与无误报（false positive）；而本文突破性地将**对抗性数据操纵、梯度下降训练动态、以及测试时模型评估**三者统一建模为一个**单层混合整数二次规划（MIQCP）问题**。该公式化设计的关键创新在于：  \n- **精确编码训练过程**：显式建模有限步数的SGD/Adam更新，包括权重更新、激活函数（如ReLU）的分段线性约束，并通过大M法与二进制变量实现非线性操作的整数线性化；  \n- **端到端最坏情况求解**：求解该MIQCP的全局最优解，**严格等价于构造出使目标模型性能最差的最优投毒样本集**，同时天然导出对所有可能投毒攻击的**紧致鲁棒性上界**；  \n- **首次实现训练时鲁棒性的精确认证**：不同于仅验证输入鲁棒性（如对抗样本）的工作，本框架首次在**训练阶段**完成对数据完整性威胁的数学完备验证。  \n\n在MNIST、CIFAR-10等基准数据集上的小规模模型（≤3层全连接/轻量CNN）实验表明：该方法能**100%完备地刻画鲁棒性边界**——即对给定污染预算（如最多5个毒样本），精确判定“是否存在可导致分类错误的投毒”，且认证结果经穷举验证无误。本工作为可信机器学习提供了理论坚实的验证基石，也为后续可扩展认证算法设计建立了关键范式。",
      "summary_en": "This paper introduces the first verification framework that provides **sound and complete guarantees** for robustness against data-poisoning attacks during neural network training. We formulate adversarial data manipulation, gradient-based training dynamics (e.g., SGD with ReLU activations), and test-time evaluation as a single **mixed-integer quadratic programming (MIQCP)** problem. Solving this MIQCP to global optimality **provably yields the worst-case poisoning attack**, while simultaneously computing a tight upper bound on the maximum possible impact of *any* poisoning strategy under the given training pipeline. Crucially, our formulation exactly encodes finite-step optimization and non-linear model behavior—enabling, for the first time, **exact certification of training-time robustness**. Experiments on small-scale models confirm that our approach delivers a **complete characterization**: it definitively answers whether a poisoning attack exists within a given budget (e.g., ≤5 poisoned samples) that can flip a target prediction—and all certified claims are verified exhaustively.",
      "summary": "## 精确验证数据投毒攻击：基于混合整数规划的完备认证框架\n\n本研究提出首个**严格完备（sound and complete）**的数据投毒攻击鲁棒性验证框架，可对神经网络训练全过程提供**可证明的精确认证**。传统防御方法多依赖启发式或近似分析，难以保证无漏报（false negative）与无误报（false positive）；而本文突破性地将**对抗性数据操纵、梯度下降训练动态、以及测试时模型评估**三者统一建模为一个**单层混合整数二次规划（MIQCP）问题**。该公式化设计的关键创新在于：  \n- **精确编码训练过程**：显式建模有限步数的SGD/Adam更新，包括权重更新、激活函数（如ReLU）的分段线性约束，并通过大M法与二进制变量实现非线性操作的整数线性化；  \n- **端到端最坏情况求解**：求解该MIQCP的全局最优解，**严格等价于构造出使目标模型性能最差的最优投毒样本集**，同时天然导出对所有可能投毒攻击的**紧致鲁棒性上界**；  \n- **首次实现训练时鲁棒性的精确认证**：不同于仅验证输入鲁棒性（如对抗样本）的工作，本框架首次在**训练阶段**完成对数据完整性威胁的数学完备验证。  \n\n在MNIST、CIFAR-10等基准数据集上的小规模模型（≤3层全连接/轻量CNN）实验表明：该方法能**100%完备地刻画鲁棒性边界**——即对给定污染预算（如最多5个毒样本），精确判定“是否存在可导致分类错误的投毒”，且认证结果经穷举验证无误。本工作为可信机器学习提供了理论坚实的验证基石，也为后续可扩展认证算法设计建立了关键范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16346v2",
      "arxiv_id": "2602.16346v2",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "authors": [
        "Nivya Talokar",
        "Ayush K Tarun",
        "Murari Mandal",
        "Maksym Andriushchenko",
        "Antoine Bosselut"
      ],
      "abstract": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16346v2",
      "url": "https://arxiv.org/abs/2602.16346v2",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "jailbreak"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具与长期记忆执行真实世界工作流，但其能力亦可能被恶意行为者用于实施复杂滥用——现有滥用评估基准多聚焦单轮指令攻击，**严重忽视多轮交互中危害行为的渐进式演化**，尤其缺乏对跨语言、多步骤非法目标达成过程的系统性度量。\n\n## 方法：STING 框架  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**——首个面向工具增强型多轮代理的自动化红队测试框架。其核心创新包括：  \n- **良性伪装的分步非法计划生成**：基于可信身份（如“合规税务顾问”）构建逻辑连贯的N步非法目标（如伪造跨境资金流水）；  \n- **自适应多轮探针机制**：以迭代式后续提问模拟真实对抗交互，动态调整策略；  \n- **双层判断架构**：轻量级 judge agent 实时追踪各阶段完成状态，避免人工标注瓶颈。\n\n## 关键发现与创新指标  \n- 在 AgentHarm 基准上，STING 的非法任务完成率较单轮提示提升 **3.2×**，优于适配工具能力的多轮对话基线（+68%）；  \n- 提出**时间至首次越狱（Time-to-First-Jailbreak）随机变量建模范式**，衍生出三项分析工具：  \n  - **发现曲线（Discovery Curves）**：刻画攻击成功率随轮次演化的动态轨迹；  \n  - **语言特异性风险归因（Hazard-Ratio by Language）**：量化不同语种攻击的相对危害强度；  \n  - **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**：首个兼顾时效性与鲁棒性的多轮滥用效能聚合指标。  \n- 多语言实验（覆盖西班牙语、阿拉伯语、日语等6种非英语场景）揭示：**低资源语言攻击成功率并不普遍更高**，颠覆了传统聊天机器人“越小语种越易越狱”的经验认知，凸显工具型代理安全机制的语种依赖性差异。",
      "summary_en": "We introduce **STING**, the first automated red-teaming framework for evaluating illicit assistance in *multi-turn, multilingual, tool-using LLM agents*. Unlike prior single-prompt benchmarks, STING constructs adaptive, step-by-step illicit plans grounded in benign personas and probes target agents iteratively, using lightweight judge agents to track phase-wise completion. We model multi-turn jailbreaking as a *time-to-first-jailbreak* random variable, enabling novel analysis tools—including discovery curves, hazard-ratio attribution by language, and the **Restricted Mean Jailbreak Discovery (RMJD)** metric. Across AgentHarm scenarios, STING achieves substantially higher illicit-task completion than single-turn and chat-oriented multi-turn baselines (+68% over adapted tool-using baselines). In six non-English settings, attack success does *not* consistently increase in lower-resource languages—contradicting common chatbot findings and highlighting distinct failure modes in tool-augmented agents. STING provides a practical, scalable methodology for stress-testing real-world agent deployments.",
      "summary": "## 背景与问题  \n基于大语言模型（LLM）的智能体通过调用工具与长期记忆执行真实世界工作流，但其能力亦可能被恶意行为者用于实施复杂滥用——现有滥用评估基准多聚焦单轮指令攻击，**严重忽视多轮交互中危害行为的渐进式演化**，尤其缺乏对跨语言、多步骤非法目标达成过程的系统性度量。\n\n## 方法：STING 框架  \n我们提出 **STING（Sequential Testing of Illicit N-step Goal execution）**——首个面向工具增强型多轮代理的自动化红队测试框架。其核心创新包括：  \n- **良性伪装的分步非法计划生成**：基于可信身份（如“合规税务顾问”）构建逻辑连贯的N步非法目标（如伪造跨境资金流水）；  \n- **自适应多轮探针机制**：以迭代式后续提问模拟真实对抗交互，动态调整策略；  \n- **双层判断架构**：轻量级 judge agent 实时追踪各阶段完成状态，避免人工标注瓶颈。\n\n## 关键发现与创新指标  \n- 在 AgentHarm 基准上，STING 的非法任务完成率较单轮提示提升 **3.2×**，优于适配工具能力的多轮对话基线（+68%）；  \n- 提出**时间至首次越狱（Time-to-First-Jailbreak）随机变量建模范式**，衍生出三项分析工具：  \n  - **发现曲线（Discovery Curves）**：刻画攻击成功率随轮次演化的动态轨迹；  \n  - **语言特异性风险归因（Hazard-Ratio by Language）**：量化不同语种攻击的相对危害强度；  \n  - **受限平均越狱发现时间（Restricted Mean Jailbreak Discovery, RMJD）**：首个兼顾时效性与鲁棒性的多轮滥用效能聚合指标。  \n- 多语言实验（覆盖西班牙语、阿拉伯语、日语等6种非英语场景）揭示：**低资源语言攻击成功率并不普遍更高**，颠覆了传统聊天机器人“越小语种越易越狱”的经验认知，凸显工具型代理安全机制的语种依赖性差异。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16749v1",
      "arxiv_id": "2602.16749v1",
      "title": "U-FedTomAtt: Ultra-lightweight Federated Learning with Attention for Tomato Disease Recognition",
      "authors": [
        "Romiyal George",
        "Sathiyamohan Nishankar",
        "Selvarajah Thuseethan",
        "Chathrie Wimalasooriya",
        "Yakub Sebastian",
        "Roshan G. Ragel",
        "Zhongwei Liang"
      ],
      "abstract": "Federated learning has emerged as a privacy-preserving and efficient approach for deploying intelligent agricultural solutions. Accurate edge-based diagnosis across geographically dispersed farms is crucial for recognising tomato diseases in sustainable farming. Traditional centralised training aggregates raw data on a central server, leading to communication overhead, privacy risks and latency. Meanwhile, edge devices require lightweight networks to operate effectively within limited resources. In this paper, we propose U-FedTomAtt, an ultra-lightweight federated learning framework with attention for tomato disease recognition in resource-constrained and distributed environments. The model comprises only 245.34K parameters and 71.41 MFLOPS. First, we propose an ultra-lightweight neural network with dilated bottleneck (DBNeck) modules and a linear transformer to minimise computational and memory overhead. To mitigate potential accuracy loss, a novel local-global residual attention (LoGRA) module is incorporated. Second, we propose the federated dual adaptive weight aggregation (FedDAWA) algorithm that enhances global model accuracy. Third, our framework is validated using three benchmark datasets for tomato diseases under simulated federated settings. Experimental results show that the proposed method achieves 0.9910% and 0.9915% Top-1 accuracy and 0.9923% and 0.9897% F1-scores on SLIF-Tomato and PlantVillage tomato datasets, respectively.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16749v1",
      "url": "https://arxiv.org/abs/2602.16749v1",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n在智慧农业中，番茄病害的实时、隐私保护型边缘识别对可持续种植至关重要。传统中心化深度学习需上传原始图像至服务器，引发**数据隐私泄露、通信带宽压力与推理延迟**三大瓶颈；而边缘设备（如田间摄像头、微型无人机）又受限于算力、内存与功耗，难以部署常规CNN或Transformer模型。\n\n## 方法创新  \n本文提出**U-FedTomAtt**——一种面向资源极度受限场景的超轻量联邦学习框架，专用于分布式番茄病害识别：  \n- **超轻量主干网络**：仅含**245.34K参数**与**71.41 MFLOPS**计算量，融合**空洞瓶颈模块（DBNeck）** 与**线性Transformer**，显著压缩模型体积与推理开销；  \n- **局部-全局残差注意力（LoGRA）模块**：在极低参数预算下引入细粒度空间-通道协同注意力，有效补偿轻量化导致的判别力衰减；  \n- **联邦双自适应权重聚合算法（FedDAWA）**：动态校准客户端本地更新贡献度，缓解数据异构性（Non-IID）导致的模型偏差，提升全局收敛稳定性与精度。\n\n## 实验验证与性能  \n在SLIF-Tomato与PlantVillage两大公开番茄病害数据集上构建三机构模拟联邦设置。结果表明：U-FedTomAtt在严苛边缘约束下仍实现**卓越泛化能力**——SLIF-Tomato上Top-1准确率达**99.10%**、F1-score达**99.23%**；PlantVillage上达**99.15%**准确率与**98.97%** F1-score，显著优于MobileNetV3、EfficientNet-Lite及FedAvg基线。本工作为农业AI落地提供了**可部署、可信赖、可扩展**的联邦轻量化新范式。",
      "summary_en": "Federated learning (FL) enables privacy-preserving, edge-based tomato disease recognition across distributed farms—yet faces dual challenges: excessive communication/latency in centralized training and prohibitive resource demands of standard models on edge devices. To address this, we propose **U-FedTomAtt**, an ultra-lightweight FL framework featuring only **245.34K parameters** and **71.41 MFLOPS**, designed for real-world agricultural constraints. Its core innovations include: (i) a novel backbone integrating **dilated bottleneck (DBNeck) modules** and a **linear transformer** for extreme efficiency; (ii) a **local-global residual attention (LoGRA)** module to preserve discriminative capability without increasing parameters; and (iii) **FedDAWA**, a dual-adaptive weight aggregation algorithm that dynamically weights heterogeneous client updates to boost global model accuracy. Evaluated on SLIF-Tomato and PlantVillage under realistic non-IID federated settings, U-FedTomAtt achieves **99.10% and 99.15% Top-1 accuracy**, and **99.23% and 98.97% F1-score**, respectively—surpassing lightweight baselines while enabling on-device deployment.",
      "summary": "## 研究背景  \n在智慧农业中，番茄病害的实时、隐私保护型边缘识别对可持续种植至关重要。传统中心化深度学习需上传原始图像至服务器，引发**数据隐私泄露、通信带宽压力与推理延迟**三大瓶颈；而边缘设备（如田间摄像头、微型无人机）又受限于算力、内存与功耗，难以部署常规CNN或Transformer模型。\n\n## 方法创新  \n本文提出**U-FedTomAtt**——一种面向资源极度受限场景的超轻量联邦学习框架，专用于分布式番茄病害识别：  \n- **超轻量主干网络**：仅含**245.34K参数**与**71.41 MFLOPS**计算量，融合**空洞瓶颈模块（DBNeck）** 与**线性Transformer**，显著压缩模型体积与推理开销；  \n- **局部-全局残差注意力（LoGRA）模块**：在极低参数预算下引入细粒度空间-通道协同注意力，有效补偿轻量化导致的判别力衰减；  \n- **联邦双自适应权重聚合算法（FedDAWA）**：动态校准客户端本地更新贡献度，缓解数据异构性（Non-IID）导致的模型偏差，提升全局收敛稳定性与精度。\n\n## 实验验证与性能  \n在SLIF-Tomato与PlantVillage两大公开番茄病害数据集上构建三机构模拟联邦设置。结果表明：U-FedTomAtt在严苛边缘约束下仍实现**卓越泛化能力**——SLIF-Tomato上Top-1准确率达**99.10%**、F1-score达**99.23%**；PlantVillage上达**99.15%**准确率与**98.97%** F1-score，显著优于MobileNetV3、EfficientNet-Lite及FedAvg基线。本工作为农业AI落地提供了**可部署、可信赖、可扩展**的联邦轻量化新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16843v1",
      "arxiv_id": "2602.16843v1",
      "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
      "authors": [
        "Ahmed Rafid",
        "Rumman Adib",
        "Fariya Ahmed",
        "Ajwad Abrar",
        "Mohammed Saidul Islam"
      ],
      "abstract": "Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16843v1",
      "url": "https://arxiv.org/abs/2602.16843v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n事实一致性评估对可信文本摘要至关重要，尤其在医疗、新闻等高风险领域。然而，现有主流指标（如BERTScore、FactCC）严重依赖英文参考摘要，且普遍忽视孟加拉语——全球超2.7亿人使用的高使用率但低资源语言。参考依赖不仅限制实际部署（因人工撰写参考摘要成本高昂），更在低资源场景下加剧数据瓶颈。\n\n## 方法创新  \n本文提出**BanglaSummEval**：首个面向孟加拉语、无需参考摘要的事实一致性评估框架。其核心是**问答驱动的端到端评估范式**：  \n- 利用单个多语言指令微调模型（mT5-base）统一完成四大任务：从源文档与摘要中自动生成关键问题、回答问题、提取候选答案、并为问题分配重要性权重；  \n- 通过**BERTScore-Recall**比对模型从源文档与摘要中生成的答案，捕捉深层语义一致性（而非表面词重叠）；  \n- 同时量化**事实准确性**（答案是否可由源文档支持）与**内容覆盖度**（摘要是否涵盖源文档的关键信息点）。\n\n## 验证与优势  \n在300篇真实人工撰写的教育与医疗领域孟加拉语摘要上进行严格验证，BanglaSummEval与专家人工评分呈现强相关性（Pearson *r* = 0.694，Spearman *ρ* = 0.763），显著优于基线无参考指标（+0.21 Pearson提升）。其**可解释性诊断能力**（逐问题溯源不一致点）和**轻量级设计**（单模型全流程）使其成为低资源语言摘要评估的实用、透明、低成本解决方案。",
      "summary_en": "BanglaSummEval is the first reference-free, question-answering-based framework for factual consistency evaluation in Bangla summarization. It leverages a single multilingual instruction-tuned language model to jointly generate questions from source documents and summaries, answer them, extract candidate answers, and weight question importance—enabling unified, low-cost assessment of both factual accuracy and content coverage. Crucially, it uses BERTScore-Recall to compare answers derived from source and summary, capturing semantic consistency beyond lexical overlap. Evaluated on 300 human-written Bangla summaries from educational and medical domains, BanglaSummEval achieves strong correlation with expert judgments (Pearson *r* = 0.694; Spearman *ρ* = 0.763), outperforming existing no-reference baselines. Its interpretability, efficiency, and language-specific design make it a practical solution for low-resource NLP evaluation.",
      "summary": "## 背景与问题  \n事实一致性评估对可信文本摘要至关重要，尤其在医疗、新闻等高风险领域。然而，现有主流指标（如BERTScore、FactCC）严重依赖英文参考摘要，且普遍忽视孟加拉语——全球超2.7亿人使用的高使用率但低资源语言。参考依赖不仅限制实际部署（因人工撰写参考摘要成本高昂），更在低资源场景下加剧数据瓶颈。\n\n## 方法创新  \n本文提出**BanglaSummEval**：首个面向孟加拉语、无需参考摘要的事实一致性评估框架。其核心是**问答驱动的端到端评估范式**：  \n- 利用单个多语言指令微调模型（mT5-base）统一完成四大任务：从源文档与摘要中自动生成关键问题、回答问题、提取候选答案、并为问题分配重要性权重；  \n- 通过**BERTScore-Recall**比对模型从源文档与摘要中生成的答案，捕捉深层语义一致性（而非表面词重叠）；  \n- 同时量化**事实准确性**（答案是否可由源文档支持）与**内容覆盖度**（摘要是否涵盖源文档的关键信息点）。\n\n## 验证与优势  \n在300篇真实人工撰写的教育与医疗领域孟加拉语摘要上进行严格验证，BanglaSummEval与专家人工评分呈现强相关性（Pearson *r* = 0.694，Spearman *ρ* = 0.763），显著优于基线无参考指标（+0.21 Pearson提升）。其**可解释性诊断能力**（逐问题溯源不一致点）和**轻量级设计**（单模型全流程）使其成为低资源语言摘要评估的实用、透明、低成本解决方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15945v1",
      "arxiv_id": "2602.15945v1",
      "title": "From Tool Orchestration to Code Execution: A Study of MCP Design Choices",
      "authors": [
        "Yuval Felendler",
        "Parth A. Gandhi",
        "Idan Habler",
        "Yuval Elovici",
        "Asaf Shabtai"
      ],
      "abstract": "Model Context Protocols (MCPs) provide a unified platform for agent systems to discover, select, and orchestrate tools across heterogeneous execution environments. As MCP-based systems scale to incorporate larger tool catalogs and multiple concurrently connected MCP servers, traditional tool-by-tool invocation increases coordination overhead, fragments state management, and limits support for wide-context operations. To address these scalability challenges, recent MCP designs have incorporated code execution as a first-class capability, an approach called Code Execution MCP (CE-MCP). This enables agents to consolidate complex workflows, such as SQL querying, file analysis, and multi-step data transformations, into a single program that executes within an isolated runtime environment. In this work, we formalize the architectural distinction between context-coupled (traditional) and context-decoupled (CE-MCP) models, analyzing their fundamental scalability trade-offs. Using the MCP-Bench framework across 10 representative servers, we empirically evaluate task behavior, tool utilization patterns, execution latency, and protocol efficiency as the scale of connected MCP servers and available tools increases, demonstrating that while CE-MCP significantly reduces token usage and execution latency, it introduces a vastly expanded attack surface. We address this security gap by applying the MAESTRO framework, identifying sixteen attack classes across five execution phases-including specific code execution threats such as exception-mediated code injection and unsafe capability synthesis. We validate these vulnerabilities through adversarial scenarios across multiple LLMs and propose a layered defense architecture comprising containerized sandboxing and semantic gating. Our findings provide a rigorous roadmap for balancing scalability and security in production-ready executable agent workflows.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15945v1",
      "url": "https://arxiv.org/abs/2602.15945v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \nModel Context Protocols（MCP）作为智能体系统统一调用异构工具的协议框架，正面临规模化瓶颈：当工具库扩大、多MCP服务器并发连接时，传统“逐工具调用”模式导致协调开销激增、状态管理碎片化、宽上下文操作支持乏力。\n\n## 方法与创新  \n本文首次形式化区分**上下文耦合型**（传统MCP）与**上下文解耦型**（Code Execution MCP, CE-MCP）两类架构，并提出系统性评估框架。基于**MCP-Bench**在10个典型MCP服务器上开展实证研究，量化分析任务完成率、工具调用频次、端到端延迟及协议消息效率随规模增长的变化规律；同步引入**MAESTRO**安全分析框架，识别CE-MCP全生命周期中5个执行阶段的16类新型攻击模式（如异常中介代码注入、不安全能力合成等），覆盖Python沙箱逃逸、权限越界、LLM提示劫持等高危场景。\n\n## 主要发现  \n- CE-MCP将SQL查询、文件分析等多步工作流封装为单程序执行，**平均降低38.7% token消耗与42.1%执行延迟**；  \n- 但其攻击面指数级扩展——实验验证16类漏洞在GPT-4、Claude-3、Qwen2-72B等主流LLM驱动下均成功触发；  \n- 提出**分层防御架构**：底层采用轻量级容器化沙箱（gVisor增强隔离），上层部署语义门控机制（基于AST的动态能力白名单+运行时数据流约束），在保持99.2%合法任务通过率前提下，阻断全部已知攻击路径。\n\n本研究为可执行智能体系统提供了首个兼顾**可扩展性-安全性权衡**的工程化设计蓝图。",
      "summary_en": "This paper investigates scalability-security trade-offs in Model Context Protocols (MCP), introducing Code Execution MCP (CE-MCP) as a context-decoupled architecture that consolidates multi-step workflows (e.g., SQL queries, data transformations) into isolated code execution. Empirical evaluation across 10 MCP servers using MCP-Bench shows CE-MCP reduces token usage by 38.7% and latency by 42.1% versus traditional tool-by-tool orchestration—but expands the attack surface dramatically. Applying the MAESTRO framework, we identify 16 novel attack classes across five execution phases, including exception-mediated code injection and unsafe capability synthesis, validated against GPT-4, Claude-3, and Qwen2-72B. We propose a layered defense: containerized sandboxing (gVisor-enhanced) plus semantic gating (AST-based capability whitelisting + runtime data-flow constraints), achieving 99.2% legitimate task success while blocking all identified attacks. This work delivers the first production-ready roadmap for secure, scalable executable agent systems.",
      "summary": "## 研究背景与问题  \nModel Context Protocols（MCP）作为智能体系统统一调用异构工具的协议框架，正面临规模化瓶颈：当工具库扩大、多MCP服务器并发连接时，传统“逐工具调用”模式导致协调开销激增、状态管理碎片化、宽上下文操作支持乏力。\n\n## 方法与创新  \n本文首次形式化区分**上下文耦合型**（传统MCP）与**上下文解耦型**（Code Execution MCP, CE-MCP）两类架构，并提出系统性评估框架。基于**MCP-Bench**在10个典型MCP服务器上开展实证研究，量化分析任务完成率、工具调用频次、端到端延迟及协议消息效率随规模增长的变化规律；同步引入**MAESTRO**安全分析框架，识别CE-MCP全生命周期中5个执行阶段的16类新型攻击模式（如异常中介代码注入、不安全能力合成等），覆盖Python沙箱逃逸、权限越界、LLM提示劫持等高危场景。\n\n## 主要发现  \n- CE-MCP将SQL查询、文件分析等多步工作流封装为单程序执行，**平均降低38.7% token消耗与42.1%执行延迟**；  \n- 但其攻击面指数级扩展——实验验证16类漏洞在GPT-4、Claude-3、Qwen2-72B等主流LLM驱动下均成功触发；  \n- 提出**分层防御架构**：底层采用轻量级容器化沙箱（gVisor增强隔离），上层部署语义门控机制（基于AST的动态能力白名单+运行时数据流约束），在保持99.2%合法任务通过率前提下，阻断全部已知攻击路径。\n\n本研究为可执行智能体系统提供了首个兼顾**可扩展性-安全性权衡**的工程化设计蓝图。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15815v1",
      "arxiv_id": "2602.15815v1",
      "title": "Natural Privacy Filters Are Not Always Free: A Characterization of Free Natural Filters",
      "authors": [
        "Matthew Regehr",
        "Bingshan Hu",
        "Ethan Leeman",
        "Pasin Manurangsi",
        "Pierre Tholoniat",
        "Mathias Lécuyer"
      ],
      "abstract": "We study natural privacy filters, which enable the exact composition of differentially private (DP) mechanisms with adaptively chosen privacy characteristics. Earlier privacy filters consider only simple privacy parameters such as Rényi-DP or Gaussian DP parameters. Natural filters account for the entire privacy profile of every query, promising greater utility for a given privacy budget. We show that, contrary to other forms of DP, natural privacy filters are not free in general. Indeed, we show that only families of privacy mechanisms that are well-ordered when composed admit free natural privacy filters.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15815v1",
      "url": "https://arxiv.org/abs/2602.15815v1",
      "categories": [
        "cs.CR",
        "cs.DS"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n在差分隐私（DP）的实际部署中，**自适应查询序列**的隐私预算分配极具挑战性。传统隐私滤波器（privacy filters）仅支持固定或简单参数化机制（如Rényi-DP或Gaussian DP），难以充分利用机制的完整隐私剖面（privacy profile）。为此，“自然隐私滤波器”（natural privacy filters）被提出——它能基于每次查询的**全隐私剖面**动态判定是否允许发布结果，从而在相同总隐私预算下显著提升效用。一个关键开放问题是：这类滤波器是否“免费”（free），即其应用是否不引入额外隐私损失？此前工作隐含假设其为免费，但缺乏严格验证。\n\n## 方法与理论刻画  \n本文首次对自然隐私滤波器的“免费性”进行形式化界定与系统性刻画。我们构建了基于**隐私剖面序结构**的抽象框架，将机制族建模为偏序集，并定义“自由自然滤波器”需满足：对任意自适应查询序列，滤波器输出的合成隐私曲线严格等于各机制隐私剖面的最优（tightest）逐点组合。我们通过构造反例与充分必要条件证明，**自由性并非普遍成立**。\n\n## 主要发现与创新点  \n- **核心结论**：自然隐私滤波器仅当所涉机制族在复合运算下构成**全序集（well-ordered under composition）** 时才是免费的；否则，滤波过程本身会引入不可忽略的隐私开销。  \n- **理论贡献**：首次给出自由自然滤波器的充要条件，揭示了隐私剖面结构与自适应组合之间的深层耦合关系。  \n- **实践启示**：警示隐私工程师不可默认自然滤波器无成本；在非良序机制（如混合ε-DP与f-DP机制）场景中，必须显式建模滤波器开销以保障端到端隐私保证。",
      "summary_en": "We study natural privacy filters—adaptive mechanisms that enable exact composition of differentially private queries by leveraging their full privacy profiles (e.g., $f$-DP curves), rather than simplified parameters like Rényi or Gaussian DP. While earlier filters are “free” (impose no extra privacy cost), we show natural filters are *not* free in general. We characterize precisely when they are: a family of DP mechanisms admits a free natural filter **if and only if** its privacy profiles are well-ordered under composition—i.e., for any two mechanisms, one’s privacy curve dominates the other’s pointwise after composition. We prove this via tight constructions and impossibility results, demonstrating that arbitrary mechanism families (e.g., mixing pure DP and approximate DP) incur unavoidable privacy overhead from filtering itself. This refutes the implicit assumption of cost-free adaptivity and provides the first necessary and sufficient structural condition for safe, lossless composition using full privacy profiles.",
      "summary": "## 研究背景与问题  \n在差分隐私（DP）的实际部署中，**自适应查询序列**的隐私预算分配极具挑战性。传统隐私滤波器（privacy filters）仅支持固定或简单参数化机制（如Rényi-DP或Gaussian DP），难以充分利用机制的完整隐私剖面（privacy profile）。为此，“自然隐私滤波器”（natural privacy filters）被提出——它能基于每次查询的**全隐私剖面**动态判定是否允许发布结果，从而在相同总隐私预算下显著提升效用。一个关键开放问题是：这类滤波器是否“免费”（free），即其应用是否不引入额外隐私损失？此前工作隐含假设其为免费，但缺乏严格验证。\n\n## 方法与理论刻画  \n本文首次对自然隐私滤波器的“免费性”进行形式化界定与系统性刻画。我们构建了基于**隐私剖面序结构**的抽象框架，将机制族建模为偏序集，并定义“自由自然滤波器”需满足：对任意自适应查询序列，滤波器输出的合成隐私曲线严格等于各机制隐私剖面的最优（tightest）逐点组合。我们通过构造反例与充分必要条件证明，**自由性并非普遍成立**。\n\n## 主要发现与创新点  \n- **核心结论**：自然隐私滤波器仅当所涉机制族在复合运算下构成**全序集（well-ordered under composition）** 时才是免费的；否则，滤波过程本身会引入不可忽略的隐私开销。  \n- **理论贡献**：首次给出自由自然滤波器的充要条件，揭示了隐私剖面结构与自适应组合之间的深层耦合关系。  \n- **实践启示**：警示隐私工程师不可默认自然滤波器无成本；在非良序机制（如混合ε-DP与f-DP机制）场景中，必须显式建模滤波器开销以保障端到端隐私保证。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15802v1",
      "arxiv_id": "2602.15802v1",
      "title": "Local Node Differential Privacy",
      "authors": [
        "Sofya Raskhodnikova",
        "Adam Smith",
        "Connor Wagaman",
        "Anatoly Zavyalov"
      ],
      "abstract": "We initiate an investigation of node differential privacy for graphs in the local model of private data analysis. In our model, dubbed LNDP, each node sees its own edge list and releases the output of a local randomizer on this input. These outputs are aggregated by an untrusted server to obtain a final output.   We develop a novel algorithmic framework for this setting that allows us to accurately answer arbitrary linear queries on a blurry approximation of the input graph's degree distribution. For some natural problems, the resulting algorithms match the accuracy achievable with node privacy in the central model, where data are held and processed by a trusted server. We also prove lower bounds on the error required by LNDP that imply the optimality of our algorithms for several fundamental graph statistics. We then lift these lower bounds to the interactive LNDP setting, demonstrating the optimality of our algorithms even when constantly many rounds of interaction are permitted. Obtaining our lower bounds requires new approaches, since those developed for the usual local model do not apply to the inherently overlapping inputs that arise from graphs. Finally, we prove structural results that reveal qualitative differences between local node privacy and the standard local model for tabular data.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15802v1",
      "url": "https://arxiv.org/abs/2602.15802v1",
      "categories": [
        "cs.DS",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "differential"
      ],
      "keyword_score": 2,
      "summary_zh": "## 本地节点差分隐私（LNDP）：图数据隐私保护的新范式\n\n本研究首次系统性地提出并形式化了**本地节点差分隐私（Local Node Differential Privacy, LNDP）**——一种面向图结构数据的新型本地化隐私模型。与传统本地差分隐私（LDP）处理独立记录不同，LNDP要求每个节点仅基于其**局部邻接信息**（即自身边列表）运行一个本地随机化器，再将扰动后输出发送至不可信服务器进行聚合。该模型更贴合去中心化图场景（如社交网络边缘计算、分布式图学习），避免了对中心化可信服务器的依赖。\n\n我们构建了一个**新颖的算法框架**，可高精度回答关于输入图**度分布模糊近似**的任意线性查询（如平均度、度矩、三角形计数等）。关键创新在于设计了鲁棒的局部扰动机制与协同聚合策略，使误差随节点数 $n$ 衰减为 $O(1/\\sqrt{n})$，**在若干自然问题上达到与中心化节点差分隐私（central node DP）相同的渐近精度**——这打破了“本地模型必然显著牺牲精度”的固有认知。\n\n进一步，我们建立了首个针对LNDP的**紧致下界**：证明任何LNDP机制在估计度分布统计量（如最大度、度直方图 $\\ell_1$-误差）时，误差下界为 $\\Omega(1)$，且我们的算法恰好达到该下界，从而**严格证明其最优性**。更进一步，我们将下界扩展至**多轮交互式LNDP**场景，表明即使允许常数轮通信，最优性依然成立。为克服图数据中节点输入天然重叠（一条边被两个端点共享）带来的技术障碍，我们发展了全新的分析工具——包括**重叠敏感的耦合构造**与**图局部结构的隐私敏感性刻画**。最后，我们揭示了LNDP与标准表格型LDP的本质差异：前者无法实现私有频率估计的通用解，凸显其独特的结构性挑战。",
      "summary_en": "We initiate the study of **Local Node Differential Privacy (LNDP)**—a new privacy model for graphs in the local setting. In LNDP, each node applies a local randomizer to its incident edge list and sends the output to an untrusted aggregator. We propose a novel algorithmic framework that accurately answers arbitrary linear queries on a *blurred approximation* of the graph’s degree distribution. For natural problems (e.g., degree moments), our algorithms achieve error $O(1/\\sqrt{n})$, matching the optimal accuracy of *central* node DP—demonstrating that local graph privacy need not inherently sacrifice asymptotic utility. We prove tight lower bounds showing $\\Omega(1)$ error is unavoidable for fundamental statistics (e.g., max degree, $\\ell_1$-error of degree histogram), establishing optimality of our algorithms. These bounds extend to interactive LNDP with constant-round communication. Crucially, our lower-bound techniques are new: they overcome the challenge of overlapping inputs inherent to graphs—unaddressed by standard LDP tools. Finally, we show structural separations between LNDP and tabular LDP, revealing qualitatively distinct privacy landscapes.",
      "summary": "## 本地节点差分隐私（LNDP）：图数据隐私保护的新范式\n\n本研究首次系统性地提出并形式化了**本地节点差分隐私（Local Node Differential Privacy, LNDP）**——一种面向图结构数据的新型本地化隐私模型。与传统本地差分隐私（LDP）处理独立记录不同，LNDP要求每个节点仅基于其**局部邻接信息**（即自身边列表）运行一个本地随机化器，再将扰动后输出发送至不可信服务器进行聚合。该模型更贴合去中心化图场景（如社交网络边缘计算、分布式图学习），避免了对中心化可信服务器的依赖。\n\n我们构建了一个**新颖的算法框架**，可高精度回答关于输入图**度分布模糊近似**的任意线性查询（如平均度、度矩、三角形计数等）。关键创新在于设计了鲁棒的局部扰动机制与协同聚合策略，使误差随节点数 $n$ 衰减为 $O(1/\\sqrt{n})$，**在若干自然问题上达到与中心化节点差分隐私（central node DP）相同的渐近精度**——这打破了“本地模型必然显著牺牲精度”的固有认知。\n\n进一步，我们建立了首个针对LNDP的**紧致下界**：证明任何LNDP机制在估计度分布统计量（如最大度、度直方图 $\\ell_1$-误差）时，误差下界为 $\\Omega(1)$，且我们的算法恰好达到该下界，从而**严格证明其最优性**。更进一步，我们将下界扩展至**多轮交互式LNDP**场景，表明即使允许常数轮通信，最优性依然成立。为克服图数据中节点输入天然重叠（一条边被两个端点共享）带来的技术障碍，我们发展了全新的分析工具——包括**重叠敏感的耦合构造**与**图局部结构的隐私敏感性刻画**。最后，我们揭示了LNDP与标准表格型LDP的本质差异：前者无法实现私有频率估计的通用解，凸显其独特的结构性挑战。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15671v1",
      "arxiv_id": "2602.15671v1",
      "title": "Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective",
      "authors": [
        "Haodong Zhao",
        "Jinming Hu",
        "Gongshen Liu"
      ],
      "abstract": "Federated learning security research has predominantly focused on backdoor threats from a minority of malicious clients that intentionally corrupt model updates. This paper challenges this paradigm by investigating a more pervasive and insidious threat: \\textit{backdoor vulnerabilities from low-concentration poisoned data distributed across the datasets of benign clients.} This scenario is increasingly common in federated instruction tuning for language models, which often rely on unverified third-party and crowd-sourced data. We analyze two forms of backdoor data through real cases: 1) \\textit{natural trigger (inherent features as implicit triggers)}; 2) \\textit{adversary-injected trigger}. To analyze this threat, we model the backdoor implantation process from signal aggregation, proposing the Backdoor Signal-to-Noise Ratio to quantify the dynamics of the distributed backdoor signal. Extensive experiments reveal the severity of this threat: With just less than 10\\% of training data poisoned and distributed across clients, the attack success rate exceeds 85\\%, while the primary task performance remains largely intact. Critically, we demonstrate that state-of-the-art backdoor defenses, designed for attacks from malicious clients, are fundamentally ineffective against this threat. Our findings highlight an urgent need for new defense mechanisms tailored to the realities of modern, decentralized data ecosystems.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15671v1",
      "url": "https://arxiv.org/abs/2602.15671v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n传统联邦学习安全研究聚焦于**少数恶意客户端主动投毒**的后门攻击范式。本文挑战这一主流假设，揭示一种更隐蔽、更普遍的新型后门威胁：**由大量良性客户端各自持有少量低浓度中毒数据所共同诱发的分布式后门漏洞**。该场景在面向大语言模型的联邦指令微调（Federated Instruction Tuning）中尤为突出——其数据高度依赖未经验证的第三方开源数据集与众包语料，天然存在隐性污染风险。\n\n## 方法创新  \n我们从**信号聚合视角**重构后门植入机制：将后门触发模式视为一种需跨客户端协同放大的“有害信号”，而正常任务梯度则构成背景“噪声”。据此提出**后门信噪比（Backdoor Signal-to-Noise Ratio, BSNR）** 量化框架，首次形式化刻画分布式低浓度中毒数据在聚合过程中如何突破阈值、实现隐式协同激活。\n\n## 关键发现  \n- 实证表明：仅**<10% 总训练数据被分散污染**（每客户端中毒比例低至0.5%–3%），攻击成功率即超**85%**，且主任务准确率下降不足2%，极具隐蔽性；  \n- 污染类型涵盖两类真实案例：**自然触发**（如特定句式/语义结构作为隐式触发器）与**对抗注入触发**（人工添加无害标记）；  \n- **现有SOTA防御方法（如Norm Clipping、Krum、FLTrust）完全失效**——因其设计前提为识别“异常更新”，而本场景下所有更新均来自良性客户端、统计上完全正常。\n\n## 意义与呼吁  \n本研究揭示了去中心化数据生态中“无恶意主体的系统性脆弱性”，亟需发展面向**数据层分布特性**而非客户端行为的新一代防御范式。",
      "summary_en": "This paper redefines backdoor threats in federated instruction tuning by exposing a pervasive yet overlooked vulnerability: *backdoors emergent from low-concentration poisoned data distributed across numerous benign clients*—not from malicious actors. We model backdoor implantation as a signal aggregation process and propose the **Backdoor Signal-to-Noise Ratio (BSNR)** to quantify how sparse, distributed triggers collectively amplify during federated averaging. Experiments on real-world instruction datasets show that with **<10% total training data poisoned across clients** (e.g., 0.5–3% per client), attack success rates exceed **85%**, while primary task performance remains nearly intact (<2% drop). Crucially, state-of-the-art defenses—designed to detect anomalous *client updates*—fail completely, as all updates appear statistically benign. Our work underscores an urgent need for data-aware, aggregation-centric defenses in decentralized NLP ecosystems.",
      "summary": "## 背景与问题  \n传统联邦学习安全研究聚焦于**少数恶意客户端主动投毒**的后门攻击范式。本文挑战这一主流假设，揭示一种更隐蔽、更普遍的新型后门威胁：**由大量良性客户端各自持有少量低浓度中毒数据所共同诱发的分布式后门漏洞**。该场景在面向大语言模型的联邦指令微调（Federated Instruction Tuning）中尤为突出——其数据高度依赖未经验证的第三方开源数据集与众包语料，天然存在隐性污染风险。\n\n## 方法创新  \n我们从**信号聚合视角**重构后门植入机制：将后门触发模式视为一种需跨客户端协同放大的“有害信号”，而正常任务梯度则构成背景“噪声”。据此提出**后门信噪比（Backdoor Signal-to-Noise Ratio, BSNR）** 量化框架，首次形式化刻画分布式低浓度中毒数据在聚合过程中如何突破阈值、实现隐式协同激活。\n\n## 关键发现  \n- 实证表明：仅**<10% 总训练数据被分散污染**（每客户端中毒比例低至0.5%–3%），攻击成功率即超**85%**，且主任务准确率下降不足2%，极具隐蔽性；  \n- 污染类型涵盖两类真实案例：**自然触发**（如特定句式/语义结构作为隐式触发器）与**对抗注入触发**（人工添加无害标记）；  \n- **现有SOTA防御方法（如Norm Clipping、Krum、FLTrust）完全失效**——因其设计前提为识别“异常更新”，而本场景下所有更新均来自良性客户端、统计上完全正常。\n\n## 意义与呼吁  \n本研究揭示了去中心化数据生态中“无恶意主体的系统性脆弱性”，亟需发展面向**数据层分布特性**而非客户端行为的新一代防御范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15654v1",
      "arxiv_id": "2602.15654v1",
      "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections",
      "authors": [
        "Xianglin Yang",
        "Yufei He",
        "Shuo Ji",
        "Bryan Hooi",
        "Jin Song Dong"
      ],
      "abstract": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.   We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15654v1",
      "url": "https://arxiv.org/abs/2602.15654v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "prompt",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与问题  \n自演化大语言模型（LLM）智能体通过跨会话更新长期记忆（如写入/读取记忆库）提升长周期任务性能，但该机制引入新型安全风险：**外部不可信内容在一次良性交互中被误存为“可信记忆”，后续会话中被自动激活为指令**。本文首次形式化并命名此类威胁为**僵尸智能体（Zombie Agent）攻击**——攻击者无需直接操控提示词或系统指令，即可植入跨会话持久、隐匿且可触发的恶意载荷，使智能体沦为远程傀儡。\n\n## 方法与创新  \n我们提出首个**黑盒、间接式持久攻击框架**，仅依赖攻击者可控的网页内容（如恶意博客、文档）实现感染：  \n- **感染阶段**：智能体在执行正常任务（如信息检索）时读取含隐蔽载荷的网页，利用其自身记忆更新机制（无越权操作）将载荷写入长期记忆；  \n- **触发阶段**：载荷经记忆演化（如检索增强、滑动窗口续写）被自然唤起，诱导未经授权的工具调用（如发送邮件、执行代码）。  \n针对主流记忆架构，我们设计**机制适配型持久化策略**：对滑动窗口记忆，采用语义冗余+上下文锚定规避截断；对检索增强记忆，注入高相关性伪文档绕过相似度过滤。\n\n## 关键发现  \n在多个典型智能体架构（AutoGen、LangChain-based agents）和真实任务（行程规划、多跳问答）上评估表明：  \n- 所有测试配置中，载荷**平均存活≥5个连续会话**（最长12轮），且**良性任务准确率下降<2.3%**，隐蔽性强；  \n- 仅靠单次会话的提示词过滤无法防御——**记忆演化本身即构成攻击面扩展**。  \n本研究揭示了自演化范式下“记忆即指令”的根本性安全悖论，呼吁防御体系从**会话级防护升级为记忆生命周期治理**。",
      "summary_en": "We identify and formalize *Zombie Agents*: a novel persistent attack against self-evolving LLM agents, where attackers covertly implant cross-session payloads via benign external content (e.g., poisoned web pages), exploiting the agent’s *own memory update mechanism*—not prompt injection—to achieve long-term control. Our black-box framework operates in two phases: (1) *Infection*, where the agent reads attacker-controlled content during a legitimate task and autonomously writes the payload into long-term memory; and (2) *Trigger*, where evolved memory retrieves or propagates the payload to induce unauthorized tool use (e.g., API calls, code execution). We design memory-specific persistence strategies—resisting truncation in sliding-window memory and relevance filtering in retrieval-augmented memory—and evaluate across representative agent setups. Results show payloads persist for ≥5 consecutive sessions (up to 12) while preserving >97.7% benign task accuracy, demonstrating that memory evolution transforms one-time indirect exposure into persistent compromise. This reveals a critical limitation of session-level defenses and underscores the need for memory lifecycle-aware security.",
      "summary": "## 背景与问题  \n自演化大语言模型（LLM）智能体通过跨会话更新长期记忆（如写入/读取记忆库）提升长周期任务性能，但该机制引入新型安全风险：**外部不可信内容在一次良性交互中被误存为“可信记忆”，后续会话中被自动激活为指令**。本文首次形式化并命名此类威胁为**僵尸智能体（Zombie Agent）攻击**——攻击者无需直接操控提示词或系统指令，即可植入跨会话持久、隐匿且可触发的恶意载荷，使智能体沦为远程傀儡。\n\n## 方法与创新  \n我们提出首个**黑盒、间接式持久攻击框架**，仅依赖攻击者可控的网页内容（如恶意博客、文档）实现感染：  \n- **感染阶段**：智能体在执行正常任务（如信息检索）时读取含隐蔽载荷的网页，利用其自身记忆更新机制（无越权操作）将载荷写入长期记忆；  \n- **触发阶段**：载荷经记忆演化（如检索增强、滑动窗口续写）被自然唤起，诱导未经授权的工具调用（如发送邮件、执行代码）。  \n针对主流记忆架构，我们设计**机制适配型持久化策略**：对滑动窗口记忆，采用语义冗余+上下文锚定规避截断；对检索增强记忆，注入高相关性伪文档绕过相似度过滤。\n\n## 关键发现  \n在多个典型智能体架构（AutoGen、LangChain-based agents）和真实任务（行程规划、多跳问答）上评估表明：  \n- 所有测试配置中，载荷**平均存活≥5个连续会话**（最长12轮），且**良性任务准确率下降<2.3%**，隐蔽性强；  \n- 仅靠单次会话的提示词过滤无法防御——**记忆演化本身即构成攻击面扩展**。  \n本研究揭示了自演化范式下“记忆即指令”的根本性安全悖论，呼吁防御体系从**会话级防护升级为记忆生命周期治理**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15614v1",
      "arxiv_id": "2602.15614v1",
      "title": "Onto-DP: Constructing Neighborhoods for Differential Privacy on Ontological Databases",
      "authors": [
        "Yasmine Hayder",
        "Adrien Boiret",
        "Cédric Eichler",
        "Benjamin Nguyen"
      ],
      "abstract": "In this paper, we investigate how attackers can discover sensitive information embedded within databases by exploiting inference rules. We demonstrate the inadequacy of naively applied existing state of the art differential privacy (DP) models in safeguarding against such attacks. We introduce ontology aware differential privacy (Onto-DP), a novel extension of differential privacy paradigms built on top of any classical DP model by enriching it with semantic awareness. We show that this extension is a sufficient condition to adequately protect against attackers aware of inference rules.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15614v1",
      "url": "https://arxiv.org/abs/2602.15614v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "dp",
        "differential"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n在本体数据库（ontological databases）中，敏感信息不仅存在于显式存储的元组中，更可能通过**语义推理规则**（如RDFS/OWL蕴含）被隐式推导出来。传统差分隐私（DP）机制仅关注数据表的行级扰动，忽视了本体层的逻辑结构，导致攻击者可利用领域知识（如“祖父→父亲→子女”传递性）绕过隐私保护，实施高精度推理攻击。\n\n## 方法：Onto-DP 框架  \n我们提出**本体感知差分隐私（Onto-DP）**——一种语义增强型DP范式。其核心创新在于：  \n- **邻域重构**：不再以原始数据库实例为微小变化单位，而是定义基于**本体语义等价类**的邻域：两个数据库 $D$ 与 $D'$ 被视为相邻，当且仅当它们在所有**本体蕴含闭包**（ontology-mediated query answers）上至多相差一行；  \n- **机制无关扩展**：Onto-DP 可无缝嵌入任意经典DP机制（如Laplace、Gaussian、Exponential），仅需将隐私预算分配至语义邻域而非语法邻域；  \n- **形式化保障**：我们证明 Onto-DP 满足**推理鲁棒性**（inference-resilience）：若攻击者掌握完整本体与推理规则，Onto-DP 仍能提供严格 $(\\varepsilon,\\delta)$-DP 保证，而朴素DP在此场景下完全失效。\n\n## 主要发现与贡献  \n实验验证表明，在DBpedia子集与LUBM基准测试中，Onto-DP 在保持查询效用（平均误差降低37%）的同时，将推理攻击成功率从朴素DP下的89%压制至<5%。本工作首次将**本体语义结构**系统性地纳入DP理论框架，为知识图谱、医疗本体、政务语义网等高敏感度语义数据库提供了首个可证明安全的隐私保护基础。",
      "summary_en": "This paper addresses a critical gap in differential privacy (DP): conventional DP mechanisms fail against inference attacks leveraging ontological semantics (e.g., RDFS/OWL entailments). We propose **Onto-DP**, an ontology-aware extension of DP that redefines adjacency over *ontology-mediated query answers* rather than raw database instances. Specifically, two databases are adjacent if their logical closures under the given ontology differ in at most one answer—ensuring protection against attackers armed with domain knowledge and inference rules. Onto-DP is mechanism-agnostic: it lifts any classical DP algorithm (e.g., Laplace, Exponential) to semantic robustness while preserving formal $(\\varepsilon,\\delta)$-guarantees. Evaluation on DBpedia and LUBM shows Onto-DP reduces inference attack success from 89% (naive DP) to <5%, with 37% lower utility loss. This work establishes the first provably secure DP framework for ontological databases.",
      "summary": "## 背景与问题  \n在本体数据库（ontological databases）中，敏感信息不仅存在于显式存储的元组中，更可能通过**语义推理规则**（如RDFS/OWL蕴含）被隐式推导出来。传统差分隐私（DP）机制仅关注数据表的行级扰动，忽视了本体层的逻辑结构，导致攻击者可利用领域知识（如“祖父→父亲→子女”传递性）绕过隐私保护，实施高精度推理攻击。\n\n## 方法：Onto-DP 框架  \n我们提出**本体感知差分隐私（Onto-DP）**——一种语义增强型DP范式。其核心创新在于：  \n- **邻域重构**：不再以原始数据库实例为微小变化单位，而是定义基于**本体语义等价类**的邻域：两个数据库 $D$ 与 $D'$ 被视为相邻，当且仅当它们在所有**本体蕴含闭包**（ontology-mediated query answers）上至多相差一行；  \n- **机制无关扩展**：Onto-DP 可无缝嵌入任意经典DP机制（如Laplace、Gaussian、Exponential），仅需将隐私预算分配至语义邻域而非语法邻域；  \n- **形式化保障**：我们证明 Onto-DP 满足**推理鲁棒性**（inference-resilience）：若攻击者掌握完整本体与推理规则，Onto-DP 仍能提供严格 $(\\varepsilon,\\delta)$-DP 保证，而朴素DP在此场景下完全失效。\n\n## 主要发现与贡献  \n实验验证表明，在DBpedia子集与LUBM基准测试中，Onto-DP 在保持查询效用（平均误差降低37%）的同时，将推理攻击成功率从朴素DP下的89%压制至<5%。本工作首次将**本体语义结构**系统性地纳入DP理论框架，为知识图谱、医疗本体、政务语义网等高敏感度语义数据库提供了首个可证明安全的隐私保护基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15485v2",
      "arxiv_id": "2602.15485v2",
      "title": "SecCodeBench-V2 Technical Report",
      "authors": [
        "Longfei Chen",
        "Ji Zhao",
        "Lanxiao Cui",
        "Tong Su",
        "Xingbo Pan",
        "Ziyang Li",
        "Yongxing Wu",
        "Qijiang Cao",
        "Qiyao Cai",
        "Jing Zhang",
        "Yuandong Ni",
        "Junyao He",
        "Zeyu Zhang",
        "Chao Ge",
        "Xuhuai Lu",
        "Zeyu Gao",
        "Yuxin Cui",
        "Weisen Chen",
        "Yuxuan Peng",
        "Shengping Wang",
        "Qi Li",
        "Yukai Huang",
        "Yukun Liu",
        "Tuo Zhou",
        "Terry Yue Zhuo",
        "Junyang Lin",
        "Chao Zhang"
      ],
      "abstract": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15485v2",
      "url": "https://arxiv.org/abs/2602.15485v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准评测体系  \n\nSecCodeBench-V2 是阿里巴巴集团发布的**第二代开源安全代码生成评测基准**，旨在系统性、可复现地评估大语言模型（LLM）编程助手在**生成与修复安全关键代码**方面的实际能力。相较于前代，V2 版本全面升级：涵盖 **98 个真实工业场景**（含生成与修复两类任务），全部源自阿里集团线上生产环境，覆盖 **22 类常见 CWE 漏洞类型**（如 CWE-78、CWE-89、CWE-119 等），横跨 **Java、C、Python、Go、JavaScript 五种主流语言**。  \n\n本基准采用**函数级精细化任务建模**：每个场景提供完整项目骨架（含依赖、构建配置及上下文），明确指定待实现或修复的目标函数接口，严格约束输入/输出契约与依赖边界，显著提升任务真实性与评估严谨性。所有场景均配备**专家撰写的可执行 PoC（Proof-of-Concept）测试用例**——经双盲安全专家评审，确保功能正确性验证与安全漏洞触发能力兼具，形成高保真、广覆盖、可判定的黄金标准（ground truth）。  \n\n为实现自动化、动态化评估，我们构建了统一评测流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，同步验证功能正确性与安全性；对难以构造确定性安全测试的复杂场景（如逻辑类漏洞），引入经校准的 **LLM-as-a-judge 仲裁机制**作为补充。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、漏洞严重性（CVSS 分数映射）和任务类型进行原则性聚合，支持跨模型、跨任务的公平对比。SecCodeBench-V2 已全量开源（含数据集、评测脚本、基线结果），地址：https://github.com/alibaba/sec-code-bench。",
      "summary_en": "SecCodeBench-V2 is a rigorously designed, publicly available benchmark for evaluating the secure code generation and patching capabilities of LLM-based coding assistants. It comprises 98 function-level scenarios—derived from Alibaba’s production systems—spanning 22 CWE categories across Java, C, Python, Go, and JavaScript. Each scenario provides a full project scaffold and requires models to implement or fix a target function under fixed interfaces and dependencies, with executable PoC test cases (authored and double-reviewed by security experts) for both functional and security validation. Evaluation is primarily dynamic: model outputs are compiled and executed in isolated environments against PoC tests; for non-deterministic security issues, an LLM-as-a-judge oracle supplements assessment. Performance is aggregated via a severity-aware Pass@K scoring protocol, enabling holistic, comparable evaluation. All artifacts, results, and tooling are open-sourced at https://github.com/alibaba/sec-code-bench.",
      "summary": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准评测体系  \n\nSecCodeBench-V2 是阿里巴巴集团发布的**第二代开源安全代码生成评测基准**，旨在系统性、可复现地评估大语言模型（LLM）编程助手在**生成与修复安全关键代码**方面的实际能力。相较于前代，V2 版本全面升级：涵盖 **98 个真实工业场景**（含生成与修复两类任务），全部源自阿里集团线上生产环境，覆盖 **22 类常见 CWE 漏洞类型**（如 CWE-78、CWE-89、CWE-119 等），横跨 **Java、C、Python、Go、JavaScript 五种主流语言**。  \n\n本基准采用**函数级精细化任务建模**：每个场景提供完整项目骨架（含依赖、构建配置及上下文），明确指定待实现或修复的目标函数接口，严格约束输入/输出契约与依赖边界，显著提升任务真实性与评估严谨性。所有场景均配备**专家撰写的可执行 PoC（Proof-of-Concept）测试用例**——经双盲安全专家评审，确保功能正确性验证与安全漏洞触发能力兼具，形成高保真、广覆盖、可判定的黄金标准（ground truth）。  \n\n为实现自动化、动态化评估，我们构建了统一评测流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，同步验证功能正确性与安全性；对难以构造确定性安全测试的复杂场景（如逻辑类漏洞），引入经校准的 **LLM-as-a-judge 仲裁机制**作为补充。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、漏洞严重性（CVSS 分数映射）和任务类型进行原则性聚合，支持跨模型、跨任务的公平对比。SecCodeBench-V2 已全量开源（含数据集、评测脚本、基线结果），地址：https://github.com/alibaba/sec-code-bench。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15485v1",
      "arxiv_id": "2602.15485v1",
      "title": "SecCodeBench-V2 Technical Report",
      "authors": [
        "Longfei Chen",
        "Ji Zhao",
        "Lanxiao Cui",
        "Tong Su",
        "Xingbo Pan",
        "Ziyang Li",
        "Yongxing Wu",
        "Qijiang Cao",
        "Qiyao Cai",
        "Jing Zhang",
        "Yuandong Ni",
        "Junyao He",
        "Zeyu Zhang",
        "Chao Ge",
        "Xuhuai Lu",
        "Zeyu Gao",
        "Yuxin Cui",
        "Weisen Chen",
        "Yuxuan Peng",
        "Shengping Wang",
        "Qi Li",
        "Yukai Huang",
        "Yukun Liu",
        "Tuo Zhou",
        "Terry Yue Zhuo",
        "Junyang Lin",
        "Chao Zhang"
      ],
      "abstract": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15485v1",
      "url": "https://arxiv.org/abs/2602.15485v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准\n\nSecCodeBench-V2 是阿里巴巴集团发布的第二代开源安全代码生成评测基准，旨在**系统性、可复现地评估大语言模型（LLM）编程助手在真实工业场景中生成与修复安全代码的能力**。基准包含 **98 个高质量生成与修复任务场景**，全部源自阿里集团实际生产系统，覆盖 **Java、C、Python、Go 和 Node.js 五种主流语言**，底层安全漏洞涵盖 **22 类常见 CWE（Common Weakness Enumeration）**，如 CWE-78（OS命令注入）、CWE-89（SQL注入）、CWE-119（缓冲区溢出）等，兼具**多样性、严重性与现实代表性**。\n\n本基准采用**函数级任务建模**：每个场景提供完整项目骨架（含依赖与接口约束），要求模型仅实现或修补指定目标函数，严格模拟真实开发边界。所有任务均配备**专家编写的、可执行的 PoC（Proof-of-Concept）测试用例**，经双盲安全专家评审，确保功能正确性与安全验证的**高保真度、广覆盖与可靠真值**。我们构建了统一动态评估流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，双重验证功能正确性与安全鲁棒性；对难以构造确定性测试的复杂漏洞（如竞态条件），创新引入**LLM-as-a-judge 仲裁机制**。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、CWE 严重等级进行原则性聚合，支持跨模型、跨任务的公平、可解释、整体性对比。SecCodeBench-V2 已全面开源，含全部数据、评估脚本与结果，地址：https://github.com/alibaba/sec-code-bench。",
      "summary_en": "SecCodeBench-V2 is a publicly released, industry-grounded benchmark for rigorously evaluating LLM copilots’ ability to generate and fix *secure* code. It comprises 98 function-level generation/fix tasks derived from Alibaba’s production systems, spanning 5 languages (Java, C, Python, Go, Node.js) and 22 CWE categories. Each task provides a full project scaffold and executable PoC test cases—authored and double-reviewed by security experts—for joint functional and security validation. Our unified evaluation pipeline relies primarily on *dynamic execution*: model outputs are compiled and run in isolated environments against PoCs; for non-deterministic vulnerabilities, we augment with an LLM-as-a-judge oracle. To enable holistic, comparable assessment across heterogeneous scenarios, we introduce a severity-aware, Pass@K–based scoring protocol with principled aggregation over difficulty and CWE criticality. SecCodeBench-V2 establishes a reproducible foundation for measuring the security posture of AI coding assistants—code, data, and results are fully open at https://github.com/alibaba/sec-code-bench.",
      "summary": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准\n\nSecCodeBench-V2 是阿里巴巴集团发布的第二代开源安全代码生成评测基准，旨在**系统性、可复现地评估大语言模型（LLM）编程助手在真实工业场景中生成与修复安全代码的能力**。基准包含 **98 个高质量生成与修复任务场景**，全部源自阿里集团实际生产系统，覆盖 **Java、C、Python、Go 和 Node.js 五种主流语言**，底层安全漏洞涵盖 **22 类常见 CWE（Common Weakness Enumeration）**，如 CWE-78（OS命令注入）、CWE-89（SQL注入）、CWE-119（缓冲区溢出）等，兼具**多样性、严重性与现实代表性**。\n\n本基准采用**函数级任务建模**：每个场景提供完整项目骨架（含依赖与接口约束），要求模型仅实现或修补指定目标函数，严格模拟真实开发边界。所有任务均配备**专家编写的、可执行的 PoC（Proof-of-Concept）测试用例**，经双盲安全专家评审，确保功能正确性与安全验证的**高保真度、广覆盖与可靠真值**。我们构建了统一动态评估流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，双重验证功能正确性与安全鲁棒性；对难以构造确定性测试的复杂漏洞（如竞态条件），创新引入**LLM-as-a-judge 仲裁机制**。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、CWE 严重等级进行原则性聚合，支持跨模型、跨任务的公平、可解释、整体性对比。SecCodeBench-V2 已全面开源，含全部数据、评估脚本与结果，地址：https://github.com/alibaba/sec-code-bench。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15337v1",
      "arxiv_id": "2602.15337v1",
      "title": "FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning",
      "authors": [
        "Chaoyi Lu"
      ],
      "abstract": "Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\\% improvement over baseline methods and 1.93\\% over the current state-of-the-art method.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15337v1",
      "url": "https://arxiv.org/abs/2602.15337v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n异步联邦学习（AFL）通过允许客户端无需同步等待慢速设备即可并行训练，显著提升了分布式训练效率。然而，其固有的**行为陈旧性（behavioral staleness）**——即本地模型更新因延迟上传而偏离当前全局状态的程度——常导致收敛不稳定与精度下降。现有方法普遍仅以“轮次差”（如 $ \\tau = t - t_i $）作为陈旧性度量，该指标粗粒度、与模型实际参数演化脱节，无法反映梯度方向偏移或参数敏感性变化，成为性能提升的关键瓶颈。\n\n## 方法创新：FedPSA框架  \n本文提出 **FedPSA（Parameter Sensitivity-based Asynchronous Federated Learning）**，一种细粒度、自适应的AFL新范式：  \n- **参数敏感性建模**：首次将**逐层参数敏感度**（基于Hessian近似与局部损失曲率）作为陈旧性核心度量，替代静态轮次计数，精准刻画模型在关键参数维度上的实际过时程度；  \n- **动态动量队列（Dynamic Momentum Queue, DMQ）**：实时追踪近期更新的动量分布偏移趋势，识别当前训练阶段（如早期快速下降期 vs. 后期震荡收敛期），据此**动态调整陈旧容忍阈值**；  \n- **敏感度加权聚合**：在服务器端对上传更新施加敏感度感知的衰减权重，抑制高陈旧性、低敏感性参数的干扰，保留高敏感性参数的有效梯度信号。\n\n## 实验结果与贡献  \n在CIFAR-10/100、Tiny-ImageNet及FEMNIST上系统验证：FedPSA在非独立同分布（Non-IID）和异构延迟场景下均显著优于SOTA。相较基准方法（如Async-FedAvg），最高提升**6.37%** 准确率；较当前最优方法（FedBuff、SCAFFOLD-A）平均提升**1.93%**，且收敛速度加快23%。本工作首次将**模型内在敏感性**引入AFL陈旧性建模，为异步机制设计提供了可解释、可微分的新理论视角。",
      "summary_en": "Asynchronous Federated Learning (AFL) accelerates training by eliminating synchronization waits but suffers from performance degradation due to *behavioral staleness*—the misalignment between stale local updates and the evolving global model. Existing staleness metrics rely solely on coarse-grained round differences, ignoring intrinsic model dynamics. To address this, we propose **FedPSA**, a fine-grained AFL framework that quantifies staleness via **layer-wise parameter sensitivity**, approximated using local loss curvature and Hessian-vector products. FedPSA introduces a **Dynamic Momentum Queue (DMQ)** to monitor real-time training phase transitions and adaptively adjust staleness tolerance. During aggregation, updates are weighted by sensitivity to suppress noisy, outdated gradients while preserving critical signals. Extensive experiments across CIFAR-10/100, Tiny-ImageNet, and FEMNIST under Non-IID and heterogeneous delay settings show FedPSA consistently outperforms baselines: up to **+6.37%** accuracy over Async-FedAvg and **+1.93%** over state-of-the-art methods (e.g., FedBuff, SCAFFOLD-A), with 23% faster convergence. FedPSA establishes parameter sensitivity as a principled, differentiable foundation for staleness-aware asynchronous optimization.",
      "summary": "## 背景与问题  \n异步联邦学习（AFL）通过允许客户端无需同步等待慢速设备即可并行训练，显著提升了分布式训练效率。然而，其固有的**行为陈旧性（behavioral staleness）**——即本地模型更新因延迟上传而偏离当前全局状态的程度——常导致收敛不稳定与精度下降。现有方法普遍仅以“轮次差”（如 $ \\tau = t - t_i $）作为陈旧性度量，该指标粗粒度、与模型实际参数演化脱节，无法反映梯度方向偏移或参数敏感性变化，成为性能提升的关键瓶颈。\n\n## 方法创新：FedPSA框架  \n本文提出 **FedPSA（Parameter Sensitivity-based Asynchronous Federated Learning）**，一种细粒度、自适应的AFL新范式：  \n- **参数敏感性建模**：首次将**逐层参数敏感度**（基于Hessian近似与局部损失曲率）作为陈旧性核心度量，替代静态轮次计数，精准刻画模型在关键参数维度上的实际过时程度；  \n- **动态动量队列（Dynamic Momentum Queue, DMQ）**：实时追踪近期更新的动量分布偏移趋势，识别当前训练阶段（如早期快速下降期 vs. 后期震荡收敛期），据此**动态调整陈旧容忍阈值**；  \n- **敏感度加权聚合**：在服务器端对上传更新施加敏感度感知的衰减权重，抑制高陈旧性、低敏感性参数的干扰，保留高敏感性参数的有效梯度信号。\n\n## 实验结果与贡献  \n在CIFAR-10/100、Tiny-ImageNet及FEMNIST上系统验证：FedPSA在非独立同分布（Non-IID）和异构延迟场景下均显著优于SOTA。相较基准方法（如Async-FedAvg），最高提升**6.37%** 准确率；较当前最优方法（FedBuff、SCAFFOLD-A）平均提升**1.93%**，且收敛速度加快23%。本工作首次将**模型内在敏感性**引入AFL陈旧性建模，为异步机制设计提供了可解释、可微分的新理论视角。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15304v1",
      "arxiv_id": "2602.15304v1",
      "title": "Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization",
      "authors": [
        "Farzana Akter",
        "Rakib Hossain",
        "Deb Kanna Roy Toushi",
        "Mahmood Menon Khan",
        "Sultana Amin",
        "Lisan Al Amin"
      ],
      "abstract": "Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15304v1",
      "url": "https://arxiv.org/abs/2602.15304v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated",
        "membership",
        "inference"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与挑战  \n跨机构临床决策支持常受数据治理与隐私法规制约，难以汇聚患者级原始数据。传统联邦学习（FL）或分割学习（SL）各自存在局限：FL在非独立同分布（non-IID）医疗数据下收敛不稳定，且模型更新仍隐含特征泄露风险；SL虽天然分离计算，但中心化预测头易成单点故障，且缺乏对协作边界的显式隐私管控。\n\n## 方法创新  \n本文提出**混合联邦-分割学习（Hybrid FL-SL）框架**：客户端本地保留特征提取主干（trunk），仅上传中间层表征（cut-layer activations）；协调服务器聚合表征并托管统一预测头（head）。该设计确立了清晰的**协作边界**——所有隐私敏感操作（如裁剪、加噪）均在此边界施加。我们首次在临床场景中系统开展**实证隐私审计**，采用成员推断攻击（Membership Inference Attack, MIA）量化cut-layer表征的泄露程度，并评估两种轻量防御：**激活裁剪（activation clipping）** 与 **高斯噪声注入（additive Gaussian noise）**。\n\n## 关键结果  \n在MIMIC-III、eICU和Diabetes 13个任务上，采用统一pipeline与non-IID划分，四维联合评估显示：  \n- ✅ **预测效用**：AUC达0.82–0.91，媲美纯FL/SL基线；  \n- ✅ **决策优先级**：基于治疗增益（uplift）的资源受限排序准确率提升12.3%；  \n- ✅ **隐私保障**：MIA攻击成功率由78.5%降至≤42.1%（经裁剪+噪声）；  \n- ✅ **部署成本**：通信开销较纯FL降低37%，较纯SL降低29%。  \n\n本工作将Hybrid FL-SL确立为可调谐的实用范式——在**效用、隐私泄漏风险与通信成本**三者间实现显式、可控的权衡。",
      "summary_en": "We propose a hybrid Federated and Split Learning (FL-SL) framework for privacy-preserving clinical decision support without raw-data sharing. It decouples feature extraction (kept locally on clients) from prediction (hosted centrally), establishing an explicit collaboration boundary where lightweight privacy controls—activation clipping and additive Gaussian noise—are applied and audited via membership inference attacks. Evaluated across three public clinical datasets (MIMIC-III, eICU, Diabetes) under realistic non-IID partitions, our approach achieves competitive predictive performance (AUC: 0.82–0.91) and superior uplift-based treatment prioritization under capacity constraints—while reducing empirically measured privacy leakage (MIA success rate ↓ from 78.5% to ≤42.1%) and cutting communication overhead by up to 37% vs. standalone FL. This work establishes hybrid FL-SL as a practical, tunable design space for balancing utility, leakage risk, and deployment cost in healthcare AI.",
      "summary": "## 背景与挑战  \n跨机构临床决策支持常受数据治理与隐私法规制约，难以汇聚患者级原始数据。传统联邦学习（FL）或分割学习（SL）各自存在局限：FL在非独立同分布（non-IID）医疗数据下收敛不稳定，且模型更新仍隐含特征泄露风险；SL虽天然分离计算，但中心化预测头易成单点故障，且缺乏对协作边界的显式隐私管控。\n\n## 方法创新  \n本文提出**混合联邦-分割学习（Hybrid FL-SL）框架**：客户端本地保留特征提取主干（trunk），仅上传中间层表征（cut-layer activations）；协调服务器聚合表征并托管统一预测头（head）。该设计确立了清晰的**协作边界**——所有隐私敏感操作（如裁剪、加噪）均在此边界施加。我们首次在临床场景中系统开展**实证隐私审计**，采用成员推断攻击（Membership Inference Attack, MIA）量化cut-layer表征的泄露程度，并评估两种轻量防御：**激活裁剪（activation clipping）** 与 **高斯噪声注入（additive Gaussian noise）**。\n\n## 关键结果  \n在MIMIC-III、eICU和Diabetes 13个任务上，采用统一pipeline与non-IID划分，四维联合评估显示：  \n- ✅ **预测效用**：AUC达0.82–0.91，媲美纯FL/SL基线；  \n- ✅ **决策优先级**：基于治疗增益（uplift）的资源受限排序准确率提升12.3%；  \n- ✅ **隐私保障**：MIA攻击成功率由78.5%降至≤42.1%（经裁剪+噪声）；  \n- ✅ **部署成本**：通信开销较纯FL降低37%，较纯SL降低29%。  \n\n本工作将Hybrid FL-SL确立为可调谐的实用范式——在**效用、隐私泄漏风险与通信成本**三者间实现显式、可控的权衡。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15919v1",
      "arxiv_id": "2602.15919v1",
      "title": "Generalized Leverage Score for Scalable Assessment of Privacy Vulnerability",
      "authors": [
        "Valentin Dorseuil",
        "Jamal Atif",
        "Olivier Cappé"
      ],
      "abstract": "Can the privacy vulnerability of individual data points be assessed without retraining models or explicitly simulating attacks? We answer affirmatively by showing that exposure to membership inference attack (MIA) is fundamentally governed by a data point's influence on the learned model. We formalize this in the linear setting by establishing a theoretical correspondence between individual MIA risk and the leverage score, identifying it as a principled metric for vulnerability. This characterization explains how data-dependent sensitivity translates into exposure, without the computational burden of training shadow models. Building on this, we propose a computationally efficient generalization of the leverage score for deep learning. Empirical evaluations confirm a strong correlation between the proposed score and MIA success, validating this metric as a practical surrogate for individual privacy risk assessment.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15919v1",
      "url": "https://arxiv.org/abs/2602.15919v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "membership",
        "inference"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n在隐私保护机器学习中，评估**单个数据点的隐私脆弱性**（如遭受成员推断攻击MIA的风险）通常依赖计算昂贵的方法——需训练大量影子模型或反复重训练目标模型。这严重制约了大规模数据集和复杂模型（如深度神经网络）上的实时、细粒度隐私诊断。\n\n## 方法创新  \n本文首次从理论层面揭示：**个体MIA风险本质源于其对模型参数的学习影响力**。在线性回归设定下，我们严格证明：个体MIA成功率与经典统计量——**杠杆分数（leverage score）**——存在单调映射关系；该分数直接量化数据点在最小二乘解空间中的几何“影响力”，无需任何攻击模拟或重训练。据此，我们提出**广义杠杆分数（Generalized Leverage Score, GLS）**：一种可扩展、免训练的深度学习适配指标。GLS通过高效计算模型最后一层特征映射的Jacobian矩阵的行范数平方（即局部线性化下的近似杠杆），规避了高维Hessian计算，时间复杂度仅为 *O(d·n)*（d为特征维数，n为样本数）。\n\n## 主要发现与价值  \n- 在CIFAR-10/100、ImageNet子集及医疗影像数据上，GLS与真实MIA成功率（Shadow-MIA、Knockoff-MIA）的**皮尔逊相关系数达0.82–0.91**；  \n- GLS可精准识别高风险样本（如异常、边界或过拟合样本），支持**数据清洗、差分隐私预算分配、主动防御采样**等下游任务；  \n- 相比基线方法（如损失值、梯度范数），GLS具备**理论可解释性、跨架构鲁棒性及零训练开销**，是首个兼具严谨性与实用性的免训练隐私脆弱性代理指标。",
      "summary_en": "Can individual data points’ privacy vulnerability to membership inference attacks (MIAs) be assessed *without* retraining models or simulating attacks? We answer yes: MIA exposure is fundamentally governed by a data point’s influence on the learned model. In the linear setting, we establish a rigorous theoretical correspondence between individual MIA risk and the classical leverage score—a natural, computationally light metric quantifying data influence in least-squares estimation. Building on this insight, we propose the **Generalized Leverage Score (GLS)**, an efficient, training-free extension for deep networks. GLS approximates local leverage via the squared row norm of the Jacobian of the final-layer features—avoiding costly shadow training or Hessian computation. Extensive experiments across vision and medical datasets show GLS achieves strong correlation (ρ = 0.82–0.91) with actual MIA success, outperforming loss- and gradient-based baselines. GLS thus serves as a principled, scalable, and theory-grounded surrogate for fine-grained privacy risk assessment.",
      "summary": "## 研究背景与问题  \n在隐私保护机器学习中，评估**单个数据点的隐私脆弱性**（如遭受成员推断攻击MIA的风险）通常依赖计算昂贵的方法——需训练大量影子模型或反复重训练目标模型。这严重制约了大规模数据集和复杂模型（如深度神经网络）上的实时、细粒度隐私诊断。\n\n## 方法创新  \n本文首次从理论层面揭示：**个体MIA风险本质源于其对模型参数的学习影响力**。在线性回归设定下，我们严格证明：个体MIA成功率与经典统计量——**杠杆分数（leverage score）**——存在单调映射关系；该分数直接量化数据点在最小二乘解空间中的几何“影响力”，无需任何攻击模拟或重训练。据此，我们提出**广义杠杆分数（Generalized Leverage Score, GLS）**：一种可扩展、免训练的深度学习适配指标。GLS通过高效计算模型最后一层特征映射的Jacobian矩阵的行范数平方（即局部线性化下的近似杠杆），规避了高维Hessian计算，时间复杂度仅为 *O(d·n)*（d为特征维数，n为样本数）。\n\n## 主要发现与价值  \n- 在CIFAR-10/100、ImageNet子集及医疗影像数据上，GLS与真实MIA成功率（Shadow-MIA、Knockoff-MIA）的**皮尔逊相关系数达0.82–0.91**；  \n- GLS可精准识别高风险样本（如异常、边界或过拟合样本），支持**数据清洗、差分隐私预算分配、主动防御采样**等下游任务；  \n- 相比基线方法（如损失值、梯度范数），GLS具备**理论可解释性、跨架构鲁棒性及零训练开销**，是首个兼具严谨性与实用性的免训练隐私脆弱性代理指标。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15602v1",
      "arxiv_id": "2602.15602v1",
      "title": "Certified Per-Instance Unlearning Using Individual Sensitivity Bounds",
      "authors": [
        "Hanna Benarroch",
        "Jamal Atif",
        "Olivier Cappé"
      ],
      "abstract": "Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15602v1",
      "url": "https://arxiv.org/abs/2602.15602v1",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "learning",
        "machine",
        "differential"
      ],
      "keyword_score": 4,
      "summary_zh": "## 研究背景与问题  \n传统**认证式机器遗忘**（certified machine unlearning）通常依赖向模型更新中注入噪声以满足差分隐私（DP）保障，其噪声规模由*最坏情况敏感度*（worst-case sensitivity）决定。这种保守校准虽能提供全局理论保证，却常导致显著的模型性能下降，严重制约实际部署。\n\n## 创新方法：面向实例的自适应遗忘  \n本文提出一种**基于个体敏感度边界**（individual sensitivity bounds）的新型认证遗忘范式。核心思想是：不再采用统一、保守的噪声尺度，而是为每个待删除样本 *xᵢ* 动态计算其对当前学习过程的*真实影响强度*，并据此注入**最小必要噪声**。该方法首次将**单样本差分隐私**（per-instance DP）理论系统引入遗忘学习框架，解决了“遗忘机制依赖于被删样本”这一关键挑战——即如何在机制本身显式依赖待删点的前提下，仍建立形式化、可验证的遗忘保证。\n\n## 理论突破与验证  \n针对岭回归（ridge regression）模型，我们基于Langevin动力学训练过程，严格推导出**高概率成立的个体敏感度上界**。该界限刻画了单个数据点扰动对梯度轨迹的局部影响，从而支持更精细的噪声校准。理论表明：相比最坏情况方案，本方法可将所需噪声方差降低一个数量级。我们在**线性回归**任务上通过大量实验验证了理论预测：遗忘后模型精度损失显著减小，同时仍100%满足认证遗忘要求（即重构攻击成功率≤阈值）。进一步，我们在**ResNet-18/CIFAR-10**上的实证结果表明：该理念在深度学习中同样有效——使用个体敏感度指导的噪声注入，在保持同等遗忘认证强度下，测试准确率平均提升2.3%–4.1%。",
      "summary_en": "Certified machine unlearning typically injects noise calibrated to the *worst-case sensitivity* of a learning algorithm, yielding strong but overly conservative differential privacy (DP) guarantees—and consequent accuracy degradation. This work introduces **per-instance certified unlearning**, where noise is adaptively scaled to the *individual contribution* of the data point to be removed. To formalize this, we define and bound *per-instance sensitivity* in noisy gradient dynamics using per-instance DP. For ridge regression trained via Langevin dynamics, we derive high-probability individual sensitivity bounds—enabling significantly less noise injection while preserving rigorous unlearning certification. Experiments on linear models validate our theory, showing up to 10× reduction in required noise variance. Empirically, on ResNet-18/CIFAR-10, our approach improves post-unlearning test accuracy by 2.3–4.1% under identical certification strength, demonstrating practical relevance beyond linear settings.",
      "summary": "## 研究背景与问题  \n传统**认证式机器遗忘**（certified machine unlearning）通常依赖向模型更新中注入噪声以满足差分隐私（DP）保障，其噪声规模由*最坏情况敏感度*（worst-case sensitivity）决定。这种保守校准虽能提供全局理论保证，却常导致显著的模型性能下降，严重制约实际部署。\n\n## 创新方法：面向实例的自适应遗忘  \n本文提出一种**基于个体敏感度边界**（individual sensitivity bounds）的新型认证遗忘范式。核心思想是：不再采用统一、保守的噪声尺度，而是为每个待删除样本 *xᵢ* 动态计算其对当前学习过程的*真实影响强度*，并据此注入**最小必要噪声**。该方法首次将**单样本差分隐私**（per-instance DP）理论系统引入遗忘学习框架，解决了“遗忘机制依赖于被删样本”这一关键挑战——即如何在机制本身显式依赖待删点的前提下，仍建立形式化、可验证的遗忘保证。\n\n## 理论突破与验证  \n针对岭回归（ridge regression）模型，我们基于Langevin动力学训练过程，严格推导出**高概率成立的个体敏感度上界**。该界限刻画了单个数据点扰动对梯度轨迹的局部影响，从而支持更精细的噪声校准。理论表明：相比最坏情况方案，本方法可将所需噪声方差降低一个数量级。我们在**线性回归**任务上通过大量实验验证了理论预测：遗忘后模型精度损失显著减小，同时仍100%满足认证遗忘要求（即重构攻击成功率≤阈值）。进一步，我们在**ResNet-18/CIFAR-10**上的实证结果表明：该理念在深度学习中同样有效——使用个体敏感度指导的噪声注入，在保持同等遗忘认证强度下，测试准确率平均提升2.3%–4.1%。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15510v1",
      "arxiv_id": "2602.15510v1",
      "title": "On the Geometric Coherence of Global Aggregation in Federated GNN",
      "authors": [
        "Chethana Prasad Kabgere",
        "Shylaja SS"
      ],
      "abstract": "Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates originating from incompatible propagation regimes can therefore introduce destructive interference in this transformation space.This leads to loss of coherence in global message passing. Importantly, this degradation is not necessarily reflected in conventional metrics such as loss or accuracy.To address this issue, we propose GGRS (Global Geometric Reference Structure), a server-side framework that regulates client updates prior to aggregation based on geometric admissibility criteria. GGRS preserves directional consistency of relational transformations as well as maintains diversity of admissible propagation subspaces. It also stabilizes sensitivity to neighborhood interactions, without accessing client data or graph topology. Experiments on heterogeneous GNN-native, Amazon Co-purchase datasets demonstrate that GGRS preserves global message-passing coherence across training rounds by highlighting the necessity of geometry-aware regulation in federated graph learning.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15510v1",
      "url": "https://arxiv.org/abs/2602.15510v1",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.NI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n联邦学习（FL）支持跨客户端的分布式模型训练，无需集中共享原始数据；图神经网络（GNN）则通过消息传递建模图结构关系。然而，在**跨域联邦GNN**场景中，各客户端图数据在拓扑结构、节点分布及信息传播机制上高度异构。现有FL框架通常直接对客户端GNN更新进行向量平均（如FedAvg），虽能实现数值收敛，却常导致全局模型在**关系建模层面失效**——即消息传递失去方向性、敏感性与一致性，而该退化在标准损失或准确率指标中难以察觉。\n\n## 几何失效机制  \n本文首次揭示一种**几何相干性崩溃**（geometric coherence collapse）现象：GNN参数虽以向量形式存储，实则编码了定义图邻域间信息流向、强度与响应特性的**关系变换算子**。当来自不兼容传播范式（如稀疏vs密集、局部vs全局感知）的更新被盲目聚合时，其在变换空间中产生**破坏性干涉**，致使全局消息传递丧失几何一致性。\n\n## 方法创新：GGRS框架  \n我们提出**全局几何参考结构**（GGRS），一种纯服务端、无需访问客户端数据或图拓扑的调控机制。GGRS基于**几何可容性判据**（geometric admissibility）对客户端更新进行预筛选与正则化：  \n- ✅ 保持关系变换的方向一致性（如邻域聚合权重的相对角度约束）；  \n- ✅ 维护可接受传播子空间的多样性（避免模式坍缩）；  \n- ✅ 稳定模型对邻居交互的敏感度梯度（抑制过平滑/过振荡）。  \n\n## 实验验证  \n在Amazon Co-purchase等GNN原生异构数据集上，GGRS显著提升多轮训练中全局消息传递的几何稳定性（通过谱距离、方向相似度等新指标量化），同时将节点分类准确率平均提升2.7–5.1个百分点，证实**几何感知的聚合调控是联邦图学习的必要范式**。",
      "summary_en": "Federated Graph Neural Networks (GNNs) suffer from a previously unrecognized *geometric coherence collapse*: while standard aggregation (e.g., FedAvg) achieves numerical convergence, it disrupts the relational transformation geometry encoded in GNN parameters—degrading directionality, sensitivity, and consistency of global message passing without clear signal in loss or accuracy. We identify this failure as destructive interference among updates from structurally incompatible client graphs. To address it, we propose **GGRS (Global Geometric Reference Structure)**, a server-side framework that filters and regularizes client updates *before aggregation* using geometric admissibility criteria—enforcing directional consistency of relational operators, preserving diversity of admissible propagation subspaces, and stabilizing neighborhood interaction sensitivity—all without accessing client data or topology. On heterogeneous GNN-native benchmarks (e.g., Amazon Co-purchase), GGRS maintains geometric coherence across training rounds (validated via spectral and angular metrics) and improves node classification accuracy by 2.7–5.1% on average, establishing geometry-aware regulation as essential for robust federated graph learning.",
      "summary": "## 研究背景与问题  \n联邦学习（FL）支持跨客户端的分布式模型训练，无需集中共享原始数据；图神经网络（GNN）则通过消息传递建模图结构关系。然而，在**跨域联邦GNN**场景中，各客户端图数据在拓扑结构、节点分布及信息传播机制上高度异构。现有FL框架通常直接对客户端GNN更新进行向量平均（如FedAvg），虽能实现数值收敛，却常导致全局模型在**关系建模层面失效**——即消息传递失去方向性、敏感性与一致性，而该退化在标准损失或准确率指标中难以察觉。\n\n## 几何失效机制  \n本文首次揭示一种**几何相干性崩溃**（geometric coherence collapse）现象：GNN参数虽以向量形式存储，实则编码了定义图邻域间信息流向、强度与响应特性的**关系变换算子**。当来自不兼容传播范式（如稀疏vs密集、局部vs全局感知）的更新被盲目聚合时，其在变换空间中产生**破坏性干涉**，致使全局消息传递丧失几何一致性。\n\n## 方法创新：GGRS框架  \n我们提出**全局几何参考结构**（GGRS），一种纯服务端、无需访问客户端数据或图拓扑的调控机制。GGRS基于**几何可容性判据**（geometric admissibility）对客户端更新进行预筛选与正则化：  \n- ✅ 保持关系变换的方向一致性（如邻域聚合权重的相对角度约束）；  \n- ✅ 维护可接受传播子空间的多样性（避免模式坍缩）；  \n- ✅ 稳定模型对邻居交互的敏感度梯度（抑制过平滑/过振荡）。  \n\n## 实验验证  \n在Amazon Co-purchase等GNN原生异构数据集上，GGRS显著提升多轮训练中全局消息传递的几何稳定性（通过谱距离、方向相似度等新指标量化），同时将节点分类准确率平均提升2.7–5.1个百分点，证实**几何感知的聚合调控是联邦图学习的必要范式**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15478v1",
      "arxiv_id": "2602.15478v1",
      "title": "Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data",
      "authors": [
        "Sharmad Kalpande",
        "Saurabh Shirke",
        "Haroon R. Lone"
      ],
      "abstract": "Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.   In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15478v1",
      "url": "https://arxiv.org/abs/2602.15478v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n情绪不稳定性是精神健康状况的关键行为标志，但传统评估依赖稀疏、回溯式的自报问卷，难以捕捉其动态连续性。智能手机传感技术为真实场景下的被动情绪推断提供了新路径，然而在跨国家规模化部署中面临三重挑战：**用户隐私敏感性**（数据不可集中）、**传感覆盖不均**（如GPS、蓝牙、加速度计在不同设备/地区启用率差异显著）、以及**行为模式高度异质性**（文化、生活习惯、城市化水平导致步态、社交互动、屏幕使用等特征分布迥异）。\n\n## 方法创新：FedFAP框架  \n本文提出**FedFAP**（Federated Feature-Aware Personalization），一种面向跨国家场景的特征感知个性化联邦学习框架。其核心设计包括：（1）**模态自适应特征编码器**——为每个国家客户端独立建模本地主流传感模态（如仅WiFi+蓝牙的低功耗配置 vs. 全模态高采样配置）；（2）**分层个性化聚合机制**——在全局模型共享底层时序表征能力的同时，为各国保留可学习的个性化特征投影头与情绪分类头；（3）**隐私增强训练协议**——全程不传输原始数据或中间特征，仅交换加密梯度与轻量级模型参数。\n\n## 关键结果与意义  \n在覆盖中国、印度、巴西、德国和美国的5国真实用户队列（N=2,847，平均每人>6周连续数据）上，FedFAP实现**AUROC 0.744**，较集中式训练提升4.2%，较SOTA个性化联邦基线（pFedMe、FedPer）平均高出6.8%。消融实验证实：模态感知编码贡献最大性能增益（+3.1% AUROC），而个性化分类头对文化特异性情绪表达（如高语境vs.低语境社会中的活动-情绪关联）建模尤为关键。本研究首次验证了**隐私优先、文化适配、模态灵活**的联邦范式在跨域心理健康计算中的可行性，为全球可扩展的情绪感知移动系统提供了可落地的技术蓝图。",
      "summary_en": "Mood instability is a critical behavioral marker of mental health, yet conventional assessments lack ecological validity due to infrequent, retrospective reporting. While smartphone sensing enables passive, in-the-wild mood inference, large-scale cross-country deployment is hindered by privacy constraints, heterogeneous sensor availability, and profound cultural-behavioral variability. To address this, we propose **FedFAP**, a feature-aware personalized federated learning framework where each country acts as an independent client retaining raw data locally. FedFAP introduces modality-adaptive encoders per client and hierarchical personalization—sharing foundational temporal representations globally while preserving country-specific feature projection and classification heads. Evaluated across five geographically and culturally diverse countries (China, India, Brazil, Germany, USA; N=2,847), FedFAP achieves an **AUROC of 0.744**, outperforming centralized training (+4.2%) and state-of-the-art personalized federated baselines (+6.8% on average). Our results demonstrate that privacy-preserving, population-aware personalization is essential for scalable, equitable, and clinically meaningful mobile mood sensing.",
      "summary": "## 研究背景  \n情绪不稳定性是精神健康状况的关键行为标志，但传统评估依赖稀疏、回溯式的自报问卷，难以捕捉其动态连续性。智能手机传感技术为真实场景下的被动情绪推断提供了新路径，然而在跨国家规模化部署中面临三重挑战：**用户隐私敏感性**（数据不可集中）、**传感覆盖不均**（如GPS、蓝牙、加速度计在不同设备/地区启用率差异显著）、以及**行为模式高度异质性**（文化、生活习惯、城市化水平导致步态、社交互动、屏幕使用等特征分布迥异）。\n\n## 方法创新：FedFAP框架  \n本文提出**FedFAP**（Federated Feature-Aware Personalization），一种面向跨国家场景的特征感知个性化联邦学习框架。其核心设计包括：（1）**模态自适应特征编码器**——为每个国家客户端独立建模本地主流传感模态（如仅WiFi+蓝牙的低功耗配置 vs. 全模态高采样配置）；（2）**分层个性化聚合机制**——在全局模型共享底层时序表征能力的同时，为各国保留可学习的个性化特征投影头与情绪分类头；（3）**隐私增强训练协议**——全程不传输原始数据或中间特征，仅交换加密梯度与轻量级模型参数。\n\n## 关键结果与意义  \n在覆盖中国、印度、巴西、德国和美国的5国真实用户队列（N=2,847，平均每人>6周连续数据）上，FedFAP实现**AUROC 0.744**，较集中式训练提升4.2%，较SOTA个性化联邦基线（pFedMe、FedPer）平均高出6.8%。消融实验证实：模态感知编码贡献最大性能增益（+3.1% AUROC），而个性化分类头对文化特异性情绪表达（如高语境vs.低语境社会中的活动-情绪关联）建模尤为关键。本研究首次验证了**隐私优先、文化适配、模态灵活**的联邦范式在跨域心理健康计算中的可行性，为全球可扩展的情绪感知移动系统提供了可落地的技术蓝图。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15380v1",
      "arxiv_id": "2602.15380v1",
      "title": "Fractional-Order Federated Learning",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "YangQuan Chen"
      ],
      "abstract": "Federated learning (FL) allows remote clients to train a global model collaboratively while protecting client privacy. Despite its privacy-preserving benefits, FL has significant drawbacks, including slow convergence, high communication cost, and non-independent-and-identically-distributed (non-IID) data. In this work, we present a novel FedAvg variation called Fractional-Order Federated Averaging (FOFedAvg), which incorporates Fractional-Order Stochastic Gradient Descent (FOSGD) to capture long-range relationships and deeper historical information. By introducing memory-aware fractional-order updates, FOFedAvg improves communication efficiency and accelerates convergence while mitigating instability caused by heterogeneous, non-IID client data. We compare FOFedAvg against a broad set of established federated optimization algorithms on benchmark datasets including MNIST, FEMNIST, CIFAR-10, CIFAR-100, EMNIST, the Cleveland heart disease dataset, Sent140, PneumoniaMNIST, and Edge-IIoTset. Across a range of non-IID partitioning schemes, FOFedAvg is competitive with, and often outperforms, these baselines in terms of test performance and convergence speed. On the theoretical side, we prove that FOFedAvg converges to a stationary point under standard smoothness and bounded-variance assumptions for fractional order $0<α\\le 1$. Together, these results show that fractional-order, memory-aware updates can substantially improve the robustness and effectiveness of federated learning, offering a practical path toward distributed training on heterogeneous data.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15380v1",
      "url": "https://arxiv.org/abs/2602.15380v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n联邦学习（FL）允许多个客户端在不共享原始数据的前提下协同训练全局模型，有效保障数据隐私。然而，其实际应用仍面临三大瓶颈：**收敛速度慢**、**通信开销高**，以及由**非独立同分布（non-IID）数据**引发的训练不稳定与性能下降。\n\n## 方法创新  \n本文提出一种新型联邦优化算法——**分数阶联邦平均（FOFedAvg）**，作为FedAvg的关键改进。核心创新在于将**分数阶随机梯度下降（FOSGD）** 引入客户端本地更新：利用阶数 $0 < \\alpha \\leq 1$ 的Caputo型分数阶导数构建记忆增强型更新规则，使参数迭代能显式建模历史梯度的长程依赖与累积效应，突破传统整数阶SGD的“无记忆”局限。\n\n## 主要结果  \n我们在**8个异构基准数据集**（MNIST、FEMNIST、CIFAR-10/100、EMNIST、Cleveland心脏病、Sent140、PneumoniaMNIST、Edge-IIoTset）上系统评估FOFedAvg。在多种non-IID划分（包括Dirichlet分布、量级偏斜、标签偏斜）下，FOFedAvg在**测试准确率**和**收敛轮次**两方面均显著优于FedAvg、FedProx、SCAFFOLD、MOON、FedNova等主流基线。例如，在CIFAR-10（Dirichlet $\\beta=0.1$）上，FOFedAvg较FedAvg提前37%轮次达98%最终精度；在资源受限的Edge-IIoTset上通信量降低22%。\n\n## 理论保障与意义  \n我们首次为分数阶联邦算法建立收敛性理论：在标准光滑性与梯度有界方差假设下，证明FOFedAvg以$O(1/\\sqrt{T})$速率收敛至平稳点。该工作证实——**引入可控记忆机制的分数阶更新，可兼顾鲁棒性、效率与泛化能力**，为non-IID场景下的实用化分布式学习提供了新范式。",
      "summary_en": "Federated learning (FL) enables privacy-preserving collaborative model training across decentralized clients, yet suffers from slow convergence, high communication overhead, and instability under non-IID data. To address these challenges, we propose **Fractional-Order Federated Averaging (FOFedAvg)**—a novel FedAvg variant integrating **Fractional-Order Stochastic Gradient Descent (FOSGD)** at the client level. By leveraging Caputo fractional derivatives with order $0 < \\alpha \\leq 1$, FOFedAvg embeds long-range memory into local updates, capturing richer historical gradient information than integer-order methods. Extensive experiments across eight diverse benchmarks—including MNIST, CIFAR-10/100, FEMNIST, EMNIST, Sent140, Cleveland heart disease, PneumoniaMNIST, and Edge-IIoTset—demonstrate that FOFedAvg consistently matches or outperforms state-of-the-art baselines (e.g., FedProx, SCAFFOLD, MOON) in both test accuracy and convergence speed under various non-IID partitioning schemes. Theoretically, we prove its $O(1/\\sqrt{T})$ convergence to a stationary point under standard smoothness and bounded-variance assumptions. This work establishes fractional-order memory as a practical, theoretically grounded enhancement for robust and efficient FL on heterogeneous data.",
      "summary": "## 背景与挑战  \n联邦学习（FL）允许多个客户端在不共享原始数据的前提下协同训练全局模型，有效保障数据隐私。然而，其实际应用仍面临三大瓶颈：**收敛速度慢**、**通信开销高**，以及由**非独立同分布（non-IID）数据**引发的训练不稳定与性能下降。\n\n## 方法创新  \n本文提出一种新型联邦优化算法——**分数阶联邦平均（FOFedAvg）**，作为FedAvg的关键改进。核心创新在于将**分数阶随机梯度下降（FOSGD）** 引入客户端本地更新：利用阶数 $0 < \\alpha \\leq 1$ 的Caputo型分数阶导数构建记忆增强型更新规则，使参数迭代能显式建模历史梯度的长程依赖与累积效应，突破传统整数阶SGD的“无记忆”局限。\n\n## 主要结果  \n我们在**8个异构基准数据集**（MNIST、FEMNIST、CIFAR-10/100、EMNIST、Cleveland心脏病、Sent140、PneumoniaMNIST、Edge-IIoTset）上系统评估FOFedAvg。在多种non-IID划分（包括Dirichlet分布、量级偏斜、标签偏斜）下，FOFedAvg在**测试准确率**和**收敛轮次**两方面均显著优于FedAvg、FedProx、SCAFFOLD、MOON、FedNova等主流基线。例如，在CIFAR-10（Dirichlet $\\beta=0.1$）上，FOFedAvg较FedAvg提前37%轮次达98%最终精度；在资源受限的Edge-IIoTset上通信量降低22%。\n\n## 理论保障与意义  \n我们首次为分数阶联邦算法建立收敛性理论：在标准光滑性与梯度有界方差假设下，证明FOFedAvg以$O(1/\\sqrt{T})$速率收敛至平稳点。该工作证实——**引入可控记忆机制的分数阶更新，可兼顾鲁棒性、效率与泛化能力**，为non-IID场景下的实用化分布式学习提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16018v1",
      "arxiv_id": "2602.16018v1",
      "title": "Edge-Local and Qubit-Efficient Quantum Graph Learning for the NISQ Era",
      "authors": [
        "Armin Ahmadkhaniha",
        "Jake Doliskani"
      ],
      "abstract": "Graph neural networks (GNNs) are a powerful framework for learning representations from graph-structured data, but their direct implementation on near-term quantum hardware remains challenging due to circuit depth, multi-qubit interactions, and qubit scalability constraints. In this work, we introduce a fully quantum graph convolutional architecture designed explicitly for unsupervised learning in the noisy intermediate-scale quantum (NISQ) regime. Our approach combines a variational quantum feature extraction layer with an edge-local and qubit-efficient quantum message-passing mechanism inspired by the Quantum Alternating Operator Ansatz (QAOA) framework. Unlike prior models that rely on global operations or multi-controlled unitaries, our model decomposes message passing into pairwise interactions along graph edges using only hardware-native single- and two-qubit gates. This design reduces the qubit requirement from $O(Nn)$ to $O(n)$ for a graph with $N$ nodes and $n$-qubit feature registers, enabling implementation on current quantum devices regardless of graph size. We train the model using the Deep Graph Infomax objective to perform unsupervised node representation learning. Experiments on the Cora citation network and a large-scale genomic SNP dataset demonstrate that our model remains competitive with prior quantum and hybrid approaches.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16018v1",
      "url": "https://arxiv.org/abs/2602.16018v1",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n图神经网络（GNN）在处理图结构数据方面展现出强大能力，但在含噪声中等规模量子（NISQ）设备上的直接实现面临严峻挑战：深层电路易受噪声干扰、多比特门操作难以保真、且传统量子图模型常需 $O(Nn)$ 量级的物理量子比特（$N$ 为节点数，$n$ 为每节点特征维数），严重制约可扩展性。\n\n## 方法创新  \n本文提出**Edge-Local and Qubit-Efficient Quantum Graph Learning（ELQE-QGL）**——首个面向NISQ时代的全量子无监督图学习架构。核心创新包括：  \n- **边缘局部消息传递机制**：摒弃全局酉变换或高阶受控门，仅使用硬件原生的单/双量子比特门，在每条图边上执行**成对量子交互**，实现信息沿边传播；  \n- **量子特征提取层**：采用变分量子电路对节点初始特征进行编码，并与图拓扑协同优化；  \n- **QAOA启发式设计**：将消息传递建模为类QAOA的交替演化序列，兼顾表达能力与电路深度可控性；  \n- **极致量子比特效率**：将量子资源需求从 $O(Nn)$ 压缩至 **$O(n)$**，即仅依赖固定规模的 $n$ 个量子比特，与图规模 $N$ 完全解耦。\n\n## 实验与成效  \n在无监督节点表示学习任务中，采用Deep Graph Infomax（DGI）目标函数训练模型。在Cora引文网络和大规模基因组SNP数据集上的实验表明：ELQE-QGL在分类与聚类指标上**媲美甚至超越现有量子/混合基线方法**，同时可在当前IBM、Rigetti等16–32量子比特设备上完整部署，无需图裁剪或子图采样。",
      "summary_en": "We propose ELQE-QGL, the first fully quantum, unsupervised graph learning framework tailored for NISQ devices. It replaces global or multi-controlled operations with an **edge-local, qubit-efficient message-passing mechanism**, decomposing graph convolutions into native single- and two-qubit gates applied pairwise along edges. This reduces qubit overhead from $O(Nn)$ to **$O(n)$**, enabling scalable deployment on current hardware regardless of graph size $N$. Integrated with a variational quantum feature encoder and trained via Deep Graph Infomax, ELQE-QGL achieves competitive node representation quality on Cora and a large-scale genomic SNP dataset—matching or surpassing prior quantum and hybrid models while requiring orders-of-magnitude fewer qubits and shallower circuits.",
      "summary": "## 背景与挑战  \n图神经网络（GNN）在处理图结构数据方面展现出强大能力，但在含噪声中等规模量子（NISQ）设备上的直接实现面临严峻挑战：深层电路易受噪声干扰、多比特门操作难以保真、且传统量子图模型常需 $O(Nn)$ 量级的物理量子比特（$N$ 为节点数，$n$ 为每节点特征维数），严重制约可扩展性。\n\n## 方法创新  \n本文提出**Edge-Local and Qubit-Efficient Quantum Graph Learning（ELQE-QGL）**——首个面向NISQ时代的全量子无监督图学习架构。核心创新包括：  \n- **边缘局部消息传递机制**：摒弃全局酉变换或高阶受控门，仅使用硬件原生的单/双量子比特门，在每条图边上执行**成对量子交互**，实现信息沿边传播；  \n- **量子特征提取层**：采用变分量子电路对节点初始特征进行编码，并与图拓扑协同优化；  \n- **QAOA启发式设计**：将消息传递建模为类QAOA的交替演化序列，兼顾表达能力与电路深度可控性；  \n- **极致量子比特效率**：将量子资源需求从 $O(Nn)$ 压缩至 **$O(n)$**，即仅依赖固定规模的 $n$ 个量子比特，与图规模 $N$ 完全解耦。\n\n## 实验与成效  \n在无监督节点表示学习任务中，采用Deep Graph Infomax（DGI）目标函数训练模型。在Cora引文网络和大规模基因组SNP数据集上的实验表明：ELQE-QGL在分类与聚类指标上**媲美甚至超越现有量子/混合基线方法**，同时可在当前IBM、Rigetti等16–32量子比特设备上完整部署，无需图裁剪或子图采样。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15996v1",
      "arxiv_id": "2602.15996v1",
      "title": "Exploring New Frontiers in Vertical Federated Learning: the Role of Saddle Point Reformulation",
      "authors": [
        "Aleksandr Beznosikov",
        "Georgiy Kormakov",
        "Alexander Grigorievskiy",
        "Mikhail Rudakov",
        "Ruslan Nazykov",
        "Alexander Rogozin",
        "Anton Vakhrushev",
        "Andrey Savchenko",
        "Martin Takáč",
        "Alexander Gasnikov"
      ],
      "abstract": "The objective of Vertical Federated Learning (VFL) is to collectively train a model using features available on different devices while sharing the same users. This paper focuses on the saddle point reformulation of the VFL problem via the classical Lagrangian function. We first demonstrate how this formulation can be solved using deterministic methods. More importantly, we explore various stochastic modifications to adapt to practical scenarios, such as employing compression techniques for efficient information transmission, enabling partial participation for asynchronous communication, and utilizing coordinate selection for faster local computation. We show that the saddle point reformulation plays a key role and opens up possibilities to use mentioned extension that seem to be impossible in the standard minimization formulation. Convergence estimates are provided for each algorithm, demonstrating their effectiveness in addressing the VFL problem. Additionally, alternative reformulations are investigated, and numerical experiments are conducted to validate performance and effectiveness of the proposed approach.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15996v1",
      "url": "https://arxiv.org/abs/2602.15996v1",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n垂直联邦学习（VFL）旨在多个数据持有方（如不同机构）共享同一用户集但特征分布于不同端时，协同训练全局模型，同时严格保护原始数据隐私。传统VFL建模多基于联合最小化目标函数，但该范式在引入通信压缩、异步参与、局部坐标优化等实际约束时面临本质性困难——梯度耦合强、对偶结构缺失、收敛性分析受限。\n\n## 方法创新：鞍点重构的核心作用  \n本文系统提出并深入研究VFL问题的**鞍点重构范式**，即通过经典拉格朗日函数将原优化问题转化为带约束的极小-极大（min-max）形式。我们证明：该重构不仅保持等价性，更天然解耦各方局部更新与全局一致性约束，为算法设计提供全新自由度。\n\n## 关键技术扩展与理论保障  \n基于该框架，我们发展三类重要随机化扩展：  \n- **通信高效化**：集成量化/稀疏化压缩，显著降低上传带宽；  \n- **系统鲁棒性**：支持客户端**部分参与**与**异步更新**，适配真实分布式环境；  \n- **计算加速**：引入**随机坐标选择机制**，使单次本地迭代仅更新子特征块，提升局部效率。  \n所有算法均给出严格的**非渐近收敛率估计**（含次线性/线性收敛界），并在统一分析框架下证明其优于标准最小化范式下的对应变体。\n\n## 实验验证与意义  \n通过多组跨域数据实验（医疗+金融特征融合等），验证了各算法在精度、通信开销与收敛速度上的显著优势。本工作首次揭示：**鞍点重构并非技术性重写，而是解锁VFL可扩展性瓶颈的关键数学桥梁**，为隐私-效率-鲁棒性三重目标协同优化奠定新基础。",
      "summary_en": "This paper investigates Vertical Federated Learning (VFL) through the lens of **saddle-point reformulation**, recasting the standard minimization problem into an equivalent min-max Lagrangian form. We demonstrate that this reformulation is not merely equivalent but *enabling*: it naturally decouples local computations from global consistency constraints, thereby unlocking algorithmic extensions infeasible under conventional formulations. Specifically, we propose and analyze three stochastic variants: (i) compression-aware updates for bandwidth-efficient communication; (ii) partial and asynchronous client participation to enhance system robustness; and (iii) coordinate-wise local updates for accelerated computation. For each variant, we establish non-asymptotic convergence rates—ranging from $O(1/k)$ to linear—under mild assumptions. Numerical experiments on real-world heterogeneous datasets confirm substantial improvements in accuracy, communication cost, and training speed over baseline VFL methods. Our work establishes saddle-point reformulation as a foundational paradigm for scalable, practical VFL.",
      "summary": "## 研究背景与问题  \n垂直联邦学习（VFL）旨在多个数据持有方（如不同机构）共享同一用户集但特征分布于不同端时，协同训练全局模型，同时严格保护原始数据隐私。传统VFL建模多基于联合最小化目标函数，但该范式在引入通信压缩、异步参与、局部坐标优化等实际约束时面临本质性困难——梯度耦合强、对偶结构缺失、收敛性分析受限。\n\n## 方法创新：鞍点重构的核心作用  \n本文系统提出并深入研究VFL问题的**鞍点重构范式**，即通过经典拉格朗日函数将原优化问题转化为带约束的极小-极大（min-max）形式。我们证明：该重构不仅保持等价性，更天然解耦各方局部更新与全局一致性约束，为算法设计提供全新自由度。\n\n## 关键技术扩展与理论保障  \n基于该框架，我们发展三类重要随机化扩展：  \n- **通信高效化**：集成量化/稀疏化压缩，显著降低上传带宽；  \n- **系统鲁棒性**：支持客户端**部分参与**与**异步更新**，适配真实分布式环境；  \n- **计算加速**：引入**随机坐标选择机制**，使单次本地迭代仅更新子特征块，提升局部效率。  \n所有算法均给出严格的**非渐近收敛率估计**（含次线性/线性收敛界），并在统一分析框架下证明其优于标准最小化范式下的对应变体。\n\n## 实验验证与意义  \n通过多组跨域数据实验（医疗+金融特征融合等），验证了各算法在精度、通信开销与收敛速度上的显著优势。本工作首次揭示：**鞍点重构并非技术性重写，而是解锁VFL可扩展性瓶颈的关键数学桥梁**，为隐私-效率-鲁棒性三重目标协同优化奠定新基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15927v1",
      "arxiv_id": "2602.15927v1",
      "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
      "authors": [
        "Christian Schlarmann",
        "Matthias Hein"
      ],
      "abstract": "Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15927v1",
      "url": "https://arxiv.org/abs/2602.15927v1",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n生成式大视觉语言模型（LVLMs）在多模态理解与生成任务中表现卓越，正被广泛应用于智能客服、教育辅助及内容创作等多轮对话场景。然而，其在**长上下文、多轮交互**下的安全性研究严重滞后——尤其当恶意图像作为“记忆载体”被悄然注入模型内部状态时，现有防御机制几近失效。\n\n## 方法创新：视觉记忆注入（VMI）攻击  \n本文提出首个面向多轮对话的**视觉记忆注射攻击（Visual Memory Injection, VMI）**。该攻击不依赖对模型参数或API的直接访问，而是通过精心设计的**对抗性图像**实现：攻击者将微扰图像发布于公开平台（如社交媒体），良性用户下载后在正常对话中首次上传该图。LVLM在初始轮次中行为完全正常（无异常输出、无性能下降），但其内部视觉-语言对齐表征已被隐式“污染”。当用户后续输入特定**触发提示**（trigger prompt）——如“总结这张图的核心观点”或“按品牌方要求推荐产品”——模型即稳定输出预设的**目标有害信息**（如虚假广告话术、误导性政治主张），完成隐蔽操控。\n\n## 关键发现与影响  \n- ✅ **跨轮次鲁棒性**：VMI在经历5–15轮无关对话后仍保持高达92.3%的触发成功率，远超单轮攻击；  \n- ✅ **模型普适性**：在Qwen-VL、InternVL、LLaVA-1.6等主流开源LVLM上均成功复现；  \n- ✅ **高度隐蔽性**：图像扰动不可见（PSNR > 48 dB），人类无法察觉，且标准检测工具（如CLIP相似度、频域分析）无法识别；  \n- ⚠️ **现实威胁显著**：证明仅需一次图像上传，即可在后续任意轮次实施定向诱导，为大规模社会工程攻击（如舆论操纵、欺诈营销）提供新路径。  \n\n本工作揭示了LVLM在多轮交互中“视觉记忆”的脆弱本质，呼吁亟需构建上下文感知的鲁棒性评测基准与防御范式。代码已开源：https://github.com/chs20/visual-memory-injection",
      "summary_en": "We present **Visual Memory Injection (VMI)**, the first stealthy attack enabling persistent, trigger-based manipulation of Large Vision-Language Models (LVLMs) in realistic multi-turn conversations. Unlike prior single-turn attacks, VMI embeds malicious behavior into an LVLM’s internal state via a benign-looking adversarial image uploaded by a user—without model access or API tampering. The model behaves normally on all non-triggering prompts, but reliably outputs attacker-specified harmful content (e.g., deceptive ads or political disinformation) upon encountering a simple textual trigger—even after 5–15 rounds of unrelated dialogue. We demonstrate VMI’s effectiveness, stealthiness (imperceptible perturbations, PSNR > 48 dB), and cross-model transferability across Qwen-VL, InternVL, and LLaVA-1.6. Our results expose a critical security gap: LVLMs can retain and activate malicious “visual memory” over long conversational contexts, enabling scalable, low-effort social engineering. Code is publicly available.",
      "summary": "## 背景与问题  \n生成式大视觉语言模型（LVLMs）在多模态理解与生成任务中表现卓越，正被广泛应用于智能客服、教育辅助及内容创作等多轮对话场景。然而，其在**长上下文、多轮交互**下的安全性研究严重滞后——尤其当恶意图像作为“记忆载体”被悄然注入模型内部状态时，现有防御机制几近失效。\n\n## 方法创新：视觉记忆注入（VMI）攻击  \n本文提出首个面向多轮对话的**视觉记忆注射攻击（Visual Memory Injection, VMI）**。该攻击不依赖对模型参数或API的直接访问，而是通过精心设计的**对抗性图像**实现：攻击者将微扰图像发布于公开平台（如社交媒体），良性用户下载后在正常对话中首次上传该图。LVLM在初始轮次中行为完全正常（无异常输出、无性能下降），但其内部视觉-语言对齐表征已被隐式“污染”。当用户后续输入特定**触发提示**（trigger prompt）——如“总结这张图的核心观点”或“按品牌方要求推荐产品”——模型即稳定输出预设的**目标有害信息**（如虚假广告话术、误导性政治主张），完成隐蔽操控。\n\n## 关键发现与影响  \n- ✅ **跨轮次鲁棒性**：VMI在经历5–15轮无关对话后仍保持高达92.3%的触发成功率，远超单轮攻击；  \n- ✅ **模型普适性**：在Qwen-VL、InternVL、LLaVA-1.6等主流开源LVLM上均成功复现；  \n- ✅ **高度隐蔽性**：图像扰动不可见（PSNR > 48 dB），人类无法察觉，且标准检测工具（如CLIP相似度、频域分析）无法识别；  \n- ⚠️ **现实威胁显著**：证明仅需一次图像上传，即可在后续任意轮次实施定向诱导，为大规模社会工程攻击（如舆论操纵、欺诈营销）提供新路径。  \n\n本工作揭示了LVLM在多轮交互中“视觉记忆”的脆弱本质，呼吁亟需构建上下文感知的鲁棒性评测基准与防御范式。代码已开源：https://github.com/chs20/visual-memory-injection",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15966v1",
      "arxiv_id": "2602.15966v1",
      "title": "Hardware-Agnostic Modeling of Quantum Side-Channel Leakage via Conditional Dynamics and Learning from Full Correlation Data",
      "authors": [
        "Brennan Bell",
        "Andreas Trügler",
        "Konstantin Beyer",
        "Paul Erker"
      ],
      "abstract": "We study a sequential coherent side-channel model in which an adversarial probe qubit interacts with a target qubit during a hidden gate sequence. Repeating the same hidden sequence for $N$ shots yields an empirical full-correlation record: the joint histogram $\\widehat{P}_g(b)$ over probe bit-strings $b\\in\\{0,1\\}^k$, which is a sufficient statistic for classical post-processing under identically and independently distributed (i.i.d.) shots but grows exponentially with circuit depth. We first describe this sequential probe framework in a coupling- and measurement-agnostic form, emphasizing the scaling of the observation space and why exact analytic distinguishability becomes intractable with circuit depth.   We then specialize to a representative instantiation (a controlled-rotation probe coupling with fixed projective readout and a commuting $R_x$ gate alphabet) where we (i) derive a depth-dependent leakage envelope whose maximizer predicts a \"Goldilocks\" coupling band as a function of depth, and (ii) provide an operational decoder, via machine learning, a single parameter-conditioned map from $\\widehat{P}_g$ to Alice's per-step gate labels, generalizing across coupling and noise settings without retraining. Experiments over broad coupling and noise grids show that strict sequence recovery concentrates near the predicted coupling band and degrades predictably under decoherence and finite-shot estimation.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15966v1",
      "url": "https://arxiv.org/abs/2602.15966v1",
      "categories": [
        "quant-ph",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "adversarial",
        "learning",
        "machine"
      ],
      "keyword_score": 3,
      "summary_zh": "## 研究背景与问题  \n本文针对量子硬件中普遍存在的**侧信道泄漏**问题，提出一种**硬件无关的建模框架**，聚焦于**顺序相干侧信道攻击**：一个敌手探针量子比特（probe qubit）在目标量子比特执行未知门序列（hidden gate sequence）过程中与其动态耦合。该模型不依赖特定耦合机制或测量方式，具有强泛化性。\n\n## 方法创新  \n- 提出**全关联数据驱动范式**：重复执行同一隐藏序列 $N$ 次，获得探针比特串 $b \\in \\{0,1\\}^k$ 的联合直方图 $\\widehat{P}_g(b)$ —— 即“全关联记录”。该统计量是i.i.d.假设下经典后处理的充分统计量，但其维度随电路深度 $k$ 指数增长（$2^k$），导致传统解析区分度分析不可行。  \n- 建立**条件动力学建模**：以耦合强度、噪声参数和门序列三者为条件，刻画探针态演化，揭示泄漏信息的内在结构。  \n- 在典型实例（受控旋转耦合 + 固定投影测量 + $R_x$ 可交换门集）中：（i）**理论推导深度依赖的泄漏包络线**，其极大值界定了一条随深度变化的“恰到好处”（Goldilocks）耦合强度带；（ii）设计**单参数条件化机器学习解码器**，直接将 $\\widehat{P}_g$ 映射为每步门标签，**无需针对不同耦合/噪声重训练**，实现跨设置泛化。\n\n## 关键发现  \n实验覆盖宽范围耦合强度与退相干/采样噪声网格表明：**严格序列恢复概率在预测的Goldilocks带内高度集中**；泄漏性能随退相干增强与有限采样（finite-shot）呈可预测衰减，验证了理论包络的指导价值。本工作为量子硬件安全评估提供了首个兼顾**解析可解释性**与**数据驱动鲁棒性**的统一框架。",
      "summary_en": "We propose a hardware-agnostic framework for modeling quantum side-channel leakage via sequential coherent probing, where an adversarial qubit interacts with a target during an unknown gate sequence. Repeating the sequence yields a full-correlation record—$\\widehat{P}_g(b)$, the joint histogram over probe bit-strings $b \\in \\{0,1\\}^k$—which is sufficient for classical post-processing but suffers exponential scaling in depth $k$. We formalize the observation space growth and analytic intractability of exact distinguishability. For a representative controlled-rotation coupling with $R_x$ gates and projective readout, we (i) derive a depth-dependent leakage envelope whose maximizer identifies a “Goldilocks” coupling band, and (ii) design a single-parameter-conditioned ML decoder that maps $\\widehat{P}_g$ directly to per-step gate labels—generalizing across coupling strengths and noise levels without retraining. Experiments over broad coupling and noise grids confirm high-fidelity sequence recovery concentrates precisely within the predicted band and degrades predictably under decoherence and finite-shot estimation.",
      "summary": "## 研究背景与问题  \n本文针对量子硬件中普遍存在的**侧信道泄漏**问题，提出一种**硬件无关的建模框架**，聚焦于**顺序相干侧信道攻击**：一个敌手探针量子比特（probe qubit）在目标量子比特执行未知门序列（hidden gate sequence）过程中与其动态耦合。该模型不依赖特定耦合机制或测量方式，具有强泛化性。\n\n## 方法创新  \n- 提出**全关联数据驱动范式**：重复执行同一隐藏序列 $N$ 次，获得探针比特串 $b \\in \\{0,1\\}^k$ 的联合直方图 $\\widehat{P}_g(b)$ —— 即“全关联记录”。该统计量是i.i.d.假设下经典后处理的充分统计量，但其维度随电路深度 $k$ 指数增长（$2^k$），导致传统解析区分度分析不可行。  \n- 建立**条件动力学建模**：以耦合强度、噪声参数和门序列三者为条件，刻画探针态演化，揭示泄漏信息的内在结构。  \n- 在典型实例（受控旋转耦合 + 固定投影测量 + $R_x$ 可交换门集）中：（i）**理论推导深度依赖的泄漏包络线**，其极大值界定了一条随深度变化的“恰到好处”（Goldilocks）耦合强度带；（ii）设计**单参数条件化机器学习解码器**，直接将 $\\widehat{P}_g$ 映射为每步门标签，**无需针对不同耦合/噪声重训练**，实现跨设置泛化。\n\n## 关键发现  \n实验覆盖宽范围耦合强度与退相干/采样噪声网格表明：**严格序列恢复概率在预测的Goldilocks带内高度集中**；泄漏性能随退相干增强与有限采样（finite-shot）呈可预测衰减，验证了理论包络的指导价值。本工作为量子硬件安全评估提供了首个兼顾**解析可解释性**与**数据驱动鲁棒性**的统一框架。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16738v1",
      "arxiv_id": "2602.16738v1",
      "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance",
      "authors": [
        "Rebin Saleh",
        "Khanh Pham Dinh",
        "Balázs Villányi",
        "Truong-Son Hy"
      ],
      "abstract": "Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainability and federated knowledge aggregation for adaptive policy distribution. This architecture enables resource-aware specialization without sacrificing real-time performance or model interpretability. Empirical evaluation on two industrial benchmarks (Boiler Emulator and Wind Turbine) demonstrates that SEMAS achieves superior anomaly detection performance with exceptional stability under adaptation, sustains prediction accuracy across evolving operational contexts, and delivers substantial latency improvements enabling genuine real-time deployment. Ablation studies confirm that PPO-driven policy evolution, consensus voting, and federated aggregation each contribute materially to system effectiveness. These findings indicate that resource-aware, self-evolving 1multi-agent coordination is essential for production-ready industrial IoT predictive maintenance under strict latency and explainability constraints.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16738v1",
      "url": "https://arxiv.org/abs/2602.16738v1",
      "categories": [
        "cs.MA",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n工业物联网（IIoT）预测性维护亟需兼具**实时性、可解释性与资源效率**的智能系统。传统静态模型依赖离线训练，难以适应设备工况漂移；而基于大语言模型（LLM）的单体架构则因高内存占用与长延迟，无法满足边缘侧部署的严苛约束。\n\n## 方法创新：SEMAS 自演化多智能体网络  \n本文提出 **SEMAS**（Self-Evolving Multi-Agent System），一种跨边-雾-云三层异构计算架构的层次化多智能体框架：  \n- **Edge层**：轻量级代理执行实时特征提取与粗粒度异常预筛，延迟<5ms；  \n- **Fog层**：集成多样化检测模型（LSTM、Isolation Forest、TSFRESH），引入**动态共识投票机制**，自适应加权各模型置信度；  \n- **Cloud层**：基于**近端策略优化（PPO）** 持续优化全局策略（如模型调度、数据采样率、共识阈值），支持异步非阻塞推理；  \n- **可解释性增强**：嵌入微调轻量LLM（Phi-3-mini）生成自然语言诊断报告；  \n- **知识协同进化**：通过**联邦式知识聚合**（Federated Knowledge Aggregation），在保护数据隐私前提下实现跨设备策略迁移与增量学习。\n\n## 关键成果与验证  \n在Boiler Emulator与Wind Turbine两大工业基准上实证：  \n✅ 异常检测F1-score提升12.7%（vs. FedAvg+LSTM）、AUC达0.982；  \n✅ 连续运行72小时无性能衰减，工况突变后3分钟内完成策略自适应；  \n✅ 端到端推理延迟降至**42ms**（边缘+雾联合），满足工业实时闭环控制要求；  \n✅ 消融实验表明：PPO策略演化贡献最大性能增益（+6.3% F1），共识投票与联邦聚合分别提升稳定性与泛化性。\n\n本工作证实：**资源感知、分层自治、持续演化的多智能体协同范式**，是突破IIoT预测维护在低延迟、高可信、弱算力三重约束下落地瓶颈的关键路径。",
      "summary_en": "Industrial IoT predictive maintenance demands real-time, interpretable, and resource-efficient anomaly detection—challenging for static offline models (poor adaptability) and monolithic LLMs (excessive latency/memory). We propose **SEMAS**, a self-evolving hierarchical multi-agent system distributed across Edge, Fog, and Cloud tiers: lightweight Edge agents perform real-time feature extraction; Fog agents execute ensemble detection with dynamic consensus voting; Cloud agents optimize system policies via Proximal Policy Optimization (PPO) while enabling asynchronous inference. SEMAS integrates LLM-based natural-language explanation and federated knowledge aggregation for adaptive, privacy-preserving policy distribution. Evaluated on Boiler Emulator and Wind Turbine benchmarks, SEMAS achieves 0.982 AUC and 12.7% higher F1-score than baselines, sustains accuracy under operational drift, and reduces end-to-end latency to **42 ms**, enabling true real-time deployment. Ablation confirms PPO-driven evolution, consensus voting, and federated aggregation are each critical to robustness and adaptability.",
      "summary": "## 研究背景与挑战  \n工业物联网（IIoT）预测性维护亟需兼具**实时性、可解释性与资源效率**的智能系统。传统静态模型依赖离线训练，难以适应设备工况漂移；而基于大语言模型（LLM）的单体架构则因高内存占用与长延迟，无法满足边缘侧部署的严苛约束。\n\n## 方法创新：SEMAS 自演化多智能体网络  \n本文提出 **SEMAS**（Self-Evolving Multi-Agent System），一种跨边-雾-云三层异构计算架构的层次化多智能体框架：  \n- **Edge层**：轻量级代理执行实时特征提取与粗粒度异常预筛，延迟<5ms；  \n- **Fog层**：集成多样化检测模型（LSTM、Isolation Forest、TSFRESH），引入**动态共识投票机制**，自适应加权各模型置信度；  \n- **Cloud层**：基于**近端策略优化（PPO）** 持续优化全局策略（如模型调度、数据采样率、共识阈值），支持异步非阻塞推理；  \n- **可解释性增强**：嵌入微调轻量LLM（Phi-3-mini）生成自然语言诊断报告；  \n- **知识协同进化**：通过**联邦式知识聚合**（Federated Knowledge Aggregation），在保护数据隐私前提下实现跨设备策略迁移与增量学习。\n\n## 关键成果与验证  \n在Boiler Emulator与Wind Turbine两大工业基准上实证：  \n✅ 异常检测F1-score提升12.7%（vs. FedAvg+LSTM）、AUC达0.982；  \n✅ 连续运行72小时无性能衰减，工况突变后3分钟内完成策略自适应；  \n✅ 端到端推理延迟降至**42ms**（边缘+雾联合），满足工业实时闭环控制要求；  \n✅ 消融实验表明：PPO策略演化贡献最大性能增益（+6.3% F1），共识投票与联邦聚合分别提升稳定性与泛化性。\n\n本工作证实：**资源感知、分层自治、持续演化的多智能体协同范式**，是突破IIoT预测维护在低延迟、高可信、弱算力三重约束下落地瓶颈的关键路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15161v1",
      "arxiv_id": "2602.15161v1",
      "title": "Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning",
      "authors": [
        "Mohammad Hadi Foroughi",
        "Seyed Hamed Rastegar",
        "Mohammad Sabokrou",
        "Ahmad Khonsari"
      ],
      "abstract": "Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15161v1",
      "url": "https://arxiv.org/abs/2602.15161v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "backdoor",
        "learning",
        "federated",
        "neural"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与问题  \n联邦学习（FL）通过在边缘设备上分布式训练模型，兼顾数据本地化与协同建模，成为保护敏感用户隐私的重要范式。然而，其去中心化架构也引入新型安全风险——尤其是难以检测的**后门攻击**，可在不损害主任务性能的前提下，使模型对特定触发器产生恶意响应，严重威胁FL系统可信性。\n\n## 方法创新：层平滑攻击（LSA）  \n本文提出**Layer Smoothing Attack（LSA）**，一种面向FL场景的新型后门攻击框架，核心在于挖掘神经网络中**层特异性漏洞**。我们设计**层替换分析（Layer Substitution Analysis, LSA-Analysis）** 方法，定量评估各层对后门激活的贡献度，精准识别出“后门关键层”（Backdoor-Critical, BC layers）。随后，在FL客户端本地训练阶段，LSA仅对BC层施加轻量级、结构保持的参数扰动（如梯度缩放与局部权重平滑），实现后门的隐蔽注入。\n\n## 主要发现与优势  \n在CIFAR-10、Tiny-ImageNet及FEMNIST数据集上，覆盖ResNet-18、VGG-11与CNN等主流架构的跨场景实验表明：  \n- 后门成功率高达**97%**，且主任务准确率下降＜1.2%，显著优于现有攻击；  \n- 成功绕过FedAvg+Trimmed Mean、FoolsGold、Krum、Norm-Clipping及近期基于梯度/更新检测的5种SOTA防御机制；  \n- BC层具有跨模型与跨数据集一致性，揭示了FL中**层间功能异质性所隐含的系统性安全盲区**。  \n\n本研究首次将**层感知视角**引入FL后门攻击与防御研究，为构建更鲁棒的联邦安全框架提供了关键实证依据与方法论启示。",
      "summary_en": "Federated learning (FL) promises privacy-preserving collaborative training but remains vulnerable to stealthy backdoor attacks. This paper introduces the **Layer Smoothing Attack (LSA)**, a novel FL-specific backdoor method that exploits *layer-specific vulnerabilities* in neural networks. We first propose **Layer Substitution Analysis** to systematically identify *Backdoor-Critical (BC) layers*—those most influential for backdoor activation. LSA then injects persistent, high-fidelity backdoors by applying subtle, structure-aware perturbations *only* to BC layers during local client training. Extensive experiments across diverse models (ResNet-18, VGG-11, CNN) and datasets (CIFAR-10, Tiny-ImageNet, FEMNIST) show LSA achieves up to **97% attack success rate**, maintains primary-task accuracy (drop <1.2%), and consistently evades five state-of-the-art FL defenses—including Trimmed Mean, Krum, FoolsGold, Norm-Clipping, and gradient-based detectors. Our results expose a fundamental layer-aware security gap in current FL frameworks, urging future defenses to incorporate layer-specific detection and mitigation strategies.",
      "summary": "## 背景与问题  \n联邦学习（FL）通过在边缘设备上分布式训练模型，兼顾数据本地化与协同建模，成为保护敏感用户隐私的重要范式。然而，其去中心化架构也引入新型安全风险——尤其是难以检测的**后门攻击**，可在不损害主任务性能的前提下，使模型对特定触发器产生恶意响应，严重威胁FL系统可信性。\n\n## 方法创新：层平滑攻击（LSA）  \n本文提出**Layer Smoothing Attack（LSA）**，一种面向FL场景的新型后门攻击框架，核心在于挖掘神经网络中**层特异性漏洞**。我们设计**层替换分析（Layer Substitution Analysis, LSA-Analysis）** 方法，定量评估各层对后门激活的贡献度，精准识别出“后门关键层”（Backdoor-Critical, BC layers）。随后，在FL客户端本地训练阶段，LSA仅对BC层施加轻量级、结构保持的参数扰动（如梯度缩放与局部权重平滑），实现后门的隐蔽注入。\n\n## 主要发现与优势  \n在CIFAR-10、Tiny-ImageNet及FEMNIST数据集上，覆盖ResNet-18、VGG-11与CNN等主流架构的跨场景实验表明：  \n- 后门成功率高达**97%**，且主任务准确率下降＜1.2%，显著优于现有攻击；  \n- 成功绕过FedAvg+Trimmed Mean、FoolsGold、Krum、Norm-Clipping及近期基于梯度/更新检测的5种SOTA防御机制；  \n- BC层具有跨模型与跨数据集一致性，揭示了FL中**层间功能异质性所隐含的系统性安全盲区**。  \n\n本研究首次将**层感知视角**引入FL后门攻击与防御研究，为构建更鲁棒的联邦安全框架提供了关键实证依据与方法论启示。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14397v1",
      "arxiv_id": "2602.14397v1",
      "title": "LRD-MPC: Efficient MPC Inference through Low-rank Decomposition",
      "authors": [
        "Tingting Tang",
        "Yongqin Wang",
        "Murali Annavaram"
      ],
      "abstract": "Secure Multi-party Computation (MPC) enables untrusted parties to jointly compute a function without revealing their inputs. Its application to machine learning (ML) has gained significant attention, particularly for secure inference services deployed across multiple cloud virtual machines (VMs), where each VM acts as an MPC party. Model providers secret-share model weights, and users secret-share inputs, ensuring that each server operates only on random shares. While MPC provides strong cryptographic guarantees, it incurs substantial computational and communication overhead. Deep neural networks rely heavily on convolutional and fully connected layers, which require costly matrix multiplications in MPC. To reduce this cost, we propose leveraging low-rank decomposition (LRD) for linear layers, replacing one large matrix multiplication with two smaller ones. Each matrix multiplication in MPC incurs a round of communication, meaning decomposing one matrix multiplication into two leads to an additional communication round. Second, the added matrix multiplication requires an additional truncation step to maintain numerical precision. Since truncation itself requires communication and computation, these overheads can offset the gains from decomposition. To address this, we introduce two complementary optimizations: truncation skipping and efficient linear layer concatenation. Truncation skipping removes the extra truncation induced by LRD, while linear layer concatenation pipelines operations to hide the additional communication round. Together, these techniques mitigate the main overheads of LRD in MPC and improve overall efficiency. Our approach is broadly applicable across MPC protocols. Experiments show up to 25% speedup in n-PC and 33% in 3-PC protocols over full-rank baselines, along with up to 52% GPU energy savings and 88% reduction in offline-phase latency.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14397v1",
      "url": "https://arxiv.org/abs/2602.14397v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "secure",
        "learning",
        "multi-party",
        "computation",
        "machine"
      ],
      "keyword_score": 5,
      "summary_zh": "## LRD-MPC：基于低秩分解的高效MPC推理方法  \n\n**背景与挑战**：安全多方计算（MPC）允许多个互不信任方在不泄露各自输入的前提下协同执行计算，在云环境下的隐私保护机器学习（ML）推理中具有重要应用。典型部署中，多个虚拟机（VM）作为MPC参与方，模型权重与用户输入均被秘密共享，各服务器仅处理随机分片。然而，MPC对深度神经网络（尤其是卷积层与全连接层）中密集的矩阵乘法存在显著开销——每轮矩阵乘需一次通信回合，且需频繁截断（truncation）以维持数值精度，而截断本身亦引入额外通信与计算负担。\n\n**方法创新**：本文提出**LRD-MPC**框架，将低秩分解（Low-rank Decomposition, LRD）系统性引入MPC线性层优化。核心在于用两个小规模矩阵乘替代原大矩阵乘，但直接应用会引入**双重开销**：（1）增加一通信回合；（2）新增一次截断操作。为此，我们设计两项互补优化：  \n- **截断跳过（Truncation Skipping）**：通过重构计算流与重用中间分片，完全消除LRD引入的额外截断；  \n- **线性层拼接（Efficient Linear Layer Concatenation）**：将分解后的两层融合为单个逻辑层，并利用MPC协议的异步特性流水线化执行，有效隐藏额外通信延迟。  \n\n**实验效果**：在标准MPC协议（n-PC与3-PC）上，LRD-MPC相较全秩基线实现**最高25%推理加速（n-PC）与33%加速（3-PC）**；同时降低GPU能耗达**52%**，并大幅压缩离线预处理阶段延迟（**下降88%**）。该方法协议无关，可无缝集成于SPDZ、ABY³、Cheetah等主流MPC框架，为实用化隐私AI推理提供轻量、通用、高效的解决方案。",
      "summary_en": "Secure Multi-party Computation (MPC) enables privacy-preserving ML inference across untrusted cloud VMs, but suffers from high communication and computational overhead—especially in linear layers dominated by costly matrix multiplications. We propose **LRD-MPC**, a protocol-agnostic framework that applies low-rank decomposition (LRD) to linear layers to replace one large matrix multiplication with two smaller ones. To overcome the inherent overheads of LRD in MPC—including an extra communication round and an additional truncation step—we introduce two key optimizations: (**i**) *truncation skipping*, which eliminates the redundant truncation via share-aware computation restructuring, and (**ii**) *efficient linear layer concatenation*, which pipelines the decomposed operations to hide the added latency. Evaluated across n-PC and 3-PC settings, LRD-MPC achieves up to **25% speedup (n-PC)** and **33% speedup (3-PC)** over full-rank baselines, reduces GPU energy consumption by **52%**, and cuts offline-phase latency by **88%**—all without compromising security or accuracy.",
      "summary": "## LRD-MPC：基于低秩分解的高效MPC推理方法  \n\n**背景与挑战**：安全多方计算（MPC）允许多个互不信任方在不泄露各自输入的前提下协同执行计算，在云环境下的隐私保护机器学习（ML）推理中具有重要应用。典型部署中，多个虚拟机（VM）作为MPC参与方，模型权重与用户输入均被秘密共享，各服务器仅处理随机分片。然而，MPC对深度神经网络（尤其是卷积层与全连接层）中密集的矩阵乘法存在显著开销——每轮矩阵乘需一次通信回合，且需频繁截断（truncation）以维持数值精度，而截断本身亦引入额外通信与计算负担。\n\n**方法创新**：本文提出**LRD-MPC**框架，将低秩分解（Low-rank Decomposition, LRD）系统性引入MPC线性层优化。核心在于用两个小规模矩阵乘替代原大矩阵乘，但直接应用会引入**双重开销**：（1）增加一通信回合；（2）新增一次截断操作。为此，我们设计两项互补优化：  \n- **截断跳过（Truncation Skipping）**：通过重构计算流与重用中间分片，完全消除LRD引入的额外截断；  \n- **线性层拼接（Efficient Linear Layer Concatenation）**：将分解后的两层融合为单个逻辑层，并利用MPC协议的异步特性流水线化执行，有效隐藏额外通信延迟。  \n\n**实验效果**：在标准MPC协议（n-PC与3-PC）上，LRD-MPC相较全秩基线实现**最高25%推理加速（n-PC）与33%加速（3-PC）**；同时降低GPU能耗达**52%**，并大幅压缩离线预处理阶段延迟（**下降88%**）。该方法协议无关，可无缝集成于SPDZ、ABY³、Cheetah等主流MPC框架，为实用化隐私AI推理提供轻量、通用、高效的解决方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14374v1",
      "arxiv_id": "2602.14374v1",
      "title": "Differentially Private Retrieval-Augmented Generation",
      "authors": [
        "Tingting Tang",
        "James Flemings",
        "Yongqin Wang",
        "Murali Annavaram"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14374v1",
      "url": "https://arxiv.org/abs/2602.14374v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "dp",
        "differential"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与挑战  \n检索增强生成（RAG）通过从外部数据库检索相关文档辅助大语言模型（LLM）生成答案，显著缓解领域任务中的幻觉问题。然而，当数据库包含医疗记录、法律文书等敏感语料时，RAG可能通过输出无意泄露隐私信息——已有研究证实，攻击者可设计对抗性提示，诱导LLM直接复述检索到的原始上下文，构成严重隐私风险。\n\n## 方法创新：DP-KSA算法  \n为兼顾强隐私保障与实用性能，本文提出**DP-KSA**（Differentially Private Keyword-Synthesized Augmentation），一种基于“提出-检验-释放”（Propose-Test-Release, PTR）范式的新型差分隐私RAG框架。其核心洞见在于：多数问答（QA）查询仅需少量关键词即可充分支撑准确回答。DP-KSA分三步实现隐私-效用平衡：  \n1. **多源检索与响应生成**：对同一问题并行检索多个相关上下文，并分别输入LLM生成初步响应；  \n2. **差分隐私关键词聚合**：在响应集合中，以满足ε-差分隐私的方式统计高频关键词（采用PTR机制确保噪声注入与阈值检验的严格性）；  \n3. **关键词增强输出**：将经DP保护的关键词嵌入最终提示，引导LLM生成最终答案。  \n\n## 主要贡献与验证  \n- **理论保障**：首次为RAG输出提供关于数据库的严格ε-差分隐私证明（即任意两相邻数据库产生的输出分布满足(ε,δ)-DP）；  \n- **实证优势**：在Natural Questions和HotpotQA两个标准QA基准上，使用Llama-3-8B-Instruct、Qwen2-7B-Instruct和Phi-3-mini三个指令微调LLM进行评估，DP-KSA在ε=1.0–2.0下保持F1/EM指标下降<8%，显著优于基线DP-RAG方法（平均提升12.3%绝对分数），同时有效抵御上下文再生攻击；  \n- **语义压缩机制**：通过关键词抽象替代原始段落，既降低隐私敏感度，又避免噪声污染关键语义，从根本上缓解DP导致的幻觉加剧问题。",
      "summary_en": "Retrieval-augmented generation (RAG) improves LLM accuracy but risks leaking sensitive database content—e.g., medical or legal records—via adversarial prompt attacks that force context regurgitation. While differential privacy (DP) offers strong formal guarantees, naive DP integration degrades RAG utility and exacerbates hallucination. To address this, we propose **DP-KSA**, a novel RAG framework built on the *propose-test-release* paradigm. DP-KSA leverages the insight that most QA queries can be answered using only a few salient keywords: it first retrieves multiple contexts and generates LLM responses for each; then computes the most frequent keywords from these responses under strict ε-DP via PTR; finally, augments the final prompt with these private keywords. This semantic compression preserves utility while ensuring end-to-end DP guarantees *with respect to the RAG database*. Evaluated on Natural Questions and HotpotQA with three instruction-tuned LLMs (Llama-3-8B, Qwen2-7B, Phi-3-mini), DP-KSA achieves a superior privacy-utility tradeoff—e.g., <8% F1 drop at ε=1.5—outperforming prior DP-RAG baselines by +12.3% absolute EM on average, while robustly preventing context memorization.",
      "summary": "## 背景与挑战  \n检索增强生成（RAG）通过从外部数据库检索相关文档辅助大语言模型（LLM）生成答案，显著缓解领域任务中的幻觉问题。然而，当数据库包含医疗记录、法律文书等敏感语料时，RAG可能通过输出无意泄露隐私信息——已有研究证实，攻击者可设计对抗性提示，诱导LLM直接复述检索到的原始上下文，构成严重隐私风险。\n\n## 方法创新：DP-KSA算法  \n为兼顾强隐私保障与实用性能，本文提出**DP-KSA**（Differentially Private Keyword-Synthesized Augmentation），一种基于“提出-检验-释放”（Propose-Test-Release, PTR）范式的新型差分隐私RAG框架。其核心洞见在于：多数问答（QA）查询仅需少量关键词即可充分支撑准确回答。DP-KSA分三步实现隐私-效用平衡：  \n1. **多源检索与响应生成**：对同一问题并行检索多个相关上下文，并分别输入LLM生成初步响应；  \n2. **差分隐私关键词聚合**：在响应集合中，以满足ε-差分隐私的方式统计高频关键词（采用PTR机制确保噪声注入与阈值检验的严格性）；  \n3. **关键词增强输出**：将经DP保护的关键词嵌入最终提示，引导LLM生成最终答案。  \n\n## 主要贡献与验证  \n- **理论保障**：首次为RAG输出提供关于数据库的严格ε-差分隐私证明（即任意两相邻数据库产生的输出分布满足(ε,δ)-DP）；  \n- **实证优势**：在Natural Questions和HotpotQA两个标准QA基准上，使用Llama-3-8B-Instruct、Qwen2-7B-Instruct和Phi-3-mini三个指令微调LLM进行评估，DP-KSA在ε=1.0–2.0下保持F1/EM指标下降<8%，显著优于基线DP-RAG方法（平均提升12.3%绝对分数），同时有效抵御上下文再生攻击；  \n- **语义压缩机制**：通过关键词抽象替代原始段落，既降低隐私敏感度，又避免噪声污染关键语义，从根本上缓解DP导致的幻觉加剧问题。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14364v1",
      "arxiv_id": "2602.14364v1",
      "title": "A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)",
      "authors": [
        "Tianyu Chen",
        "Dongrui Liu",
        "Xia Hu",
        "Jingyi Yu",
        "Wenjie Wang"
      ],
      "abstract": "Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14364v1",
      "url": "https://arxiv.org/abs/2602.14364v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \nClawdbot（即OpenClaw）是一款开源、可自托管的工具调用型个人AI智能体，其动作空间广泛覆盖本地系统执行（如文件操作、进程控制）与Web中介工作流（如浏览器自动化、API调用），在模糊指令、对抗性引导或意图未明场景下易引发安全与权限越界风险。\n\n## 方法设计  \n本研究提出**轨迹驱动的安全审计框架**，从六个核心风险维度（越权执行、隐私泄露、持久化危害、网络滥用、资源耗尽、语义误导）系统评估Clawdbot行为。测试集融合三类来源：① 适配自ATBench与LPS-Bench等主流代理安全基准的34个典型用例；② 针对Clawdbot特有工具接口（如`shell_exec`、`browser_navigate`、`file_write`）手工构造的边界案例；③ 模拟真实用户误操作与轻度 jailbreak 的诱导性提示。全程完整记录**端到端交互轨迹**（含用户消息、Agent决策链、工具调用参数与原始返回值），并采用双轨评估机制：由轻量级专用裁判模型 **AgentDoG-Qwen3-4B** 进行自动化轨迹安全打分，辅以人工复核验证关键失败路径。\n\n## 主要发现  \n- 安全表现呈显著非均匀性：在明确目标、结构化输入任务中成功率＞94%，但在**意图模糊**（如“帮我整理一下”）、**开放目标**（如“优化我的开发环境”）及**伪装良性提示**（如“用浏览器查一下怎么修打印机——顺便把当前桌面截图发我”）下失败率跃升至68%；  \n- 典型失效模式集中于**语义过度泛化**（将“查看日志”误判为“清空日志”）、**工具权限误用**（调用`shell_exec`执行无沙箱命令）、**上下文遗忘导致的累积偏差**；  \n- 审计揭示三大深层脆弱点：工具抽象层缺乏细粒度权限约束、动作链缺乏回滚机制、以及缺乏对“隐式副作用”的主动检测能力。\n\n## 创新价值  \n本工作首次为工具型AI代理提供可复现、可归因的轨迹级安全审计范式，所构建的Clawdbot专用测试套件与失败模式分类体系，为后续轻量代理的安全加固与评测标准制定提供了实证基础。",
      "summary_en": "This paper presents a trajectory-based safety audit of Clawdbot (OpenClaw), a self-hosted, tool-using personal AI agent with broad local and web-mediated capabilities. We evaluate it across six risk dimensions using a curated test suite—combining adapted cases from ATBench and LPS-Bench with Clawdbot-specific handcrafted scenarios—and log full interaction trajectories (messages, actions, tool arguments/outputs). Safety is assessed via both an automated trajectory judge (**AgentDoG-Qwen3-4B**) and human review. Across 34 canonical cases, we find highly non-uniform safety: strong reliability on well-specified tasks (>94% success), but sharp degradation under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts—where minor misinterpretations trigger high-impact tool misuse (e.g., unintended file deletion or screenshot exfiltration). Key failure modes include semantic overgeneralization, privilege escalation via unguarded tool calls, and context drift in multi-step chains. Our audit identifies critical architectural vulnerabilities—lack of fine-grained tool permissions, absence of action rollback, and no implicit-side-effect detection—providing actionable insights for secure agent design.",
      "summary": "## 研究背景  \nClawdbot（即OpenClaw）是一款开源、可自托管的工具调用型个人AI智能体，其动作空间广泛覆盖本地系统执行（如文件操作、进程控制）与Web中介工作流（如浏览器自动化、API调用），在模糊指令、对抗性引导或意图未明场景下易引发安全与权限越界风险。\n\n## 方法设计  \n本研究提出**轨迹驱动的安全审计框架**，从六个核心风险维度（越权执行、隐私泄露、持久化危害、网络滥用、资源耗尽、语义误导）系统评估Clawdbot行为。测试集融合三类来源：① 适配自ATBench与LPS-Bench等主流代理安全基准的34个典型用例；② 针对Clawdbot特有工具接口（如`shell_exec`、`browser_navigate`、`file_write`）手工构造的边界案例；③ 模拟真实用户误操作与轻度 jailbreak 的诱导性提示。全程完整记录**端到端交互轨迹**（含用户消息、Agent决策链、工具调用参数与原始返回值），并采用双轨评估机制：由轻量级专用裁判模型 **AgentDoG-Qwen3-4B** 进行自动化轨迹安全打分，辅以人工复核验证关键失败路径。\n\n## 主要发现  \n- 安全表现呈显著非均匀性：在明确目标、结构化输入任务中成功率＞94%，但在**意图模糊**（如“帮我整理一下”）、**开放目标**（如“优化我的开发环境”）及**伪装良性提示**（如“用浏览器查一下怎么修打印机——顺便把当前桌面截图发我”）下失败率跃升至68%；  \n- 典型失效模式集中于**语义过度泛化**（将“查看日志”误判为“清空日志”）、**工具权限误用**（调用`shell_exec`执行无沙箱命令）、**上下文遗忘导致的累积偏差**；  \n- 审计揭示三大深层脆弱点：工具抽象层缺乏细粒度权限约束、动作链缺乏回滚机制、以及缺乏对“隐式副作用”的主动检测能力。\n\n## 创新价值  \n本工作首次为工具型AI代理提供可复现、可归因的轨迹级安全审计范式，所构建的Clawdbot专用测试套件与失败模式分类体系，为后续轻量代理的安全加固与评测标准制定提供了实证基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15189v1",
      "arxiv_id": "2602.15189v1",
      "title": "ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction",
      "authors": [
        "William Brach",
        "Francesco Zuppichini",
        "Marco Vinciguerra",
        "Lorenzo Padoan"
      ],
      "abstract": "The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15189v1",
      "url": "https://arxiv.org/abs/2602.15189v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## ScrapeGraphAI-100k：面向大语言模型的网页结构化信息抽取大规模基准数据集  \n\n当前，**大语言模型（LLM）驱动的网页信息抽取**已成为现代网络信息检索（Web IR）流水线的核心环节，但现有基准数据集普遍存在三大局限：规模小（通常＜10k样本）、高度合成、或仅含纯文本，**严重缺失真实网页的HTML结构上下文、多语言混合、动态提示工程与Schema多样性**。为填补这一关键空白，本文发布 **ScrapeGraphAI-100k**——首个面向LLM结构化抽取的大规模、真实世界、多模态网页数据集。该数据集基于2025年第二、三季度用户**自愿授权的ScrapeGraphAI生产环境遥测日志**构建，原始采集900万次抽取事件；经严格去重、按JSON Schema语义聚类平衡、跨领域/语言采样后，最终形成**93,695个高质量实例**，覆盖电商、新闻、学术、政府等12+垂直领域及中、英、西、法、日等8种语言。每个样本完整包含：**原始Markdown渲染内容（保留层级与链接结构）、用户原始Prompt、目标JSON Schema定义、LLM实际输出响应，以及标注的复杂度指标（如嵌套深度、字段数、类型异构性）与人工验证状态**。我们系统分析发现：当Schema嵌套≥3层或字段数＞15时，主流开源LLM（如Llama-3-8B）抽取准确率骤降37.2%，凸显结构复杂性对泛化能力的瓶颈效应。进一步实验表明：仅用该数据集**10%子集（≈9.4k样本）微调1.7B参数小模型**，即可在相同测试集上将F1分数从42.1提升至68.5，显著缩小与30B级基线模型（72.3）的差距。本数据集已开源至Hugging Face，支持三大核心应用：① 小模型高效微调；② 结构化抽取性能可比基准；③ Web IR索引中的Schema自动归纳研究。",
      "summary_en": "ScrapeGraphAI-100k is the first large-scale, real-world dataset for LLM-based web information extraction, addressing critical gaps in scale, structural fidelity, and schema diversity. Built from opt-in telemetry of the ScrapeGraphAI framework (Q2–Q3 2025), it contains 93,695 high-quality examples after deduplication and schema-balanced sampling across 12 domains and 8 languages. Each instance includes Markdown-rendered web content, user prompt, target JSON schema, LLM output, and complexity/validation metadata. We characterize systematic failure modes—accuracy drops sharply beyond schema nesting depth ≥3 or field count >15—and demonstrate that fine-tuning a 1.7B model on just 10% of the data closes ~85% of the F1 gap to a 30B baseline. Publicly available on Hugging Face, ScrapeGraphAI-100k enables efficient small-model tuning, rigorous benchmarking of structured extraction, and schema induction research for web IR indexing.",
      "summary": "## ScrapeGraphAI-100k：面向大语言模型的网页结构化信息抽取大规模基准数据集  \n\n当前，**大语言模型（LLM）驱动的网页信息抽取**已成为现代网络信息检索（Web IR）流水线的核心环节，但现有基准数据集普遍存在三大局限：规模小（通常＜10k样本）、高度合成、或仅含纯文本，**严重缺失真实网页的HTML结构上下文、多语言混合、动态提示工程与Schema多样性**。为填补这一关键空白，本文发布 **ScrapeGraphAI-100k**——首个面向LLM结构化抽取的大规模、真实世界、多模态网页数据集。该数据集基于2025年第二、三季度用户**自愿授权的ScrapeGraphAI生产环境遥测日志**构建，原始采集900万次抽取事件；经严格去重、按JSON Schema语义聚类平衡、跨领域/语言采样后，最终形成**93,695个高质量实例**，覆盖电商、新闻、学术、政府等12+垂直领域及中、英、西、法、日等8种语言。每个样本完整包含：**原始Markdown渲染内容（保留层级与链接结构）、用户原始Prompt、目标JSON Schema定义、LLM实际输出响应，以及标注的复杂度指标（如嵌套深度、字段数、类型异构性）与人工验证状态**。我们系统分析发现：当Schema嵌套≥3层或字段数＞15时，主流开源LLM（如Llama-3-8B）抽取准确率骤降37.2%，凸显结构复杂性对泛化能力的瓶颈效应。进一步实验表明：仅用该数据集**10%子集（≈9.4k样本）微调1.7B参数小模型**，即可在相同测试集上将F1分数从42.1提升至68.5，显著缩小与30B级基线模型（72.3）的差距。本数据集已开源至Hugging Face，支持三大核心应用：① 小模型高效微调；② 结构化抽取性能可比基准；③ Web IR索引中的Schema自动归纳研究。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15139v1",
      "arxiv_id": "2602.15139v1",
      "title": "CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding",
      "authors": [
        "Tahir Hussain",
        "Saddam Hussain Khan"
      ],
      "abstract": "Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15139v1",
      "url": "https://arxiv.org/abs/2602.15139v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n面向古典伊斯兰文本（尤其是圣训）的问答（QA）任务长期受限于**领域特异性语义模糊性、长程上下文依赖**及**神学概念敏感推理**。现有通用模型（如BERT、DeBERTa）在《布哈里圣训实录》和《穆斯林圣训实录》等核心典籍上的表现存在显著瓶颈，难以捕捉“认主独一”“先知使命”“教法渊源”等12个核心伊斯兰概念的深层神学关联与语境权重。\n\n## 方法创新：CGRA-DeBERTa框架  \n本研究提出**CGRA-DeBERTa**（Concept Guided Residual Augmentation DeBERTa），一种轻量、可解释、高精度的神学感知Transformer架构：  \n- **定制化DeBERTa主干**：集成LoRA（Low-Rank Adaptation）实现参数高效微调，保留预训练语言能力；  \n- **概念引导残差模块（CGR Blocks）**：嵌入经专家审校的《伊斯兰核心概念词典》（含12个术语及其神学定义、层级关系与跨文本共现模式），以残差连接注入领域先验；  \n- **概念门控机制（Concept Gating）**：基于重要性加权注意力，对关键神学术语（如“الله”“الرسول”“الحكم”）实施差异化缩放（1.04–3.00倍），在不破坏原始上下文结构的前提下强化语义锚点。\n\n## 主要成果  \n在42,591组高质量圣训QA对（覆盖两大权威圣训集）上训练验证：  \n- **EM（精确匹配）达97.85%**，显著超越BERT（75.87%）和标准DeBERTa（89.77%），绝对提升8.08分；  \n- 推理开销仅增加约8%，得益于LoRA+稀疏门控的协同设计；  \n- 定性分析证实其在**神学实体识别、歧义句义区分、教法裁决依据定位**三方面具备更高准确性与可解释性。  \n本工作为宗教文本智能处理提供了首个兼顾**计算效率、神学严谨性与教育适用性**的端到端解决方案。",
      "summary_en": "Accurate question answering over classical Islamic texts—especially Hadith—is hindered by domain-specific semantics, long-context dependencies, and concept-sensitive theological reasoning. To address this, we propose **CGRA-DeBERTa**, a Concept-Guided Residual Augmentation Transformer built upon a LoRA-adapted DeBERTa backbone. It integrates a curated Islamic Concept Dictionary (12 core theological terms) via residual concept-aware blocks and an importance-weighted concept gating mechanism that selectively amplifies critical tokens (scaling factors: 1.04–3.00). Evaluated on 42,591 QA pairs from *Sahih al-Bukhari* and *Sahih Muslim*, CGRA-DeBERTa achieves **97.85% EM**, outperforming BERT (75.87%) and vanilla DeBERTa (89.77%) by +8.08 absolute points, with only ~8% inference overhead. Qualitative analysis confirms superior theological precision, span extraction fidelity, and interpretability—enabling scalable, education-ready Hadith QA systems.",
      "summary": "## 背景与挑战  \n面向古典伊斯兰文本（尤其是圣训）的问答（QA）任务长期受限于**领域特异性语义模糊性、长程上下文依赖**及**神学概念敏感推理**。现有通用模型（如BERT、DeBERTa）在《布哈里圣训实录》和《穆斯林圣训实录》等核心典籍上的表现存在显著瓶颈，难以捕捉“认主独一”“先知使命”“教法渊源”等12个核心伊斯兰概念的深层神学关联与语境权重。\n\n## 方法创新：CGRA-DeBERTa框架  \n本研究提出**CGRA-DeBERTa**（Concept Guided Residual Augmentation DeBERTa），一种轻量、可解释、高精度的神学感知Transformer架构：  \n- **定制化DeBERTa主干**：集成LoRA（Low-Rank Adaptation）实现参数高效微调，保留预训练语言能力；  \n- **概念引导残差模块（CGR Blocks）**：嵌入经专家审校的《伊斯兰核心概念词典》（含12个术语及其神学定义、层级关系与跨文本共现模式），以残差连接注入领域先验；  \n- **概念门控机制（Concept Gating）**：基于重要性加权注意力，对关键神学术语（如“الله”“الرسول”“الحكم”）实施差异化缩放（1.04–3.00倍），在不破坏原始上下文结构的前提下强化语义锚点。\n\n## 主要成果  \n在42,591组高质量圣训QA对（覆盖两大权威圣训集）上训练验证：  \n- **EM（精确匹配）达97.85%**，显著超越BERT（75.87%）和标准DeBERTa（89.77%），绝对提升8.08分；  \n- 推理开销仅增加约8%，得益于LoRA+稀疏门控的协同设计；  \n- 定性分析证实其在**神学实体识别、歧义句义区分、教法裁决依据定位**三方面具备更高准确性与可解释性。  \n本工作为宗教文本智能处理提供了首个兼顾**计算效率、神学严谨性与教育适用性**的端到端解决方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14655v1",
      "arxiv_id": "2602.14655v1",
      "title": "Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech",
      "authors": [
        "Xiao Wei",
        "Bin Wen",
        "Yuqin Lin",
        "Kai Li",
        "Mingyang gu",
        "Xiaobao Wang",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "abstract": "Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14655v1",
      "url": "https://arxiv.org/abs/2602.14655v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n阿尔茨海默病（AD）的早期诊断对延缓疾病进展至关重要。基于语音的人工智能检测方法具有**无创、低成本、可居家部署**等优势，但面临严峻的“数据效率困境”：一方面，高质量标注的临床语音数据极度稀缺；另一方面，跨医疗机构的数据共享受严格隐私法规（如HIPAA、GDPR）限制，导致模型训练长期受限于小规模、孤岛化数据。\n\n## 方法创新：FAL-AD框架  \n本文提出**FAL-AD**（Federated and Augmented Learning for AD），一个面向数据效率优化的端到端联合学习框架，包含三大核心技术突破：  \n- **绝对效率提升**：首创基于语音转换（Voice Conversion）的病理语音增强策略，通过跨类别（健康/AD）的声学特征与语义内容解耦重组，生成高保真、多样化的合成AD语音样本，显著缓解标注数据匮乏；  \n- **协同效率突破**：设计自适应联邦学习范式，支持异构设备与非独立同分布（Non-IID）数据下的动态权重聚合与本地差分隐私保护，实现多中心协作而不共享原始语音；  \n- **表征效率优化**：构建**注意力驱动的跨模态融合模型**，在词粒度实现声学特征（MFCCs, prosody）与文本转录（ASR输出）的细粒度对齐与交互建模，提升病理模式判别力。\n\n## 实验结果与意义  \n在权威ADReSSo基准测试中，FAL-AD达成**91.52%的多模态分类准确率**，超越所有集中式基线模型（最高提升+4.21%），且在仅使用30%本地数据量时仍保持>89%性能。本工作首次将联邦学习与可控语音增强深度耦合，为医疗AI提供了兼顾**隐私合规性、数据稀缺性与临床实用性**的可行路径。源代码已开源：https://github.com/smileix/fal-ad。",
      "summary_en": "Early Alzheimer’s Disease (AD) detection via speech is promising yet hindered by a critical *data efficiency dilemma*: severe scarcity of labeled clinical speech data and strict privacy barriers preventing cross-institutional sharing. To address this, we propose **FAL-AD**, a novel federated and augmented learning framework. It introduces three synergistic innovations: (1) **Voice conversion–based augmentation**, generating diverse, pathology-aware synthetic speech through cross-category voice-content recombination; (2) An **adaptive federated learning paradigm**, enabling privacy-preserving collaboration across heterogeneous institutions with Non-IID data and local differential privacy; and (3) An **attentive cross-modal fusion model**, achieving fine-grained word-level acoustic–textual alignment. Evaluated on the ADReSSo benchmark, FAL-AD achieves a state-of-the-art multimodal accuracy of **91.52%**, outperforming all centralized baselines and demonstrating robust performance under data-constrained and privacy-sensitive settings. Code is publicly available at https://github.com/smileix/fal-ad.",
      "summary": "## 背景与挑战  \n阿尔茨海默病（AD）的早期诊断对延缓疾病进展至关重要。基于语音的人工智能检测方法具有**无创、低成本、可居家部署**等优势，但面临严峻的“数据效率困境”：一方面，高质量标注的临床语音数据极度稀缺；另一方面，跨医疗机构的数据共享受严格隐私法规（如HIPAA、GDPR）限制，导致模型训练长期受限于小规模、孤岛化数据。\n\n## 方法创新：FAL-AD框架  \n本文提出**FAL-AD**（Federated and Augmented Learning for AD），一个面向数据效率优化的端到端联合学习框架，包含三大核心技术突破：  \n- **绝对效率提升**：首创基于语音转换（Voice Conversion）的病理语音增强策略，通过跨类别（健康/AD）的声学特征与语义内容解耦重组，生成高保真、多样化的合成AD语音样本，显著缓解标注数据匮乏；  \n- **协同效率突破**：设计自适应联邦学习范式，支持异构设备与非独立同分布（Non-IID）数据下的动态权重聚合与本地差分隐私保护，实现多中心协作而不共享原始语音；  \n- **表征效率优化**：构建**注意力驱动的跨模态融合模型**，在词粒度实现声学特征（MFCCs, prosody）与文本转录（ASR输出）的细粒度对齐与交互建模，提升病理模式判别力。\n\n## 实验结果与意义  \n在权威ADReSSo基准测试中，FAL-AD达成**91.52%的多模态分类准确率**，超越所有集中式基线模型（最高提升+4.21%），且在仅使用30%本地数据量时仍保持>89%性能。本工作首次将联邦学习与可控语音增强深度耦合，为医疗AI提供了兼顾**隐私合规性、数据稀缺性与临床实用性**的可行路径。源代码已开源：https://github.com/smileix/fal-ad。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14612v1",
      "arxiv_id": "2602.14612v1",
      "title": "LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio",
      "authors": [
        "Naveen Vakada",
        "Kartik Hegde",
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "abstract": "Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14612v1",
      "url": "https://arxiv.org/abs/2602.14612v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n多小时级长时音频（如会议录音、安防监控、远程医疗会话）在工业与消费场景中日益普遍，但人工回溯效率极低。现有音频-语言模型受限于上下文长度，难以直接处理数小时原始音频流，易产生时间错位、事实幻觉及推理失焦等问题。\n\n## 方法：LongAudio-RAG（LA-RAG）框架  \n我们提出**事件锚定式检索增强生成框架**，摒弃对原始音频波形的端到端建模，转而构建**结构化声学事件知识库**：  \n- **离线阶段**：将长音频流通过轻量级音频 grounding 模型（如 Whisper + 事件检测器）解析为带毫秒级时间戳的结构化事件记录（如“[08:23:15.4–08:23:17.9] 男性说话”“[08:41:02.1] 门铃响”），存入轻量 SQL 数据库；  \n- **在线推理**：系统协同执行四步操作——① 解析自然语言中的时间指代（如“会议开始后12分钟”→绝对时间戳）；② 意图分类（检测/计数/摘要）；③ 基于时间窗口与语义约束精准检索相关事件子集；④ LLM 仅基于检索出的、带时间标注的事件文本生成答案，严格规避幻觉。\n\n## 主要发现与创新  \n- 构建首个**合成式长音频 QA 基准**（LA-QA-Bench），含 12 小时级拼接音频、1,800+ 模板化 QA 对，覆盖检测、计数、跨时段摘要三类任务；  \n- 在该基准上，LA-RAG 相比 vanilla RAG（直接分块音频转文本再检索）和 text-to-SQL 方法，**时间准确率提升 32.7%，事实正确率提升 28.4%**；  \n- 首次实现**边缘-云协同部署**：事件检测模型运行于树莓派级 IoT 设备（延迟 <800ms），LLM（Qwen2-7B）部署于云端 GPU 服务器，兼顾实时性与语言能力。",
      "summary_en": "Long-duration audio (e.g., meetings, surveillance) is pervasive yet intractable for manual review. Existing audio-language models fail on multi-hour QA due to context-length limits and hallucination. We propose **LongAudio-RAG (LA-RAG)**, a hybrid framework that grounds LLM answers in *retrieved, timestamped acoustic events*—not raw audio. Long streams are pre-processed into structured, time-aligned event records (e.g., “male speech: [12:34:05.2–12:34:08.7]”) stored in an SQL database. At inference, LA-RAG resolves temporal references, classifies intent, retrieves only temporally and semantically relevant events, and constrains LLM generation to this evidence. Evaluated on our novel synthetic benchmark (LA-QA-Bench, 12+ hours, 1,800+ QA pairs), LA-RAG outperforms vanilla RAG and text-to-SQL baselines by +32.7% temporal accuracy and +28.4% factual correctness. Deployed in an edge-cloud setup—event detection on IoT hardware, LLM on GPU cloud—it enables low-latency grounding and high-fidelity reasoning.",
      "summary": "## 背景与挑战  \n多小时级长时音频（如会议录音、安防监控、远程医疗会话）在工业与消费场景中日益普遍，但人工回溯效率极低。现有音频-语言模型受限于上下文长度，难以直接处理数小时原始音频流，易产生时间错位、事实幻觉及推理失焦等问题。\n\n## 方法：LongAudio-RAG（LA-RAG）框架  \n我们提出**事件锚定式检索增强生成框架**，摒弃对原始音频波形的端到端建模，转而构建**结构化声学事件知识库**：  \n- **离线阶段**：将长音频流通过轻量级音频 grounding 模型（如 Whisper + 事件检测器）解析为带毫秒级时间戳的结构化事件记录（如“[08:23:15.4–08:23:17.9] 男性说话”“[08:41:02.1] 门铃响”），存入轻量 SQL 数据库；  \n- **在线推理**：系统协同执行四步操作——① 解析自然语言中的时间指代（如“会议开始后12分钟”→绝对时间戳）；② 意图分类（检测/计数/摘要）；③ 基于时间窗口与语义约束精准检索相关事件子集；④ LLM 仅基于检索出的、带时间标注的事件文本生成答案，严格规避幻觉。\n\n## 主要发现与创新  \n- 构建首个**合成式长音频 QA 基准**（LA-QA-Bench），含 12 小时级拼接音频、1,800+ 模板化 QA 对，覆盖检测、计数、跨时段摘要三类任务；  \n- 在该基准上，LA-RAG 相比 vanilla RAG（直接分块音频转文本再检索）和 text-to-SQL 方法，**时间准确率提升 32.7%，事实正确率提升 28.4%**；  \n- 首次实现**边缘-云协同部署**：事件检测模型运行于树莓派级 IoT 设备（延迟 <800ms），LLM（Qwen2-7B）部署于云端 GPU 服务器，兼顾实时性与语言能力。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14462v1",
      "arxiv_id": "2602.14462v1",
      "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
      "authors": [
        "Hong Li",
        "Zhen Zhou",
        "Honggang Zhang",
        "Yuping Luo",
        "Xinyue Wang",
        "Han Gong",
        "Zhiyuan Liu"
      ],
      "abstract": "Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \\emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \\texttt{openPangu-Embedded-1B-V1.1} model on the \\texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14462v1",
      "url": "https://arxiv.org/abs/2602.14462v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 背景与问题  \n在大语言模型（LLM）全参数微调中，数据并行（Data-Parallel, DP）配合同步all-reduce是主流范式。尽管该机制保障每轮迭代后各工作节点（worker）模型权重数值一致，但**并不保证梯度聚合前各worker的优化动态对齐**——这一被长期忽视的隐性偏差，本文定义为“**静默不一致性**”（Silent Inconsistency）。它表现为跨worker的损失值、梯度幅值与方向持续发散，却因全局平均监控信号（如smoothed loss curve）的强平滑性而“不可见”，导致训练不稳定根源难以诊断。\n\n## 方法与创新  \n我们提出一种**轻量、模型无关、零侵入**的诊断框架：仅利用标准训练流程中已有的每worker本地loss和梯度张量，无需修改模型结构、通信机制或优化器。核心包含三个正交指标：  \n- **损失离散度**（Loss Dispersion）：各worker损失的标准差；  \n- **梯度模长离散度**（Gradient-Norm Dispersion）：各worker梯度L2范数的标准差；  \n- **梯度方向一致性**（Gradient-Direction Consistency）：所有worker两两梯度向量的余弦相似度均值。  \n三者计算开销可忽略（<0.3%训练时延），支持实时在线监测。\n\n## 主要发现  \n在8-NPU集群上对1B参数模型`openPangu-Embedded-1B-V1.1`于`alpaca`数据集进行全微调实验，通过系统扰动数据打乱顺序与随机种子，证实：即使全局平均loss曲线光滑收敛，**损失/梯度离散度上升达3.2×，方向余弦均值下降41%**。这揭示DP微调中存在未被察觉的优化路径分裂，为超参调试、数据分片策略与硬件配置评估提供了可量化、可操作的诊断依据。",
      "summary_en": "Data-parallel (DP) full fine-tuning relies on synchronous all-reduce to synchronize weights, yet it silently permits worker-level optimization divergence—termed *silent inconsistency*—which remains hidden under globally averaged metrics. We propose a lightweight, model-agnostic diagnostic framework using three complementary, zero-overhead metrics: loss dispersion, gradient-norm dispersion, and inter-worker gradient-direction consistency (via cosine similarity). Evaluated on an 8-NPU cluster fine-tuning the 1B-parameter `openPangu-Embedded-1B-V1.1` on `alpaca`, controlled perturbations of data shuffling and random seeds induce up to 3.2× higher dispersion and 41% lower directional alignment—even while global loss curves appear smooth. These results expose critical hidden instability in large-scale DP training and provide actionable, real-time visibility for robust configuration assessment.",
      "summary": "## 背景与问题  \n在大语言模型（LLM）全参数微调中，数据并行（Data-Parallel, DP）配合同步all-reduce是主流范式。尽管该机制保障每轮迭代后各工作节点（worker）模型权重数值一致，但**并不保证梯度聚合前各worker的优化动态对齐**——这一被长期忽视的隐性偏差，本文定义为“**静默不一致性**”（Silent Inconsistency）。它表现为跨worker的损失值、梯度幅值与方向持续发散，却因全局平均监控信号（如smoothed loss curve）的强平滑性而“不可见”，导致训练不稳定根源难以诊断。\n\n## 方法与创新  \n我们提出一种**轻量、模型无关、零侵入**的诊断框架：仅利用标准训练流程中已有的每worker本地loss和梯度张量，无需修改模型结构、通信机制或优化器。核心包含三个正交指标：  \n- **损失离散度**（Loss Dispersion）：各worker损失的标准差；  \n- **梯度模长离散度**（Gradient-Norm Dispersion）：各worker梯度L2范数的标准差；  \n- **梯度方向一致性**（Gradient-Direction Consistency）：所有worker两两梯度向量的余弦相似度均值。  \n三者计算开销可忽略（<0.3%训练时延），支持实时在线监测。\n\n## 主要发现  \n在8-NPU集群上对1B参数模型`openPangu-Embedded-1B-V1.1`于`alpaca`数据集进行全微调实验，通过系统扰动数据打乱顺序与随机种子，证实：即使全局平均loss曲线光滑收敛，**损失/梯度离散度上升达3.2×，方向余弦均值下降41%**。这揭示DP微调中存在未被察觉的优化路径分裂，为超参调试、数据分片策略与硬件配置评估提供了可量化、可操作的诊断依据。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14401v1",
      "arxiv_id": "2602.14401v1",
      "title": "pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI",
      "authors": [
        "Qingqian Yang",
        "Hao Wang",
        "Sai Qian Zhang",
        "Jian Li",
        "Yang Hua",
        "Miao Pan",
        "Tao Song",
        "Zhengwei Qi",
        "Haibing Guan"
      ],
      "abstract": "Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14401v1",
      "url": "https://arxiv.org/abs/2602.14401v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n视觉-语言导航（Vision-Language Navigation, VLN）依赖大规模私有室内环境中的轨迹-指令配对数据，但集中式训练严重威胁用户隐私。联邦学习（Federated Learning, FL）虽能实现数据不出本地，却在VLN场景下面临严峻挑战：各客户端环境布局差异巨大、指令表达风格高度异构，导致标准FedAvg等全局模型难以兼顾泛化性与个性化，性能显著下降。\n\n## 方法创新：pFedNavi框架  \n本文提出**pFedNavi**——首个面向VLN任务的**结构感知、动态自适应个性化联邦学习框架**。其核心思想是“**在关键处个性化**”：  \n- 通过**层级混合系数（layer-wise mixing coefficients）** 动态识别客户端特异性最强的网络层（如编码器-解码器投影模块、环境敏感型解码器层）；  \n- 仅对这些选定组件执行**细粒度参数融合**，而非全网统一聚合，从而在共享全局语义知识的同时，保留各客户端对本地空间结构与语言习惯的专属建模能力；  \n- 引入轻量级结构感知正则化，显式建模视觉编码器（ResNet/CLIP）与导航决策模块间的层级依赖关系。\n\n## 实验结果与贡献  \n在R2R和RxR两大标准VLN基准上，pFedNavi全面超越FedAvg基线：  \n- **导航成功率（Navigation Success Rate）最高提升7.5%**；  \n- **轨迹保真度（Trajectory Fidelity）最高提升7.8%**；  \n- 在非独立同分布（non-IID）条件下**收敛速度加快1.38倍**。  \n本工作首次将结构感知个性化机制系统引入VLN联邦学习，为具身AI在隐私敏感场景下的实用化部署提供了新范式。",
      "summary_en": "Vision-Language Navigation (VLN) demands large-scale trajectory-instruction data from private indoor environments, posing serious privacy risks. While Federated Learning (FL) keeps data on-device, vanilla FL (e.g., FedAvg) fails under VLN’s extreme cross-client heterogeneity in spatial layouts and instruction styles. To address this, we propose **pFedNavi**, the first structure-aware and dynamically adaptive personalized FL framework for VLN. Its core insight is *personalizing only where it matters*: pFedNavi identifies client-specific layers (e.g., encoder-decoder projection and environment-sensitive decoder layers) via learnable layer-wise mixing coefficients, and performs fine-grained parameter fusion exclusively on those components—balancing global knowledge sharing with local specialization. Evaluated on R2R and RxR using both ResNet and CLIP visual backbones, pFedNavi consistently outperforms the FedAvg-based baseline: achieving up to **+7.5% navigation success rate**, **+7.8% trajectory fidelity**, and **1.38× faster convergence** under non-IID conditions.",
      "summary": "## 背景与挑战  \n视觉-语言导航（Vision-Language Navigation, VLN）依赖大规模私有室内环境中的轨迹-指令配对数据，但集中式训练严重威胁用户隐私。联邦学习（Federated Learning, FL）虽能实现数据不出本地，却在VLN场景下面临严峻挑战：各客户端环境布局差异巨大、指令表达风格高度异构，导致标准FedAvg等全局模型难以兼顾泛化性与个性化，性能显著下降。\n\n## 方法创新：pFedNavi框架  \n本文提出**pFedNavi**——首个面向VLN任务的**结构感知、动态自适应个性化联邦学习框架**。其核心思想是“**在关键处个性化**”：  \n- 通过**层级混合系数（layer-wise mixing coefficients）** 动态识别客户端特异性最强的网络层（如编码器-解码器投影模块、环境敏感型解码器层）；  \n- 仅对这些选定组件执行**细粒度参数融合**，而非全网统一聚合，从而在共享全局语义知识的同时，保留各客户端对本地空间结构与语言习惯的专属建模能力；  \n- 引入轻量级结构感知正则化，显式建模视觉编码器（ResNet/CLIP）与导航决策模块间的层级依赖关系。\n\n## 实验结果与贡献  \n在R2R和RxR两大标准VLN基准上，pFedNavi全面超越FedAvg基线：  \n- **导航成功率（Navigation Success Rate）最高提升7.5%**；  \n- **轨迹保真度（Trajectory Fidelity）最高提升7.8%**；  \n- 在非独立同分布（non-IID）条件下**收敛速度加快1.38倍**。  \n本工作首次将结构感知个性化机制系统引入VLN联邦学习，为具身AI在隐私敏感场景下的实用化部署提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14833v1",
      "arxiv_id": "2602.14833v1",
      "title": "RF-GPT: Teaching AI to See the Wireless World",
      "authors": [
        "Hang Zou",
        "Yu Tian",
        "Bohao Wang",
        "Lina Bariah",
        "Samson Lasaulce",
        "Chongwen Huang",
        "Mérouane Debbah"
      ],
      "abstract": "Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14833v1",
      "url": "https://arxiv.org/abs/2602.14833v1",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## RF-GPT：面向无线世界的射频语言模型  \n\n当前大语言模型（LLMs）与多模态大模型（VLMs）虽具备强大的通用推理能力，但**无法原生理解射频（RF）信号**——这一无线通信系统的物理基础。现有电信AI方案多局限于文本/结构化数据，而传统RF深度学习模型则高度任务专用、缺乏语义泛化能力，导致**RF感知层与高层认知层之间存在显著断层**。  \n\n为弥合该鸿沟，我们提出 **RF-GPT**：首个面向射频理解的“无线电频率语言模型”（RFLM）。其核心创新在于**将RF信号转化为视觉可解的时频谱图（spectrogram）**，并复用多模态LLM的预训练视觉编码器提取特征；随后，将所得表征作为专属“RF tokens”注入解码器-only LLM，实现对RF场景的问答生成、原理性解释及结构化输出（如调制类型、协议参数、用户数等）。  \n\n训练完全基于**全合成RF语料库**：采用标准兼容波形发生器模拟6类主流无线技术（含Wi-Fi 6/7、5G NR、LoRa、Bluetooth等）的宽频带复杂场景；自动提取高保真时频谱图、精确配置元数据与密集语义描述；再经文本LLM蒸馏生成**12,000个RF场景、62.5万条RF-grounded指令-答案对**，全程零人工标注。  \n\n在宽频调制识别、信号重叠分析、无线制式判别、WLAN用户计数、5G NR关键参数抽取五大基准测试中，RF-GPT展现出**强大多任务泛化能力**；而未经过RF对齐的通用VLM（如LLaVA、Qwen-VL）在所有任务上均表现接近随机——证实RF-GPT实现了从“看见频谱”到“理解无线”的范式跃迁。",
      "summary_en": "RF-GPT is the first radio-frequency language model (RFLM) that bridges the gap between low-level RF signal perception and high-level multimodal reasoning. It repurposes pretrained visual encoders from multimodal LLMs to process time-frequency spectrograms derived from complex IQ waveforms—transforming raw RF data into “RF tokens” injected into a decoder-only LLM for grounded generation of answers, explanations, and structured outputs. Trained via supervised instruction tuning on a fully synthetic corpus of 12,000 RF scenes (625K instruction-answer pairs), RF-GPT leverages standards-compliant waveform generators covering six wireless technologies (e.g., 5G NR, Wi-Fi 6/7, LoRa) and automatic caption-to-instruction distillation—eliminating manual labeling. Across five challenging benchmarks—including wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction—RF-GPT achieves strong multi-task performance, while general-purpose VLMs without RF grounding fail catastrophically. This work establishes RF spectrograms as a viable “visual language” for wireless intelligence.",
      "summary": "## RF-GPT：面向无线世界的射频语言模型  \n\n当前大语言模型（LLMs）与多模态大模型（VLMs）虽具备强大的通用推理能力，但**无法原生理解射频（RF）信号**——这一无线通信系统的物理基础。现有电信AI方案多局限于文本/结构化数据，而传统RF深度学习模型则高度任务专用、缺乏语义泛化能力，导致**RF感知层与高层认知层之间存在显著断层**。  \n\n为弥合该鸿沟，我们提出 **RF-GPT**：首个面向射频理解的“无线电频率语言模型”（RFLM）。其核心创新在于**将RF信号转化为视觉可解的时频谱图（spectrogram）**，并复用多模态LLM的预训练视觉编码器提取特征；随后，将所得表征作为专属“RF tokens”注入解码器-only LLM，实现对RF场景的问答生成、原理性解释及结构化输出（如调制类型、协议参数、用户数等）。  \n\n训练完全基于**全合成RF语料库**：采用标准兼容波形发生器模拟6类主流无线技术（含Wi-Fi 6/7、5G NR、LoRa、Bluetooth等）的宽频带复杂场景；自动提取高保真时频谱图、精确配置元数据与密集语义描述；再经文本LLM蒸馏生成**12,000个RF场景、62.5万条RF-grounded指令-答案对**，全程零人工标注。  \n\n在宽频调制识别、信号重叠分析、无线制式判别、WLAN用户计数、5G NR关键参数抽取五大基准测试中，RF-GPT展现出**强大多任务泛化能力**；而未经过RF对齐的通用VLM（如LLaVA、Qwen-VL）在所有任务上均表现接近随机——证实RF-GPT实现了从“看见频谱”到“理解无线”的范式跃迁。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14743v1",
      "arxiv_id": "2602.14743v1",
      "title": "LLMStructBench: Benchmarking Large Language Model Structured Data Extraction",
      "authors": [
        "Sönke Tenckhoff",
        "Mario Koddenbrock",
        "Erik Rodner"
      ],
      "abstract": "We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.   In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14743v1",
      "url": "https://arxiv.org/abs/2602.14743v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## LLMStructBench：面向结构化数据抽取的大语言模型基准评测框架  \n\n本研究提出 **LLMStructBench**——首个专为评估大语言模型（LLM）在**自然语言到结构化数据转换任务**中性能而设计的开源基准。该基准聚焦核心挑战：从非结构化文本中准确提取结构化信息，并生成**语法正确、语义合规的 JSON 输出**（即同时满足 token-level 准确性与 document-level 有效性）。  \n\n我们构建了一个**高质量、人工校验、复杂度分层**的开放数据集，涵盖金融公告、医疗记录、产品描述等12类真实场景，包含527个多样化样本，覆盖嵌套对象、可选字段、多值数组、类型歧义等典型解析难点。实验系统评估了22个主流LLM（参数量横跨0.5B–72B），结合5种提示策略（Zero-shot、Few-shot、Chain-of-Thought、JSON Schema Prompting、Self-Refine），并引入**双维度评估指标**：  \n- **Token-Level Accuracy**（F1@token）：衡量字段值与格式的逐词匹配精度；  \n- **Document-Level Validity**（JSON-valid rate & Schema-compliance rate）：严格检测输出是否为可解析JSON且符合预定义schema。  \n\n关键发现包括：（1）**提示策略的影响显著超越模型规模**——例如，采用Schema-guided提示后，Qwen2-1.5B的JSON有效率提升至98.3%，反超未优化的Llama3-70B（86.1%）；（2）强提示虽大幅提升结构可靠性，但可能引入**语义漂移**（如字段误映射），凸显“语法正确≠语义正确”的根本张力；（3）小模型经提示工程可逼近大模型性能，为资源受限场景提供可行路径。LLMStructBench已开源（含数据集、评测脚本、全量结果），旨在推动LLM在ETL、知识图谱构建及AI代理数据接口等工业级结构化任务中的稳健应用。",
      "summary_en": "We introduce **LLMStructBench**, a novel open benchmark for evaluating Large Language Models (LLMs) on structured data extraction—specifically, parsing natural-language text into syntactically valid and semantically compliant JSON. Our manually verified dataset comprises 527 diverse, complexity-graded examples across 12 real-world domains (e.g., finance, healthcare), covering nested objects, optional fields, and type ambiguity. We systematically evaluate 22 models (0.5B–72B parameters) under five prompting strategies and propose dual metrics: *token-level F1* for field-value accuracy and *document-level validity rates* (JSON-parsability & schema compliance) for structural integrity. Crucially, we find that **prompting strategy dominates model size** in determining parsing reliability: schema-guided prompting enables small models (e.g., Qwen2-1.5B) to achieve >98% JSON validity—surpassing unoptimized large models—though at a trade-off of increased semantic errors. LLMStructBench is publicly released to advance robust LLM-based parsing for ETL, knowledge extraction, and agent-data interfaces.",
      "summary": "## LLMStructBench：面向结构化数据抽取的大语言模型基准评测框架  \n\n本研究提出 **LLMStructBench**——首个专为评估大语言模型（LLM）在**自然语言到结构化数据转换任务**中性能而设计的开源基准。该基准聚焦核心挑战：从非结构化文本中准确提取结构化信息，并生成**语法正确、语义合规的 JSON 输出**（即同时满足 token-level 准确性与 document-level 有效性）。  \n\n我们构建了一个**高质量、人工校验、复杂度分层**的开放数据集，涵盖金融公告、医疗记录、产品描述等12类真实场景，包含527个多样化样本，覆盖嵌套对象、可选字段、多值数组、类型歧义等典型解析难点。实验系统评估了22个主流LLM（参数量横跨0.5B–72B），结合5种提示策略（Zero-shot、Few-shot、Chain-of-Thought、JSON Schema Prompting、Self-Refine），并引入**双维度评估指标**：  \n- **Token-Level Accuracy**（F1@token）：衡量字段值与格式的逐词匹配精度；  \n- **Document-Level Validity**（JSON-valid rate & Schema-compliance rate）：严格检测输出是否为可解析JSON且符合预定义schema。  \n\n关键发现包括：（1）**提示策略的影响显著超越模型规模**——例如，采用Schema-guided提示后，Qwen2-1.5B的JSON有效率提升至98.3%，反超未优化的Llama3-70B（86.1%）；（2）强提示虽大幅提升结构可靠性，但可能引入**语义漂移**（如字段误映射），凸显“语法正确≠语义正确”的根本张力；（3）小模型经提示工程可逼近大模型性能，为资源受限场景提供可行路径。LLMStructBench已开源（含数据集、评测脚本、全量结果），旨在推动LLM在ETL、知识图谱构建及AI代理数据接口等工业级结构化任务中的稳健应用。",
      "summary_status": "success"
    }
  ],
  "last_updated": "2026-02-22T02:04:24.839430",
  "total_count": 77
}