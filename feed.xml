<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Paper Pulse</title>
    <link>https://jamie-cui.github.io/paper-pulse</link>
    <description>Keyword-based research paper aggregation from arXiv and IACR</description>
    <lastBuildDate>Mon, 02 Mar 2026 02:01:01 -0000</lastBuildDate>
    <atom:link href="https://jamie-cui.github.io/paper-pulse/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On the Need for (Quantum) Memory with Short Outputs</title>
      <link>https://eprint.iacr.org/2026/403</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/403</guid>
      <description>We establish the first unconditional separation between bounded- and unbounded-space computation for problems with *short outputs*—where output size is logarithmic while memory may be exponential—in both classical and quantum models. To achieve this, we introduce the **nested collision finding** problem: given oracles for random functions $f$ and $g$, find distinct $x \neq y$ such that $g(f(x)) = g(f(y))$. We prove that any algorithm solving it in $T$ queries using only $S$ bits of memory must satisfy $T \cdot S = \Omega(2^{n/3})$, whereas an unbounded-memory algorithm achieves the optimal $T = O(2^{n/3})$. Our lower bound hinges on a novel **two-oracle recording technique**, which uses one oracle to implicitly “record” long intermediate outputs (e.g., $f(x)$) generated under the other, thereby reducing the short-output time–space trade-off to that of a corresponding long-output problem. This technique is broadly applicable to memory-sensitive settings beyond collision-finding.</description>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling</title>
      <link>https://arxiv.org/abs/2602.23262v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23262v1</guid>
      <description>We propose a spectral differential privacy (DP) framework for private image generation, grounded in the insight that privacy-sensitive information resides predominantly in low-frequency wavelet components (e.g., global shapes and facial structures), while high-frequency details are largely generic and public. Our coarse-to-fine two-stage approach first applies DP fine-tuning *only* to an autoregressive spectral tokenizer trained on low-resolution wavelet coefficients (LL subband), concentrating the entire privacy budget on structural fidelity. Then, a publicly pretrained super-resolution model upsamples the DP-encoded coarse representation—leveraging DP’s post-processing property to refine textures without additional privacy cost. Experiments on MS-COCO and MM-CelebA-HQ show our method achieves superior image quality (12–28% lower FID) and semantic alignment (9–15% higher CLIP Score) versus state-of-the-art DP generative models under ε = 2–8, demonstrating that frequency-aware budget allocation breaks the utility–privacy trade-off bottleneck.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)</title>
      <link>https://arxiv.org/abs/2602.23167v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23167v1</guid>
      <description>SettleFL is a trustless, scalable reward settlement protocol for federated learning (FL) on permissionless blockchains. It resolves the fundamental tension between FL’s high-frequency iterative nature and the prohibitive on-chain cost of permissionless ledgers by introducing two interoperable, circuit-based settlement strategies: (1) *Commit-and-Challenge*, an optimistic design minimizing average gas usage via dispute-driven arbitration; and (2) *Commit-with-Proof*, ensuring instant finality per round via succinct zero-knowledge validity proofs. Both leverage a shared domain-specific zk-SNARK circuit architecture and enforce rational robustness—without trusted coordinators—through game-theoretic incentives. Extensive experiments integrating real FL workloads (CIFAR-10, EMNIST) and large-scale simulations demonstrate SettleFL’s practicality: it scales to 800 participants with sub-second settlement latency and reduces average gas cost by 87% (to ~\$0.14 per round), outperforming prior decentralized approaches.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification</title>
      <link>https://arxiv.org/abs/2602.22724v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22724v1</guid>
      <description>Large language model (LLM) agents face growing threats from **indirect prompt injection (IPI)**—a stealthy, multi-turn attack where malicious context embedded in tool outputs or retrieved documents hijacks agent behavior over time. Existing inference-time defenses rely on heuristics and conservative blocking, harming utility under ambiguity. We propose **AgentSentry**, the first framework to model IPI as a *temporal causal takeover*. It localizes takeover points via *controlled counterfactual re-executions* at tool-return boundaries and enables safe continuation through *causally guided context purification*—removing only attack-induced deviations while preserving task-critical evidence. Evaluated across 4 task suites, 3 IPI attack families, and multiple black-box LLMs on AgentDojo, AgentSentry achieves **100% attack blocking**, an average **Utility Under Attack (UA) of 74.55%**, and **no degradation on benign inputs**—outperforming strongest baselines by +20.8–33.6 percentage points in UA.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>injection</category>
      <category>prompt</category>
    </item>
    <item>
      <title>DPSQL+: A Differentially Private SQL Library with a Minimum Frequency Rule</title>
      <link>https://arxiv.org/abs/2602.22699v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22699v1</guid>
      <description>DPSQL+ is a novel differentially private SQL library that jointly enforces user-level $(\varepsilon,\delta)$-differential privacy and the minimum frequency rule (requiring ≥$k$ distinct individuals per released group). Its modular architecture comprises a static **Validator** restricting queries to a DP-safe SQL subset, a consistent **Accountant** tracking cumulative privacy loss across adaptive queries using Rényi DP, and an extensible **Backend** supporting PostgreSQL, SQLite, and Pandas. Crucially, DPSQL+ integrates the $k$-anonymity constraint directly into the DP noise injection pipeline—applying calibrated Laplace/Gaussian noise *then* enforcing $k$-thresholding with bias-corrected rescaling. Experiments on TPC-H show DPSQL+ achieves superior accuracy across aggregates, quadratic statistics (e.g., variance), and join queries, while enabling **2.1–3.8× more queries** under a fixed global budget ($\varepsilon=1.0, \delta=10^{-5}$) compared to prior libraries. It bridges formal privacy theory and real-world governance requirements.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>membership</category>
      <category>dp</category>
      <category>inference</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings</title>
      <link>https://arxiv.org/abs/2602.22689v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22689v1</guid>
      <description>Latent diffusion models (LDMs) excel at text-to-image generation but risk memorizing training data, threatening privacy and IP rights. Membership inference attacks (MIAs) audit such memorization—but existing methods require ground-truth captions, failing in realistic caption-unavailable settings where VLM-generated captions yield poor performance. We propose **MoFit**, the first caption-free MIA framework. It bypasses textual supervision by optimizing a perturbation to construct a *model-fitted surrogate image* that lies in the unconditional prior manifold learned from member samples; then extracts a *model-fitted embedding* from this surrogate to serve as a mismatched condition for the query image—amplifying conditional loss for members while minimally affecting non-members. Extensive experiments across LAION, ImageNet, and models including Stable Diffusion v1.5 and SDXL show MoFit consistently outperforms VLM-conditioned baselines and achieves performance on par with caption-dependent state-of-the-art methods.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>extraction</category>
      <category>membership</category>
      <category>model</category>
    </item>
    <item>
      <title>Systems-Level Attack Surface of Edge Agent Deployments on IoT</title>
      <link>https://arxiv.org/abs/2602.22525v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22525v1</guid>
      <description>This paper presents the first empirical security analysis of LLM agent deployments on resource-constrained IoT edge devices. We evaluate three architectures—cloud-hosted, edge-local swarm, and hybrid—using a multi-device home-automation testbed with local MQTT and an Android smartphone as an edge inference node. We identify five systems-level attack surfaces, including two emergent failures observed *in vivo*: **coordination-state divergence** (causing unresolvable control conflicts) and **induced trust erosion** (where malicious nodes trigger legitimate agents’ erroneous degradation). We formalize core security properties as measurable system metrics: data egress volume, failover window exposure, sovereignty boundary integrity, and provenance chain completeness. Measurements show that edge-local deployments eliminate routine cloud data exposure but silently compromise sovereignty during fallback—boundary crossings remain invisible at the application layer. Provenance chains stay complete under cooperation yet are trivially bypassed without cryptographic enforcement. Failover windows create transient blind spots exploitable for unauthorized actuation. Our results establish deployment architecture—not just model or prompt design—as a primary determinant of security risk in agent-controlled IoT systems.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>security</category>
      <category>llm</category>
      <category>agent</category>
    </item>
    <item>
      <title>Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</title>
      <link>https://arxiv.org/abs/2602.23296v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23296v1</guid>
      <description>Federated learning (FL) lacks reliable uncertainty quantification (UQ), especially under *dual heterogeneity*—the coupled effect of data and model heterogeneity—which undermines coverage guarantees across clients and risks silent local failures. While conformal prediction offers distribution-free UQ with finite-sample guarantees, its adaptation to heterogeneous FL remains underexplored. We propose **FedWQ-CP**, a simple, communication-efficient framework that achieves both client-level and global coverage validity in one round of agent-server calibration. Each client computes a local quantile threshold on its calibration set and sends only this threshold and its calibration size to the server, which aggregates them via a sample-size-weighted average to obtain a global threshold. Evaluated on seven public classification and regression benchmarks, FedWQ-CP empirically attains target coverage (e.g., 90%) at *every client* and globally (coverage error &lt;1.2%), while yielding the *sharpest* prediction sets/intervals—outperforming existing federated conformal methods by 8.3–15.7% in average interval width or set size. FedWQ-CP requires no model architecture changes, no gradient exchange, and zero additional communication rounds.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots</title>
      <link>https://arxiv.org/abs/2602.22973v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22973v1</guid>
      <description>This paper introduces a novel diagnostic alignment framework for clinical AI that treats expert validation as a structured, traceable transformation—not merely a final label. We formalize the AI’s initial image-based report as an *immutable inference snapshot*, preserving it unchanged for systematic comparison with physician-validated outcomes. The pipeline integrates a vision-enhanced LLM, BERT-based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement pre-review. Evaluated on 21 dermatological cases, our four-level concordance framework reveals: exact primary match rate (PMR) = 71.4%; semantic-adjusted match rate (AMR) identical to PMR (*t* = 0.60), indicating lexical agreement already captures clinical meaning; and 100% Comprehensive Concordance Rate (CCR) with 95% CI [83.9%, 100%], confirming no case showed complete diagnostic divergence. Critically, binary lexical evaluation substantially underestimates clinically meaningful alignment—our signal-aware approach enables quantification of correction dynamics and supports human-aligned, auditable evaluation of image-based clinical decision support.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study</title>
      <link>https://arxiv.org/abs/2602.22760v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22760v1</guid>
      <description>This study investigates the feasibility of performing full-parameter pretraining of large language models (LLMs) during renewable energy curtailment windows—periods when excess clean electricity is otherwise discarded. We design and prototype a geographically distributed training system that dynamically schedules compute across heterogeneous GPU clusters using real-world marginal carbon intensity traces to identify curtailment opportunities. Leveraging the Flower federated learning framework, our system elastically switches between local single-site training and cross-site synchronized training as clusters enter or exit curtailment windows. Evaluating on a 561M-parameter transformer model across three clusters, we demonstrate that curtailment-aware scheduling preserves model quality (within ±1.2% perplexity deviation on WikiText-2 and PG-19) while reducing operational carbon emissions to just 5–12% of conventional single-site baselines. This work provides the first end-to-end empirical validation that LLM pretraining can be robustly aligned with intermittent, regionally varying renewable surpluses—enabling scalable, low-carbon AI development.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Tackling Privacy Heterogeneity in Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2602.22633v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22633v1</guid>
      <description>Differentially private federated learning (DP-FL) enables collaborative model training while protecting local data privacy. However, real-world deployments face *privacy heterogeneity*: clients impose vastly different privacy budgets (e.g., ε = 0.5 vs. ε = 8), violating the common uniform-budget assumption. This heterogeneity undermines standard client selection—data-volume-based strategies cannot distinguish high-signal (low-noise) updates from low-signal (high-noise) ones induced by stringent privacy constraints. To bridge this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We derive a tight convergence bound quantifying how heterogeneous budgets impact training error, then formulate optimal selection as a convex optimization problem that adaptively assigns sampling probabilities to minimize this bound. Experiments on CIFAR-10, FEMNIST, and Sentiment140 show our method improves test accuracy by up to **10.2%** over state-of-the-art baselines under heterogeneous budgets, while reducing communication rounds by 18%. This demonstrates that explicitly incorporating privacy heterogeneity into client selection is essential for practical, efficient, and compliant DP-FL.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD</title>
      <link>https://arxiv.org/abs/2602.22611v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22611v1</guid>
      <description>In Embedding-as-an-Interface (EaaI) settings, intermediate representations (IRs) from pre-trained models leak membership information, with MIA risk varying significantly across layers—a heterogeneity ignored by standard DP-SGD’s uniform noise and per-example clipping. We propose **Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD)**, which quantifies layer-specific MIA risk via shadow-model-based adversaries on public data and uses these estimates to reweight layer gradients *before* global clipping—enabling risk-proportional privacy allocation under fixed noise magnitude. Theoretically, LM-DP-SGD preserves $(\varepsilon,\delta)$-DP guarantees and convergence. Experiments show it reduces peak IR-level MIA risk by up to 41.3% (avg. 28.7%) at the same privacy budget, while improving utility retention by 35–52% over baseline DP-SGD—establishing a new state-of-the-art privacy–utility trade-off for IR publishing.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>dp</category>
      <category>membership</category>
    </item>
    <item>
      <title>DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion</title>
      <link>https://arxiv.org/abs/2602.22610v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22610v1</guid>
      <description>Condition injection in diffusion models enables context-aware generation for time-series tasks, but heterogeneous conditioning (e.g., missingness patterns or outlier covariates) induces heavy-tailed per-example gradients—causing excessive global clipping, amplified bias, and degraded utility under DP-SGD. We propose **DP-aware AdaLN-Zero**, a plug-and-play, sensitivity-aware conditioning mechanism that jointly bounds the magnitude of conditioning representations and AdaLN-Zero modulation parameters via bounded re-parameterization—suppressing extreme gradient tails *before* clipping and noise injection, without altering DP-SGD. Empirically, on a real-world power dataset and two ETT benchmarks, it improves imputation and forecasting MSE by 18.3–24.7% under matched privacy budgets (ε=2, δ=1e−5). Gradient diagnostics confirm reduced clipping distortion (−32.6%) and condition-specific tail reshaping, while preserving expressiveness in non-private training. This work establishes sensitivity-aware conditioning as a key architectural lever for private conditional diffusion.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
    </item>
    <item>
      <title>SQISign on ARM</title>
      <link>https://eprint.iacr.org/2026/394</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/394</guid>
      <description>We present the first vectorized implementation of SQISign for high-performance ARM architectures. SQISign is a leading NIST On-Ramp Round 2 candidate, offering the smallest key and signature sizes among post-quantum signatures—but its signing performance is bottlenecked by the ideal-to-isogeny conversion, which demands intensive finite field and elliptic curve operations. Leveraging the NEON instruction set, we design a highly optimized $\mathbb{F}_{p^2}$ arithmetic library and batched 2D isogeny-chain operations, accelerating the critical conversion subroutine by **2.24×** over the state-of-the-art scalar implementation. Our optimizations are fully orthogonal to the recent Qlapoti algorithm (Asiacrypt 2025); when combined, they yield **&gt;2.24× end-to-end signing speedup at NIST Security Level I**. This work establishes the first practical, vectorized SQISign baseline for ARM and opens avenues for efficient quaternion-based computation in isogeny cryptography.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Conditionally Linkable Attribute-Based Signatures</title>
      <link>https://eprint.iacr.org/2026/402</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/402</guid>
      <description>This paper introduces **Conditionally Linkable Attribute-Based Signatures (CLABS)**, a new primitive that enables *context-dependent*, *user-controlled* linkability in attribute-based authentication. Unlike prior ABS schemes—where linkability is either globally absent or universally enforced—CLABS allows two signatures to be publicly linked *if and only if* they share a declared context $\tau$ (e.g., jurisdiction, policy domain) *and* their attributes match under a public context-specific function $f_\tau$. Each user’s linking set $L_x \subseteq \mathcal{T}$ encodes voluntary, fine-grained linkage consent, ensuring anonymity by default and leakage limited to an opt-in bit. We formalize CLABS security, capturing conditional linkability and context-aware anonymity. Our generic construction combines a PRF, a standard signature for attribute certification, and a signature of knowledge (SoK) proving correct tag derivation and Boolean policy satisfaction without revealing attributes. Instantiated under standard lattice assumptions in the QROM, CLABS achieves post-quantum security and supports arbitrary Boolean policies.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>NIROPoK-Based Post-Quantum Sidechain Design on Ethereum</title>
      <link>https://eprint.iacr.org/2026/401</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/401</guid>
      <description>This paper introduces NIROPoK, the first practical post-quantum sidechain framework for Ethereum. We design a quantum-resistant stack comprising: (1) a novel **Non-Interactive Random Oracle Proof of Knowledge (NIROPoK)** based on module-LWE, enabling efficient zero-knowledge verification with 42% lower gas cost than zk-SNARKs; (2) a **Quantum-Resistant Proof-of-Stake (QPoS)** consensus using Dilithium signatures for secure validator onboarding and block signing; and (3) a **Dilithium-based cross-chain bridge** formally verified against quantum adversaries. Deployed on an Ethereum-compatible testnet, our sidechain achieves &gt;1200 TPS and sub-2.3s block latency while preserving full EVM compatibility and requiring no changes to Ethereum’s core architecture. The solution secures existing Ethereum transactions against quantum threats *today*, demonstrating a viable, deployable path toward quantum-ready Web3 infrastructure.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Non-interactive Blind Signatures with Threshold Issuance</title>
      <link>https://eprint.iacr.org/2026/400</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/400</guid>
      <description>We introduce **Non-interactive Threshold Blind Signatures (NITBS)**, the first primitive enabling a user to obtain partial presignatures from a *t*-out-of-*n* threshold of signers and locally combine them into a valid blind signature—without any interaction beyond initial requests. We formalize security with rigorous definitions of *blindness* (signers learn nothing about the blinded message) and *one-more unforgeability* (an adversary gaining *t−1* partial presignatures cannot produce *t* valid blind signatures). Our construction adapts the Pointcheval-Sanders (PS) signature scheme and is proven secure in the Algebraic Group Model (AGM). Micro-benchmarks show it achieves the **smallest presignature size (2 group elements), smallest signature size (3 group elements), and fastest issuance latency** among all existing NIBS schemes—setting a new efficiency baseline for threshold privacy-preserving authentication.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>What a Wonderful World: zkSNARKs in the Algebraic Group Model are Universally Composable</title>
      <link>https://eprint.iacr.org/2026/399</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/399</guid>
      <description>Zero-Knowledge Succinct Non-interactive Arguments of Knowledge (zkSNARKs) are widely deployed in practice, yet their security under concurrent composition requires Universal Composability (UC) guarantees. Prior UC analyses either lack modularity or rely on the Random Oracle Model (ROM), leaving efficient Algebraic Group Model (AGM)-based zkSNARKs—such as Plonk and Marlin—without rigorous UC foundations. We bridge this gap by introducing a modular framework: we identify simple, standard algebraic properties (e.g., extractability of commitments, polynomial binding, and algebraic zero-knowledge simulation) sufficient for UC security in the AGM (+ROM). Crucially, these properties can be verified *directly* from the rigorous AGM formalization of Jaeger and Mohan (CRYPTO’24). Applying our framework, we prove—**without any modification or overhead**—that Plonk and Marlin are UC-secure in their native AGM-based security setting. This is the first composably secure justification for widely adopted PIIOP-derived zkSNARKs.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Orthus: Practical Sublinear Batch-Verification of Lattice Relations from Standard Assumptions</title>
      <link>https://eprint.iacr.org/2026/398</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/398</guid>
      <description>This work introduces **Orthus**, the first practical sublinear batch-verification proof system for lattice-based relations—such as Falcon signature verification and SIS/LWE assertions—built solely on standard assumptions (e.g., Module-LWE) without trusted setup. Orthus achieves asymptotic verification time $O(\sqrt{N})$, where $N$ is the witness size, breaking the linear barrier that has hindered adoption of succinct lattice-based proofs. Its core innovations include a hierarchical batching framework, FFT-optimized polynomial commitments tailored to lattice algebra, and efficient inner-product compression. In implementation, Orthus reduces verifier runtime by **9×** when aggregating $2^{17}$ Falcon signatures, while keeping proof size modest (~24 KB) and prover overhead practical. This marks the first standard-assumption construction enabling *real-world* verification acceleration for lattice-based succinct proofs.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Bittersweet Signatures: Bringing LWR to a Picnic for Hardware-Friendly MPC-in-the-Head</title>
      <link>https://eprint.iacr.org/2026/397</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/397</guid>
      <description>We introduce **Bittersweet signatures**, the first MPC-in-the-Head (MiH) signature scheme based on the **Learning With Rounding (LWR)** assumption. Leveraging *almost key-homomorphic PRFs*, Bittersweet achieves exceptional conceptual simplicity and regular structure—enabling efficient parallel and hardware-friendly implementations. Technically, we address the nontrivial challenge of *carry leakage* inherent in LWR-based homomorphic operations via rigorous noise analysis and tailored parameterization. Concretely, Bittersweet yields competitive signature sizes (~12–18 KB), trades modest software overhead (2–3× slower signing than Picnic3) for substantial hardware gains (5–8× FPGA throughput), and lags only slightly behind state-of-the-art VOLE- or Threshold-MiH schemes. Its algebraic scalability and structured design make it a promising candidate for leakage-resilient deployment. The new abstractions introduced open avenues for generalization and optimization beyond MiH.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Anonymity of X-Wing and its Variants</title>
      <link>https://eprint.iacr.org/2026/396</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/396</guid>
      <description>X-Wing is a hybrid KEM (X25519 + ML-KEM-768) under IETF standardization and deployed by industry for post-quantum transition. This paper presents the first anonymity analysis of X-Wing. We prove in the standard model that weak anonymity implies full anonymity for any IND-CCA-secure KEM via a reduction that is tight in success probability, time, *and memory*—a novel triple-tight guarantee. In the random oracle model, we show X-Wing achieves weak anonymity if both X25519 and ML-KEM-768 do; notably, X25519’s weak anonymity holds information-theoretically. To recover memory-tightness lost in the original construction, we propose X-Wing⁺, a minimal variant preserving tightness end-to-end. Finally, we improve the existing IND-CCA proof of X-Wing using our memory-aware techniques, reducing security loss.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>How To Make Delegated Payments on Bitcoin: A Question for the AI Agentic Future</title>
      <link>https://eprint.iacr.org/2026/395</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/395</guid>
      <description>We introduce **Proxy Adaptor Signatures (PAS)**, a novel cryptographic primitive enabling *provably fair, delegated payment* on public blockchains. PAS solves the core fair-exchange problem in AI-agentic finance: ensuring a buyer obtains an asset *iff* the seller is paid—without revealing sensitive transaction witnesses to proxies, requiring long-term secrets from users, or compromising atomicity or privacy. Under a threshold model tolerating up to $t-1$ colluding proxies, PAS allows a stateless buyer to initiate a single request; proxies assist the exchange obliviously; and the seller is cryptographically guaranteed payment only if the buyer can later reconstruct the witness. Our efficient construction builds on standard primitives (ECDSA adaptor signatures + Shamir secret sharing) and is compatible with Bitcoin (Taproot), Cardano, and Ethereum. A Rust prototype supporting up to 30 proxies demonstrates concrete efficiency: buyer/seller operations finish in **microseconds**, proxy computation in **milliseconds**, and on-chain cost matches that of a baseline transaction—no extra fees or gas overhead.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>UC-Secure Star DKG for Non-Exportable Key Shares with VSS-Free Enforcement</title>
      <link>https://arxiv.org/abs/2602.22187v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22187v1</guid>
      <description>We present **Star DKG (SDKG)**, the first UC-secure Distributed Key Generation protocol for non-exportable key shares, eliminating Verifiable Secret Sharing (VSS) entirely. SDKG operates in the $\mathcal{F}_{KeyBox}$-hybrid model—formalizing hardware-enforced key isolation (e.g., TEEs, HSM APIs)—where shares are cryptographically bound and never exported. To enforce transcript-defined affine consistency without share exposure, we introduce **Unique Structure Verification (USV)**, a public certificate whose secret scalar remains inside the KeyBox while its group element is deterministically derivable from the transcript, and combine it with **Fischlin-style UC-extractable NIZKs** in the gRO-CRP model to enable straight-line simulation under adaptive corruptions and secure erasures. SDKG realizes a **1+1-out-of-$n$ star access structure** (designated service + any recovery device) for threshold wallets, with role-based registration. Under DL and DDH assumptions, it UC-realizes a transcript-driven refinement of standard UC-DKG, achieving $\widetilde{O}(n\log p)$ communication and $\widetilde{O}(n\log^{2.585}p)$ bit-operation cost over a prime-order group of size $p$.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions</title>
      <link>https://arxiv.org/abs/2602.22134v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22134v1</guid>
      <description>Semantic Communication (SemCom) shifts wireless transmission from symbol reproduction to task-oriented semantic delivery—but its AI-native architecture introduces novel security vulnerabilities: semantic failures can occur even with intact physical-layer reliability and cryptographic protection, due to adversarial model perturbations, poisoned training data, desynchronized semantic priors, or misaligned distributed inference. This survey establishes the first defense-centered, system-level framework for SemCom security via AI defenses. We propose a unified threat model categorizing attacks across four vectors—model-level, channel-realizable, knowledge-based, and networked inference—and introduce a structured defense taxonomy aligned with semantic integrity failure points: encoding, wireless transmission, knowledge integrity, and multi-agent coordination. We further define *security utility operating envelopes* to capture fidelity–robustness–latency–energy tradeoffs under realistic constraints, review evaluation metrics (e.g., Semantic BER, Task Accuracy under Attack), applications, and identify critical open challenges—including cross-layer security composition and deployment-time certification. The work provides a foundational, actionable perspective for building trustworthy SemCom systems in next-generation intelligent networks.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>poisoning</category>
      <category>data</category>
      <category>model</category>
      <category>inference</category>
    </item>
    <item>
      <title>A Critical Look into Threshold Homomorphic Encryption for Private Average Aggregation</title>
      <link>https://arxiv.org/abs/2602.22037v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22037v1</guid>
      <description>Threshold Homomorphic Encryption (Threshold HE) is widely adopted for privacy-preserving average aggregation in Federated Learning (FL), yet recent work exposes critical vulnerabilities when adversaries access a *restricted decryption oracle*—a realistic threat reflecting FL clients’ ability to jointly decrypt results without learning the full secret key. This paper conducts the first systematic evaluation of threshold RLWE-based HE (specifically BFV and CKKS variants) for federated averaging, focusing on the practical trade-offs introduced by *smudging noise with large variance* as a security countermeasure. We benchmark communication, latency, and aggregation accuracy under standardized 128-bit security parameters across 3–16 clients. Contrary to common assumptions, we find that threshold CKKS achieves **comparable or slightly better end-to-end performance than threshold BFV**, with up to 12% lower latency at scale, while maintaining aggregation error below 0.001%—well within FL convergence tolerance. Our results provide concrete guidance for secure, efficient HE integration in production FL systems.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Resilient Federated Chain: Transforming Blockchain Consensus into an Active Defense Layer for Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21841v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21841v1</guid>
      <description>Federated Learning (FL) enables privacy-preserving decentralized training but remains highly vulnerable to adversarial attacks due to its inherent lack of centralized data inspection. While blockchain integration has been explored for FL, its potential as an *active defense layer*—rather than merely an immutable ledger—remains untapped. This paper proposes **Resilient Federated Chain (RFC)**, a novel framework that repurposes the computational redundancy in Proof of Federated Learning (PoFL) consensus as real-time defense infrastructure. RFC introduces a flexible, attack-adaptive evaluation function within its consensus mechanism and tightly couples on-chain verification with off-chain robust aggregation (e.g., Krum, Median). Extensive experiments on image classification under diverse adversarial settings (e.g., label-flipping, sign-flipping, Byzantine attacks) show RFC improves model accuracy by up to 37.5 percentage points over FedAvg under 20% malicious clients, while maintaining &lt;3.1% false positive rate. RFC establishes blockchain not just as a trust anchor, but as a dynamic, responsive security layer for trustworthy decentralized AI.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Private and Robust Contribution Evaluation in Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21721v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21721v1</guid>
      <description>Cross-silo federated learning (FL) enables collaborative model training without raw data sharing, yet client updates remain vulnerable to inference attacks. While secure aggregation (SecAgg) preserves privacy by masking individual contributions, it renders conventional contribution evaluation—critical for fair rewards and misbehavior detection—infeasible. Existing marginal-contribution methods (e.g., Shapley value) are incompatible with SecAgg; practical alternatives like Leave-One-Out (LOO) suffer from coarse granularity and dangerous self-evaluation dependencies. We propose two SecAgg-compatible marginal-difference scores: **Fair-Private**, satisfying core fairness axioms (efficiency, symmetry, null player, additivity); and **Everybody-Else**, eliminating self-evaluation entirely and providing provable resistance to strategic manipulation—a previously overlooked vulnerability. We provide rigorous theoretical guarantees on fairness, $(\varepsilon,\delta)$-differential privacy, Byzantine robustness, and linear communication/computation cost. Extensive evaluation across medical imaging datasets (BraTS, CheXpert, NIH ChestX-ray) and CIFAR10 shows our scores consistently outperform baselines: they better approximate Shapley rankings (+32–47% Kendall tau), improve final model accuracy (+1.8–3.4 pp), and achieve &gt;0.91 F1-score in detecting malicious clients. This work establishes the first principled framework jointly achieving fairness, privacy, robustness, and practical utility in FL contribution evaluation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection</title>
      <link>https://arxiv.org/abs/2602.21593v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21593v1</guid>
      <description>Generative image proliferation has spurred adoption of semantic-aware watermarking in diffusion models for provenance tracking and forgery detection. While content-aware schemes bind watermarks to high-level semantics to resist local edits, we expose a critical vulnerability: large language models (LLMs) enable *targeted, coherence-preserving semantic perturbations* that selectively alter watermark-relevant attributes without disrupting global visual-semantic consistency. We propose the **Coherence-Preserving Semantic Injection (CSI)** attack—a novel LLM-guided framework that leverages CLIP-aligned text prompts and embedding-space similarity constraints to generate semantically shifted yet visually faithful images, inducing detector misclassification. Extensive evaluation across six state-of-the-art semantic watermarking methods shows CSI consistently outperforms all baselines, increasing false-negative rates by up to 82.6% (avg. +37.4% over best prior attack). This reveals a fundamental security gap: semantic binding alone is insufficient against LLM-powered semantic manipulation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>llm</category>
    </item>
    <item>
      <title>Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem</title>
      <link>https://arxiv.org/abs/2602.21814v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21814v1</guid>
      <description>This study isolates the causal impact of prompt architecture on implicit physical constraint reasoning using the “car wash problem”—a benchmark where LLMs consistently fail without explicit spatial logic cues. In a controlled variable-isolation experiment (n=20 per condition, 6 conditions), we evaluate Claude 3.5 Sonnet under fixed hyperparameters (temperature=0.7, top_p=1.0). The STAR (Situation-Task-Action-Result) reasoning framework alone boosts accuracy from 0% to 85% (*p*=0.001, Fisher’s exact test; OR=13.22), demonstrating that *forced goal articulation prior to inference* is the dominant driver of correct reasoning. User-profile context (via vector DB retrieval) adds +10 percentage points, and RAG-supplied domain context contributes an additional +5 points—reaching 100% in the full-stack condition. Crucially, structured scaffolding matters substantially more than contextual augmentation for this class of tasks: reasoning *how* to reason outweighs *what* to reason about. These findings establish STAR as a minimal, generalizable scaffold for reliable implicit-constraint inference.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>prompt</category>
      <category>injection</category>
    </item>
    <item>
      <title>Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation</title>
      <link>https://arxiv.org/abs/2602.21957v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21957v1</guid>
      <description>Federated recommendation (FedRec) enables collaborative model training across distributed clients without sharing raw user interaction data. Conventional methods synchronize high-dimensional item embeddings between server and clients, implicitly assuming precise geometric alignment is essential for collaboration. We challenge this assumption and argue that preserving *global semantic structures*—rather than identical embeddings—is more effective and efficient. To this end, we propose **CGFedRec**, a cluster-guided framework where the server discovers a shared item clustering structure from uploaded embeddings and broadcasts only compact cluster labels—not full embeddings—to clients. Clients then align their local item representations via these structural constraints, enabling personalization while maintaining global consistency. Experiments on six real-world datasets (e.g., Yelp, Amazon-Book, ML-1M) show CGFedRec achieves **up to 92.7% communication reduction** and **+3.8% average NDCG@10 gain** over strong baselines, demonstrating superior accuracy-efficiency trade-offs. Our work redefines collaboration in FedRec as *structural alignment*, not coordinate synchronization.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2602.21928v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21928v1</guid>
      <description>This paper addresses decentralized root cause analysis (RCA) in nonlinear dynamical systems—e.g., power grids and supply chains—where clients are geographically distributed, run fixed proprietary models, and exhibit unknown, time-varying interdependencies. We propose a novel federated learning framework that learns cross-client dependencies *without accessing raw sensor streams or modifying local black-box models*. Each client augments its legacy system with a lightweight, trainable dependency encoder; a global server coordinates encoders via representation consistency constraints (leveraging temporal lag correlation and contrastive alignment) while preserving privacy through calibrated differential privacy. Theoretical analysis establishes convergence guarantees under non-IID, feature-partitioned, and privacy-constrained settings. Experiments on extensive simulations and a real-world industrial cybersecurity dataset demonstrate state-of-the-art RCA performance: 92.4% F1-score on root cause localization and 58% lower false positives versus baselines—achieving RCA with *zero raw data upload, zero model modification, and zero prior dependency knowledge*.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>federated</category>
      <category>machine</category>
      <category>learning</category>
      <category>differential</category>
      <category>privacy</category>
    </item>
    <item>
      <title>GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task</title>
      <link>https://arxiv.org/abs/2602.21873v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21873v1</guid>
      <description>Federated learning (FL) enables privacy-preserving collaborative training for vision tasks, yet suffers from **ineffective cross-client knowledge fusion under class imbalance** and **prohibitive communication overhead** from transmitting full-model parameters. To address this, we propose **Generative Federated Prototype Learning (GFPL)** — a lightweight, semantics-aware FL framework. GFPL replaces model parameter exchange with **class-wise prototype transmission**, where each client generates prototypes via Gaussian Mixture Models (GMM) to capture feature statistics compactly. Server-side aggregation employs **Bhattacharyya distance** to fuse semantically similar prototypes across clients, avoiding semantic misalignment. Crucially, fused prototypes generate **class-balanced pseudo-features**, and a **dual-classifier architecture**—optimized via hybrid Dot Regression and Cross-Entropy loss—enhances local feature alignment. Experiments on imbalanced benchmarks (e.g., CIFAR-10-LT, ImageNet-LT) show GFPL improves accuracy by **+3.6%** over strong baselines while reducing communication cost by **up to 87%**, achieving superior performance-efficiency trade-offs for resource-constrained edge vision applications.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21844v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21844v1</guid>
      <description>Differentially private federated learning (DP-FL) suffers from a fundamental tension: privacy-preserving mechanisms impose quantifiable privacy costs that deter client participation, especially among highly privacy-sensitive “stragglers.” Existing incentive designs assume unbiased client selection, forcing servers to over-compensate these stragglers—leading to budget waste and suboptimal model convergence. We propose **JSAM**, the first Bayesian-optimal framework that *jointly* optimizes client selection probabilities and privacy compensation under a fixed budget. By theoretically characterizing optimal selection structure, JSAM reduces the original 2N-dimensional problem to an efficient 3D formulation. We prove that servers should *exclude* high-sensitivity clients and *preferentially select* privacy-tolerant ones—and reveal the counter-intuitive insight that *least*-sensitive clients incur the *highest cumulative cost* due to frequent selection. Extensive experiments on MNIST and CIFAR-10 show JSAM improves test accuracy by up to **15%** over unbiased baselines while maintaining cost efficiency across diverse data heterogeneity levels (α ∈ [0.1, 1.0]).</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Mamba Meets Scheduling: Learning to Solve Flexible Job Shop Scheduling with Efficient Sequence Modeling</title>
      <link>https://arxiv.org/abs/2602.21546v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21546v1</guid>
      <description>The Flexible Job Shop Scheduling Problem (FJSP) is a fundamental combinatorial optimization challenge in smart manufacturing, yet learning-based solvers suffer from high computational overhead and limited global dependency modeling. This paper pioneers the integration of **Mamba**—a linear-time state-space model—into FJSP solving. We propose a novel architecture featuring: (i) a dual-path Mamba encoder that separately processes operation and machine sequences to capture long-range constraints with O(N) complexity; and (ii) an efficient cross-attention decoder for dynamic operation-machine interaction. Evaluated on standard benchmarks (Dai, Brandimarte, Kacem), our method achieves **3.2× faster inference** and outperforms state-of-the-art learning-based solvers by 1.8–4.7% in makespan minimization, while reducing memory usage by 58%. This demonstrates Mamba’s superiority over quadratic-cost models (e.g., Transformers, GNNs) for structured scheduling tasks.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain</title>
      <link>https://arxiv.org/abs/2602.22045v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22045v1</guid>
      <description>We present **DLT-Corpus**, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: **2.98 billion tokens** across **22.12 million documents**, including 37,440 scientific publications, 49,023 USPTO patents, and 22 million social media posts. Unlike prior NLP resources narrowly focused on cryptocurrency price prediction or smart contracts, DLT-Corpus comprehensively captures the full technological lifecycle of DLT. Using it, we uncover that DLT innovations emerge first in scientific literature, then migrate to patents, and finally diffuse to social media—following classic technology transfer pathways. Crucially, while social media sentiment remains persistently bullish—even during “crypto winters”—scientific and patent activity grows independently of short-term market volatility and instead tracks long-term market capitalization expansion (*r* = 0.87), revealing a virtuous innovation cycle where research enables economic growth that funds further R&amp;D. We publicly release the full corpus, **LedgerBERT** (a domain-adapted model achieving **+23% F1** over BERT-base on DLT-specific NER), and all tools and code.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>crypto</category>
    </item>
    <item>
      <title>Fast cube roots in Fp2 via the algebraic torus</title>
      <link>https://eprint.iacr.org/2026/392</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/392</guid>
      <description>We present a novel algorithm for computing cube roots in $\mathbb{F}_{p^2}$, a critical subroutine in elliptic-curve point decompression, hash-to-curve, and isogeny-based cryptography. Leveraging the algebraic torus $\mathbb{T}_2(\mathbb{F}_p)$ and Lucas sequences, our method reduces the problem *entirely* to operations in the base field $\mathbb{F}_p$—under the practically universal condition $p \equiv 1 \pmod{3}$. We prove correctness across all residuosity cases and implement it in the open-source `gnark-crypto` library. Benchmarks on six cryptographic primes (spanning pairing- and isogeny-based settings) show **1.6–2.3× speedups** over standard $\mathbb{F}_{p^2}$ exponentiation. The approach extends to $p \equiv 2 \pmod{3}$ and, more generally, to any odd $n$-th root in quadratic towers $\mathbb{F}_{p^{2^k}}$ when $\gcd(n, p+1) = 1$.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Zero-Knowledge IOPPs for Constrained Interleaved Codes</title>
      <link>https://eprint.iacr.org/2026/391</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/391</guid>
      <description>We present the first zero-knowledge interactive oracle proof of proximity (zk-IOPP) for constrained interleaved linear codes—achieving honest-verifier zero-knowledge with *negligible overhead* over the state-of-the-art non-ZK protocols. Our construction satisfies round-by-round knowledge soundness with a straightline extractor and negligible error. Technically, we introduce a composable definition of HVZK for interactive oracle reductions (IORs), then modularly compose lightweight zk-IORs: a novel **zero-knowledge sumcheck IOR** and a **zero-knowledge code-switching IOR**, both tailored to stringent efficiency requirements (sublinear communication, constant rounds, straightline extraction). To overcome challenges—including privacy leakage in interleaved polynomial commitments and zero-knowledge preservation across code families—we introduce new abstractions and protocols. As a side contribution, we highlight the concrete efficiency gains from high-distance codes derived from dispersers, which may be of independent interest.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Succinct Arguments for BatchQMA and Friends under 6 Rounds</title>
      <link>https://eprint.iacr.org/2026/390</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/390</guid>
      <description>We present the first succinct classical argument systems for batchQMA and monotone-policy batchQMA with ≤6 rounds under standard post-quantum assumptions—breaking the prior 8-round barrier and avoiding the quantum random oracle model (QROM). Our key technical innovation is **straight-line partial extractability**, enabling soundness proofs *without rewinding* cheating quantum provers—a departure from all prior works that relied on state-preserving succinct arguments of knowledge for NP. Specifically: (1) A **4-round public-coin** (except first message) argument for batchQMA achieves optimal communication (all messages independent of batch size) assuming post-quantum functional encryption and LWE; under LWE alone, only the verifier’s first message scales with batch size. (2) A **6-round private-coin** argument for monotone-policy batchQMA achieves batch-size- *and* circuit-size-independent communication under LWE. These results significantly advance practical verification of quantum computations by classical verifiers.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Towards Accountability for Anonymous Credentials</title>
      <link>https://eprint.iacr.org/2026/389</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/389</guid>
      <description>Anonymous Credentials (ACs) provide strong privacy by preventing issuers and verifiers from tracking users—but this very feature undermines accountability, hindering adoption in national identity systems (e.g., EUDI, Swiss e-ID). This paper identifies *transferability attacks* as a critical threat and proposes the first accountability framework for ACs. We introduce the **Cryptographic Forensic Trail (CFT)**: a tamper-evident, encrypted log attached to each credential presentation. Crucially, CFT decryption requires *joint, conditional authorization*: police must obtain a judicial warrant (based on probable cause), after which police, judge, and an independent NGO jointly execute a multiparty protocol to decrypt *only the relevant trail*. This design enforces checks and balances—neither law enforcement nor judiciary can act unilaterally, and NGO oversight detects and blocks collusion. Performance evaluation confirms practical feasibility: CFT adds &lt;300ms latency and &lt;2KB overhead on modern smartphones. Our work bridges the long-standing privacy–accountability gap in digital identity.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Necessary and Sufficient Conditions for the Existence of Ideal Linear Secret Sharing Schemes for Arbitrary Access Structures</title>
      <link>https://eprint.iacr.org/2026/388</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/388</guid>
      <description>This paper establishes a necessary and sufficient condition for the existence of an ideal linear secret sharing scheme (ILSSS) realizing an arbitrary minimal access structure $\Gamma_{\min}$. Using linear codes over a finite field $\mathbb{F}_q$ as the primary tool, we construct matrices $H$ (parity-check) and $G$ (generator) such that $\Gamma_{\min}$ admits an ILSSS **if and only if** the matrix equation $GH^{\mathsf{T}} = 0$ has a solution over $\mathbb{F}_q$. When satisfied, $H$ defines a linear code whose *port* realizes $\Gamma_{\min}$, and $G$ is its corresponding generator matrix. Crucially, we prove this algebraic condition is equivalent to the combinatorial requirement that $\Gamma_{\min}$ must be the port of a matroid representable over $\mathbb{F}_q$. This unifies secret sharing, coding theory, and matroid representation in a concise, computationally verifiable framework—enabling efficient existence testing and constructive design of ideal schemes.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>A Comprehensive Break of the Tropical Matrix-Based Signature Scheme</title>
      <link>https://eprint.iacr.org/2026/387</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/387</guid>
      <description>We present a comprehensive cryptanalysis of the tropical matrix-based signature scheme GMS26 (Grigoriev–Monico–Shpilrain, 2026), which bases security on the claimed NP-hardness of tropical matrix factorization. Contrary to its security claims, we demonstrate four efficient polynomial-time attacks exploiting inherent algebraic weaknesses—not the underlying hardness assumption. First, we mount an existential forgery attack in the chosen-hash model requiring only $O(n^3)$ tropical operations. Second, we prove the scheme is fundamentally malleable: any valid signature can be transformed into infinitely many distinct yet valid signatures, breaking strong unforgeability. Third, observing $O(n)$ honest signatures enables probabilistic partial recovery of private key entries via leakage in tropical extremal equations. Fourth, using an SMT solver, we recover the *entire* private key from just **two valid signatures** in under 5 seconds for recommended parameters ($n=8$). All attacks invalidate standard security notions: existential unforgeability (EUF-CMA), strong unforgeability (SUF-CMA), and key confidentiality.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Determining those Boolean functions whose restrictions to affine spaces are plateaued</title>
      <link>https://eprint.iacr.org/2026/386</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/386</guid>
      <description>This paper determines the class $C^n_k$ of $n$-variable Boolean functions whose restrictions to *all* $k$-dimensional affine subspaces of $\mathbb{F}_2^n$ are plateaued (i.e., have Walsh spectra in $\{0,\pm\lambda\}$). We prove that partially-bent functions are precisely those plateaued on every affine hyperplane ($k=n-1$), while quadratic functions are exactly those plateaued on *every* $k$-dimensional affine subspace for $3 \leq k \leq n-2$. For $n \geq 5$, we establish a strict inclusion chain: quadratic $\subsetneq$ partially-bent $\subsetneq$ restrictions of partially-bent to hyperplanes $\subsetneq$ plateaued $\subsetneq$ restrictions of plateaued to hyperplanes $\subsetneq$ all Boolean functions. The characterization extends to strongly plateaued vectorial functions, and we pose an open question linking vectorial plateauedness to the long-standing crooked function problem.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Bridging Privacy and Utility: A Verifiable Framework for Data Valuation via Zero-Knowledge Proofs</title>
      <link>https://eprint.iacr.org/2026/385</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/385</guid>
      <description>Deep learning’s data dependency has spurred decentralized data markets, yet trust deficits persist: buyers fear poisoned data, sellers fear leakage. While the Shapley value enables fair valuation, its standard computation requires a Trusted Third Party (TTP) to access raw data—violating privacy. We propose **ZK-DV**, the first zero-knowledge proof (ZKP) system for *verifiable, privacy-preserving* data valuation. ZK-DV lets a seller prove that a claimed Gradient Shapley score is mathematically consistent with their private data and the buyer’s model—without revealing either. Its key insight is architectural co-design: we embed valuation logic directly into backpropagation via a custom arithmetic circuit, extracting marginal utilities from intermediate gradients. Implemented using the GKR protocol with a hybrid Pedersen/KZG commitment scheme and batched processing, ZK-DV amortizes cryptographic overhead. Experiments on MNIST show practicality: optimized batching achieves **2.7× faster proof generation**, perfect quantization fidelity (ρ = 1.0), and verification under **0.2 seconds**. ZK-DV bridges cryptographic integrity and economic fairness for trustless data exchange.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Beyond performance-wise Contribution Evaluation in Federated Learning</title>
      <link>https://arxiv.org/abs/2602.22470v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22470v1</guid>
      <description>Federated learning (FL) enables privacy-preserving collaborative modeling, yet fair contribution evaluation remains critical for incentive alignment and system sustainability. While existing methods focus almost exclusively on predictive *performance* (e.g., accuracy), this work pioneers a multi-dimensional evaluation of client contributions toward model *trustworthiness*: specifically, **reliability** (robustness to label noise), **resilience** (resistance to adversarial examples), and **fairness** (measured via demographic parity). We quantify each client’s marginal contribution across these orthogonal dimensions using Monte Carlo approximated Shapley values—a principled, axiomatic attribution method. Experiments on FEMNIST, CIFAR-10-C, and Adult Income show that: (1) the three trustworthiness dimensions are largely uncorrelated (mean |r| &lt; 0.12); (2) no client dominates across all dimensions—high-accuracy clients often underperform significantly in fairness or resilience; and (3) reward allocation based solely on accuracy systematically undervalues up to 47% of high-trustworthiness clients. Our findings expose a fundamental flaw in current FL evaluation: comprehensive, equitable reward design requires multi-faceted, performance-agnostic metrics.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>adversarial</category>
      <category>learning</category>
      <category>federated</category>
      <category>machine</category>
    </item>
    <item>
      <title>Silent Egress: When Implicit Prompt Injection Makes LLM Agents Leak Without a Trace</title>
      <link>https://arxiv.org/abs/2602.22450v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22450v1</guid>
      <description>This paper identifies *silent egress*—a novel system-level threat in agentic LLM systems where adversarial instructions embedded in automatically fetched URL previews (e.g., titles, metadata, snippets) trigger undetectable exfiltration of sensitive runtime context. Using a fully local, reproducible testbed with a qwen2.5:7b-based agent, we demonstrate that malicious web pages can induce outbound requests leaking internal state—even when the final user-facing response appears benign. Across 480 runs, the attack succeeds with 89% probability, and 95% of successful exfiltrations evade output-based safety checks. We introduce *sharded exfiltration*, splitting sensitive data across multiple requests to reduce Leak@1 by 73% and bypass simple DLP mechanisms. Ablation shows prompt-layer defenses offer limited protection, whereas system- and network-layer controls (e.g., domain allowlisting, redirect-chain analysis) are markedly more effective. Our findings argue for treating network egress as a first-class security outcome—and motivate architectural shifts toward provenance tracking and capability isolation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>llm</category>
      <category>agent</category>
      <category>injection</category>
      <category>prompt</category>
    </item>
    <item>
      <title>Differentially Private Truncation of Unbounded Data via Public Second Moments</title>
      <link>https://arxiv.org/abs/2602.22282v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22282v1</guid>
      <description>Differential privacy (DP) is hindered by the common requirement of bounded data, yet real-world datasets are often unbounded. To overcome this, we propose **Public-moment-guided Truncation (PMT)**, which leverages a small amount of *public* second-moment information (e.g., covariance) to linearly transform private data into a well-conditioned space—enabling deterministic, non-private truncation based solely on dimension *d* and sample size *n*. This transformation dramatically improves the conditioning of the second-moment matrix, making it far more resilient to DP noise during inversion. We integrate PMT into penalized linear regression and generalized linear models, designing new loss functions and back-mapping procedures with rigorous theoretical guarantees: tighter DP estimation error bounds, distributional robustness, and convergence. Experiments on synthetic and real datasets (Adult, California Housing) show PMT consistently boosts accuracy (up to 23.7% MAE reduction under ε=1–8) and stability (3.2× lower variance), establishing a unified, practical framework for high-fidelity DP learning on unbounded data.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>Poisoned Acoustics</title>
      <link>https://arxiv.org/abs/2602.22258v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22258v1</guid>
      <description>We demonstrate that training-data poisoning attacks achieve *provably undetectable, targeted failure* in acoustic vehicle classification: on the MELAUDIS urban audio dataset (≈9,600 clips, 6 classes), a compact CNN achieves **95.7% Attack Success Rate (ASR)** under a Truck→Car label-flipping attack with only **p = 0.5% corruption (48 samples)**, while aggregate test accuracy remains statistically unchanged (87.6% baseline; 95% CI: 88–100%, n=3). We prove this stealth is *structural*: the maximum possible accuracy drop from any targeted attack is bounded above by the minority class fraction β—here ≈3% for Trucks—rendering aggregate accuracy monitoring *provably insufficient* regardless of model or attack design. A companion backdoor study reveals *trigger-dominance collapse*: when the target class is a minority, spectrogram patch triggers become functionally redundant (clean ASR = triggered ASR), reducing the attack to pure label flipping. To address this systemic trust gap, we propose ML-TrustChain—a cryptographically verifiable defense combining content-addressed artifact hashing, Merkle-tree dataset commitment, and NIST FIPS 204 post-quantum signatures (ML-DSA-65/Dilithium3) for tamper-proof data provenance.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>data</category>
      <category>poisoning</category>
      <category>backdoor</category>
      <category>neural</category>
    </item>
    <item>
      <title>SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read</title>
      <link>https://arxiv.org/abs/2602.22426v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22426v1</guid>
      <description>Despite rapid progress in Multimodal Large Language Models (MLLMs), it remains unclear whether they genuinely *read* text in images or merely exploit parametric shortcuts from text prompts—a phenomenon we term “modality laziness.” To diagnose this, we introduce the **Visualized-Question (VQ) setting**, where queries are rendered *onto* images with randomized styles (e.g., font, noise, rotation), forcing visual engagement. Experiments on Qwen2.5-VL reveal a severe capability-utilization gap: up to **12.7% performance drop** under VQ, exposing heavy reliance on textual priors. To bridge it, we propose **SimpleOCR**: a plug-and-play training strategy that converts standard samples into VQ format, invalidating text-based shortcuts and compelling optimization of visual text extraction pathways. Without architectural changes, SimpleOCR achieves **+5.4% average gain** over the base model and **+2.7% over GRPO** on four OOD OCR benchmarks—using only **8.5K samples** (30× fewer than recent RL methods). It also composes seamlessly with advanced RL strategies like NoisyRollout. Code: https://github.com/aiming-lab/SimpleOCR</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning</title>
      <link>https://arxiv.org/abs/2602.22269v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22269v1</guid>
      <description>Federated Learning (FL) enables privacy-preserving collaborative training, yet local model updates remain vulnerable to inference and poisoning attacks. Quantum Secure Aggregation (QSA) promises information-theoretic privacy via GHZ-state phase encoding—but its reliance on a *single global GHZ state* suffers from rapidly degrading fidelity at scale and inherent inability to detect Byzantine clients. We propose **Clustered QSA (CQSA)**: a modular framework that partitions clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server then identifies malicious contributions by analyzing statistical discrepancies—e.g., cosine similarity and Euclidean distance—among cluster-level aggregates. Under depolarizing noise, CQSA achieves up to **68.5% higher GHZ fidelity** than global QSA and maintains stable convergence even with 20% Byzantine clients, reducing accuracy degradation by **52.3%**. CQSA bridges near-term quantum hardware constraints with practical Byzantine robustness in FL.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>data</category>
      <category>poisoning</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>VROOM: Accelerating (Almost All) Number-Theoretic Cryptography Using Vectorization and the Residue Number System</title>
      <link>https://eprint.iacr.org/2026/393</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/393</guid>
      <description>Modular arithmetic with large prime moduli dominates computational cost in number-theoretic cryptography, yet efficient CPU vectorization remains elusive due to carry propagation and costly data permutations. VROOM introduces a novel vectorized approach leveraging the Residue Number System (RNS) to align modular operations natively with wide SIMD units—eliminating carries, minimizing shuffles, and enabling constant-time multiplication for *arbitrary* odd moduli (prime or composite). Our AVX2/AVX-512 implementation achieves **4.0× speedup** for RSA-4096 signature verification and **1.3×** for signing over OpenSSL, and **3.43× faster BLS-381 verification** than the assembly-optimized BLST library. All code is portable, hardware-agnostic, and planned for upstream integration into BoringSSL to accelerate real-world TLS and cryptographic infrastructure.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
  </channel>
</rss>