<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Paper Pulse</title>
    <link>https://jamie-cui.github.io/paper-pulse</link>
    <description>Keyword-based research paper aggregation from arXiv and IACR</description>
    <lastBuildDate>Sun, 01 Mar 2026 02:08:26 -0000</lastBuildDate>
    <atom:link href="https://jamie-cui.github.io/paper-pulse/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling</title>
      <link>https://arxiv.org/abs/2602.23262v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23262v1</guid>
      <description>We propose a spectral differential privacy (DP) framework for private image generation, grounded in the insight that privacy-sensitive information resides predominantly in low-frequency wavelet components (e.g., global shapes and facial structures), while high-frequency details are largely generic and public. Our coarse-to-fine two-stage approach first applies DP fine-tuning *only* to an autoregressive spectral tokenizer trained on low-resolution wavelet coefficients (LL subband), concentrating the entire privacy budget on structural fidelity. Then, a publicly pretrained super-resolution model upsamples the DP-encoded coarse representation—leveraging DP’s post-processing property to refine textures without additional privacy cost. Experiments on MS-COCO and MM-CelebA-HQ show our method achieves superior image quality (12–28% lower FID) and semantic alignment (9–15% higher CLIP Score) versus state-of-the-art DP generative models under ε = 2–8, demonstrating that frequency-aware budget allocation breaks the utility–privacy trade-off bottleneck.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)</title>
      <link>https://arxiv.org/abs/2602.23167v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23167v1</guid>
      <description>SettleFL is a trustless, scalable reward settlement protocol for federated learning (FL) on permissionless blockchains. It resolves the fundamental tension between FL’s high-frequency iterative nature and the prohibitive on-chain cost of permissionless ledgers by introducing two interoperable, circuit-based settlement strategies: (1) *Commit-and-Challenge*, an optimistic design minimizing average gas usage via dispute-driven arbitration; and (2) *Commit-with-Proof*, ensuring instant finality per round via succinct zero-knowledge validity proofs. Both leverage a shared domain-specific zk-SNARK circuit architecture and enforce rational robustness—without trusted coordinators—through game-theoretic incentives. Extensive experiments integrating real FL workloads (CIFAR-10, EMNIST) and large-scale simulations demonstrate SettleFL’s practicality: it scales to 800 participants with sub-second settlement latency and reduces average gas cost by 87% (to ~\$0.14 per round), outperforming prior decentralized approaches.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification</title>
      <link>https://arxiv.org/abs/2602.22724v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22724v1</guid>
      <description>Large language model (LLM) agents face growing threats from **indirect prompt injection (IPI)**—a stealthy, multi-turn attack where malicious context embedded in tool outputs or retrieved documents hijacks agent behavior over time. Existing inference-time defenses rely on heuristics and conservative blocking, harming utility under ambiguity. We propose **AgentSentry**, the first framework to model IPI as a *temporal causal takeover*. It localizes takeover points via *controlled counterfactual re-executions* at tool-return boundaries and enables safe continuation through *causally guided context purification*—removing only attack-induced deviations while preserving task-critical evidence. Evaluated across 4 task suites, 3 IPI attack families, and multiple black-box LLMs on AgentDojo, AgentSentry achieves **100% attack blocking**, an average **Utility Under Attack (UA) of 74.55%**, and **no degradation on benign inputs**—outperforming strongest baselines by +20.8–33.6 percentage points in UA.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>injection</category>
      <category>prompt</category>
    </item>
    <item>
      <title>DPSQL+: A Differentially Private SQL Library with a Minimum Frequency Rule</title>
      <link>https://arxiv.org/abs/2602.22699v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22699v1</guid>
      <description>DPSQL+ is a novel differentially private SQL library that jointly enforces user-level $(\varepsilon,\delta)$-differential privacy and the minimum frequency rule (requiring ≥$k$ distinct individuals per released group). Its modular architecture comprises a static **Validator** restricting queries to a DP-safe SQL subset, a consistent **Accountant** tracking cumulative privacy loss across adaptive queries using Rényi DP, and an extensible **Backend** supporting PostgreSQL, SQLite, and Pandas. Crucially, DPSQL+ integrates the $k$-anonymity constraint directly into the DP noise injection pipeline—applying calibrated Laplace/Gaussian noise *then* enforcing $k$-thresholding with bias-corrected rescaling. Experiments on TPC-H show DPSQL+ achieves superior accuracy across aggregates, quadratic statistics (e.g., variance), and join queries, while enabling **2.1–3.8× more queries** under a fixed global budget ($\varepsilon=1.0, \delta=10^{-5}$) compared to prior libraries. It bridges formal privacy theory and real-world governance requirements.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>membership</category>
      <category>dp</category>
      <category>inference</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings</title>
      <link>https://arxiv.org/abs/2602.22689v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22689v1</guid>
      <description>Latent diffusion models (LDMs) excel at text-to-image generation but risk memorizing training data, threatening privacy and IP rights. Membership inference attacks (MIAs) audit such memorization—but existing methods require ground-truth captions, failing in realistic caption-unavailable settings where VLM-generated captions yield poor performance. We propose **MoFit**, the first caption-free MIA framework. It bypasses textual supervision by optimizing a perturbation to construct a *model-fitted surrogate image* that lies in the unconditional prior manifold learned from member samples; then extracts a *model-fitted embedding* from this surrogate to serve as a mismatched condition for the query image—amplifying conditional loss for members while minimally affecting non-members. Extensive experiments across LAION, ImageNet, and models including Stable Diffusion v1.5 and SDXL show MoFit consistently outperforms VLM-conditioned baselines and achieves performance on par with caption-dependent state-of-the-art methods.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>extraction</category>
      <category>membership</category>
      <category>model</category>
    </item>
    <item>
      <title>Systems-Level Attack Surface of Edge Agent Deployments on IoT</title>
      <link>https://arxiv.org/abs/2602.22525v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22525v1</guid>
      <description>This paper presents the first empirical security analysis of LLM agent deployments on resource-constrained IoT edge devices. We evaluate three architectures—cloud-hosted, edge-local swarm, and hybrid—using a multi-device home-automation testbed with local MQTT and an Android smartphone as an edge inference node. We identify five systems-level attack surfaces, including two emergent failures observed *in vivo*: **coordination-state divergence** (causing unresolvable control conflicts) and **induced trust erosion** (where malicious nodes trigger legitimate agents’ erroneous degradation). We formalize core security properties as measurable system metrics: data egress volume, failover window exposure, sovereignty boundary integrity, and provenance chain completeness. Measurements show that edge-local deployments eliminate routine cloud data exposure but silently compromise sovereignty during fallback—boundary crossings remain invisible at the application layer. Provenance chains stay complete under cooperation yet are trivially bypassed without cryptographic enforcement. Failover windows create transient blind spots exploitable for unauthorized actuation. Our results establish deployment architecture—not just model or prompt design—as a primary determinant of security risk in agent-controlled IoT systems.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>security</category>
      <category>llm</category>
      <category>agent</category>
    </item>
    <item>
      <title>Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</title>
      <link>https://arxiv.org/abs/2602.23296v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23296v1</guid>
      <description>Federated learning (FL) lacks reliable uncertainty quantification (UQ), especially under *dual heterogeneity*—the coupled effect of data and model heterogeneity—which undermines coverage guarantees across clients and risks silent local failures. While conformal prediction offers distribution-free UQ with finite-sample guarantees, its adaptation to heterogeneous FL remains underexplored. We propose **FedWQ-CP**, a simple, communication-efficient framework that achieves both client-level and global coverage validity in one round of agent-server calibration. Each client computes a local quantile threshold on its calibration set and sends only this threshold and its calibration size to the server, which aggregates them via a sample-size-weighted average to obtain a global threshold. Evaluated on seven public classification and regression benchmarks, FedWQ-CP empirically attains target coverage (e.g., 90%) at *every client* and globally (coverage error &lt;1.2%), while yielding the *sharpest* prediction sets/intervals—outperforming existing federated conformal methods by 8.3–15.7% in average interval width or set size. FedWQ-CP requires no model architecture changes, no gradient exchange, and zero additional communication rounds.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots</title>
      <link>https://arxiv.org/abs/2602.22973v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22973v1</guid>
      <description>This paper introduces a novel diagnostic alignment framework for clinical AI that treats expert validation as a structured, traceable transformation—not merely a final label. We formalize the AI’s initial image-based report as an *immutable inference snapshot*, preserving it unchanged for systematic comparison with physician-validated outcomes. The pipeline integrates a vision-enhanced LLM, BERT-based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement pre-review. Evaluated on 21 dermatological cases, our four-level concordance framework reveals: exact primary match rate (PMR) = 71.4%; semantic-adjusted match rate (AMR) identical to PMR (*t* = 0.60), indicating lexical agreement already captures clinical meaning; and 100% Comprehensive Concordance Rate (CCR) with 95% CI [83.9%, 100%], confirming no case showed complete diagnostic divergence. Critically, binary lexical evaluation substantially underestimates clinically meaningful alignment—our signal-aware approach enables quantification of correction dynamics and supports human-aligned, auditable evaluation of image-based clinical decision support.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study</title>
      <link>https://arxiv.org/abs/2602.22760v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22760v1</guid>
      <description>This study investigates the feasibility of performing full-parameter pretraining of large language models (LLMs) during renewable energy curtailment windows—periods when excess clean electricity is otherwise discarded. We design and prototype a geographically distributed training system that dynamically schedules compute across heterogeneous GPU clusters using real-world marginal carbon intensity traces to identify curtailment opportunities. Leveraging the Flower federated learning framework, our system elastically switches between local single-site training and cross-site synchronized training as clusters enter or exit curtailment windows. Evaluating on a 561M-parameter transformer model across three clusters, we demonstrate that curtailment-aware scheduling preserves model quality (within ±1.2% perplexity deviation on WikiText-2 and PG-19) while reducing operational carbon emissions to just 5–12% of conventional single-site baselines. This work provides the first end-to-end empirical validation that LLM pretraining can be robustly aligned with intermittent, regionally varying renewable surpluses—enabling scalable, low-carbon AI development.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Tackling Privacy Heterogeneity in Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2602.22633v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22633v1</guid>
      <description>Differentially private federated learning (DP-FL) enables collaborative model training while protecting local data privacy. However, real-world deployments face *privacy heterogeneity*: clients impose vastly different privacy budgets (e.g., ε = 0.5 vs. ε = 8), violating the common uniform-budget assumption. This heterogeneity undermines standard client selection—data-volume-based strategies cannot distinguish high-signal (low-noise) updates from low-signal (high-noise) ones induced by stringent privacy constraints. To bridge this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We derive a tight convergence bound quantifying how heterogeneous budgets impact training error, then formulate optimal selection as a convex optimization problem that adaptively assigns sampling probabilities to minimize this bound. Experiments on CIFAR-10, FEMNIST, and Sentiment140 show our method improves test accuracy by up to **10.2%** over state-of-the-art baselines under heterogeneous budgets, while reducing communication rounds by 18%. This demonstrates that explicitly incorporating privacy heterogeneity into client selection is essential for practical, efficient, and compliant DP-FL.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD</title>
      <link>https://arxiv.org/abs/2602.22611v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22611v1</guid>
      <description>In Embedding-as-an-Interface (EaaI) settings, intermediate representations (IRs) from pre-trained models leak membership information, with MIA risk varying significantly across layers—a heterogeneity ignored by standard DP-SGD’s uniform noise and per-example clipping. We propose **Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD)**, which quantifies layer-specific MIA risk via shadow-model-based adversaries on public data and uses these estimates to reweight layer gradients *before* global clipping—enabling risk-proportional privacy allocation under fixed noise magnitude. Theoretically, LM-DP-SGD preserves $(\varepsilon,\delta)$-DP guarantees and convergence. Experiments show it reduces peak IR-level MIA risk by up to 41.3% (avg. 28.7%) at the same privacy budget, while improving utility retention by 35–52% over baseline DP-SGD—establishing a new state-of-the-art privacy–utility trade-off for IR publishing.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>dp</category>
      <category>membership</category>
    </item>
    <item>
      <title>DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion</title>
      <link>https://arxiv.org/abs/2602.22610v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22610v1</guid>
      <description>Condition injection in diffusion models enables context-aware generation for time-series tasks, but heterogeneous conditioning (e.g., missingness patterns or outlier covariates) induces heavy-tailed per-example gradients—causing excessive global clipping, amplified bias, and degraded utility under DP-SGD. We propose **DP-aware AdaLN-Zero**, a plug-and-play, sensitivity-aware conditioning mechanism that jointly bounds the magnitude of conditioning representations and AdaLN-Zero modulation parameters via bounded re-parameterization—suppressing extreme gradient tails *before* clipping and noise injection, without altering DP-SGD. Empirically, on a real-world power dataset and two ETT benchmarks, it improves imputation and forecasting MSE by 18.3–24.7% under matched privacy budgets (ε=2, δ=1e−5). Gradient diagnostics confirm reduced clipping distortion (−32.6%) and condition-specific tail reshaping, while preserving expressiveness in non-private training. This work establishes sensitivity-aware conditioning as a key architectural lever for private conditional diffusion.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
    </item>
    <item>
      <title>SQISign on ARM</title>
      <link>https://eprint.iacr.org/2026/394</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/394</guid>
      <description>We present the first vectorized implementation of SQISign for high-performance ARM architectures. SQISign is a leading NIST On-Ramp Round 2 candidate, offering the smallest key and signature sizes among post-quantum signatures—but its signing performance is bottlenecked by the ideal-to-isogeny conversion, which demands intensive finite field and elliptic curve operations. Leveraging the NEON instruction set, we design a highly optimized $\mathbb{F}_{p^2}$ arithmetic library and batched 2D isogeny-chain operations, accelerating the critical conversion subroutine by **2.24×** over the state-of-the-art scalar implementation. Our optimizations are fully orthogonal to the recent Qlapoti algorithm (Asiacrypt 2025); when combined, they yield **&gt;2.24× end-to-end signing speedup at NIST Security Level I**. This work establishes the first practical, vectorized SQISign baseline for ARM and opens avenues for efficient quaternion-based computation in isogeny cryptography.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>UC-Secure Star DKG for Non-Exportable Key Shares with VSS-Free Enforcement</title>
      <link>https://arxiv.org/abs/2602.22187v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22187v1</guid>
      <description>We present **Star DKG (SDKG)**, the first UC-secure Distributed Key Generation protocol for non-exportable key shares, eliminating Verifiable Secret Sharing (VSS) entirely. SDKG operates in the $\mathcal{F}_{KeyBox}$-hybrid model—formalizing hardware-enforced key isolation (e.g., TEEs, HSM APIs)—where shares are cryptographically bound and never exported. To enforce transcript-defined affine consistency without share exposure, we introduce **Unique Structure Verification (USV)**, a public certificate whose secret scalar remains inside the KeyBox while its group element is deterministically derivable from the transcript, and combine it with **Fischlin-style UC-extractable NIZKs** in the gRO-CRP model to enable straight-line simulation under adaptive corruptions and secure erasures. SDKG realizes a **1+1-out-of-$n$ star access structure** (designated service + any recovery device) for threshold wallets, with role-based registration. Under DL and DDH assumptions, it UC-realizes a transcript-driven refinement of standard UC-DKG, achieving $\widetilde{O}(n\log p)$ communication and $\widetilde{O}(n\log^{2.585}p)$ bit-operation cost over a prime-order group of size $p$.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions</title>
      <link>https://arxiv.org/abs/2602.22134v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22134v1</guid>
      <description>Semantic Communication (SemCom) shifts wireless transmission from symbol reproduction to task-oriented semantic delivery—but its AI-native architecture introduces novel security vulnerabilities: semantic failures can occur even with intact physical-layer reliability and cryptographic protection, due to adversarial model perturbations, poisoned training data, desynchronized semantic priors, or misaligned distributed inference. This survey establishes the first defense-centered, system-level framework for SemCom security via AI defenses. We propose a unified threat model categorizing attacks across four vectors—model-level, channel-realizable, knowledge-based, and networked inference—and introduce a structured defense taxonomy aligned with semantic integrity failure points: encoding, wireless transmission, knowledge integrity, and multi-agent coordination. We further define *security utility operating envelopes* to capture fidelity–robustness–latency–energy tradeoffs under realistic constraints, review evaluation metrics (e.g., Semantic BER, Task Accuracy under Attack), applications, and identify critical open challenges—including cross-layer security composition and deployment-time certification. The work provides a foundational, actionable perspective for building trustworthy SemCom systems in next-generation intelligent networks.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>poisoning</category>
      <category>data</category>
      <category>model</category>
      <category>inference</category>
    </item>
    <item>
      <title>A Critical Look into Threshold Homomorphic Encryption for Private Average Aggregation</title>
      <link>https://arxiv.org/abs/2602.22037v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22037v1</guid>
      <description>Threshold Homomorphic Encryption (Threshold HE) is widely adopted for privacy-preserving average aggregation in Federated Learning (FL), yet recent work exposes critical vulnerabilities when adversaries access a *restricted decryption oracle*—a realistic threat reflecting FL clients’ ability to jointly decrypt results without learning the full secret key. This paper conducts the first systematic evaluation of threshold RLWE-based HE (specifically BFV and CKKS variants) for federated averaging, focusing on the practical trade-offs introduced by *smudging noise with large variance* as a security countermeasure. We benchmark communication, latency, and aggregation accuracy under standardized 128-bit security parameters across 3–16 clients. Contrary to common assumptions, we find that threshold CKKS achieves **comparable or slightly better end-to-end performance than threshold BFV**, with up to 12% lower latency at scale, while maintaining aggregation error below 0.001%—well within FL convergence tolerance. Our results provide concrete guidance for secure, efficient HE integration in production FL systems.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Resilient Federated Chain: Transforming Blockchain Consensus into an Active Defense Layer for Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21841v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21841v1</guid>
      <description>Federated Learning (FL) enables privacy-preserving decentralized training but remains highly vulnerable to adversarial attacks due to its inherent lack of centralized data inspection. While blockchain integration has been explored for FL, its potential as an *active defense layer*—rather than merely an immutable ledger—remains untapped. This paper proposes **Resilient Federated Chain (RFC)**, a novel framework that repurposes the computational redundancy in Proof of Federated Learning (PoFL) consensus as real-time defense infrastructure. RFC introduces a flexible, attack-adaptive evaluation function within its consensus mechanism and tightly couples on-chain verification with off-chain robust aggregation (e.g., Krum, Median). Extensive experiments on image classification under diverse adversarial settings (e.g., label-flipping, sign-flipping, Byzantine attacks) show RFC improves model accuracy by up to 37.5 percentage points over FedAvg under 20% malicious clients, while maintaining &lt;3.1% false positive rate. RFC establishes blockchain not just as a trust anchor, but as a dynamic, responsive security layer for trustworthy decentralized AI.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Private and Robust Contribution Evaluation in Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21721v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21721v1</guid>
      <description>Cross-silo federated learning (FL) enables collaborative model training without raw data sharing, yet client updates remain vulnerable to inference attacks. While secure aggregation (SecAgg) preserves privacy by masking individual contributions, it renders conventional contribution evaluation—critical for fair rewards and misbehavior detection—infeasible. Existing marginal-contribution methods (e.g., Shapley value) are incompatible with SecAgg; practical alternatives like Leave-One-Out (LOO) suffer from coarse granularity and dangerous self-evaluation dependencies. We propose two SecAgg-compatible marginal-difference scores: **Fair-Private**, satisfying core fairness axioms (efficiency, symmetry, null player, additivity); and **Everybody-Else**, eliminating self-evaluation entirely and providing provable resistance to strategic manipulation—a previously overlooked vulnerability. We provide rigorous theoretical guarantees on fairness, $(\varepsilon,\delta)$-differential privacy, Byzantine robustness, and linear communication/computation cost. Extensive evaluation across medical imaging datasets (BraTS, CheXpert, NIH ChestX-ray) and CIFAR10 shows our scores consistently outperform baselines: they better approximate Shapley rankings (+32–47% Kendall tau), improve final model accuracy (+1.8–3.4 pp), and achieve &gt;0.91 F1-score in detecting malicious clients. This work establishes the first principled framework jointly achieving fairness, privacy, robustness, and practical utility in FL contribution evaluation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection</title>
      <link>https://arxiv.org/abs/2602.21593v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21593v1</guid>
      <description>Generative image proliferation has spurred adoption of semantic-aware watermarking in diffusion models for provenance tracking and forgery detection. While content-aware schemes bind watermarks to high-level semantics to resist local edits, we expose a critical vulnerability: large language models (LLMs) enable *targeted, coherence-preserving semantic perturbations* that selectively alter watermark-relevant attributes without disrupting global visual-semantic consistency. We propose the **Coherence-Preserving Semantic Injection (CSI)** attack—a novel LLM-guided framework that leverages CLIP-aligned text prompts and embedding-space similarity constraints to generate semantically shifted yet visually faithful images, inducing detector misclassification. Extensive evaluation across six state-of-the-art semantic watermarking methods shows CSI consistently outperforms all baselines, increasing false-negative rates by up to 82.6% (avg. +37.4% over best prior attack). This reveals a fundamental security gap: semantic binding alone is insufficient against LLM-powered semantic manipulation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>llm</category>
    </item>
    <item>
      <title>Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem</title>
      <link>https://arxiv.org/abs/2602.21814v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21814v1</guid>
      <description>This study isolates the causal impact of prompt architecture on implicit physical constraint reasoning using the “car wash problem”—a benchmark where LLMs consistently fail without explicit spatial logic cues. In a controlled variable-isolation experiment (n=20 per condition, 6 conditions), we evaluate Claude 3.5 Sonnet under fixed hyperparameters (temperature=0.7, top_p=1.0). The STAR (Situation-Task-Action-Result) reasoning framework alone boosts accuracy from 0% to 85% (*p*=0.001, Fisher’s exact test; OR=13.22), demonstrating that *forced goal articulation prior to inference* is the dominant driver of correct reasoning. User-profile context (via vector DB retrieval) adds +10 percentage points, and RAG-supplied domain context contributes an additional +5 points—reaching 100% in the full-stack condition. Crucially, structured scaffolding matters substantially more than contextual augmentation for this class of tasks: reasoning *how* to reason outweighs *what* to reason about. These findings establish STAR as a minimal, generalizable scaffold for reliable implicit-constraint inference.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>prompt</category>
      <category>injection</category>
    </item>
    <item>
      <title>Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation</title>
      <link>https://arxiv.org/abs/2602.21957v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21957v1</guid>
      <description>Federated recommendation (FedRec) enables collaborative model training across distributed clients without sharing raw user interaction data. Conventional methods synchronize high-dimensional item embeddings between server and clients, implicitly assuming precise geometric alignment is essential for collaboration. We challenge this assumption and argue that preserving *global semantic structures*—rather than identical embeddings—is more effective and efficient. To this end, we propose **CGFedRec**, a cluster-guided framework where the server discovers a shared item clustering structure from uploaded embeddings and broadcasts only compact cluster labels—not full embeddings—to clients. Clients then align their local item representations via these structural constraints, enabling personalization while maintaining global consistency. Experiments on six real-world datasets (e.g., Yelp, Amazon-Book, ML-1M) show CGFedRec achieves **up to 92.7% communication reduction** and **+3.8% average NDCG@10 gain** over strong baselines, demonstrating superior accuracy-efficiency trade-offs. Our work redefines collaboration in FedRec as *structural alignment*, not coordinate synchronization.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2602.21928v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21928v1</guid>
      <description>This paper addresses decentralized root cause analysis (RCA) in nonlinear dynamical systems—e.g., power grids and supply chains—where clients are geographically distributed, run fixed proprietary models, and exhibit unknown, time-varying interdependencies. We propose a novel federated learning framework that learns cross-client dependencies *without accessing raw sensor streams or modifying local black-box models*. Each client augments its legacy system with a lightweight, trainable dependency encoder; a global server coordinates encoders via representation consistency constraints (leveraging temporal lag correlation and contrastive alignment) while preserving privacy through calibrated differential privacy. Theoretical analysis establishes convergence guarantees under non-IID, feature-partitioned, and privacy-constrained settings. Experiments on extensive simulations and a real-world industrial cybersecurity dataset demonstrate state-of-the-art RCA performance: 92.4% F1-score on root cause localization and 58% lower false positives versus baselines—achieving RCA with *zero raw data upload, zero model modification, and zero prior dependency knowledge*.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>federated</category>
      <category>machine</category>
      <category>learning</category>
      <category>differential</category>
      <category>privacy</category>
    </item>
    <item>
      <title>GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task</title>
      <link>https://arxiv.org/abs/2602.21873v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21873v1</guid>
      <description>Federated learning (FL) enables privacy-preserving collaborative training for vision tasks, yet suffers from **ineffective cross-client knowledge fusion under class imbalance** and **prohibitive communication overhead** from transmitting full-model parameters. To address this, we propose **Generative Federated Prototype Learning (GFPL)** — a lightweight, semantics-aware FL framework. GFPL replaces model parameter exchange with **class-wise prototype transmission**, where each client generates prototypes via Gaussian Mixture Models (GMM) to capture feature statistics compactly. Server-side aggregation employs **Bhattacharyya distance** to fuse semantically similar prototypes across clients, avoiding semantic misalignment. Crucially, fused prototypes generate **class-balanced pseudo-features**, and a **dual-classifier architecture**—optimized via hybrid Dot Regression and Cross-Entropy loss—enhances local feature alignment. Experiments on imbalanced benchmarks (e.g., CIFAR-10-LT, ImageNet-LT) show GFPL improves accuracy by **+3.6%** over strong baselines while reducing communication cost by **up to 87%**, achieving superior performance-efficiency trade-offs for resource-constrained edge vision applications.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21844v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21844v1</guid>
      <description>Differentially private federated learning (DP-FL) suffers from a fundamental tension: privacy-preserving mechanisms impose quantifiable privacy costs that deter client participation, especially among highly privacy-sensitive “stragglers.” Existing incentive designs assume unbiased client selection, forcing servers to over-compensate these stragglers—leading to budget waste and suboptimal model convergence. We propose **JSAM**, the first Bayesian-optimal framework that *jointly* optimizes client selection probabilities and privacy compensation under a fixed budget. By theoretically characterizing optimal selection structure, JSAM reduces the original 2N-dimensional problem to an efficient 3D formulation. We prove that servers should *exclude* high-sensitivity clients and *preferentially select* privacy-tolerant ones—and reveal the counter-intuitive insight that *least*-sensitive clients incur the *highest cumulative cost* due to frequent selection. Extensive experiments on MNIST and CIFAR-10 show JSAM improves test accuracy by up to **15%** over unbiased baselines while maintaining cost efficiency across diverse data heterogeneity levels (α ∈ [0.1, 1.0]).</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Mamba Meets Scheduling: Learning to Solve Flexible Job Shop Scheduling with Efficient Sequence Modeling</title>
      <link>https://arxiv.org/abs/2602.21546v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21546v1</guid>
      <description>The Flexible Job Shop Scheduling Problem (FJSP) is a fundamental combinatorial optimization challenge in smart manufacturing, yet learning-based solvers suffer from high computational overhead and limited global dependency modeling. This paper pioneers the integration of **Mamba**—a linear-time state-space model—into FJSP solving. We propose a novel architecture featuring: (i) a dual-path Mamba encoder that separately processes operation and machine sequences to capture long-range constraints with O(N) complexity; and (ii) an efficient cross-attention decoder for dynamic operation-machine interaction. Evaluated on standard benchmarks (Dai, Brandimarte, Kacem), our method achieves **3.2× faster inference** and outperforms state-of-the-art learning-based solvers by 1.8–4.7% in makespan minimization, while reducing memory usage by 58%. This demonstrates Mamba’s superiority over quadratic-cost models (e.g., Transformers, GNNs) for structured scheduling tasks.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain</title>
      <link>https://arxiv.org/abs/2602.22045v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22045v1</guid>
      <description>We present **DLT-Corpus**, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: **2.98 billion tokens** across **22.12 million documents**, including 37,440 scientific publications, 49,023 USPTO patents, and 22 million social media posts. Unlike prior NLP resources narrowly focused on cryptocurrency price prediction or smart contracts, DLT-Corpus comprehensively captures the full technological lifecycle of DLT. Using it, we uncover that DLT innovations emerge first in scientific literature, then migrate to patents, and finally diffuse to social media—following classic technology transfer pathways. Crucially, while social media sentiment remains persistently bullish—even during “crypto winters”—scientific and patent activity grows independently of short-term market volatility and instead tracks long-term market capitalization expansion (*r* = 0.87), revealing a virtuous innovation cycle where research enables economic growth that funds further R&amp;D. We publicly release the full corpus, **LedgerBERT** (a domain-adapted model achieving **+23% F1** over BERT-base on DLT-specific NER), and all tools and code.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>crypto</category>
    </item>
    <item>
      <title>Fast cube roots in Fp2 via the algebraic torus</title>
      <link>https://eprint.iacr.org/2026/392</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/392</guid>
      <description>We present a novel algorithm for computing cube roots in $\mathbb{F}_{p^2}$, a critical subroutine in elliptic-curve point decompression, hash-to-curve, and isogeny-based cryptography. Leveraging the algebraic torus $\mathbb{T}_2(\mathbb{F}_p)$ and Lucas sequences, our method reduces the problem *entirely* to operations in the base field $\mathbb{F}_p$—under the practically universal condition $p \equiv 1 \pmod{3}$. We prove correctness across all residuosity cases and implement it in the open-source `gnark-crypto` library. Benchmarks on six cryptographic primes (spanning pairing- and isogeny-based settings) show **1.6–2.3× speedups** over standard $\mathbb{F}_{p^2}$ exponentiation. The approach extends to $p \equiv 2 \pmod{3}$ and, more generally, to any odd $n$-th root in quadratic towers $\mathbb{F}_{p^{2^k}}$ when $\gcd(n, p+1) = 1$.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Zero-Knowledge IOPPs for Constrained Interleaved Codes</title>
      <link>https://eprint.iacr.org/2026/391</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/391</guid>
      <description>We present the first zero-knowledge interactive oracle proof of proximity (zk-IOPP) for constrained interleaved linear codes—achieving honest-verifier zero-knowledge with *negligible overhead* over the state-of-the-art non-ZK protocols. Our construction satisfies round-by-round knowledge soundness with a straightline extractor and negligible error. Technically, we introduce a composable definition of HVZK for interactive oracle reductions (IORs), then modularly compose lightweight zk-IORs: a novel **zero-knowledge sumcheck IOR** and a **zero-knowledge code-switching IOR**, both tailored to stringent efficiency requirements (sublinear communication, constant rounds, straightline extraction). To overcome challenges—including privacy leakage in interleaved polynomial commitments and zero-knowledge preservation across code families—we introduce new abstractions and protocols. As a side contribution, we highlight the concrete efficiency gains from high-distance codes derived from dispersers, which may be of independent interest.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Succinct Arguments for BatchQMA and Friends under 6 Rounds</title>
      <link>https://eprint.iacr.org/2026/390</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/390</guid>
      <description>We present the first succinct classical argument systems for batchQMA and monotone-policy batchQMA with ≤6 rounds under standard post-quantum assumptions—breaking the prior 8-round barrier and avoiding the quantum random oracle model (QROM). Our key technical innovation is **straight-line partial extractability**, enabling soundness proofs *without rewinding* cheating quantum provers—a departure from all prior works that relied on state-preserving succinct arguments of knowledge for NP. Specifically: (1) A **4-round public-coin** (except first message) argument for batchQMA achieves optimal communication (all messages independent of batch size) assuming post-quantum functional encryption and LWE; under LWE alone, only the verifier’s first message scales with batch size. (2) A **6-round private-coin** argument for monotone-policy batchQMA achieves batch-size- *and* circuit-size-independent communication under LWE. These results significantly advance practical verification of quantum computations by classical verifiers.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Towards Accountability for Anonymous Credentials</title>
      <link>https://eprint.iacr.org/2026/389</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/389</guid>
      <description>Anonymous Credentials (ACs) provide strong privacy by preventing issuers and verifiers from tracking users—but this very feature undermines accountability, hindering adoption in national identity systems (e.g., EUDI, Swiss e-ID). This paper identifies *transferability attacks* as a critical threat and proposes the first accountability framework for ACs. We introduce the **Cryptographic Forensic Trail (CFT)**: a tamper-evident, encrypted log attached to each credential presentation. Crucially, CFT decryption requires *joint, conditional authorization*: police must obtain a judicial warrant (based on probable cause), after which police, judge, and an independent NGO jointly execute a multiparty protocol to decrypt *only the relevant trail*. This design enforces checks and balances—neither law enforcement nor judiciary can act unilaterally, and NGO oversight detects and blocks collusion. Performance evaluation confirms practical feasibility: CFT adds &lt;300ms latency and &lt;2KB overhead on modern smartphones. Our work bridges the long-standing privacy–accountability gap in digital identity.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Necessary and Sufficient Conditions for the Existence of Ideal Linear Secret Sharing Schemes for Arbitrary Access Structures</title>
      <link>https://eprint.iacr.org/2026/388</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/388</guid>
      <description>This paper establishes a necessary and sufficient condition for the existence of an ideal linear secret sharing scheme (ILSSS) realizing an arbitrary minimal access structure $\Gamma_{\min}$. Using linear codes over a finite field $\mathbb{F}_q$ as the primary tool, we construct matrices $H$ (parity-check) and $G$ (generator) such that $\Gamma_{\min}$ admits an ILSSS **if and only if** the matrix equation $GH^{\mathsf{T}} = 0$ has a solution over $\mathbb{F}_q$. When satisfied, $H$ defines a linear code whose *port* realizes $\Gamma_{\min}$, and $G$ is its corresponding generator matrix. Crucially, we prove this algebraic condition is equivalent to the combinatorial requirement that $\Gamma_{\min}$ must be the port of a matroid representable over $\mathbb{F}_q$. This unifies secret sharing, coding theory, and matroid representation in a concise, computationally verifiable framework—enabling efficient existence testing and constructive design of ideal schemes.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>A Comprehensive Break of the Tropical Matrix-Based Signature Scheme</title>
      <link>https://eprint.iacr.org/2026/387</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/387</guid>
      <description>We present a comprehensive cryptanalysis of the tropical matrix-based signature scheme GMS26 (Grigoriev–Monico–Shpilrain, 2026), which bases security on the claimed NP-hardness of tropical matrix factorization. Contrary to its security claims, we demonstrate four efficient polynomial-time attacks exploiting inherent algebraic weaknesses—not the underlying hardness assumption. First, we mount an existential forgery attack in the chosen-hash model requiring only $O(n^3)$ tropical operations. Second, we prove the scheme is fundamentally malleable: any valid signature can be transformed into infinitely many distinct yet valid signatures, breaking strong unforgeability. Third, observing $O(n)$ honest signatures enables probabilistic partial recovery of private key entries via leakage in tropical extremal equations. Fourth, using an SMT solver, we recover the *entire* private key from just **two valid signatures** in under 5 seconds for recommended parameters ($n=8$). All attacks invalidate standard security notions: existential unforgeability (EUF-CMA), strong unforgeability (SUF-CMA), and key confidentiality.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Determining those Boolean functions whose restrictions to affine spaces are plateaued</title>
      <link>https://eprint.iacr.org/2026/386</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/386</guid>
      <description>This paper determines the class $C^n_k$ of $n$-variable Boolean functions whose restrictions to *all* $k$-dimensional affine subspaces of $\mathbb{F}_2^n$ are plateaued (i.e., have Walsh spectra in $\{0,\pm\lambda\}$). We prove that partially-bent functions are precisely those plateaued on every affine hyperplane ($k=n-1$), while quadratic functions are exactly those plateaued on *every* $k$-dimensional affine subspace for $3 \leq k \leq n-2$. For $n \geq 5$, we establish a strict inclusion chain: quadratic $\subsetneq$ partially-bent $\subsetneq$ restrictions of partially-bent to hyperplanes $\subsetneq$ plateaued $\subsetneq$ restrictions of plateaued to hyperplanes $\subsetneq$ all Boolean functions. The characterization extends to strongly plateaued vectorial functions, and we pose an open question linking vectorial plateauedness to the long-standing crooked function problem.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Bridging Privacy and Utility: A Verifiable Framework for Data Valuation via Zero-Knowledge Proofs</title>
      <link>https://eprint.iacr.org/2026/385</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/385</guid>
      <description>Deep learning’s data dependency has spurred decentralized data markets, yet trust deficits persist: buyers fear poisoned data, sellers fear leakage. While the Shapley value enables fair valuation, its standard computation requires a Trusted Third Party (TTP) to access raw data—violating privacy. We propose **ZK-DV**, the first zero-knowledge proof (ZKP) system for *verifiable, privacy-preserving* data valuation. ZK-DV lets a seller prove that a claimed Gradient Shapley score is mathematically consistent with their private data and the buyer’s model—without revealing either. Its key insight is architectural co-design: we embed valuation logic directly into backpropagation via a custom arithmetic circuit, extracting marginal utilities from intermediate gradients. Implemented using the GKR protocol with a hybrid Pedersen/KZG commitment scheme and batched processing, ZK-DV amortizes cryptographic overhead. Experiments on MNIST show practicality: optimized batching achieves **2.7× faster proof generation**, perfect quantization fidelity (ρ = 1.0), and verification under **0.2 seconds**. ZK-DV bridges cryptographic integrity and economic fairness for trustless data exchange.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Beyond performance-wise Contribution Evaluation in Federated Learning</title>
      <link>https://arxiv.org/abs/2602.22470v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22470v1</guid>
      <description>Federated learning (FL) enables privacy-preserving collaborative modeling, yet fair contribution evaluation remains critical for incentive alignment and system sustainability. While existing methods focus almost exclusively on predictive *performance* (e.g., accuracy), this work pioneers a multi-dimensional evaluation of client contributions toward model *trustworthiness*: specifically, **reliability** (robustness to label noise), **resilience** (resistance to adversarial examples), and **fairness** (measured via demographic parity). We quantify each client’s marginal contribution across these orthogonal dimensions using Monte Carlo approximated Shapley values—a principled, axiomatic attribution method. Experiments on FEMNIST, CIFAR-10-C, and Adult Income show that: (1) the three trustworthiness dimensions are largely uncorrelated (mean |r| &lt; 0.12); (2) no client dominates across all dimensions—high-accuracy clients often underperform significantly in fairness or resilience; and (3) reward allocation based solely on accuracy systematically undervalues up to 47% of high-trustworthiness clients. Our findings expose a fundamental flaw in current FL evaluation: comprehensive, equitable reward design requires multi-faceted, performance-agnostic metrics.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>adversarial</category>
      <category>learning</category>
      <category>federated</category>
      <category>machine</category>
    </item>
    <item>
      <title>Silent Egress: When Implicit Prompt Injection Makes LLM Agents Leak Without a Trace</title>
      <link>https://arxiv.org/abs/2602.22450v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22450v1</guid>
      <description>This paper identifies *silent egress*—a novel system-level threat in agentic LLM systems where adversarial instructions embedded in automatically fetched URL previews (e.g., titles, metadata, snippets) trigger undetectable exfiltration of sensitive runtime context. Using a fully local, reproducible testbed with a qwen2.5:7b-based agent, we demonstrate that malicious web pages can induce outbound requests leaking internal state—even when the final user-facing response appears benign. Across 480 runs, the attack succeeds with 89% probability, and 95% of successful exfiltrations evade output-based safety checks. We introduce *sharded exfiltration*, splitting sensitive data across multiple requests to reduce Leak@1 by 73% and bypass simple DLP mechanisms. Ablation shows prompt-layer defenses offer limited protection, whereas system- and network-layer controls (e.g., domain allowlisting, redirect-chain analysis) are markedly more effective. Our findings argue for treating network egress as a first-class security outcome—and motivate architectural shifts toward provenance tracking and capability isolation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>llm</category>
      <category>agent</category>
      <category>injection</category>
      <category>prompt</category>
    </item>
    <item>
      <title>Differentially Private Truncation of Unbounded Data via Public Second Moments</title>
      <link>https://arxiv.org/abs/2602.22282v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22282v1</guid>
      <description>Differential privacy (DP) is hindered by the common requirement of bounded data, yet real-world datasets are often unbounded. To overcome this, we propose **Public-moment-guided Truncation (PMT)**, which leverages a small amount of *public* second-moment information (e.g., covariance) to linearly transform private data into a well-conditioned space—enabling deterministic, non-private truncation based solely on dimension *d* and sample size *n*. This transformation dramatically improves the conditioning of the second-moment matrix, making it far more resilient to DP noise during inversion. We integrate PMT into penalized linear regression and generalized linear models, designing new loss functions and back-mapping procedures with rigorous theoretical guarantees: tighter DP estimation error bounds, distributional robustness, and convergence. Experiments on synthetic and real datasets (Adult, California Housing) show PMT consistently boosts accuracy (up to 23.7% MAE reduction under ε=1–8) and stability (3.2× lower variance), establishing a unified, practical framework for high-fidelity DP learning on unbounded data.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>Poisoned Acoustics</title>
      <link>https://arxiv.org/abs/2602.22258v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22258v1</guid>
      <description>We demonstrate that training-data poisoning attacks achieve *provably undetectable, targeted failure* in acoustic vehicle classification: on the MELAUDIS urban audio dataset (≈9,600 clips, 6 classes), a compact CNN achieves **95.7% Attack Success Rate (ASR)** under a Truck→Car label-flipping attack with only **p = 0.5% corruption (48 samples)**, while aggregate test accuracy remains statistically unchanged (87.6% baseline; 95% CI: 88–100%, n=3). We prove this stealth is *structural*: the maximum possible accuracy drop from any targeted attack is bounded above by the minority class fraction β—here ≈3% for Trucks—rendering aggregate accuracy monitoring *provably insufficient* regardless of model or attack design. A companion backdoor study reveals *trigger-dominance collapse*: when the target class is a minority, spectrogram patch triggers become functionally redundant (clean ASR = triggered ASR), reducing the attack to pure label flipping. To address this systemic trust gap, we propose ML-TrustChain—a cryptographically verifiable defense combining content-addressed artifact hashing, Merkle-tree dataset commitment, and NIST FIPS 204 post-quantum signatures (ML-DSA-65/Dilithium3) for tamper-proof data provenance.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>data</category>
      <category>poisoning</category>
      <category>backdoor</category>
      <category>neural</category>
    </item>
    <item>
      <title>SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read</title>
      <link>https://arxiv.org/abs/2602.22426v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22426v1</guid>
      <description>Despite rapid progress in Multimodal Large Language Models (MLLMs), it remains unclear whether they genuinely *read* text in images or merely exploit parametric shortcuts from text prompts—a phenomenon we term “modality laziness.” To diagnose this, we introduce the **Visualized-Question (VQ) setting**, where queries are rendered *onto* images with randomized styles (e.g., font, noise, rotation), forcing visual engagement. Experiments on Qwen2.5-VL reveal a severe capability-utilization gap: up to **12.7% performance drop** under VQ, exposing heavy reliance on textual priors. To bridge it, we propose **SimpleOCR**: a plug-and-play training strategy that converts standard samples into VQ format, invalidating text-based shortcuts and compelling optimization of visual text extraction pathways. Without architectural changes, SimpleOCR achieves **+5.4% average gain** over the base model and **+2.7% over GRPO** on four OOD OCR benchmarks—using only **8.5K samples** (30× fewer than recent RL methods). It also composes seamlessly with advanced RL strategies like NoisyRollout. Code: https://github.com/aiming-lab/SimpleOCR</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning</title>
      <link>https://arxiv.org/abs/2602.22269v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22269v1</guid>
      <description>Federated Learning (FL) enables privacy-preserving collaborative training, yet local model updates remain vulnerable to inference and poisoning attacks. Quantum Secure Aggregation (QSA) promises information-theoretic privacy via GHZ-state phase encoding—but its reliance on a *single global GHZ state* suffers from rapidly degrading fidelity at scale and inherent inability to detect Byzantine clients. We propose **Clustered QSA (CQSA)**: a modular framework that partitions clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server then identifies malicious contributions by analyzing statistical discrepancies—e.g., cosine similarity and Euclidean distance—among cluster-level aggregates. Under depolarizing noise, CQSA achieves up to **68.5% higher GHZ fidelity** than global QSA and maintains stable convergence even with 20% Byzantine clients, reducing accuracy degradation by **52.3%**. CQSA bridges near-term quantum hardware constraints with practical Byzantine robustness in FL.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>data</category>
      <category>poisoning</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>VROOM: Accelerating (Almost All) Number-Theoretic Cryptography Using Vectorization and the Residue Number System</title>
      <link>https://eprint.iacr.org/2026/393</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/393</guid>
      <description>Modular arithmetic with large prime moduli dominates computational cost in number-theoretic cryptography, yet efficient CPU vectorization remains elusive due to carry propagation and costly data permutations. VROOM introduces a novel vectorized approach leveraging the Residue Number System (RNS) to align modular operations natively with wide SIMD units—eliminating carries, minimizing shuffles, and enabling constant-time multiplication for *arbitrary* odd moduli (prime or composite). Our AVX2/AVX-512 implementation achieves **4.0× speedup** for RSA-4096 signature verification and **1.3×** for signing over OpenSSL, and **3.43× faster BLS-381 verification** than the assembly-optimized BLST library. All code is portable, hardware-agnostic, and planned for upstream integration into BoringSSL to accelerate real-world TLS and cryptographic infrastructure.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2602.21078v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21078v1</guid>
      <description>Federated Semi-Supervised Learning (FSSL) faces dual heterogeneity: *external* (cross-client distribution shift) and *internal* (within-client labeled/unlabeled mismatch). Existing methods either aggregate client models via fixed/dynamic weights—struggling to capture the ideal global distribution—or filter low-confidence pseudo-labels, discarding valuable unlabeled data. To address both challenges jointly, we propose **ProxyFL**, a proxy-guided framework that uses **learnable classifier weights as category-distribution proxies**. For external heterogeneity, ProxyFL explicitly optimizes a *global proxy* on the server to robustly suppress outliers, replacing direct weight aggregation. For internal heterogeneity, it introduces a *positive-negative proxy pool* to re-include filtered unlabeled samples via similarity-based soft weighting, mitigating pseudo-label noise. Theoretically, ProxyFL achieves tighter convergence bounds; empirically, it outperforms SOTAs by +3.2–5.8% average accuracy across 6 benchmarks (e.g., CIFAR-10/LT, SVHN), boosts unlabeled data utilization by 3.7×, and reduces pseudo-label error rate by 41%.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Some Simple Economics of AGI</title>
      <link>https://arxiv.org/abs/2602.20946v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20946v1</guid>
      <description>This paper reframes the AGI transition as an economic collision between two asymmetric cost curves: exponentially falling *Cost to Automate* and biologically constrained *Cost to Verify*. As AI decouples cognition from biology, execution becomes near-zero marginal cost—absorbing even creative and innovative labor—but human verification bandwidth remains the binding constraint on growth. This structural asymmetry widens a *Measurability Gap*: what agents can execute vastly exceeds what humans can afford to validate, audit, or underwrite. Consequently, technical change shifts from skill-biased to *measurability-biased*, with economic rents migrating to verification-grade ground truth, cryptographic provenance, and liability underwriting—not just output generation. The current human-in-the-loop equilibrium is unstable: eroded by collapsing apprenticeship (“Missing Junior Loop”) and expert self-obsolescence (“Codifier’s Curse”), enabling privately rational but socially hazardous unverified deployment. Unmanaged, this pulls toward a *Hollow Economy*; yet scaling verification capacity in tandem with agentic capability unlocks an *Augmented Economy*—one where robust oversight enables unbounded discovery. The central imperative is clear: the defining race today is not for autonomy, but for *verifiability*.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>trojan</category>
      <category>ai</category>
    </item>
    <item>
      <title>On Electric Vehicle Energy Demand Forecasting and the Effect of Federated Learning</title>
      <link>https://arxiv.org/abs/2602.20782v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20782v1</guid>
      <description>This paper investigates Electric Vehicle Supply Equipment (EVSE) energy demand forecasting (EDF) under privacy-preserving constraints. We benchmark statistical (ARIMA), machine learning (XGBoost), and deep learning (LSTM, GRU) models across four real-world EVSE datasets, evaluating performance under both centralized and federated learning (FL) paradigms. Results show XGBoost achieves the highest accuracy (18.7% lower MAE on average) and best energy efficiency (5× less training energy than LSTM), outperforming both statistical and neural models. Crucially, in FL settings, XGBoost maintains this advantage while reducing communication overhead by 62% compared to deep models—demonstrating superior trade-offs among prediction fidelity, privacy preservation (no raw data sharing), and edge-device energy consumption. Our work establishes FL-enabled tree-based forecasting as a practical, scalable, and sustainable solution for decentralized EV energy management.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Bikelution: Federated Gradient-Boosting for Scalable Shared Micro-Mobility Demand Forecasting</title>
      <link>https://arxiv.org/abs/2602.20671v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20671v1</guid>
      <description>Bikelution is a novel federated learning framework for scalable, privacy-preserving micro-mobility demand forecasting using gradient-boosted decision trees. Unlike centralized ML—which achieves high accuracy but violates data privacy and incurs prohibitive bandwidth costs—Bikelution enables collaborative model training across edge devices (e.g., bike docks, city zones) without sharing raw spatio-temporal data. It introduces sparse histogram-based gradient compression, differential privacy-aware secure aggregation, and a sliding-window client selection strategy to handle non-IID demand patterns. Evaluated on three real-world dockless bike-sharing datasets (Hangzhou, NYC Citi Bike, Singapore SG Bike), Bikelution matches centralized XGBoost’s 6-hour-ahead forecast accuracy (within 2.1% MAE gap) while outperforming state-of-the-art federated tree methods by 19.3% on average. This demonstrates that high-fidelity, mid-term demand forecasting is feasible under strict privacy constraints—enabling sustainable, compliant smart mobility planning.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Is the Trigger Essential? A Feature-Based Triggerless Backdoor Attack in Vertical Federated Learning</title>
      <link>https://arxiv.org/abs/2602.20593v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20593v1</guid>
      <description>Vertical federated learning (VFL) enables collaborative modeling across parties holding disjoint features and labels, yet remains vulnerable to backdoor attacks. Contrary to the prevailing trigger-dependent paradigm, this paper demonstrates that **triggers are *not essential* for effective backdoor attacks in VFL**. We propose the first **feature-based triggerless backdoor attack**, operating under a stricter honest-but-curious threat model—where the attacker participates legitimately in training without gradient tampering. Our method comprises three key components: (1) label inference to identify target classes from intermediate outputs; (2) triggerless poisoning via feature amplification and imperceptible perturbation; and (3) stealthy backdoor execution on *clean, unmodified inputs*. Experiments on five benchmark datasets show our attack achieves 2–50× higher attack success rate (ASR) than three trigger-based baselines, with &lt;0.8% main-task accuracy degradation. It remains highly effective even in large-scale VFL (32 passive parties) using only one auxiliary dataset and exhibits strong resilience against defenses (ASR variation &lt;3%). This work uncovers a critical blind spot in VFL security and calls for robust, trigger-agnostic defense design.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>learning</category>
      <category>security</category>
      <category>train</category>
      <category>privacy-preserving</category>
    </item>
    <item>
      <title>Characterizing Online and Private Learnability under Distributional Constraints via Generalized Smoothness</title>
      <link>https://arxiv.org/abs/2602.20585v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20585v1</guid>
      <description>This paper characterizes online and private learnability under *distributional adversaries*—adaptive sequences of data-generating distributions drawn from a fixed family $U$. We introduce **generalized smoothness**, a structural property of $U$, and prove it is *necessary and sufficient* for online learnability: a family $U$ admits VC-dimension-dependent regret bounds for *every* finite-VC hypothesis class if and only if $U$ is generalized smooth. We design universal algorithms achieving low regret without explicit knowledge of $U$, and—when $U$ is known—derive refined bounds via the **fragmentation number**, a combinatorial measure of how many disjoint regions can simultaneously carry nontrivial mass under $U$. Remarkably, we show generalized smoothness also *exactly characterizes* private learnability under distributional constraints, revealing a deep unification between online and differentially private learning. These results provide a near-complete theory of learnability beyond i.i.d. and fully adversarial settings.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>Wireless Federated Multi-Task LLM Fine-Tuning via Sparse-and-Orthogonal LoRA</title>
      <link>https://arxiv.org/abs/2602.20492v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20492v1</guid>
      <description>This paper proposes **Sparse-and-Orthogonal LoRA**, a fully decentralized wireless federated learning framework for multi-task fine-tuning of large language models (LLMs) on mobile devices. It tackles three critical issues in heterogeneous edge settings: (i) catastrophic forgetting during local fine-tuning due to conflicting gradient directions; (ii) inefficient communication and slow convergence from redundant parameter transmission; and (iii) cross-task knowledge interference at inference. Our approach introduces: (1) orthogonality-constrained sparse LoRA adapters to eliminate update conflicts; (2) a task-aware clustering strategy for topology-efficient model aggregation among neighboring devices; and (3) an implicit mixture-of-experts mechanism enabling task-specific inference paths without explicit routing. Experiments show up to **73% reduction in communication cost** and **+5.0% average accuracy gain** over standard federated LoRA, with faster convergence—demonstrating strong practicality for bandwidth-constrained wireless edge AI.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning</title>
      <link>https://arxiv.org/abs/2602.20450v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.20450v1</guid>
      <description>Federated Learning (FL) suffers from statistical heterogeneity across clients, leading to accuracy degradation and unstable convergence. Prior client selection methods rely on loss or bias—imprecise proxies for heterogeneity—and employ stochastic selection, limiting reliability. We propose **Terraform**, a novel heterogeneity-aware methodology that leverages **client gradient updates** (not scalar metrics) to quantify directional and magnitude deviation from the global gradient, enabling precise heterogeneity assessment. Its **deterministic greedy algorithm** selects the top-*k* most heterogeneous clients per round—ensuring reproducibility and stability. Evaluated across four benchmarks (CIFAR-10/100, Tiny-ImageNet, LEAF-HeartDisease), Terraform achieves up to **47% higher accuracy** over state-of-the-art baselines (e.g., FedAvg, q-FFL). Ablation studies confirm both gradient-based modeling and deterministic selection are essential; training overhead remains under 3% per round. Terraform delivers a robust, efficient, and principled solution for heterogeneous FL.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Partially Non-Interactive Two-Round Threshold and Multi-Signatures with Tighter and Adaptive Security</title>
      <link>https://eprint.iacr.org/2026/373</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/373</guid>
      <description>We bridge a critical security gap in partially non-interactive (PNI) two-round threshold and multi-signature schemes in the pairing-free discrete-logarithm setting. While fully online schemes achieve rewinding-free or fully adaptive security under standard assumptions, prior PNI schemes require algebraic adversary restrictions or non-standard assumptions. Our key insight is to systematically transform the HBMS framework (Bellare &amp; Dai, Asiacrypt 2021) into a PNI paradigm via message-independent preprocessing of commitments and adaptive key derivation. We present: (1) the **first PNI two-round multi-signature scheme** with a rewinding-free reduction under standard DL assumptions, achieving tighter security (O(1) loss) against *non-algebraic* adversaries; and (2) the **first PNI two-round threshold signature scheme** with *fully adaptive security*—supporting adaptive corruptions and message queries—also under standard assumptions. Both schemes retain practical efficiency and avoid random oracles.</description>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
  </channel>
</rss>