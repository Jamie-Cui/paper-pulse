<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Paper Pulse</title>
    <link>https://jamie-cui.github.io/paper-pulse</link>
    <description>Keyword-based research paper aggregation from arXiv and IACR</description>
    <lastBuildDate>Mon, 02 Mar 2026 02:30:13 -0000</lastBuildDate>
    <atom:link href="https://jamie-cui.github.io/paper-pulse/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On the Need for (Quantum) Memory with Short Outputs</title>
      <link>https://eprint.iacr.org/2026/403</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/403</guid>
      <description>We establish the first unconditional separation between bounded- and unbounded-space computation for problems with *short outputs*—where output size is logarithmic while memory may be exponential—in both classical and quantum models. To achieve this, we introduce the **nested collision finding** problem: given oracles for random functions $f$ and $g$, find distinct $x \neq y$ such that $g(f(x)) = g(f(y))$. We prove that any algorithm solving it in $T$ queries using only $S$ bits of memory must satisfy $T \cdot S = \Omega(2^{n/3})$, whereas an unbounded-memory algorithm achieves the optimal $T = O(2^{n/3})$. Our lower bound hinges on a novel **two-oracle recording technique**, which uses one oracle to implicitly “record” long intermediate outputs (e.g., $f(x)$) generated under the other, thereby reducing the short-output time–space trade-off to that of a corresponding long-output problem. This technique is broadly applicable to memory-sensitive settings beyond collision-finding.</description>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Jailbreak Foundry: From Papers to Runnable Attacks for Reproducible Benchmarking</title>
      <link>https://arxiv.org/abs/2602.24009v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.24009v1</guid>
      <description>Jailbreak Foundry (JBF) is a reproducible benchmarking system that bridges the widening gap between rapidly evolving LLM jailbreak techniques and stagnant evaluation infrastructure. It introduces a multi-agent workflow—JBF-FORGE—to automatically translate jailbreak papers into standardized, executable modules, backed by JBF-LIB (shared utilities &amp; contracts) and JBF-EVAL (unified, GPT-4o–judged evaluation). Across 30 reproduced attacks, JBF achieves high fidelity with a mean ASR deviation of only +0.26 percentage points versus reported results. It cuts attack-specific implementation code by nearly half and attains an 82.5% mean code reuse ratio. Crucially, JBF enables the first standardized AdvBench evaluation of all 30 attacks across 10 victim models under consistent judging—transforming benchmarks from static snapshots into scalable, living evaluations that evolve with the threat landscape.</description>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>agent</category>
    </item>
    <item>
      <title>MemEmo: Evaluating Emotion in Memory Systems of Agents</title>
      <link>https://arxiv.org/abs/2602.23944v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23944v1</guid>
      <description>MemEmo introduces the first benchmark dedicated to evaluating how memory systems in LLM-based agents process, update, and reason over emotional information—a critical gap given humans’ emotion-embedded memory cognition. We propose the **HLME dataset** (Human-Like Memory Emotion), assessing systems across three dimensions: (1) *emotional information extraction*, (2) *emotional memory updating*, and (3) *emotional memory question answering*. Experiments on 8 state-of-the-art memory architectures—including retrieval-augmented, episodic, semantic, and hybrid designs—reveal that **no system achieves robust performance across all three tasks** (average F1 &lt; 62%; inter-task standard deviation &gt; 18%). Performance degrades notably in dynamic emotion updating and affective reasoning, exposing fundamental limitations in current memory design. MemEmo provides an objective, reproducible evaluation framework and a clear roadmap for emotion-aware memory optimization.</description>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>NAU-QMUL: Utilizing BERT and CLIP for Multi-modal AI-Generated Image Detection</title>
      <link>https://arxiv.org/abs/2602.23863v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23863v1</guid>
      <description>We propose **NAU-QMUL**, a novel multi-modal multi-task framework for detecting AI-generated images and identifying their underlying generative models (e.g., SDXL, DALL·E 3, MidJourney v6). It jointly leverages frozen **BERT** to encode textual prompts and **CLIP-ViT/L-14** to extract visual features, followed by cross-modal attention fusion and a custom multi-task loss combining focal loss (for binary detection) and KL divergence (for fine-grained model attribution). To address data scarcity, we introduce a confidence-thresholded pseudo-labeling strategy that expands training data with high-quality synthetic labels. Evaluated on the CT2 competition benchmark, NAU-QMUL achieves **5th place in both Task A (F1 = 83.16%) and Task B (F1 = 48.88%)**, demonstrating state-of-the-art performance in real-world AI image attribution. The code is publicly available at https://github.com/xxxxxxxxy/AIGeneratedImageDetection.</description>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>TRIZ-RAGNER: A Retrieval-Augmented Large Language Model for TRIZ-Aware Named Entity Recognition in Patent-Based Contradiction Mining</title>
      <link>https://arxiv.org/abs/2602.23656v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23656v1</guid>
      <description>TRIZ-based contradiction mining is essential for systematic innovation but remains challenging due to semantic ambiguity in patent language and the lack of reliable TRIZ knowledge grounding in LLMs. To address this, we propose **TRIZ-RAGNER**, a retrieval-augmented LLM framework that reformulates contradiction mining as a TRIZ-aware named entity recognition (NER) task. It integrates dense retrieval over a structured TRIZ knowledge base, cross-encoder reranking for contextual refinement, and domain-informed prompting to extract *improving* and *worsening* technical parameters from patent sentences. Evaluated on the PaTRIZ dataset, TRIZ-RAGNER achieves **85.6% precision, 82.9% recall, and 84.2% F1-score**, outperforming strong LLM and traditional NER baselines by up to **+7.3 F1 points**. This demonstrates that retrieval-augmented TRIZ knowledge injection significantly enhances accuracy, consistency, and interpretability in patent-based contradiction analysis.</description>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling</title>
      <link>https://arxiv.org/abs/2602.23262v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23262v1</guid>
      <description>We propose a spectral differential privacy (DP) framework for private image generation, grounded in the insight that privacy-sensitive information resides predominantly in low-frequency wavelet components (e.g., global shapes and facial structures), while high-frequency details are largely generic and public. Our coarse-to-fine two-stage approach first applies DP fine-tuning *only* to an autoregressive spectral tokenizer trained on low-resolution wavelet coefficients (LL subband), concentrating the entire privacy budget on structural fidelity. Then, a publicly pretrained super-resolution model upsamples the DP-encoded coarse representation—leveraging DP’s post-processing property to refine textures without additional privacy cost. Experiments on MS-COCO and MM-CelebA-HQ show our method achieves superior image quality (12–28% lower FID) and semantic alignment (9–15% higher CLIP Score) versus state-of-the-art DP generative models under ε = 2–8, demonstrating that frequency-aware budget allocation breaks the utility–privacy trade-off bottleneck.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)</title>
      <link>https://arxiv.org/abs/2602.23167v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23167v1</guid>
      <description>SettleFL is a trustless, scalable reward settlement protocol for federated learning (FL) on permissionless blockchains. It resolves the fundamental tension between FL’s high-frequency iterative nature and the prohibitive on-chain cost of permissionless ledgers by introducing two interoperable, circuit-based settlement strategies: (1) *Commit-and-Challenge*, an optimistic design minimizing average gas usage via dispute-driven arbitration; and (2) *Commit-with-Proof*, ensuring instant finality per round via succinct zero-knowledge validity proofs. Both leverage a shared domain-specific zk-SNARK circuit architecture and enforce rational robustness—without trusted coordinators—through game-theoretic incentives. Extensive experiments integrating real FL workloads (CIFAR-10, EMNIST) and large-scale simulations demonstrate SettleFL’s practicality: it scales to 800 participants with sub-second settlement latency and reduces average gas cost by 87% (to ~\$0.14 per round), outperforming prior decentralized approaches.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification</title>
      <link>https://arxiv.org/abs/2602.22724v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22724v1</guid>
      <description>Large language model (LLM) agents face growing threats from **indirect prompt injection (IPI)**—a stealthy, multi-turn attack where malicious context embedded in tool outputs or retrieved documents hijacks agent behavior over time. Existing inference-time defenses rely on heuristics and conservative blocking, harming utility under ambiguity. We propose **AgentSentry**, the first framework to model IPI as a *temporal causal takeover*. It localizes takeover points via *controlled counterfactual re-executions* at tool-return boundaries and enables safe continuation through *causally guided context purification*—removing only attack-induced deviations while preserving task-critical evidence. Evaluated across 4 task suites, 3 IPI attack families, and multiple black-box LLMs on AgentDojo, AgentSentry achieves **100% attack blocking**, an average **Utility Under Attack (UA) of 74.55%**, and **no degradation on benign inputs**—outperforming strongest baselines by +20.8–33.6 percentage points in UA.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>injection</category>
      <category>prompt</category>
    </item>
    <item>
      <title>DPSQL+: A Differentially Private SQL Library with a Minimum Frequency Rule</title>
      <link>https://arxiv.org/abs/2602.22699v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22699v1</guid>
      <description>DPSQL+ is a novel differentially private SQL library that jointly enforces user-level $(\varepsilon,\delta)$-differential privacy and the minimum frequency rule (requiring ≥$k$ distinct individuals per released group). Its modular architecture comprises a static **Validator** restricting queries to a DP-safe SQL subset, a consistent **Accountant** tracking cumulative privacy loss across adaptive queries using Rényi DP, and an extensible **Backend** supporting PostgreSQL, SQLite, and Pandas. Crucially, DPSQL+ integrates the $k$-anonymity constraint directly into the DP noise injection pipeline—applying calibrated Laplace/Gaussian noise *then* enforcing $k$-thresholding with bias-corrected rescaling. Experiments on TPC-H show DPSQL+ achieves superior accuracy across aggregates, quadratic statistics (e.g., variance), and join queries, while enabling **2.1–3.8× more queries** under a fixed global budget ($\varepsilon=1.0, \delta=10^{-5}$) compared to prior libraries. It bridges formal privacy theory and real-world governance requirements.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>membership</category>
      <category>dp</category>
      <category>inference</category>
      <category>privacy</category>
      <category>differential</category>
    </item>
    <item>
      <title>No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings</title>
      <link>https://arxiv.org/abs/2602.22689v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22689v1</guid>
      <description>Latent diffusion models (LDMs) excel at text-to-image generation but risk memorizing training data, threatening privacy and IP rights. Membership inference attacks (MIAs) audit such memorization—but existing methods require ground-truth captions, failing in realistic caption-unavailable settings where VLM-generated captions yield poor performance. We propose **MoFit**, the first caption-free MIA framework. It bypasses textual supervision by optimizing a perturbation to construct a *model-fitted surrogate image* that lies in the unconditional prior manifold learned from member samples; then extracts a *model-fitted embedding* from this surrogate to serve as a mismatched condition for the query image—amplifying conditional loss for members while minimally affecting non-members. Extensive experiments across LAION, ImageNet, and models including Stable Diffusion v1.5 and SDXL show MoFit consistently outperforms VLM-conditioned baselines and achieves performance on par with caption-dependent state-of-the-art methods.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>extraction</category>
      <category>membership</category>
      <category>model</category>
    </item>
    <item>
      <title>Systems-Level Attack Surface of Edge Agent Deployments on IoT</title>
      <link>https://arxiv.org/abs/2602.22525v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22525v1</guid>
      <description>This paper presents the first empirical security analysis of LLM agent deployments on resource-constrained IoT edge devices. We evaluate three architectures—cloud-hosted, edge-local swarm, and hybrid—using a multi-device home-automation testbed with local MQTT and an Android smartphone as an edge inference node. We identify five systems-level attack surfaces, including two emergent failures observed *in vivo*: **coordination-state divergence** (causing unresolvable control conflicts) and **induced trust erosion** (where malicious nodes trigger legitimate agents’ erroneous degradation). We formalize core security properties as measurable system metrics: data egress volume, failover window exposure, sovereignty boundary integrity, and provenance chain completeness. Measurements show that edge-local deployments eliminate routine cloud data exposure but silently compromise sovereignty during fallback—boundary crossings remain invisible at the application layer. Provenance chains stay complete under cooperation yet are trivially bypassed without cryptographic enforcement. Failover windows create transient blind spots exploitable for unauthorized actuation. Our results establish deployment architecture—not just model or prompt design—as a primary determinant of security risk in agent-controlled IoT systems.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>security</category>
      <category>llm</category>
      <category>agent</category>
    </item>
    <item>
      <title>Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</title>
      <link>https://arxiv.org/abs/2602.23296v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23296v1</guid>
      <description>Federated learning (FL) lacks reliable uncertainty quantification (UQ), especially under *dual heterogeneity*—the coupled effect of data and model heterogeneity—which undermines coverage guarantees across clients and risks silent local failures. While conformal prediction offers distribution-free UQ with finite-sample guarantees, its adaptation to heterogeneous FL remains underexplored. We propose **FedWQ-CP**, a simple, communication-efficient framework that achieves both client-level and global coverage validity in one round of agent-server calibration. Each client computes a local quantile threshold on its calibration set and sends only this threshold and its calibration size to the server, which aggregates them via a sample-size-weighted average to obtain a global threshold. Evaluated on seven public classification and regression benchmarks, FedWQ-CP empirically attains target coverage (e.g., 90%) at *every client* and globally (coverage error &lt;1.2%), while yielding the *sharpest* prediction sets/intervals—outperforming existing federated conformal methods by 8.3–15.7% in average interval width or set size. FedWQ-CP requires no model architecture changes, no gradient exchange, and zero additional communication rounds.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots</title>
      <link>https://arxiv.org/abs/2602.22973v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22973v1</guid>
      <description>This paper introduces a novel diagnostic alignment framework for clinical AI that treats expert validation as a structured, traceable transformation—not merely a final label. We formalize the AI’s initial image-based report as an *immutable inference snapshot*, preserving it unchanged for systematic comparison with physician-validated outcomes. The pipeline integrates a vision-enhanced LLM, BERT-based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement pre-review. Evaluated on 21 dermatological cases, our four-level concordance framework reveals: exact primary match rate (PMR) = 71.4%; semantic-adjusted match rate (AMR) identical to PMR (*t* = 0.60), indicating lexical agreement already captures clinical meaning; and 100% Comprehensive Concordance Rate (CCR) with 95% CI [83.9%, 100%], confirming no case showed complete diagnostic divergence. Critically, binary lexical evaluation substantially underestimates clinically meaningful alignment—our signal-aware approach enables quantification of correction dynamics and supports human-aligned, auditable evaluation of image-based clinical decision support.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>extraction</category>
      <category>model</category>
    </item>
    <item>
      <title>Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study</title>
      <link>https://arxiv.org/abs/2602.22760v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22760v1</guid>
      <description>This study investigates the feasibility of performing full-parameter pretraining of large language models (LLMs) during renewable energy curtailment windows—periods when excess clean electricity is otherwise discarded. We design and prototype a geographically distributed training system that dynamically schedules compute across heterogeneous GPU clusters using real-world marginal carbon intensity traces to identify curtailment opportunities. Leveraging the Flower federated learning framework, our system elastically switches between local single-site training and cross-site synchronized training as clusters enter or exit curtailment windows. Evaluating on a 561M-parameter transformer model across three clusters, we demonstrate that curtailment-aware scheduling preserves model quality (within ±1.2% perplexity deviation on WikiText-2 and PG-19) while reducing operational carbon emissions to just 5–12% of conventional single-site baselines. This work provides the first end-to-end empirical validation that LLM pretraining can be robustly aligned with intermittent, regionally varying renewable surpluses—enabling scalable, low-carbon AI development.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Tackling Privacy Heterogeneity in Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2602.22633v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22633v1</guid>
      <description>Differentially private federated learning (DP-FL) enables collaborative model training while protecting local data privacy. However, real-world deployments face *privacy heterogeneity*: clients impose vastly different privacy budgets (e.g., ε = 0.5 vs. ε = 8), violating the common uniform-budget assumption. This heterogeneity undermines standard client selection—data-volume-based strategies cannot distinguish high-signal (low-noise) updates from low-signal (high-noise) ones induced by stringent privacy constraints. To bridge this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We derive a tight convergence bound quantifying how heterogeneous budgets impact training error, then formulate optimal selection as a convex optimization problem that adaptively assigns sampling probabilities to minimize this bound. Experiments on CIFAR-10, FEMNIST, and Sentiment140 show our method improves test accuracy by up to **10.2%** over state-of-the-art baselines under heterogeneous budgets, while reducing communication rounds by 18%. This demonstrates that explicitly incorporating privacy heterogeneity into client selection is essential for practical, efficient, and compliant DP-FL.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD</title>
      <link>https://arxiv.org/abs/2602.22611v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22611v1</guid>
      <description>In Embedding-as-an-Interface (EaaI) settings, intermediate representations (IRs) from pre-trained models leak membership information, with MIA risk varying significantly across layers—a heterogeneity ignored by standard DP-SGD’s uniform noise and per-example clipping. We propose **Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD)**, which quantifies layer-specific MIA risk via shadow-model-based adversaries on public data and uses these estimates to reweight layer gradients *before* global clipping—enabling risk-proportional privacy allocation under fixed noise magnitude. Theoretically, LM-DP-SGD preserves $(\varepsilon,\delta)$-DP guarantees and convergence. Experiments show it reduces peak IR-level MIA risk by up to 41.3% (avg. 28.7%) at the same privacy budget, while improving utility retention by 35–52% over baseline DP-SGD—establishing a new state-of-the-art privacy–utility trade-off for IR publishing.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>dp</category>
      <category>membership</category>
    </item>
    <item>
      <title>DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion</title>
      <link>https://arxiv.org/abs/2602.22610v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22610v1</guid>
      <description>Condition injection in diffusion models enables context-aware generation for time-series tasks, but heterogeneous conditioning (e.g., missingness patterns or outlier covariates) induces heavy-tailed per-example gradients—causing excessive global clipping, amplified bias, and degraded utility under DP-SGD. We propose **DP-aware AdaLN-Zero**, a plug-and-play, sensitivity-aware conditioning mechanism that jointly bounds the magnitude of conditioning representations and AdaLN-Zero modulation parameters via bounded re-parameterization—suppressing extreme gradient tails *before* clipping and noise injection, without altering DP-SGD. Empirically, on a real-world power dataset and two ETT benchmarks, it improves imputation and forecasting MSE by 18.3–24.7% under matched privacy budgets (ε=2, δ=1e−5). Gradient diagnostics confirm reduced clipping distortion (−32.6%) and condition-specific tail reshaping, while preserving expressiveness in non-private training. This work establishes sensitivity-aware conditioning as a key architectural lever for private conditional diffusion.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
    </item>
    <item>
      <title>SQISign on ARM</title>
      <link>https://eprint.iacr.org/2026/394</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/394</guid>
      <description>We present the first vectorized implementation of SQISign for high-performance ARM architectures. SQISign is a leading NIST On-Ramp Round 2 candidate, offering the smallest key and signature sizes among post-quantum signatures—but its signing performance is bottlenecked by the ideal-to-isogeny conversion, which demands intensive finite field and elliptic curve operations. Leveraging the NEON instruction set, we design a highly optimized $\mathbb{F}_{p^2}$ arithmetic library and batched 2D isogeny-chain operations, accelerating the critical conversion subroutine by **2.24×** over the state-of-the-art scalar implementation. Our optimizations are fully orthogonal to the recent Qlapoti algorithm (Asiacrypt 2025); when combined, they yield **&gt;2.24× end-to-end signing speedup at NIST Security Level I**. This work establishes the first practical, vectorized SQISign baseline for ARM and opens avenues for efficient quaternion-based computation in isogeny cryptography.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Conditionally Linkable Attribute-Based Signatures</title>
      <link>https://eprint.iacr.org/2026/402</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/402</guid>
      <description>This paper introduces **Conditionally Linkable Attribute-Based Signatures (CLABS)**, a new primitive that enables *context-dependent*, *user-controlled* linkability in attribute-based authentication. Unlike prior ABS schemes—where linkability is either globally absent or universally enforced—CLABS allows two signatures to be publicly linked *if and only if* they share a declared context $\tau$ (e.g., jurisdiction, policy domain) *and* their attributes match under a public context-specific function $f_\tau$. Each user’s linking set $L_x \subseteq \mathcal{T}$ encodes voluntary, fine-grained linkage consent, ensuring anonymity by default and leakage limited to an opt-in bit. We formalize CLABS security, capturing conditional linkability and context-aware anonymity. Our generic construction combines a PRF, a standard signature for attribute certification, and a signature of knowledge (SoK) proving correct tag derivation and Boolean policy satisfaction without revealing attributes. Instantiated under standard lattice assumptions in the QROM, CLABS achieves post-quantum security and supports arbitrary Boolean policies.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>NIROPoK-Based Post-Quantum Sidechain Design on Ethereum</title>
      <link>https://eprint.iacr.org/2026/401</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/401</guid>
      <description>This paper introduces NIROPoK, the first practical post-quantum sidechain framework for Ethereum. We design a quantum-resistant stack comprising: (1) a novel **Non-Interactive Random Oracle Proof of Knowledge (NIROPoK)** based on module-LWE, enabling efficient zero-knowledge verification with 42% lower gas cost than zk-SNARKs; (2) a **Quantum-Resistant Proof-of-Stake (QPoS)** consensus using Dilithium signatures for secure validator onboarding and block signing; and (3) a **Dilithium-based cross-chain bridge** formally verified against quantum adversaries. Deployed on an Ethereum-compatible testnet, our sidechain achieves &gt;1200 TPS and sub-2.3s block latency while preserving full EVM compatibility and requiring no changes to Ethereum’s core architecture. The solution secures existing Ethereum transactions against quantum threats *today*, demonstrating a viable, deployable path toward quantum-ready Web3 infrastructure.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Non-interactive Blind Signatures with Threshold Issuance</title>
      <link>https://eprint.iacr.org/2026/400</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/400</guid>
      <description>We introduce **Non-interactive Threshold Blind Signatures (NITBS)**, the first primitive enabling a user to obtain partial presignatures from a *t*-out-of-*n* threshold of signers and locally combine them into a valid blind signature—without any interaction beyond initial requests. We formalize security with rigorous definitions of *blindness* (signers learn nothing about the blinded message) and *one-more unforgeability* (an adversary gaining *t−1* partial presignatures cannot produce *t* valid blind signatures). Our construction adapts the Pointcheval-Sanders (PS) signature scheme and is proven secure in the Algebraic Group Model (AGM). Micro-benchmarks show it achieves the **smallest presignature size (2 group elements), smallest signature size (3 group elements), and fastest issuance latency** among all existing NIBS schemes—setting a new efficiency baseline for threshold privacy-preserving authentication.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>What a Wonderful World: zkSNARKs in the Algebraic Group Model are Universally Composable</title>
      <link>https://eprint.iacr.org/2026/399</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/399</guid>
      <description>Zero-Knowledge Succinct Non-interactive Arguments of Knowledge (zkSNARKs) are widely deployed in practice, yet their security under concurrent composition requires Universal Composability (UC) guarantees. Prior UC analyses either lack modularity or rely on the Random Oracle Model (ROM), leaving efficient Algebraic Group Model (AGM)-based zkSNARKs—such as Plonk and Marlin—without rigorous UC foundations. We bridge this gap by introducing a modular framework: we identify simple, standard algebraic properties (e.g., extractability of commitments, polynomial binding, and algebraic zero-knowledge simulation) sufficient for UC security in the AGM (+ROM). Crucially, these properties can be verified *directly* from the rigorous AGM formalization of Jaeger and Mohan (CRYPTO’24). Applying our framework, we prove—**without any modification or overhead**—that Plonk and Marlin are UC-secure in their native AGM-based security setting. This is the first composably secure justification for widely adopted PIIOP-derived zkSNARKs.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Orthus: Practical Sublinear Batch-Verification of Lattice Relations from Standard Assumptions</title>
      <link>https://eprint.iacr.org/2026/398</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/398</guid>
      <description>This work introduces **Orthus**, the first practical sublinear batch-verification proof system for lattice-based relations—such as Falcon signature verification and SIS/LWE assertions—built solely on standard assumptions (e.g., Module-LWE) without trusted setup. Orthus achieves asymptotic verification time $O(\sqrt{N})$, where $N$ is the witness size, breaking the linear barrier that has hindered adoption of succinct lattice-based proofs. Its core innovations include a hierarchical batching framework, FFT-optimized polynomial commitments tailored to lattice algebra, and efficient inner-product compression. In implementation, Orthus reduces verifier runtime by **9×** when aggregating $2^{17}$ Falcon signatures, while keeping proof size modest (~24 KB) and prover overhead practical. This marks the first standard-assumption construction enabling *real-world* verification acceleration for lattice-based succinct proofs.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Bittersweet Signatures: Bringing LWR to a Picnic for Hardware-Friendly MPC-in-the-Head</title>
      <link>https://eprint.iacr.org/2026/397</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/397</guid>
      <description>We introduce **Bittersweet signatures**, the first MPC-in-the-Head (MiH) signature scheme based on the **Learning With Rounding (LWR)** assumption. Leveraging *almost key-homomorphic PRFs*, Bittersweet achieves exceptional conceptual simplicity and regular structure—enabling efficient parallel and hardware-friendly implementations. Technically, we address the nontrivial challenge of *carry leakage* inherent in LWR-based homomorphic operations via rigorous noise analysis and tailored parameterization. Concretely, Bittersweet yields competitive signature sizes (~12–18 KB), trades modest software overhead (2–3× slower signing than Picnic3) for substantial hardware gains (5–8× FPGA throughput), and lags only slightly behind state-of-the-art VOLE- or Threshold-MiH schemes. Its algebraic scalability and structured design make it a promising candidate for leakage-resilient deployment. The new abstractions introduced open avenues for generalization and optimization beyond MiH.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Anonymity of X-Wing and its Variants</title>
      <link>https://eprint.iacr.org/2026/396</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/396</guid>
      <description>X-Wing is a hybrid KEM (X25519 + ML-KEM-768) under IETF standardization and deployed by industry for post-quantum transition. This paper presents the first anonymity analysis of X-Wing. We prove in the standard model that weak anonymity implies full anonymity for any IND-CCA-secure KEM via a reduction that is tight in success probability, time, *and memory*—a novel triple-tight guarantee. In the random oracle model, we show X-Wing achieves weak anonymity if both X25519 and ML-KEM-768 do; notably, X25519’s weak anonymity holds information-theoretically. To recover memory-tightness lost in the original construction, we propose X-Wing⁺, a minimal variant preserving tightness end-to-end. Finally, we improve the existing IND-CCA proof of X-Wing using our memory-aware techniques, reducing security loss.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>How To Make Delegated Payments on Bitcoin: A Question for the AI Agentic Future</title>
      <link>https://eprint.iacr.org/2026/395</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/395</guid>
      <description>We introduce **Proxy Adaptor Signatures (PAS)**, a novel cryptographic primitive enabling *provably fair, delegated payment* on public blockchains. PAS solves the core fair-exchange problem in AI-agentic finance: ensuring a buyer obtains an asset *iff* the seller is paid—without revealing sensitive transaction witnesses to proxies, requiring long-term secrets from users, or compromising atomicity or privacy. Under a threshold model tolerating up to $t-1$ colluding proxies, PAS allows a stateless buyer to initiate a single request; proxies assist the exchange obliviously; and the seller is cryptographically guaranteed payment only if the buyer can later reconstruct the witness. Our efficient construction builds on standard primitives (ECDSA adaptor signatures + Shamir secret sharing) and is compatible with Bitcoin (Taproot), Cardano, and Ethereum. A Rust prototype supporting up to 30 proxies demonstrates concrete efficiency: buyer/seller operations finish in **microseconds**, proxy computation in **milliseconds**, and on-chain cost matches that of a baseline transaction—no extra fees or gas overhead.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Lap2: Revisiting Laplace DP-SGD for High Dimensions via Majorization Theory</title>
      <link>https://arxiv.org/abs/2602.23516v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23516v1</guid>
      <description>Differentially Private Stochastic Gradient Descent (DP-SGD) relies heavily on the Gaussian mechanism, while the Laplace mechanism remains impractical for high-dimensional models due to its requirement of L1-norm gradient clipping—causing noise scale to grow as √n and degrading utility severely. This paper introduces **Lap2**, the first Laplace-based DP-SGD framework supporting **L2-norm clipping** with rigorous (ε, δ)-privacy guarantees. Leveraging coordinate-wise moment bounds and **majorization theory**, Lap2 exploits the Schur-convexity of the moment accountant to construct a tight, data-independent global upper bound under L2 constraints—enabling scalable multivariate privacy accounting with thousands of moments. Experiments show Lap2 significantly outperforms standard Laplace DP-SGD and matches or exceeds Gaussian DP-SGD under strong privacy: fine-tuning RoBERTa-base on SST-2 achieves **87.88% accuracy at ε = 0.54**, surpassing Gaussian (87.16%) and standard Laplace (48.97%). Lap2 establishes Laplace mechanisms as viable—and competitive—for high-dimensional private learning.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>dp</category>
    </item>
    <item>
      <title>Learning to Generate Secure Code via Token-Level Rewards</title>
      <link>https://arxiv.org/abs/2602.23407v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23407v1</guid>
      <description>Large language models (LLMs) excel at code generation but often produce vulnerable code. Prior approaches are hindered by scarce high-quality security data and coarse instance-level reinforcement learning (RL) rewards. To address this, we propose **Vul2Safe**, a framework that leverages LLM self-reflection to automatically construct high-confidence vulnerability–repair pairs from real-world CVEs and generate diverse implicit prompts, yielding the **PrimeVul+** dataset (127K samples). Concurrently, we introduce **SRCode**, the first RL framework for secure code generation using **token-level rewards**: security analyzers assign fine-grained scores to each generated token (e.g., penalizing unsafe string concatenation, rewarding sanitization calls), enabling precise optimization of local security patterns. Experiments across six benchmarks show SRCode trained on PrimeVul+ reduces average vulnerability rates by **58.3%** over SOTA RL baselines while improving functional correctness (+9.2%) and maintainability. Our work establishes token-level reward modeling and synthetically curated security data as essential levers for trustworthy code generation.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>llm</category>
    </item>
    <item>
      <title>Lifecycle-Integrated Security for AI-Cloud Convergence in Cyber-Physical Infrastructure</title>
      <link>https://arxiv.org/abs/2602.23397v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23397v1</guid>
      <description>This paper addresses the security fragmentation arising from AI-cloud convergence in cyber-physical infrastructure (CPI), where disjointed AI governance (e.g., NIST AI RMF, OWASP AI Exchange), cloud security (e.g., CSA MAESTRO), and industrial control standards (e.g., NERC CIP) enable cross-layer attacks threatening safety-critical operations. We contribute: (i) a lifecycle-integrated threat taxonomy structured by explicit attacker capability tiers (data poisoning → model stealing → prompt injection → agent privilege escalation → physical impact); (ii) a Unified Reference Architecture comprising a Secure Data Factory, hardened model supply chain, and runtime governance layer with automated policy enforcement; and (iii) Grid-Guard—a validated case study in a hybrid transmission system operator environment—demonstrating end-to-end autonomous defense against a multi-tier physical-financial manipulation campaign using coordinated controls drawn from NIST AI RMF, MITRE ATLAS, OWASP AI Exchange/GenAI, CSA MAESTRO, and NERC CIP. All controls are rigorously mapped to all five frameworks and current NERC CIP requirements, proving that a single cloud-native architecture can simultaneously satisfy AI governance, adversarial robustness, agentic safety, and industrial regulatory compliance.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>inference</category>
      <category>security</category>
    </item>
    <item>
      <title>FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments</title>
      <link>https://arxiv.org/abs/2602.23504v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23504v1</guid>
      <description>Federated Learning (FL) suffers from performance degradation under data heterogeneity, motivating clustered FL approaches that group similar clients. However, existing methods rely *either* on data similarity *or* gradient similarity—yielding incomplete client assessment—and strictly isolate knowledge within clusters, missing cross-cluster representation benefits. To address this, we propose **FedDAG**, a clustered FL framework featuring: (1) a **weighted, class-wise similarity metric** that jointly integrates local data distributions and global gradient signals for more holistic clustering; and (2) a **dual-encoder architecture**, where each cluster’s primary encoder is trained on its own clients’ data, while a secondary encoder is refined via gradients from semantically complementary clusters—enabling controlled cross-cluster feature transfer without sacrificing specialization. Extensive experiments across CIFAR-10/100, Tiny-ImageNet, and LEAF-FEMNIST under diverse heterogeneity settings demonstrate that FedDAG consistently outperforms state-of-the-art clustered FL baselines, achieving absolute accuracy gains of **2.3–5.7 percentage points**, with up to **9.1% relative improvement** in low-resource clusters.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>federated</category>
      <category>learning</category>
    </item>
    <item>
      <title>Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</title>
      <link>https://arxiv.org/abs/2602.23296v2</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23296v2</guid>
      <description>Federated learning (FL) suffers from unreliable uncertainty quantification (UQ) under dual heterogeneity—where both data distributions and model architectures vary across agents—leading to overconfident yet erroneous local predictions despite acceptable global accuracy. To address this, we propose **FedWQ-CP**, a conformalized neural network framework that achieves rigorous, distribution-free UQ with minimal communication. FedWQ-CP performs agent-server calibration in **one round**: each agent computes its local conformity score quantile $\hat{q}_i$ and sample size $n_i$, then transmits only these two scalars; the server aggregates via weighted average $q_{\text{global}} = \sum_i (n_i / \sum_j n_j)\, \hat{q}_i$. Experiments across seven public classification and regression benchmarks show FedWQ-CP maintains **empirical coverage within ±0.8% per agent and &lt;0.5% globally**, while yielding the **smallest prediction sets/intervals**—reducing set size by 12–34% (classification) and interval width by 18–29% (regression) versus state-of-the-art federated CP methods.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>federated</category>
      <category>learning</category>
    </item>
    <item>
      <title>IDP Accelerator: Agentic Document Intelligence from Extraction to Compliance Validation</title>
      <link>https://arxiv.org/abs/2602.23481v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.23481v1</guid>
      <description>IDP Accelerator is an open-source agentic framework for end-to-end document intelligence, bridging extraction, analytics, and compliance validation. It introduces four novel components: (1) **DocSplit**, a multimodal benchmark and classifier using BIO tagging to segment heterogeneous document packets; (2) a **configurable multimodal LLM-based Extraction Module** for zero-shot, context-aware structured data generation; (3) an **Agentic Analytics Module** compliant with the Model Context Protocol (MCP), enabling secure, sandboxed code execution for dynamic data exploration; and (4) a **Rule Validation Module** that replaces brittle deterministic engines with LLM-driven, interpretable logic for complex regulatory checks. Evaluated in production at a leading healthcare provider, IDP Accelerator achieves **98% classification accuracy**, **80% lower latency**, and **77% reduced operational cost** versus legacy systems. A live web demo and full source code are publicly available.</description>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>UC-Secure Star DKG for Non-Exportable Key Shares with VSS-Free Enforcement</title>
      <link>https://arxiv.org/abs/2602.22187v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22187v1</guid>
      <description>We present **Star DKG (SDKG)**, the first UC-secure Distributed Key Generation protocol for non-exportable key shares, eliminating Verifiable Secret Sharing (VSS) entirely. SDKG operates in the $\mathcal{F}_{KeyBox}$-hybrid model—formalizing hardware-enforced key isolation (e.g., TEEs, HSM APIs)—where shares are cryptographically bound and never exported. To enforce transcript-defined affine consistency without share exposure, we introduce **Unique Structure Verification (USV)**, a public certificate whose secret scalar remains inside the KeyBox while its group element is deterministically derivable from the transcript, and combine it with **Fischlin-style UC-extractable NIZKs** in the gRO-CRP model to enable straight-line simulation under adaptive corruptions and secure erasures. SDKG realizes a **1+1-out-of-$n$ star access structure** (designated service + any recovery device) for threshold wallets, with role-based registration. Under DL and DDH assumptions, it UC-realizes a transcript-driven refinement of standard UC-DKG, achieving $\widetilde{O}(n\log p)$ communication and $\widetilde{O}(n\log^{2.585}p)$ bit-operation cost over a prime-order group of size $p$.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions</title>
      <link>https://arxiv.org/abs/2602.22134v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22134v1</guid>
      <description>Semantic Communication (SemCom) shifts wireless transmission from symbol reproduction to task-oriented semantic delivery—but its AI-native architecture introduces novel security vulnerabilities: semantic failures can occur even with intact physical-layer reliability and cryptographic protection, due to adversarial model perturbations, poisoned training data, desynchronized semantic priors, or misaligned distributed inference. This survey establishes the first defense-centered, system-level framework for SemCom security via AI defenses. We propose a unified threat model categorizing attacks across four vectors—model-level, channel-realizable, knowledge-based, and networked inference—and introduce a structured defense taxonomy aligned with semantic integrity failure points: encoding, wireless transmission, knowledge integrity, and multi-agent coordination. We further define *security utility operating envelopes* to capture fidelity–robustness–latency–energy tradeoffs under realistic constraints, review evaluation metrics (e.g., Semantic BER, Task Accuracy under Attack), applications, and identify critical open challenges—including cross-layer security composition and deployment-time certification. The work provides a foundational, actionable perspective for building trustworthy SemCom systems in next-generation intelligent networks.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>poisoning</category>
      <category>data</category>
      <category>model</category>
      <category>inference</category>
    </item>
    <item>
      <title>A Critical Look into Threshold Homomorphic Encryption for Private Average Aggregation</title>
      <link>https://arxiv.org/abs/2602.22037v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22037v1</guid>
      <description>Threshold Homomorphic Encryption (Threshold HE) is widely adopted for privacy-preserving average aggregation in Federated Learning (FL), yet recent work exposes critical vulnerabilities when adversaries access a *restricted decryption oracle*—a realistic threat reflecting FL clients’ ability to jointly decrypt results without learning the full secret key. This paper conducts the first systematic evaluation of threshold RLWE-based HE (specifically BFV and CKKS variants) for federated averaging, focusing on the practical trade-offs introduced by *smudging noise with large variance* as a security countermeasure. We benchmark communication, latency, and aggregation accuracy under standardized 128-bit security parameters across 3–16 clients. Contrary to common assumptions, we find that threshold CKKS achieves **comparable or slightly better end-to-end performance than threshold BFV**, with up to 12% lower latency at scale, while maintaining aggregation error below 0.001%—well within FL convergence tolerance. Our results provide concrete guidance for secure, efficient HE integration in production FL systems.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Resilient Federated Chain: Transforming Blockchain Consensus into an Active Defense Layer for Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21841v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21841v1</guid>
      <description>Federated Learning (FL) enables privacy-preserving decentralized training but remains highly vulnerable to adversarial attacks due to its inherent lack of centralized data inspection. While blockchain integration has been explored for FL, its potential as an *active defense layer*—rather than merely an immutable ledger—remains untapped. This paper proposes **Resilient Federated Chain (RFC)**, a novel framework that repurposes the computational redundancy in Proof of Federated Learning (PoFL) consensus as real-time defense infrastructure. RFC introduces a flexible, attack-adaptive evaluation function within its consensus mechanism and tightly couples on-chain verification with off-chain robust aggregation (e.g., Krum, Median). Extensive experiments on image classification under diverse adversarial settings (e.g., label-flipping, sign-flipping, Byzantine attacks) show RFC improves model accuracy by up to 37.5 percentage points over FedAvg under 20% malicious clients, while maintaining &lt;3.1% false positive rate. RFC establishes blockchain not just as a trust anchor, but as a dynamic, responsive security layer for trustworthy decentralized AI.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Private and Robust Contribution Evaluation in Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21721v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21721v1</guid>
      <description>Cross-silo federated learning (FL) enables collaborative model training without raw data sharing, yet client updates remain vulnerable to inference attacks. While secure aggregation (SecAgg) preserves privacy by masking individual contributions, it renders conventional contribution evaluation—critical for fair rewards and misbehavior detection—infeasible. Existing marginal-contribution methods (e.g., Shapley value) are incompatible with SecAgg; practical alternatives like Leave-One-Out (LOO) suffer from coarse granularity and dangerous self-evaluation dependencies. We propose two SecAgg-compatible marginal-difference scores: **Fair-Private**, satisfying core fairness axioms (efficiency, symmetry, null player, additivity); and **Everybody-Else**, eliminating self-evaluation entirely and providing provable resistance to strategic manipulation—a previously overlooked vulnerability. We provide rigorous theoretical guarantees on fairness, $(\varepsilon,\delta)$-differential privacy, Byzantine robustness, and linear communication/computation cost. Extensive evaluation across medical imaging datasets (BraTS, CheXpert, NIH ChestX-ray) and CIFAR10 shows our scores consistently outperform baselines: they better approximate Shapley rankings (+32–47% Kendall tau), improve final model accuracy (+1.8–3.4 pp), and achieve &gt;0.91 F1-score in detecting malicious clients. This work establishes the first principled framework jointly achieving fairness, privacy, robustness, and practical utility in FL contribution evaluation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection</title>
      <link>https://arxiv.org/abs/2602.21593v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21593v1</guid>
      <description>Generative image proliferation has spurred adoption of semantic-aware watermarking in diffusion models for provenance tracking and forgery detection. While content-aware schemes bind watermarks to high-level semantics to resist local edits, we expose a critical vulnerability: large language models (LLMs) enable *targeted, coherence-preserving semantic perturbations* that selectively alter watermark-relevant attributes without disrupting global visual-semantic consistency. We propose the **Coherence-Preserving Semantic Injection (CSI)** attack—a novel LLM-guided framework that leverages CLIP-aligned text prompts and embedding-space similarity constraints to generate semantically shifted yet visually faithful images, inducing detector misclassification. Extensive evaluation across six state-of-the-art semantic watermarking methods shows CSI consistently outperforms all baselines, increasing false-negative rates by up to 82.6% (avg. +37.4% over best prior attack). This reveals a fundamental security gap: semantic binding alone is insufficient against LLM-powered semantic manipulation.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>security</category>
      <category>llm</category>
    </item>
    <item>
      <title>Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem</title>
      <link>https://arxiv.org/abs/2602.21814v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21814v1</guid>
      <description>This study isolates the causal impact of prompt architecture on implicit physical constraint reasoning using the “car wash problem”—a benchmark where LLMs consistently fail without explicit spatial logic cues. In a controlled variable-isolation experiment (n=20 per condition, 6 conditions), we evaluate Claude 3.5 Sonnet under fixed hyperparameters (temperature=0.7, top_p=1.0). The STAR (Situation-Task-Action-Result) reasoning framework alone boosts accuracy from 0% to 85% (*p*=0.001, Fisher’s exact test; OR=13.22), demonstrating that *forced goal articulation prior to inference* is the dominant driver of correct reasoning. User-profile context (via vector DB retrieval) adds +10 percentage points, and RAG-supplied domain context contributes an additional +5 points—reaching 100% in the full-stack condition. Crucially, structured scaffolding matters substantially more than contextual augmentation for this class of tasks: reasoning *how* to reason outweighs *what* to reason about. These findings establish STAR as a minimal, generalizable scaffold for reliable implicit-constraint inference.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>prompt</category>
      <category>injection</category>
    </item>
    <item>
      <title>Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation</title>
      <link>https://arxiv.org/abs/2602.21957v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21957v1</guid>
      <description>Federated recommendation (FedRec) enables collaborative model training across distributed clients without sharing raw user interaction data. Conventional methods synchronize high-dimensional item embeddings between server and clients, implicitly assuming precise geometric alignment is essential for collaboration. We challenge this assumption and argue that preserving *global semantic structures*—rather than identical embeddings—is more effective and efficient. To this end, we propose **CGFedRec**, a cluster-guided framework where the server discovers a shared item clustering structure from uploaded embeddings and broadcasts only compact cluster labels—not full embeddings—to clients. Clients then align their local item representations via these structural constraints, enabling personalization while maintaining global consistency. Experiments on six real-world datasets (e.g., Yelp, Amazon-Book, ML-1M) show CGFedRec achieves **up to 92.7% communication reduction** and **+3.8% average NDCG@10 gain** over strong baselines, demonstrating superior accuracy-efficiency trade-offs. Our work redefines collaboration in FedRec as *structural alignment*, not coordinate synchronization.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2602.21928v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21928v1</guid>
      <description>This paper addresses decentralized root cause analysis (RCA) in nonlinear dynamical systems—e.g., power grids and supply chains—where clients are geographically distributed, run fixed proprietary models, and exhibit unknown, time-varying interdependencies. We propose a novel federated learning framework that learns cross-client dependencies *without accessing raw sensor streams or modifying local black-box models*. Each client augments its legacy system with a lightweight, trainable dependency encoder; a global server coordinates encoders via representation consistency constraints (leveraging temporal lag correlation and contrastive alignment) while preserving privacy through calibrated differential privacy. Theoretical analysis establishes convergence guarantees under non-IID, feature-partitioned, and privacy-constrained settings. Experiments on extensive simulations and a real-world industrial cybersecurity dataset demonstrate state-of-the-art RCA performance: 92.4% F1-score on root cause localization and 58% lower false positives versus baselines—achieving RCA with *zero raw data upload, zero model modification, and zero prior dependency knowledge*.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>federated</category>
      <category>machine</category>
      <category>learning</category>
      <category>differential</category>
      <category>privacy</category>
    </item>
    <item>
      <title>GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task</title>
      <link>https://arxiv.org/abs/2602.21873v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21873v1</guid>
      <description>Federated learning (FL) enables privacy-preserving collaborative training for vision tasks, yet suffers from **ineffective cross-client knowledge fusion under class imbalance** and **prohibitive communication overhead** from transmitting full-model parameters. To address this, we propose **Generative Federated Prototype Learning (GFPL)** — a lightweight, semantics-aware FL framework. GFPL replaces model parameter exchange with **class-wise prototype transmission**, where each client generates prototypes via Gaussian Mixture Models (GMM) to capture feature statistics compactly. Server-side aggregation employs **Bhattacharyya distance** to fuse semantically similar prototypes across clients, avoiding semantic misalignment. Crucially, fused prototypes generate **class-balanced pseudo-features**, and a **dual-classifier architecture**—optimized via hybrid Dot Regression and Cross-Entropy loss—enhances local feature alignment. Experiments on imbalanced benchmarks (e.g., CIFAR-10-LT, ImageNet-LT) show GFPL improves accuracy by **+3.6%** over strong baselines while reducing communication cost by **up to 87%**, achieving superior performance-efficiency trade-offs for resource-constrained edge vision applications.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21844v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21844v1</guid>
      <description>Differentially private federated learning (DP-FL) suffers from a fundamental tension: privacy-preserving mechanisms impose quantifiable privacy costs that deter client participation, especially among highly privacy-sensitive “stragglers.” Existing incentive designs assume unbiased client selection, forcing servers to over-compensate these stragglers—leading to budget waste and suboptimal model convergence. We propose **JSAM**, the first Bayesian-optimal framework that *jointly* optimizes client selection probabilities and privacy compensation under a fixed budget. By theoretically characterizing optimal selection structure, JSAM reduces the original 2N-dimensional problem to an efficient 3D formulation. We prove that servers should *exclude* high-sensitivity clients and *preferentially select* privacy-tolerant ones—and reveal the counter-intuitive insight that *least*-sensitive clients incur the *highest cumulative cost* due to frequent selection. Extensive experiments on MNIST and CIFAR-10 show JSAM improves test accuracy by up to **15%** over unbiased baselines while maintaining cost efficiency across diverse data heterogeneity levels (α ∈ [0.1, 1.0]).</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>learning</category>
      <category>federated</category>
    </item>
    <item>
      <title>Mamba Meets Scheduling: Learning to Solve Flexible Job Shop Scheduling with Efficient Sequence Modeling</title>
      <link>https://arxiv.org/abs/2602.21546v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.21546v1</guid>
      <description>The Flexible Job Shop Scheduling Problem (FJSP) is a fundamental combinatorial optimization challenge in smart manufacturing, yet learning-based solvers suffer from high computational overhead and limited global dependency modeling. This paper pioneers the integration of **Mamba**—a linear-time state-space model—into FJSP solving. We propose a novel architecture featuring: (i) a dual-path Mamba encoder that separately processes operation and machine sequences to capture long-range constraints with O(N) complexity; and (ii) an efficient cross-attention decoder for dynamic operation-machine interaction. Evaluated on standard benchmarks (Dai, Brandimarte, Kacem), our method achieves **3.2× faster inference** and outperforms state-of-the-art learning-based solvers by 1.8–4.7% in makespan minimization, while reducing memory usage by 58%. This demonstrates Mamba’s superiority over quadratic-cost models (e.g., Transformers, GNNs) for structured scheduling tasks.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>model</category>
      <category>extraction</category>
    </item>
    <item>
      <title>DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain</title>
      <link>https://arxiv.org/abs/2602.22045v1</link>
      <guid isPermaLink="true">https://arxiv.org/abs/2602.22045v1</guid>
      <description>We present **DLT-Corpus**, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: **2.98 billion tokens** across **22.12 million documents**, including 37,440 scientific publications, 49,023 USPTO patents, and 22 million social media posts. Unlike prior NLP resources narrowly focused on cryptocurrency price prediction or smart contracts, DLT-Corpus comprehensively captures the full technological lifecycle of DLT. Using it, we uncover that DLT innovations emerge first in scientific literature, then migrate to patents, and finally diffuse to social media—following classic technology transfer pathways. Crucially, while social media sentiment remains persistently bullish—even during “crypto winters”—scientific and patent activity grows independently of short-term market volatility and instead tracks long-term market capitalization expansion (*r* = 0.87), revealing a virtuous innovation cycle where research enables economic growth that funds further R&amp;D. We publicly release the full corpus, **LedgerBERT** (a domain-adapted model achieving **+23% F1** over BERT-base on DLT-specific NER), and all tools and code.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>arXiv</category>
      <category>crypto</category>
    </item>
    <item>
      <title>Fast cube roots in Fp2 via the algebraic torus</title>
      <link>https://eprint.iacr.org/2026/392</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/392</guid>
      <description>We present a novel algorithm for computing cube roots in $\mathbb{F}_{p^2}$, a critical subroutine in elliptic-curve point decompression, hash-to-curve, and isogeny-based cryptography. Leveraging the algebraic torus $\mathbb{T}_2(\mathbb{F}_p)$ and Lucas sequences, our method reduces the problem *entirely* to operations in the base field $\mathbb{F}_p$—under the practically universal condition $p \equiv 1 \pmod{3}$. We prove correctness across all residuosity cases and implement it in the open-source `gnark-crypto` library. Benchmarks on six cryptographic primes (spanning pairing- and isogeny-based settings) show **1.6–2.3× speedups** over standard $\mathbb{F}_{p^2}$ exponentiation. The approach extends to $p \equiv 2 \pmod{3}$ and, more generally, to any odd $n$-th root in quadratic towers $\mathbb{F}_{p^{2^k}}$ when $\gcd(n, p+1) = 1$.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Zero-Knowledge IOPPs for Constrained Interleaved Codes</title>
      <link>https://eprint.iacr.org/2026/391</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/391</guid>
      <description>We present the first zero-knowledge interactive oracle proof of proximity (zk-IOPP) for constrained interleaved linear codes—achieving honest-verifier zero-knowledge with *negligible overhead* over the state-of-the-art non-ZK protocols. Our construction satisfies round-by-round knowledge soundness with a straightline extractor and negligible error. Technically, we introduce a composable definition of HVZK for interactive oracle reductions (IORs), then modularly compose lightweight zk-IORs: a novel **zero-knowledge sumcheck IOR** and a **zero-knowledge code-switching IOR**, both tailored to stringent efficiency requirements (sublinear communication, constant rounds, straightline extraction). To overcome challenges—including privacy leakage in interleaved polynomial commitments and zero-knowledge preservation across code families—we introduce new abstractions and protocols. As a side contribution, we highlight the concrete efficiency gains from high-distance codes derived from dispersers, which may be of independent interest.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Succinct Arguments for BatchQMA and Friends under 6 Rounds</title>
      <link>https://eprint.iacr.org/2026/390</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/390</guid>
      <description>We present the first succinct classical argument systems for batchQMA and monotone-policy batchQMA with ≤6 rounds under standard post-quantum assumptions—breaking the prior 8-round barrier and avoiding the quantum random oracle model (QROM). Our key technical innovation is **straight-line partial extractability**, enabling soundness proofs *without rewinding* cheating quantum provers—a departure from all prior works that relied on state-preserving succinct arguments of knowledge for NP. Specifically: (1) A **4-round public-coin** (except first message) argument for batchQMA achieves optimal communication (all messages independent of batch size) assuming post-quantum functional encryption and LWE; under LWE alone, only the verifier’s first message scales with batch size. (2) A **6-round private-coin** argument for monotone-policy batchQMA achieves batch-size- *and* circuit-size-independent communication under LWE. These results significantly advance practical verification of quantum computations by classical verifiers.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Towards Accountability for Anonymous Credentials</title>
      <link>https://eprint.iacr.org/2026/389</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/389</guid>
      <description>Anonymous Credentials (ACs) provide strong privacy by preventing issuers and verifiers from tracking users—but this very feature undermines accountability, hindering adoption in national identity systems (e.g., EUDI, Swiss e-ID). This paper identifies *transferability attacks* as a critical threat and proposes the first accountability framework for ACs. We introduce the **Cryptographic Forensic Trail (CFT)**: a tamper-evident, encrypted log attached to each credential presentation. Crucially, CFT decryption requires *joint, conditional authorization*: police must obtain a judicial warrant (based on probable cause), after which police, judge, and an independent NGO jointly execute a multiparty protocol to decrypt *only the relevant trail*. This design enforces checks and balances—neither law enforcement nor judiciary can act unilaterally, and NGO oversight detects and blocks collusion. Performance evaluation confirms practical feasibility: CFT adds &lt;300ms latency and &lt;2KB overhead on modern smartphones. Our work bridges the long-standing privacy–accountability gap in digital identity.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
    <item>
      <title>Necessary and Sufficient Conditions for the Existence of Ideal Linear Secret Sharing Schemes for Arbitrary Access Structures</title>
      <link>https://eprint.iacr.org/2026/388</link>
      <guid isPermaLink="true">https://eprint.iacr.org/2026/388</guid>
      <description>This paper establishes a necessary and sufficient condition for the existence of an ideal linear secret sharing scheme (ILSSS) realizing an arbitrary minimal access structure $\Gamma_{\min}$. Using linear codes over a finite field $\mathbb{F}_q$ as the primary tool, we construct matrices $H$ (parity-check) and $G$ (generator) such that $\Gamma_{\min}$ admits an ILSSS **if and only if** the matrix equation $GH^{\mathsf{T}} = 0$ has a solution over $\mathbb{F}_q$. When satisfied, $H$ defines a linear code whose *port* realizes $\Gamma_{\min}$, and $G$ is its corresponding generator matrix. Crucially, we prove this algebraic condition is equivalent to the combinatorial requirement that $\Gamma_{\min}$ must be the port of a matroid representable over $\mathbb{F}_q$. This unifies secret sharing, coding theory, and matroid representation in a concise, computationally verifiable framework—enabling efficient existence testing and constructive design of ideal schemes.</description>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0000</pubDate>
      <category>IACR</category>
    </item>
  </channel>
</rss>