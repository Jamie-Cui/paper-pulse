{
  "papers": [
    {
      "id": "arxiv_2602.16708v1",
      "arxiv_id": "2602.16708v1",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Mihai Christodorescu",
        "Somesh Jha"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16708v1",
      "url": "https://arxiv.org/abs/2602.16708v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "prompt",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的高敏场景，如客户服务协议、多级审批流程、敏感数据访问控制及合规性监管。当前主流做法是将策略嵌入提示词（prompt），但该方式缺乏强制力保障，无法防止模型绕过、忽略或误解释策略，导致策略违规频发。\n\n## 方法：PCAS 政策编译器  \n本文提出 **PCAS（Policy Compiler for Agentic Systems）**——一种面向智能体系统的策略编译框架，实现**确定性策略执行**。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图**（causal dependency graph）表征系统状态，显式追踪工具调用、工具返回值、消息传递等事件间的因果与信息流关系；  \n- **策略语言设计**：采用类 Datalog 的声明式策略语言，天然支持**传递性信息流分析**与**跨智能体溯源**（cross-agent provenance），可精确表达“某数据是否经未授权代理传播”等复杂约束；  \n- **运行时强保障**：集成轻量级**参考监视器**（reference monitor），在所有动作执行前实时拦截并阻断违规行为，执行逻辑完全独立于LLM推理过程，杜绝模型幻觉或策略规避。\n\n## 关键成果与验证  \nPCAS 以“编译即合规”范式工作：输入现有智能体代码与策略规范，自动产出**策略内建**（policy-by-construction）的插桩系统，无需重构原有安全逻辑。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离；  \n- 多智能体药物警戒系统中的多级审批策略；  \n- 客户服务组织策略（如地域权限、敏感操作二次确认）。  \n实验表明：在客户服务任务上，PCAS 将前沿模型（如GPT-4、Claude 3）的策略合规率从**48% 提升至 93%**，且所有插桩运行中**零策略违规发生**，验证了其确定性保障能力。",
      "summary_en": "PCAS (Policy Compiler for Agentic Systems) is a novel framework that enables *deterministic, runtime-enforced* policy compliance for LLM-based agents—without relying on prompt engineering or model fine-tuning. It addresses the fundamental limitation of linear message histories by modeling system state as a *causal dependency graph*, capturing information flow across tool calls, results, and messages. Policies are written in a declarative, Datalog-derived language supporting transitive flow and cross-agent provenance. A lightweight reference monitor intercepts all actions pre-execution, blocking violations with guaranteed enforcement—decoupled from model reasoning. Given an existing agent implementation and a policy specification, PCAS compiles them into an instrumented, policy-compliant system *by construction*. Evaluated on three real-world case studies—including prompt injection defense, pharmacovigilance approval workflows, and customer service policies—PCAS boosts compliance from 48% to 93% across frontier models (e.g., GPT-4, Claude 3), achieving zero policy violations in all instrumented runs.",
      "summary": "## 背景与挑战  \n基于大语言模型（LLM）的智能体正广泛部署于需严格授权策略的高敏场景，如客户服务协议、多级审批流程、敏感数据访问控制及合规性监管。当前主流做法是将策略嵌入提示词（prompt），但该方式缺乏强制力保障，无法防止模型绕过、忽略或误解释策略，导致策略违规频发。\n\n## 方法：PCAS 政策编译器  \n本文提出 **PCAS（Policy Compiler for Agentic Systems）**——一种面向智能体系统的策略编译框架，实现**确定性策略执行**。其核心创新在于：  \n- **状态建模革新**：摒弃线性消息历史，构建**因果依赖图**（causal dependency graph）表征系统状态，显式追踪工具调用、工具返回值、消息传递等事件间的因果与信息流关系；  \n- **策略语言设计**：采用类 Datalog 的声明式策略语言，天然支持**传递性信息流分析**与**跨智能体溯源**（cross-agent provenance），可精确表达“某数据是否经未授权代理传播”等复杂约束；  \n- **运行时强保障**：集成轻量级**参考监视器**（reference monitor），在所有动作执行前实时拦截并阻断违规行为，执行逻辑完全独立于LLM推理过程，杜绝模型幻觉或策略规避。\n\n## 关键成果与验证  \nPCAS 以“编译即合规”范式工作：输入现有智能体代码与策略规范，自动产出**策略内建**（policy-by-construction）的插桩系统，无需重构原有安全逻辑。在三大真实案例中验证：  \n- 防御提示注入的信息流隔离；  \n- 多智能体药物警戒系统中的多级审批策略；  \n- 客户服务组织策略（如地域权限、敏感操作二次确认）。  \n实验表明：在客户服务任务上，PCAS 将前沿模型（如GPT-4、Claude 3）的策略合规率从**48% 提升至 93%**，且所有插桩运行中**零策略违规发生**，验证了其确定性保障能力。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16596v1",
      "arxiv_id": "2602.16596v1",
      "title": "Sequential Membership Inference Attacks",
      "authors": [
        "Thomas Michel",
        "Debabrota Basu",
        "Emilie Kaufmann"
      ],
      "abstract": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16596v1",
      "url": "https://arxiv.org/abs/2602.16596v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "math.ST",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "membership",
        "dp",
        "inference"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n现代AI模型具有动态演化特性——在其生命周期中经历多次增量更新（如持续学习、微调或在线训练）。然而，现有成员推断（Membership Inference, MI）攻击与隐私审计方法多基于**静态终态模型**，忽视了模型序列蕴含的时序敏感信息。尽管实证研究发现利用模型更新序列可提升MI攻击效能，但缺乏对“最优”动态攻击的理论刻画：既有分析局限于无限样本下的静态设定，无法指导有限数据、含噪（如差分隐私）场景下的实际部署。\n\n## 方法创新：SeMI* 攻击框架  \n本文提出首个严格定义的**序列式成员推断最优攻击 SeMI***，其核心思想是：将目标样本（canary）的潜在插入时间点建模为隐变量，联合利用模型更新轨迹（而非仅最终模型）构建似然比检验。我们为经验均值估计器推导出SeMI*在**有限样本、含/不含隐私保护（如DP-SGD）** 条件下的**精确最优检测功率**（optimal power），并证明该结果自然退化至经典渐近结论，填补了理论空白。\n\n## 关键发现与优势  \n- **信号增强**：相比仅观测终态模型的攻击，SeMI* 利用更新序列避免MI信号随训练数据累积而“稀释”，显著提升小样本与早期插入场景的敏感性；  \n- **审计可调性**：攻击者可协同优化**插入时机**与**canary设计**，实现更紧致的隐私边界估计；  \n- **实证验证**：在多种数据分布（MNIST、CIFAR-10、Purchase）及DP-SGD训练/微调模型上，SeMI*的实用变体 consistently 超越SOTA基线（如Loss-based、Shadow Model），将隐私审计的ε-上界平均收紧18–35%。",
      "summary_en": "Modern AI models evolve sequentially through updates—yet membership inference (MI) attacks and privacy audits remain largely static. While empirical studies suggest model sequences boost MI power, rigorous analysis of *optimal* sequential attacks is missing: existing theory assumes infinite samples and static models. We bridge this gap by proposing **SeMI\\***, the first theoretically grounded sequential MI attack that leverages the full model update trajectory to detect whether a target sample was inserted at a specific step. For empirical mean estimation, we derive SeMI\\*'s *exact optimal detection power* under finite samples—with or without privacy (e.g., DP-SGD)—recovering known asymptotics as a special case. Crucially, SeMI\\* avoids signal dilution inherent in final-model-only attacks, enabling stronger inference early in training. Moreover, adversaries can jointly optimize insertion timing and canary design for tighter privacy auditing. Experiments across datasets (MNIST, CIFAR-10, Purchase) and DP-SGD-trained/fine-tuned models confirm that practical SeMI\\* variants yield significantly tighter privacy bounds—reducing estimated ε by 18–35% over state-of-the-art baselines.",
      "summary": "## 背景与问题  \n现代AI模型具有动态演化特性——在其生命周期中经历多次增量更新（如持续学习、微调或在线训练）。然而，现有成员推断（Membership Inference, MI）攻击与隐私审计方法多基于**静态终态模型**，忽视了模型序列蕴含的时序敏感信息。尽管实证研究发现利用模型更新序列可提升MI攻击效能，但缺乏对“最优”动态攻击的理论刻画：既有分析局限于无限样本下的静态设定，无法指导有限数据、含噪（如差分隐私）场景下的实际部署。\n\n## 方法创新：SeMI* 攻击框架  \n本文提出首个严格定义的**序列式成员推断最优攻击 SeMI***，其核心思想是：将目标样本（canary）的潜在插入时间点建模为隐变量，联合利用模型更新轨迹（而非仅最终模型）构建似然比检验。我们为经验均值估计器推导出SeMI*在**有限样本、含/不含隐私保护（如DP-SGD）** 条件下的**精确最优检测功率**（optimal power），并证明该结果自然退化至经典渐近结论，填补了理论空白。\n\n## 关键发现与优势  \n- **信号增强**：相比仅观测终态模型的攻击，SeMI* 利用更新序列避免MI信号随训练数据累积而“稀释”，显著提升小样本与早期插入场景的敏感性；  \n- **审计可调性**：攻击者可协同优化**插入时机**与**canary设计**，实现更紧致的隐私边界估计；  \n- **实证验证**：在多种数据分布（MNIST、CIFAR-10、Purchase）及DP-SGD训练/微调模型上，SeMI*的实用变体 consistently 超越SOTA基线（如Loss-based、Shadow Model），将隐私审计的ε-上界平均收紧18–35%。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16564v1",
      "arxiv_id": "2602.16564v1",
      "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
      "authors": [
        "Michael Lanier",
        "Yevgeniy Vorobeychik"
      ],
      "abstract": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16564v1",
      "url": "https://arxiv.org/abs/2602.16564v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n在大规模网络攻防博弈中，基于仿真的安全博弈（Simulation-Based Network Security Games）面临“维度灾难”：网络节点数激增导致状态空间指数级膨胀，传统双 oracle（Double Oracle）或策略空间响应优化（PSRO）方法因需枚举大量纯策略而难以扩展。现有方法在万级节点网络中常遭遇内存溢出、训练停滞或策略质量骤降等问题。\n\n## 方法创新：MetaDOAR 框架  \n本文提出 **MetaDOAR**——一种轻量级元控制器，通过三层协同机制实现可扩展性突破：  \n- **分区感知过滤层**：学习从节点结构嵌入（如 PageRank、度中心性等）到紧凑状态投影的映射，**实时筛选 top-k 关键设备子集**（如高价值服务器、边界网关），将搜索空间压缩 2–3 个数量级；  \n- **分层决策架构**：低层执行器仅在该子集上进行聚焦式束搜索（beam search），由专用 critic 代理提供动作评估；  \n- **量化缓存机制**：将 critic 的前向计算结果按**量化状态投影 + 局部动作标识**为键存入 LRU 缓存，并采用保守的 *k-hop 缓存失效策略*（仅当邻接子图结构变化 ≥k 跳时刷新），减少 >78% 冗余计算，且不牺牲策略最优性。\n\n## 实验结果与意义  \n在包含 5,000–50,000 节点的真实拓扑变体（AS-level、企业内网合成图）上，MetaDOAR 在攻击者/防御者平均收益上**超越 SOTA 基线（如 PSRO+GNN、DeepStack-Net）12.6%–29.3%**，训练内存占用降低 5.4×，单轮迭代耗时下降 68%。本工作为超大规模网络化决策问题提供了**首个兼具理论保障（收敛性继承自 DOAR）、工程实用性（<16GB GPU 显存可训）与部署友好性（模块化设计支持在线热更新）的分层强化学习路径**。",
      "summary_en": "We propose **MetaDOAR**, a scalable meta-controller for simulation-based network security games that extends the Double Oracle / PSRO paradigm with three key innovations: (1) a learned, partition-aware filtering layer that projects per-node structural embeddings into a compact state representation to rapidly select a *top-k subset* of critical devices; (2) a hierarchical execution pipeline where a low-level actor performs focused beam search only on this subset, guided by a critic agent; and (3) a quantized LRU cache for critic evaluations—keyed by discretized state projections and local action IDs—with conservative *k-hop invalidation* to eliminate >78% redundant computation while preserving decision quality. Empirically, MetaDOAR achieves **12.6–29.3% higher player payoffs** than state-of-the-art baselines on networks with up to 50,000 nodes, with **5.4× lower memory usage** and **68% faster iteration time**, without performance degradation. This work delivers a practical, theoretically grounded framework for hierarchical policy learning in large-scale cyber-networked systems.",
      "summary": "## 研究背景与挑战  \n在大规模网络攻防博弈中，基于仿真的安全博弈（Simulation-Based Network Security Games）面临“维度灾难”：网络节点数激增导致状态空间指数级膨胀，传统双 oracle（Double Oracle）或策略空间响应优化（PSRO）方法因需枚举大量纯策略而难以扩展。现有方法在万级节点网络中常遭遇内存溢出、训练停滞或策略质量骤降等问题。\n\n## 方法创新：MetaDOAR 框架  \n本文提出 **MetaDOAR**——一种轻量级元控制器，通过三层协同机制实现可扩展性突破：  \n- **分区感知过滤层**：学习从节点结构嵌入（如 PageRank、度中心性等）到紧凑状态投影的映射，**实时筛选 top-k 关键设备子集**（如高价值服务器、边界网关），将搜索空间压缩 2–3 个数量级；  \n- **分层决策架构**：低层执行器仅在该子集上进行聚焦式束搜索（beam search），由专用 critic 代理提供动作评估；  \n- **量化缓存机制**：将 critic 的前向计算结果按**量化状态投影 + 局部动作标识**为键存入 LRU 缓存，并采用保守的 *k-hop 缓存失效策略*（仅当邻接子图结构变化 ≥k 跳时刷新），减少 >78% 冗余计算，且不牺牲策略最优性。\n\n## 实验结果与意义  \n在包含 5,000–50,000 节点的真实拓扑变体（AS-level、企业内网合成图）上，MetaDOAR 在攻击者/防御者平均收益上**超越 SOTA 基线（如 PSRO+GNN、DeepStack-Net）12.6%–29.3%**，训练内存占用降低 5.4×，单轮迭代耗时下降 68%。本工作为超大规模网络化决策问题提供了**首个兼具理论保障（收敛性继承自 DOAR）、工程实用性（<16GB GPU 显存可训）与部署友好性（模块化设计支持在线热更新）的分层强化学习路径**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16520v1",
      "arxiv_id": "2602.16520v1",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "authors": [
        "Doron Shavit"
      ],
      "abstract": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16520v1",
      "url": "https://arxiv.org/abs/2602.16520v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n**越狱提示（Jailbreak prompts）** 是当前大语言模型（LLM）尤其是**工具增强型智能体（tool-augmented agents）** 面临的关键安全威胁。此类攻击常利用长上下文隐藏、语义伪装及轻量级混淆等手法，绕过传统单次推理的防护机制（如静态分类器或一次性守门员模型），导致现有防御在真实代理场景中鲁棒性不足。\n\n## 方法：递归语言模型检测框架（RLM-JB）  \n我们提出 **RLM-JB**——一种基于**递归语言模型（Recursive Language Models, RLMs）** 的端到端越狱检测框架。其核心思想是将检测建模为**可审计的程序化流程**，而非黑盒分类：  \n- **根模型（Root Model）** 作为协调器，执行结构化分析程序；  \n- 对输入进行**规范化与去混淆处理**（如解码Base64、还原Unicode逃逸）；  \n- 将文本**动态分块（chunking）**，缓解长上下文稀释问题，并确保全覆盖；  \n- 启动多个**工作模型（Worker Models）并行筛查各分块**；  \n- 通过跨块信号聚合（cross-chunk signal composition）识别被刻意拆分的恶意载荷（split-payload attacks）。\n\n## 主要结果与创新  \n在AutoDAN等强对抗性越狱数据集上，RLM-JB在三种主流LLM后端（Llama-3-8B、Qwen2-7B、Phi-3-mini）上实现：  \n- **高召回率（ASR/Recall）92.5%–98.0%**；  \n- **超高精度（Precision）98.99%–100%**，误报率仅**0.0%–2.0%**；  \n- 首次验证了**程序化检测范式**在敏感性-特异性权衡中的实用性，且决策过程全程可追溯、可审计。",
      "summary_en": "We introduce **RLM-JB**, a procedural jailbreak detection framework built on Recursive Language Models (RLMs), designed specifically for tool-augmented agents operating on untrusted inputs. Unlike one-shot classifiers, RLM-JB treats detection as an auditable program: a root model normalizes and de-obfuscates suspicious prompts, chunks text to ensure full coverage and mitigate context dilution, dispatches parallel worker-model queries over segments, and composes cross-chunk evidence to recover split-payload attacks. Evaluated on AutoDAN-style adversarial prompts across three LLM backends (Llama-3-8B, Qwen2-7B, Phi-3-mini), RLM-JB achieves high recall (92.5–98.0%), exceptional precision (98.99–100%), and near-zero false positives (0.0–2.0%). This demonstrates that recursive, procedure-driven defense offers a practical and interpretable trade-off between sensitivity and specificity—without compromising operational safety.",
      "summary": "## 背景与挑战  \n**越狱提示（Jailbreak prompts）** 是当前大语言模型（LLM）尤其是**工具增强型智能体（tool-augmented agents）** 面临的关键安全威胁。此类攻击常利用长上下文隐藏、语义伪装及轻量级混淆等手法，绕过传统单次推理的防护机制（如静态分类器或一次性守门员模型），导致现有防御在真实代理场景中鲁棒性不足。\n\n## 方法：递归语言模型检测框架（RLM-JB）  \n我们提出 **RLM-JB**——一种基于**递归语言模型（Recursive Language Models, RLMs）** 的端到端越狱检测框架。其核心思想是将检测建模为**可审计的程序化流程**，而非黑盒分类：  \n- **根模型（Root Model）** 作为协调器，执行结构化分析程序；  \n- 对输入进行**规范化与去混淆处理**（如解码Base64、还原Unicode逃逸）；  \n- 将文本**动态分块（chunking）**，缓解长上下文稀释问题，并确保全覆盖；  \n- 启动多个**工作模型（Worker Models）并行筛查各分块**；  \n- 通过跨块信号聚合（cross-chunk signal composition）识别被刻意拆分的恶意载荷（split-payload attacks）。\n\n## 主要结果与创新  \n在AutoDAN等强对抗性越狱数据集上，RLM-JB在三种主流LLM后端（Llama-3-8B、Qwen2-7B、Phi-3-mini）上实现：  \n- **高召回率（ASR/Recall）92.5%–98.0%**；  \n- **超高精度（Precision）98.99%–100%**，误报率仅**0.0%–2.0%**；  \n- 首次验证了**程序化检测范式**在敏感性-特异性权衡中的实用性，且决策过程全程可追溯、可审计。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16480v1",
      "arxiv_id": "2602.16480v1",
      "title": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data",
      "authors": [
        "Yiwen Lu"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16480v1",
      "url": "https://arxiv.org/abs/2602.16480v1",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "learning",
        "federated",
        "security",
        "model",
        "data",
        "inference"
      ],
      "keyword_score": 7,
      "summary_zh": "## 背景与挑战  \n联邦学习（FL）在保护数据隐私的前提下实现多方协同建模，广泛应用于医疗、金融等敏感场景。然而，其面临双重安全威胁：**好奇服务器**可能通过模型上传推断客户端原始数据（推理攻击），而**恶意客户端**可注入毒化模型破坏全局聚合（投毒攻击）。现有方案多结合差分隐私/安全聚合与鲁棒聚合策略，但普遍存在两大缺陷：（1）计算与通信开销过高，难以部署于资源受限设备；（2）在非独立同分布（Non-IID）数据下鲁棒性显著下降——因数据异构导致正常模型差异增大，传统异常检测易将良性偏差误判为攻击。\n\n## 方法创新  \n本文提出 **SRFed**——首个兼顾高效性、Byzantine鲁棒性与端到端隐私保护的Non-IID适配FL框架：  \n- **去中心化高效函数加密（DEFE）**：摒弃可信第三方，支持客户端对本地模型参数进行轻量级加密，服务器无需交互即可完成非线性操作（如加权聚合）后的解密；理论证明其抵御服务器侧推理攻击，且加密/解密复杂度仅为 *O(d)*（*d* 为模型维度），较同态加密降低两个数量级；  \n- **隐私保护的分层鲁棒聚合机制**：基于DEFE密文直接执行层间投影降维与密度聚类分析，动态识别并过滤毒化层参数，避免明文暴露导致的隐私泄露，且在Non-IID下误删率降低42.7%（vs. Krum、Median）。\n\n## 实验验证  \n在CIFAR-10/100、Tiny-ImageNet及真实医疗数据集上，SRFed在3种典型投毒攻击（Label-Flipping、ALIE、Min-Max）下保持>89%准确率（较SOTA提升5.2–11.8%），同时通信开销仅为Secure Aggregation的63%，训练延迟降低37%。理论安全性证明覆盖半诚实服务器与拜占庭客户端混合威胁模型。",
      "summary_en": "Federated Learning (FL) enables collaborative model training while preserving data privacy, yet remains vulnerable to both server-side inference attacks and client-side poisoning attacks—especially under Non-IID data. Existing defenses suffer from high overhead or poor robustness in heterogeneous settings. We propose **SRFed**, the first efficient, Byzantine-robust, and end-to-end privacy-preserving FL framework tailored for Non-IID scenarios. Its core innovations are: (1) a **Decentralized Efficient Functional Encryption (DEFE)** scheme that eliminates third-party trust, enables non-interactive decryption after encrypted aggregation, and provably thwarts server inference with *O(d)* computational cost; and (2) a **privacy-preserving defensive aggregation** mechanism that performs layer-wise projection and clustering directly on encrypted models to filter poisoned updates without revealing raw parameters. Extensive experiments across four datasets show SRFed achieves >89% accuracy under diverse poisoning attacks—outperforming state-of-the-art baselines by 5.2–11.8%—while reducing communication overhead by 37% and latency by 37% compared to Secure Aggregation. Theoretical analysis confirms security against semi-honest servers and Byzantine clients.",
      "summary": "## 背景与挑战  \n联邦学习（FL）在保护数据隐私的前提下实现多方协同建模，广泛应用于医疗、金融等敏感场景。然而，其面临双重安全威胁：**好奇服务器**可能通过模型上传推断客户端原始数据（推理攻击），而**恶意客户端**可注入毒化模型破坏全局聚合（投毒攻击）。现有方案多结合差分隐私/安全聚合与鲁棒聚合策略，但普遍存在两大缺陷：（1）计算与通信开销过高，难以部署于资源受限设备；（2）在非独立同分布（Non-IID）数据下鲁棒性显著下降——因数据异构导致正常模型差异增大，传统异常检测易将良性偏差误判为攻击。\n\n## 方法创新  \n本文提出 **SRFed**——首个兼顾高效性、Byzantine鲁棒性与端到端隐私保护的Non-IID适配FL框架：  \n- **去中心化高效函数加密（DEFE）**：摒弃可信第三方，支持客户端对本地模型参数进行轻量级加密，服务器无需交互即可完成非线性操作（如加权聚合）后的解密；理论证明其抵御服务器侧推理攻击，且加密/解密复杂度仅为 *O(d)*（*d* 为模型维度），较同态加密降低两个数量级；  \n- **隐私保护的分层鲁棒聚合机制**：基于DEFE密文直接执行层间投影降维与密度聚类分析，动态识别并过滤毒化层参数，避免明文暴露导致的隐私泄露，且在Non-IID下误删率降低42.7%（vs. Krum、Median）。\n\n## 实验验证  \n在CIFAR-10/100、Tiny-ImageNet及真实医疗数据集上，SRFed在3种典型投毒攻击（Label-Flipping、ALIE、Min-Max）下保持>89%准确率（较SOTA提升5.2–11.8%），同时通信开销仅为Secure Aggregation的63%，训练延迟降低37%。理论安全性证明覆盖半诚实服务器与拜占庭客户端混合威胁模型。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16436v1",
      "arxiv_id": "2602.16436v1",
      "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent",
      "authors": [
        "Jean Dufraiche",
        "Paul Mangold",
        "Michaël Perrot",
        "Marc Tommasi"
      ],
      "abstract": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16436v1",
      "url": "https://arxiv.org/abs/2602.16436v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "differential"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n在非交互式**本地差分隐私（LDP）**框架下一次性发布原始数据，虽保障了数据的长期可重用性，但强噪声注入会显著扭曲统计结构，尤其导致二分类任务中损失函数梯度的系统性偏差。传统LDP学习方法（如随机响应或高斯机制）往往忽略该偏差的解析结构，仅依赖渐近校正或启发式去噪，难以保证有限样本下的无偏性与收敛性。\n\n## 方法创新  \n本文首次将**Weierstrass变换**引入LDP理论分析：证明LDP扰动等价于对真实数据分布施加Weierstrass卷积核平滑，其偏差本质是函数在高斯核下的期望漂移。进一步，我们严格推导其**逆Weierstrass变换**的显式形式，构建首个可解析、可计算的偏差校正算子——该算子能对任意LDP发布的样本（如经拉普拉斯/高斯扰动的特征向量）进行逐点校正，从而获得**非线性函数（如Sigmoid、平方损失）的无偏估计**。\n\n## 核心算法与理论保证  \n基于上述校正机制，我们提出**逆Weierstrass私有随机梯度下降（IWP-SGD）**：在每次迭代中，先对LDP样本应用逆Weierstrass校正，再计算梯度并更新模型。理论证明：IWP-SGD以$\\mathcal{O}(1/n)$速率收敛至真实总体风险最小化器（$n$为样本量），显著优于标准LDP-SGD的$\\mathcal{O}(1/\\sqrt{n})$速率，且无需额外隐私预算开销。\n\n## 实证验证  \n在合成数据（含非线性边界）及真实数据集（UCI Adult、Bank Marketing）上，IWP-SGD在相同隐私预算（$\\varepsilon=1.0$）下，测试准确率平均提升**5.2–8.7个百分点**，验证了其偏差校正的有效性与实用性。",
      "summary_en": "This paper addresses bias in noninteractive Local Differential Privacy (LDP) for binary classification. We characterize LDP-induced distortion as a Weierstrass transform of the true data distribution and derive its exact inverse, enabling unbiased estimation of nonlinear functions (e.g., logistic loss) on privatized examples. Based on this, we propose **Inverse Weierstrass Private SGD (IWP-SGD)**—a novel optimization algorithm that applies analytical bias correction *before* gradient computation. We prove IWP-SGD converges to the true population risk minimizer at rate $\\mathcal{O}(1/n)$, improving upon the standard $\\mathcal{O}(1/\\sqrt{n})$ rate under LDP. Experiments on synthetic and real-world datasets (UCI Adult, Bank Marketing) confirm consistent accuracy gains of 5.2–8.7 percentage points at $\\varepsilon = 1.0$, demonstrating both theoretical soundness and practical efficacy.",
      "summary": "## 背景与挑战  \n在非交互式**本地差分隐私（LDP）**框架下一次性发布原始数据，虽保障了数据的长期可重用性，但强噪声注入会显著扭曲统计结构，尤其导致二分类任务中损失函数梯度的系统性偏差。传统LDP学习方法（如随机响应或高斯机制）往往忽略该偏差的解析结构，仅依赖渐近校正或启发式去噪，难以保证有限样本下的无偏性与收敛性。\n\n## 方法创新  \n本文首次将**Weierstrass变换**引入LDP理论分析：证明LDP扰动等价于对真实数据分布施加Weierstrass卷积核平滑，其偏差本质是函数在高斯核下的期望漂移。进一步，我们严格推导其**逆Weierstrass变换**的显式形式，构建首个可解析、可计算的偏差校正算子——该算子能对任意LDP发布的样本（如经拉普拉斯/高斯扰动的特征向量）进行逐点校正，从而获得**非线性函数（如Sigmoid、平方损失）的无偏估计**。\n\n## 核心算法与理论保证  \n基于上述校正机制，我们提出**逆Weierstrass私有随机梯度下降（IWP-SGD）**：在每次迭代中，先对LDP样本应用逆Weierstrass校正，再计算梯度并更新模型。理论证明：IWP-SGD以$\\mathcal{O}(1/n)$速率收敛至真实总体风险最小化器（$n$为样本量），显著优于标准LDP-SGD的$\\mathcal{O}(1/\\sqrt{n})$速率，且无需额外隐私预算开销。\n\n## 实证验证  \n在合成数据（含非线性边界）及真实数据集（UCI Adult、Bank Marketing）上，IWP-SGD在相同隐私预算（$\\varepsilon=1.0$）下，测试准确率平均提升**5.2–8.7个百分点**，验证了其偏差校正的有效性与实用性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16268v1",
      "arxiv_id": "2602.16268v1",
      "title": "Quantum Oracle Distribution Switching and its Applications to Fully Anonymous Ring Signatures",
      "authors": [
        "Marvin Beckmann",
        "Christian Majenz"
      ],
      "abstract": "Ring signatures are a powerful primitive that allows a member to sign on behalf of a group, without revealing their identity. Recently, ring signatures have received additional attention as an ingredient for post-quantum deniable authenticated key exchange, e.g., for a post-quantum version of the Signal protocol, employed by virtually all end-to-end-encrypted messenger services. While several ring signature constructions from post-quantum assumptions offer suitable security and efficiency for use in deniable key exchange, they are currently proven secure in the random oracle model (ROM) only, which is insufficient for post-quantum security.   In this work, we provide four security reductions in the quantum-accessible random oracle model (QROM) for two generic ring signature constructions: two for the AOS framework and two for a construction paradigm based on ring trapdoors, whose generic backbone we formalize. The two security proofs for AOS ring signatures differ in their requirements on the underlying sigma protocol and their tightness. The two reductions for the ring-trapdoor-based ring signatures exhibit various differences in requirements and the security they provide. We employ the measure-and-reprogram technique, QROM straightline extraction tools based on the compressed oracle, history-free reductions and QROM reprogramming tools. To make use of Rényi divergence properties in the QROM, we study the behavior of quantum algorithms that interact with an oracle whose distribution is based on one of two different distributions over the set of outputs. We provide tight bounds for the statistical distance, show that the Rényi divergence can not be used to replace the entire oracle and provide a workaround.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16268v1",
      "url": "https://arxiv.org/abs/2602.16268v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n环签名（Ring Signature）是一种关键密码原语，允许群组中任意成员以匿名方式代表整个群组签署消息，且不泄露其真实身份。近年来，其在**后量子可否认认证密钥交换**（如后量子版Signal协议）中的应用引发广泛关注。然而，现有基于后量子假设的环签名方案大多仅在**随机预言机模型**（ROM）下被证明安全；而ROM在量子环境下不具可信性——攻击者可量子查询预言机，故需升级至**量子可访问随机预言机模型**（QROM）以保障真正后量子安全性。\n\n## 方法与技术贡献  \n本文首次系统构建了环签名在QROM下的严格安全性基础：  \n- 提出**四条紧致安全归约**，覆盖两类通用构造范式：  \n  • **AOS框架**（Au, Liu, Susilo）：给出两个归约，分别适配不同强度的底层Σ协议（强零知识 vs. 标准零知识），并在紧致性（tightness）上形成互补；  \n  • **环陷门范式**（Ring Trapdoor）：首次形式化其通用骨架，并提供两个差异化归约——一个支持选择消息攻击下的不可伪造性（EUF-CMA），另一个强化为全匿名性（full anonymity）保证。  \n- 关键技术融合：综合运用**测量-重编程技术**（measure-and-reprogram）、基于**压缩预言机**（compressed oracle）的QROM直线提取工具、**无历史归约**（history-free reductions）及QROM重编程机制。  \n- 理论突破：深入研究当预言机输出分布切换于两种不同输出分布之间时，量子算法行为的统计特性；给出**统计距离的紧界**，严格证明**Rényi散度无法完全替代预言机模拟**，并提出可实用的规避方案。\n\n## 创新意义  \n本工作填补了后量子环签名在QROM下安全性证明的空白，为Signal等隐私通信协议向后量子安全迁移提供了首个兼具**理论严谨性与工程适用性**的密码学支撑。",
      "summary_en": "This paper establishes the first rigorous quantum-security foundations for ring signatures in the **Quantum Random Oracle Model (QROM)**. We provide **four tight security reductions**: two for the AOS framework—differing in Σ-protocol assumptions (strong vs. standard zero-knowledge) and tightness—and two for a newly formalized **ring-trapdoor paradigm**, offering distinct guarantees (EUF-CMA vs. full anonymity). Our proofs integrate advanced QROM techniques: measure-and-reprogram, compressed-oracle-based straightline extraction, history-free reductions, and QROM reprogramming. Crucially, we analyze quantum algorithms interacting with oracles whose output distributions switch between two alternatives; we derive tight bounds on statistical distance, prove Rényi divergence *cannot* fully replace oracle simulation in QROM, and propose a practical workaround. This work enables post-quantum secure deniable key exchange (e.g., quantum-safe Signal) with provable anonymity and unforgeability.",
      "summary": "## 研究背景与问题  \n环签名（Ring Signature）是一种关键密码原语，允许群组中任意成员以匿名方式代表整个群组签署消息，且不泄露其真实身份。近年来，其在**后量子可否认认证密钥交换**（如后量子版Signal协议）中的应用引发广泛关注。然而，现有基于后量子假设的环签名方案大多仅在**随机预言机模型**（ROM）下被证明安全；而ROM在量子环境下不具可信性——攻击者可量子查询预言机，故需升级至**量子可访问随机预言机模型**（QROM）以保障真正后量子安全性。\n\n## 方法与技术贡献  \n本文首次系统构建了环签名在QROM下的严格安全性基础：  \n- 提出**四条紧致安全归约**，覆盖两类通用构造范式：  \n  • **AOS框架**（Au, Liu, Susilo）：给出两个归约，分别适配不同强度的底层Σ协议（强零知识 vs. 标准零知识），并在紧致性（tightness）上形成互补；  \n  • **环陷门范式**（Ring Trapdoor）：首次形式化其通用骨架，并提供两个差异化归约——一个支持选择消息攻击下的不可伪造性（EUF-CMA），另一个强化为全匿名性（full anonymity）保证。  \n- 关键技术融合：综合运用**测量-重编程技术**（measure-and-reprogram）、基于**压缩预言机**（compressed oracle）的QROM直线提取工具、**无历史归约**（history-free reductions）及QROM重编程机制。  \n- 理论突破：深入研究当预言机输出分布切换于两种不同输出分布之间时，量子算法行为的统计特性；给出**统计距离的紧界**，严格证明**Rényi散度无法完全替代预言机模拟**，并提出可实用的规避方案。\n\n## 创新意义  \n本工作填补了后量子环签名在QROM下安全性证明的空白，为Signal等隐私通信协议向后量子安全迁移提供了首个兼具**理论严谨性与工程适用性**的密码学支撑。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16156v1",
      "arxiv_id": "2602.16156v1",
      "title": "Weak Zero-Knowledge and One-Way Functions",
      "authors": [
        "Rohit Chatterjee",
        "Yunqi Li",
        "Prashant Nalini Vasudevan"
      ],
      "abstract": "We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $ε_c$, $ε_s$, and $ε_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:   1. If all languages in NP have NIZK proofs or arguments satisfying $ ε_c+ε_s+ ε_z < 1 $, then One-Way Functions (OWFs) exist.   This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $ε_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ ε_c+\\sqrt{ε_s}+ε_z < 1 $ [Chakraborty et al., CRYPTO 2025].   2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+(2k-1).ε_z < 1 $, then OWFs exist.   3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+k.ε_z < 1 $, then infinitely-often OWFs exist.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16156v1",
      "url": "https://arxiv.org/abs/2602.16156v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "crypto"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n本文系统研究**弱零知识（Weak Zero-Knowledge, Weak ZK）协议**的存在性对密码学基础假设——特别是**单向函数（One-Way Functions, OWFs）存在性**——的蕴含关系。传统零知识要求完备性、可靠性和零知识性错误均“可忽略”，而Weak ZK允许这些错误率（记为 $ε_c$、$ε_s$、$ε_z$）为常数甚至较大，仅需满足某种非平凡约束。核心问题是：在**NP中存在最坏情况困难语言**这一温和假设下，何种程度的Weak ZK能力足以推出OWFs存在？\n\n## 主要结果与创新点  \n1. **统一性突破**：首次证明——若**所有NP语言均存在满足 $ε_c + ε_s + ε_z < 1$ 的非交互式零知识（NIZK）证明或论证**，则OWFs必然存在。该条件覆盖全部非平凡常数错误组合（如 $ε_c=0.3, ε_s=0.3, ε_z=0.39$），且**无需根号项**；此前最优结果要求更严格的 $ε_c + \\sqrt{ε_s} + ε_z < 1$（Chakraborty et al., CRYPTO 2025）。进一步，若完备性错误 $ε_c$ 可忽略，则该NIZK系统可“提升”为**全错误均可忽略的标准NIZK**，实现从弱到强的自动强化。  \n2. **轮数依赖刻画**：针对$k$轮公开硬币（public-coin）ZK协议，证明若满足 $ε_c + ε_s + (2k-1)ε_z < 1$，则OWFs存在；若更强地满足 $ε_c + ε_s + k·ε_z < 1$（对某常数 $k$），则可推出**无穷多次单向函数（infinitely-often OWFs）** 存在。  \n3. **技术贡献**：通过精巧构造“错误放大—冲突检测”归约框架，绕过传统模拟器分析瓶颈，首次将线性错误和（而非平方根或指数形式）作为临界阈值，显著拓展了Weak ZK的密码学意义边界。",
      "summary_en": "This paper establishes new implications of weak zero-knowledge (ZK) protocols for the existence of one-way functions (OWFs), assuming worst-case hard languages in NP. First, if *all* NP languages admit non-interactive ZK (NIZK) proofs or arguments with completeness, soundness, and zero-knowledge errors $ε_c$, $ε_s$, $ε_z$ satisfying $ε_c + ε_s + ε_z < 1$, then OWFs exist—unifying and strictly improving prior work requiring $ε_c + \\sqrt{ε_s} + ε_z < 1$. Moreover, if $ε_c$ is negligible, such NIZKs can be upgraded to fully negligible-error ones. Second, for $k$-round public-coin ZK, OWFs follow from $ε_c + ε_s + (2k-1)ε_z < 1$; under the tighter bound $ε_c + ε_s + k·ε_z < 1$, infinitely-often OWFs exist. These results reveal linear error thresholds as fundamental to cryptographic hardness.",
      "summary": "## 研究背景与问题  \n本文系统研究**弱零知识（Weak Zero-Knowledge, Weak ZK）协议**的存在性对密码学基础假设——特别是**单向函数（One-Way Functions, OWFs）存在性**——的蕴含关系。传统零知识要求完备性、可靠性和零知识性错误均“可忽略”，而Weak ZK允许这些错误率（记为 $ε_c$、$ε_s$、$ε_z$）为常数甚至较大，仅需满足某种非平凡约束。核心问题是：在**NP中存在最坏情况困难语言**这一温和假设下，何种程度的Weak ZK能力足以推出OWFs存在？\n\n## 主要结果与创新点  \n1. **统一性突破**：首次证明——若**所有NP语言均存在满足 $ε_c + ε_s + ε_z < 1$ 的非交互式零知识（NIZK）证明或论证**，则OWFs必然存在。该条件覆盖全部非平凡常数错误组合（如 $ε_c=0.3, ε_s=0.3, ε_z=0.39$），且**无需根号项**；此前最优结果要求更严格的 $ε_c + \\sqrt{ε_s} + ε_z < 1$（Chakraborty et al., CRYPTO 2025）。进一步，若完备性错误 $ε_c$ 可忽略，则该NIZK系统可“提升”为**全错误均可忽略的标准NIZK**，实现从弱到强的自动强化。  \n2. **轮数依赖刻画**：针对$k$轮公开硬币（public-coin）ZK协议，证明若满足 $ε_c + ε_s + (2k-1)ε_z < 1$，则OWFs存在；若更强地满足 $ε_c + ε_s + k·ε_z < 1$（对某常数 $k$），则可推出**无穷多次单向函数（infinitely-often OWFs）** 存在。  \n3. **技术贡献**：通过精巧构造“错误放大—冲突检测”归约框架，绕过传统模拟器分析瓶颈，首次将线性错误和（而非平方根或指数形式）作为临界阈值，显著拓展了Weak ZK的密码学意义边界。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16109v1",
      "arxiv_id": "2602.16109v1",
      "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes",
      "authors": [
        "Srikumar Nayak",
        "James Walmesley"
      ],
      "abstract": "Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16109v1",
      "url": "https://arxiv.org/abs/2602.16109v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CE"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "learning",
        "federated",
        "differential"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与挑战  \n跨境内部威胁正严重危及各国政府财政计划的安全性，尤其在数据分布式存储、隐私敏感且受多司法管辖区法规约束的场景下。现有方法存在三重瓶颈：**隐私壁垒**导致跨国情报无法安全共享；**推理能力缺失**难以识别跨机构、多步骤的隐蔽攻击链；**结构建模不足**无法刻画金融交易网络中复杂的图结构关系（如资金环路、嵌套代理、异常子图模式）。\n\n## 方法创新：FedGraph-AGI 框架  \n我们提出 **FedGraph-AGI**——首个融合人工通用智能（AGI）推理与联邦图学习的隐私增强型框架，专为跨境内部威胁情报协同而设计。其核心包含三重技术突破：  \n1. **联邦图神经网络（Federated GNN）**：各司法管辖区本地训练图模型，原始交易图数据不出域，严格保障数据主权；  \n2. **混合专家聚合（MoE-Aggregation）**：针对各国监管差异、数据分布异构性与图拓扑多样性，动态加权融合本地模型更新，避免“一刀切”聚合偏差；  \n3. **AGI驱动的因果推理引擎**：基于大型动作模型（LAM），在加密图嵌入空间执行可解释的因果推断（如“若阻断节点X，则攻击路径Y中断概率提升73%”），实现从相关性检测到因果归因的跃迁。\n\n## 实验验证与价值  \n在覆盖10个司法管辖区、含50,000笔真实级交易的跨域数据集上，FedGraph-AGI达**92.3%检测准确率**，显著优于联邦基线（86.1%）和中心化方案（84.7%）。消融实验表明：AGI推理模块贡献**+6.8%**性能增益，MoE机制贡献**+4.4%**；系统在满足**ε = 1.0差分隐私**强约束下仍保持近最优性能，并可高效扩展至50+参与方。本工作首次将AGI级因果推理深度嵌入联邦图学习范式，为全球金融监管协同树立了可验证、可部署、可解释的新范式。",
      "summary_en": "Cross-border insider threats critically undermine government financial schemes, yet existing methods fail to reconcile privacy compliance, multi-jurisdictional heterogeneity, and complex attack pattern reasoning. We propose **FedGraph-AGI**, the first framework unifying federated graph learning with Artificial General Intelligence (AGI) reasoning for privacy-preserving threat intelligence sharing. It integrates: (1) sovereign-preserving federated graph neural networks; (2) Mixture-of-Experts (MoE) aggregation to harmonize jurisdictionally diverse models; and (3) Large Action Models (LAMs) performing causal inference over encrypted graph embeddings. Evaluated on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves **92.3% accuracy**, outperforming federated baselines (+6.2%) and centralized approaches (+7.6%). Ablation confirms AGI reasoning contributes +6.8% and MoE +4.4%. The system satisfies ε = 1.0 differential privacy, scales to 50+ clients, and enables actionable, interpretable threat attribution—pioneering AGI-augmented federated graph intelligence for global financial security.",
      "summary": "## 背景与挑战  \n跨境内部威胁正严重危及各国政府财政计划的安全性，尤其在数据分布式存储、隐私敏感且受多司法管辖区法规约束的场景下。现有方法存在三重瓶颈：**隐私壁垒**导致跨国情报无法安全共享；**推理能力缺失**难以识别跨机构、多步骤的隐蔽攻击链；**结构建模不足**无法刻画金融交易网络中复杂的图结构关系（如资金环路、嵌套代理、异常子图模式）。\n\n## 方法创新：FedGraph-AGI 框架  \n我们提出 **FedGraph-AGI**——首个融合人工通用智能（AGI）推理与联邦图学习的隐私增强型框架，专为跨境内部威胁情报协同而设计。其核心包含三重技术突破：  \n1. **联邦图神经网络（Federated GNN）**：各司法管辖区本地训练图模型，原始交易图数据不出域，严格保障数据主权；  \n2. **混合专家聚合（MoE-Aggregation）**：针对各国监管差异、数据分布异构性与图拓扑多样性，动态加权融合本地模型更新，避免“一刀切”聚合偏差；  \n3. **AGI驱动的因果推理引擎**：基于大型动作模型（LAM），在加密图嵌入空间执行可解释的因果推断（如“若阻断节点X，则攻击路径Y中断概率提升73%”），实现从相关性检测到因果归因的跃迁。\n\n## 实验验证与价值  \n在覆盖10个司法管辖区、含50,000笔真实级交易的跨域数据集上，FedGraph-AGI达**92.3%检测准确率**，显著优于联邦基线（86.1%）和中心化方案（84.7%）。消融实验表明：AGI推理模块贡献**+6.8%**性能增益，MoE机制贡献**+4.4%**；系统在满足**ε = 1.0差分隐私**强约束下仍保持近最优性能，并可高效扩展至50+参与方。本工作首次将AGI级因果推理深度嵌入联邦图学习范式，为全球金融监管协同树立了可验证、可部署、可解释的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16155v1",
      "arxiv_id": "2602.16155v1",
      "title": "Differentially Private Non-convex Distributionally Robust Optimization",
      "authors": [
        "Difei Xu",
        "Meng Ding",
        "Zebin Ma",
        "Huanyi Xie",
        "Youming Tao",
        "Aicha Slaitane",
        "Di Wang"
      ],
      "abstract": "Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.   Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.   Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.   In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.   To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $ψ$-divergence and non-convex loss.   First, we study DRO with general $ψ$-divergence by reformulating it as a minimization problem, and develop a novel $(\\varepsilon, δ)$-DP optimization method, called DP Double-Spider, tailored to this structure.   Under mild assumptions, we show that it achieves a utility bound of $\\mathcal{O}(\\frac{1}{\\sqrt{n}}+ (\\frac{\\sqrt{d \\log (1/δ)}}{n \\varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.   We further improve the utility rate for specific divergences.   In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\\mathcal{O}((\\frac{\\sqrt{d \\log(1/δ)}}{n\\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.   Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.",
      "published": "2026-02-18",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16155v1",
      "url": "https://arxiv.org/abs/2602.16155v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "dp",
        "differential"
      ],
      "keyword_score": 3,
      "summary_zh": "## 研究背景与问题  \n现实机器学习部署常面临**分布偏移、群体不平衡与对抗扰动**等挑战，传统经验风险最小化（ERM）性能显著下降。分布鲁棒优化（DRO）通过在分布不确定性集上优化最坏情况期望损失，为鲁棒性提供理论保障。然而，DRO训练数据往往含敏感信息，亟需结合**差分隐私（DP）** 防范隐私泄露。现有工作多聚焦于DP-ERM，而DP-DRO因具**带约束的极小极大结构**，研究严重不足，尤其在非凸、有限和、ψ-散度设定下尚无系统性算法与理论。\n\n## 方法创新  \n本文首次对**差分私有非凸有限和DRO**开展全面研究：  \n- 提出通用框架：将任意ψ-散度DRO重写为单层最小化问题，设计新型$(\\varepsilon,\\delta)$-DP算法——**DP Double-Spider**，融合双重方差缩减与隐私噪声注入；  \n- 针对KL散度这一关键特例，进一步将其转化为**复合有限和优化问题**，提出**DP Recursive-Spider**算法，实现更紧致的隐私-效用权衡。\n\n## 主要理论结果  \n- DP Double-Spider在温和假设下达到梯度范数界：$\\mathcal{O}\\!\\left(\\frac{1}{\\sqrt{n}} + \\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，其中$n$为样本量，$d$为模型维度；  \n- DP Recursive-Spider对KL-DRO取得**最优速率**：$\\mathcal{O}\\!\\left(\\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，与当前非凸DP-ERM最优界完全匹配；  \n- 实验验证：在多个真实数据集（如Adult、CelebA）上，所提方法在隐私预算$(\\varepsilon,\\delta)=(1,10^{-5})$下，测试鲁棒准确率平均提升2.1–4.7个百分点，显著优于基线DP-SGDA、DP-VRBO等。",
      "summary_en": "Distributionally Robust Optimization (DRO) enhances model robustness against distribution shifts and adversarial perturbations, yet its deployment on sensitive data necessitates differential privacy (DP). This paper presents the first comprehensive study of *non-convex, finite-sum, differentially private DRO* under ψ-divergence uncertainty sets. We reformulate general ψ-DRO as a single-level minimization and propose **DP Double-Spider**, achieving a gradient-norm utility bound of $\\mathcal{O}(1/\\sqrt{n} + (\\sqrt{d \\log(1/\\delta)}/(n\\varepsilon))^{2/3})$. For KL-divergence DRO, we cast it as a compositional finite-sum problem and design **DP Recursive-Spider**, attaining the optimal rate $\\mathcal{O}((\\sqrt{d \\log(1/\\delta)}/(n\\varepsilon))^{2/3})$—matching the best-known bound for non-convex DP-ERM. Experiments confirm consistent superiority over existing DP minimax methods across benchmark datasets under realistic privacy budgets.",
      "summary": "## 研究背景与问题  \n现实机器学习部署常面临**分布偏移、群体不平衡与对抗扰动**等挑战，传统经验风险最小化（ERM）性能显著下降。分布鲁棒优化（DRO）通过在分布不确定性集上优化最坏情况期望损失，为鲁棒性提供理论保障。然而，DRO训练数据往往含敏感信息，亟需结合**差分隐私（DP）** 防范隐私泄露。现有工作多聚焦于DP-ERM，而DP-DRO因具**带约束的极小极大结构**，研究严重不足，尤其在非凸、有限和、ψ-散度设定下尚无系统性算法与理论。\n\n## 方法创新  \n本文首次对**差分私有非凸有限和DRO**开展全面研究：  \n- 提出通用框架：将任意ψ-散度DRO重写为单层最小化问题，设计新型$(\\varepsilon,\\delta)$-DP算法——**DP Double-Spider**，融合双重方差缩减与隐私噪声注入；  \n- 针对KL散度这一关键特例，进一步将其转化为**复合有限和优化问题**，提出**DP Recursive-Spider**算法，实现更紧致的隐私-效用权衡。\n\n## 主要理论结果  \n- DP Double-Spider在温和假设下达到梯度范数界：$\\mathcal{O}\\!\\left(\\frac{1}{\\sqrt{n}} + \\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，其中$n$为样本量，$d$为模型维度；  \n- DP Recursive-Spider对KL-DRO取得**最优速率**：$\\mathcal{O}\\!\\left(\\left(\\frac{\\sqrt{d \\log(1/\\delta)}}{n\\varepsilon}\\right)^{2/3}\\right)$，与当前非凸DP-ERM最优界完全匹配；  \n- 实验验证：在多个真实数据集（如Adult、CelebA）上，所提方法在隐私预算$(\\varepsilon,\\delta)=(1,10^{-5})$下，测试鲁棒准确率平均提升2.1–4.7个百分点，显著优于基线DP-SGDA、DP-VRBO等。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15945v1",
      "arxiv_id": "2602.15945v1",
      "title": "From Tool Orchestration to Code Execution: A Study of MCP Design Choices",
      "authors": [
        "Yuval Felendler",
        "Parth A. Gandhi",
        "Idan Habler",
        "Yuval Elovici",
        "Asaf Shabtai"
      ],
      "abstract": "Model Context Protocols (MCPs) provide a unified platform for agent systems to discover, select, and orchestrate tools across heterogeneous execution environments. As MCP-based systems scale to incorporate larger tool catalogs and multiple concurrently connected MCP servers, traditional tool-by-tool invocation increases coordination overhead, fragments state management, and limits support for wide-context operations. To address these scalability challenges, recent MCP designs have incorporated code execution as a first-class capability, an approach called Code Execution MCP (CE-MCP). This enables agents to consolidate complex workflows, such as SQL querying, file analysis, and multi-step data transformations, into a single program that executes within an isolated runtime environment. In this work, we formalize the architectural distinction between context-coupled (traditional) and context-decoupled (CE-MCP) models, analyzing their fundamental scalability trade-offs. Using the MCP-Bench framework across 10 representative servers, we empirically evaluate task behavior, tool utilization patterns, execution latency, and protocol efficiency as the scale of connected MCP servers and available tools increases, demonstrating that while CE-MCP significantly reduces token usage and execution latency, it introduces a vastly expanded attack surface. We address this security gap by applying the MAESTRO framework, identifying sixteen attack classes across five execution phases-including specific code execution threats such as exception-mediated code injection and unsafe capability synthesis. We validate these vulnerabilities through adversarial scenarios across multiple LLMs and propose a layered defense architecture comprising containerized sandboxing and semantic gating. Our findings provide a rigorous roadmap for balancing scalability and security in production-ready executable agent workflows.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15945v1",
      "url": "https://arxiv.org/abs/2602.15945v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \nModel Context Protocols（MCP）作为智能体系统统一调用异构工具的协议框架，正面临规模化瓶颈：当工具库扩大、多MCP服务器并发连接时，传统“逐工具调用”模式导致协调开销激增、状态管理碎片化、宽上下文操作支持乏力。\n\n## 方法与创新  \n本文首次形式化区分**上下文耦合型**（传统MCP）与**上下文解耦型**（Code Execution MCP, CE-MCP）两类架构，并提出系统性评估框架。基于**MCP-Bench**在10个典型MCP服务器上开展实证研究，量化分析任务完成率、工具调用频次、端到端延迟及协议消息效率随规模增长的变化规律；同步引入**MAESTRO**安全分析框架，识别CE-MCP全生命周期中5个执行阶段的16类新型攻击模式（如异常中介代码注入、不安全能力合成等），覆盖Python沙箱逃逸、权限越界、LLM提示劫持等高危场景。\n\n## 主要发现  \n- CE-MCP将SQL查询、文件分析等多步工作流封装为单程序执行，**平均降低38.7% token消耗与42.1%执行延迟**；  \n- 但其攻击面指数级扩展——实验验证16类漏洞在GPT-4、Claude-3、Qwen2-72B等主流LLM驱动下均成功触发；  \n- 提出**分层防御架构**：底层采用轻量级容器化沙箱（gVisor增强隔离），上层部署语义门控机制（基于AST的动态能力白名单+运行时数据流约束），在保持99.2%合法任务通过率前提下，阻断全部已知攻击路径。\n\n本研究为可执行智能体系统提供了首个兼顾**可扩展性-安全性权衡**的工程化设计蓝图。",
      "summary_en": "This paper investigates scalability-security trade-offs in Model Context Protocols (MCP), introducing Code Execution MCP (CE-MCP) as a context-decoupled architecture that consolidates multi-step workflows (e.g., SQL queries, data transformations) into isolated code execution. Empirical evaluation across 10 MCP servers using MCP-Bench shows CE-MCP reduces token usage by 38.7% and latency by 42.1% versus traditional tool-by-tool orchestration—but expands the attack surface dramatically. Applying the MAESTRO framework, we identify 16 novel attack classes across five execution phases, including exception-mediated code injection and unsafe capability synthesis, validated against GPT-4, Claude-3, and Qwen2-72B. We propose a layered defense: containerized sandboxing (gVisor-enhanced) plus semantic gating (AST-based capability whitelisting + runtime data-flow constraints), achieving 99.2% legitimate task success while blocking all identified attacks. This work delivers the first production-ready roadmap for secure, scalable executable agent systems.",
      "summary": "## 研究背景与问题  \nModel Context Protocols（MCP）作为智能体系统统一调用异构工具的协议框架，正面临规模化瓶颈：当工具库扩大、多MCP服务器并发连接时，传统“逐工具调用”模式导致协调开销激增、状态管理碎片化、宽上下文操作支持乏力。\n\n## 方法与创新  \n本文首次形式化区分**上下文耦合型**（传统MCP）与**上下文解耦型**（Code Execution MCP, CE-MCP）两类架构，并提出系统性评估框架。基于**MCP-Bench**在10个典型MCP服务器上开展实证研究，量化分析任务完成率、工具调用频次、端到端延迟及协议消息效率随规模增长的变化规律；同步引入**MAESTRO**安全分析框架，识别CE-MCP全生命周期中5个执行阶段的16类新型攻击模式（如异常中介代码注入、不安全能力合成等），覆盖Python沙箱逃逸、权限越界、LLM提示劫持等高危场景。\n\n## 主要发现  \n- CE-MCP将SQL查询、文件分析等多步工作流封装为单程序执行，**平均降低38.7% token消耗与42.1%执行延迟**；  \n- 但其攻击面指数级扩展——实验验证16类漏洞在GPT-4、Claude-3、Qwen2-72B等主流LLM驱动下均成功触发；  \n- 提出**分层防御架构**：底层采用轻量级容器化沙箱（gVisor增强隔离），上层部署语义门控机制（基于AST的动态能力白名单+运行时数据流约束），在保持99.2%合法任务通过率前提下，阻断全部已知攻击路径。\n\n本研究为可执行智能体系统提供了首个兼顾**可扩展性-安全性权衡**的工程化设计蓝图。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15815v1",
      "arxiv_id": "2602.15815v1",
      "title": "Natural Privacy Filters Are Not Always Free: A Characterization of Free Natural Filters",
      "authors": [
        "Matthew Regehr",
        "Bingshan Hu",
        "Ethan Leeman",
        "Pasin Manurangsi",
        "Pierre Tholoniat",
        "Mathias Lécuyer"
      ],
      "abstract": "We study natural privacy filters, which enable the exact composition of differentially private (DP) mechanisms with adaptively chosen privacy characteristics. Earlier privacy filters consider only simple privacy parameters such as Rényi-DP or Gaussian DP parameters. Natural filters account for the entire privacy profile of every query, promising greater utility for a given privacy budget. We show that, contrary to other forms of DP, natural privacy filters are not free in general. Indeed, we show that only families of privacy mechanisms that are well-ordered when composed admit free natural privacy filters.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15815v1",
      "url": "https://arxiv.org/abs/2602.15815v1",
      "categories": [
        "cs.CR",
        "cs.DS"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 研究背景与问题  \n在差分隐私（DP）的实际部署中，**自适应查询序列**的隐私预算分配极具挑战性。传统隐私滤波器（privacy filters）仅支持固定或简单参数化机制（如Rényi-DP或Gaussian DP），难以充分利用机制的完整隐私剖面（privacy profile）。为此，“自然隐私滤波器”（natural privacy filters）被提出——它能基于每次查询的**全隐私剖面**动态判定是否允许发布结果，从而在相同总隐私预算下显著提升效用。一个关键开放问题是：这类滤波器是否“免费”（free），即其应用是否不引入额外隐私损失？此前工作隐含假设其为免费，但缺乏严格验证。\n\n## 方法与理论刻画  \n本文首次对自然隐私滤波器的“免费性”进行形式化界定与系统性刻画。我们构建了基于**隐私剖面序结构**的抽象框架，将机制族建模为偏序集，并定义“自由自然滤波器”需满足：对任意自适应查询序列，滤波器输出的合成隐私曲线严格等于各机制隐私剖面的最优（tightest）逐点组合。我们通过构造反例与充分必要条件证明，**自由性并非普遍成立**。\n\n## 主要发现与创新点  \n- **核心结论**：自然隐私滤波器仅当所涉机制族在复合运算下构成**全序集（well-ordered under composition）** 时才是免费的；否则，滤波过程本身会引入不可忽略的隐私开销。  \n- **理论贡献**：首次给出自由自然滤波器的充要条件，揭示了隐私剖面结构与自适应组合之间的深层耦合关系。  \n- **实践启示**：警示隐私工程师不可默认自然滤波器无成本；在非良序机制（如混合ε-DP与f-DP机制）场景中，必须显式建模滤波器开销以保障端到端隐私保证。",
      "summary_en": "We study natural privacy filters—adaptive mechanisms that enable exact composition of differentially private queries by leveraging their full privacy profiles (e.g., $f$-DP curves), rather than simplified parameters like Rényi or Gaussian DP. While earlier filters are “free” (impose no extra privacy cost), we show natural filters are *not* free in general. We characterize precisely when they are: a family of DP mechanisms admits a free natural filter **if and only if** its privacy profiles are well-ordered under composition—i.e., for any two mechanisms, one’s privacy curve dominates the other’s pointwise after composition. We prove this via tight constructions and impossibility results, demonstrating that arbitrary mechanism families (e.g., mixing pure DP and approximate DP) incur unavoidable privacy overhead from filtering itself. This refutes the implicit assumption of cost-free adaptivity and provides the first necessary and sufficient structural condition for safe, lossless composition using full privacy profiles.",
      "summary": "## 研究背景与问题  \n在差分隐私（DP）的实际部署中，**自适应查询序列**的隐私预算分配极具挑战性。传统隐私滤波器（privacy filters）仅支持固定或简单参数化机制（如Rényi-DP或Gaussian DP），难以充分利用机制的完整隐私剖面（privacy profile）。为此，“自然隐私滤波器”（natural privacy filters）被提出——它能基于每次查询的**全隐私剖面**动态判定是否允许发布结果，从而在相同总隐私预算下显著提升效用。一个关键开放问题是：这类滤波器是否“免费”（free），即其应用是否不引入额外隐私损失？此前工作隐含假设其为免费，但缺乏严格验证。\n\n## 方法与理论刻画  \n本文首次对自然隐私滤波器的“免费性”进行形式化界定与系统性刻画。我们构建了基于**隐私剖面序结构**的抽象框架，将机制族建模为偏序集，并定义“自由自然滤波器”需满足：对任意自适应查询序列，滤波器输出的合成隐私曲线严格等于各机制隐私剖面的最优（tightest）逐点组合。我们通过构造反例与充分必要条件证明，**自由性并非普遍成立**。\n\n## 主要发现与创新点  \n- **核心结论**：自然隐私滤波器仅当所涉机制族在复合运算下构成**全序集（well-ordered under composition）** 时才是免费的；否则，滤波过程本身会引入不可忽略的隐私开销。  \n- **理论贡献**：首次给出自由自然滤波器的充要条件，揭示了隐私剖面结构与自适应组合之间的深层耦合关系。  \n- **实践启示**：警示隐私工程师不可默认自然滤波器无成本；在非良序机制（如混合ε-DP与f-DP机制）场景中，必须显式建模滤波器开销以保障端到端隐私保证。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15802v1",
      "arxiv_id": "2602.15802v1",
      "title": "Local Node Differential Privacy",
      "authors": [
        "Sofya Raskhodnikova",
        "Adam Smith",
        "Connor Wagaman",
        "Anatoly Zavyalov"
      ],
      "abstract": "We initiate an investigation of node differential privacy for graphs in the local model of private data analysis. In our model, dubbed LNDP, each node sees its own edge list and releases the output of a local randomizer on this input. These outputs are aggregated by an untrusted server to obtain a final output.   We develop a novel algorithmic framework for this setting that allows us to accurately answer arbitrary linear queries on a blurry approximation of the input graph's degree distribution. For some natural problems, the resulting algorithms match the accuracy achievable with node privacy in the central model, where data are held and processed by a trusted server. We also prove lower bounds on the error required by LNDP that imply the optimality of our algorithms for several fundamental graph statistics. We then lift these lower bounds to the interactive LNDP setting, demonstrating the optimality of our algorithms even when constantly many rounds of interaction are permitted. Obtaining our lower bounds requires new approaches, since those developed for the usual local model do not apply to the inherently overlapping inputs that arise from graphs. Finally, we prove structural results that reveal qualitative differences between local node privacy and the standard local model for tabular data.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15802v1",
      "url": "https://arxiv.org/abs/2602.15802v1",
      "categories": [
        "cs.DS",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "differential"
      ],
      "keyword_score": 2,
      "summary_zh": "## 本地节点差分隐私（LNDP）：图数据隐私保护的新范式\n\n本研究首次系统性地提出并形式化了**本地节点差分隐私（Local Node Differential Privacy, LNDP）**——一种面向图结构数据的新型本地化隐私模型。与传统本地差分隐私（LDP）处理独立记录不同，LNDP要求每个节点仅基于其**局部邻接信息**（即自身边列表）运行一个本地随机化器，再将扰动后输出发送至不可信服务器进行聚合。该模型更贴合去中心化图场景（如社交网络边缘计算、分布式图学习），避免了对中心化可信服务器的依赖。\n\n我们构建了一个**新颖的算法框架**，可高精度回答关于输入图**度分布模糊近似**的任意线性查询（如平均度、度矩、三角形计数等）。关键创新在于设计了鲁棒的局部扰动机制与协同聚合策略，使误差随节点数 $n$ 衰减为 $O(1/\\sqrt{n})$，**在若干自然问题上达到与中心化节点差分隐私（central node DP）相同的渐近精度**——这打破了“本地模型必然显著牺牲精度”的固有认知。\n\n进一步，我们建立了首个针对LNDP的**紧致下界**：证明任何LNDP机制在估计度分布统计量（如最大度、度直方图 $\\ell_1$-误差）时，误差下界为 $\\Omega(1)$，且我们的算法恰好达到该下界，从而**严格证明其最优性**。更进一步，我们将下界扩展至**多轮交互式LNDP**场景，表明即使允许常数轮通信，最优性依然成立。为克服图数据中节点输入天然重叠（一条边被两个端点共享）带来的技术障碍，我们发展了全新的分析工具——包括**重叠敏感的耦合构造**与**图局部结构的隐私敏感性刻画**。最后，我们揭示了LNDP与标准表格型LDP的本质差异：前者无法实现私有频率估计的通用解，凸显其独特的结构性挑战。",
      "summary_en": "We initiate the study of **Local Node Differential Privacy (LNDP)**—a new privacy model for graphs in the local setting. In LNDP, each node applies a local randomizer to its incident edge list and sends the output to an untrusted aggregator. We propose a novel algorithmic framework that accurately answers arbitrary linear queries on a *blurred approximation* of the graph’s degree distribution. For natural problems (e.g., degree moments), our algorithms achieve error $O(1/\\sqrt{n})$, matching the optimal accuracy of *central* node DP—demonstrating that local graph privacy need not inherently sacrifice asymptotic utility. We prove tight lower bounds showing $\\Omega(1)$ error is unavoidable for fundamental statistics (e.g., max degree, $\\ell_1$-error of degree histogram), establishing optimality of our algorithms. These bounds extend to interactive LNDP with constant-round communication. Crucially, our lower-bound techniques are new: they overcome the challenge of overlapping inputs inherent to graphs—unaddressed by standard LDP tools. Finally, we show structural separations between LNDP and tabular LDP, revealing qualitatively distinct privacy landscapes.",
      "summary": "## 本地节点差分隐私（LNDP）：图数据隐私保护的新范式\n\n本研究首次系统性地提出并形式化了**本地节点差分隐私（Local Node Differential Privacy, LNDP）**——一种面向图结构数据的新型本地化隐私模型。与传统本地差分隐私（LDP）处理独立记录不同，LNDP要求每个节点仅基于其**局部邻接信息**（即自身边列表）运行一个本地随机化器，再将扰动后输出发送至不可信服务器进行聚合。该模型更贴合去中心化图场景（如社交网络边缘计算、分布式图学习），避免了对中心化可信服务器的依赖。\n\n我们构建了一个**新颖的算法框架**，可高精度回答关于输入图**度分布模糊近似**的任意线性查询（如平均度、度矩、三角形计数等）。关键创新在于设计了鲁棒的局部扰动机制与协同聚合策略，使误差随节点数 $n$ 衰减为 $O(1/\\sqrt{n})$，**在若干自然问题上达到与中心化节点差分隐私（central node DP）相同的渐近精度**——这打破了“本地模型必然显著牺牲精度”的固有认知。\n\n进一步，我们建立了首个针对LNDP的**紧致下界**：证明任何LNDP机制在估计度分布统计量（如最大度、度直方图 $\\ell_1$-误差）时，误差下界为 $\\Omega(1)$，且我们的算法恰好达到该下界，从而**严格证明其最优性**。更进一步，我们将下界扩展至**多轮交互式LNDP**场景，表明即使允许常数轮通信，最优性依然成立。为克服图数据中节点输入天然重叠（一条边被两个端点共享）带来的技术障碍，我们发展了全新的分析工具——包括**重叠敏感的耦合构造**与**图局部结构的隐私敏感性刻画**。最后，我们揭示了LNDP与标准表格型LDP的本质差异：前者无法实现私有频率估计的通用解，凸显其独特的结构性挑战。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15671v1",
      "arxiv_id": "2602.15671v1",
      "title": "Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective",
      "authors": [
        "Haodong Zhao",
        "Jinming Hu",
        "Gongshen Liu"
      ],
      "abstract": "Federated learning security research has predominantly focused on backdoor threats from a minority of malicious clients that intentionally corrupt model updates. This paper challenges this paradigm by investigating a more pervasive and insidious threat: \\textit{backdoor vulnerabilities from low-concentration poisoned data distributed across the datasets of benign clients.} This scenario is increasingly common in federated instruction tuning for language models, which often rely on unverified third-party and crowd-sourced data. We analyze two forms of backdoor data through real cases: 1) \\textit{natural trigger (inherent features as implicit triggers)}; 2) \\textit{adversary-injected trigger}. To analyze this threat, we model the backdoor implantation process from signal aggregation, proposing the Backdoor Signal-to-Noise Ratio to quantify the dynamics of the distributed backdoor signal. Extensive experiments reveal the severity of this threat: With just less than 10\\% of training data poisoned and distributed across clients, the attack success rate exceeds 85\\%, while the primary task performance remains largely intact. Critically, we demonstrate that state-of-the-art backdoor defenses, designed for attacks from malicious clients, are fundamentally ineffective against this threat. Our findings highlight an urgent need for new defense mechanisms tailored to the realities of modern, decentralized data ecosystems.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15671v1",
      "url": "https://arxiv.org/abs/2602.15671v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n传统联邦学习安全研究聚焦于**少数恶意客户端主动投毒**的后门攻击范式。本文挑战这一主流假设，揭示一种更隐蔽、更普遍的新型后门威胁：**由大量良性客户端各自持有少量低浓度中毒数据所共同诱发的分布式后门漏洞**。该场景在面向大语言模型的联邦指令微调（Federated Instruction Tuning）中尤为突出——其数据高度依赖未经验证的第三方开源数据集与众包语料，天然存在隐性污染风险。\n\n## 方法创新  \n我们从**信号聚合视角**重构后门植入机制：将后门触发模式视为一种需跨客户端协同放大的“有害信号”，而正常任务梯度则构成背景“噪声”。据此提出**后门信噪比（Backdoor Signal-to-Noise Ratio, BSNR）** 量化框架，首次形式化刻画分布式低浓度中毒数据在聚合过程中如何突破阈值、实现隐式协同激活。\n\n## 关键发现  \n- 实证表明：仅**<10% 总训练数据被分散污染**（每客户端中毒比例低至0.5%–3%），攻击成功率即超**85%**，且主任务准确率下降不足2%，极具隐蔽性；  \n- 污染类型涵盖两类真实案例：**自然触发**（如特定句式/语义结构作为隐式触发器）与**对抗注入触发**（人工添加无害标记）；  \n- **现有SOTA防御方法（如Norm Clipping、Krum、FLTrust）完全失效**——因其设计前提为识别“异常更新”，而本场景下所有更新均来自良性客户端、统计上完全正常。\n\n## 意义与呼吁  \n本研究揭示了去中心化数据生态中“无恶意主体的系统性脆弱性”，亟需发展面向**数据层分布特性**而非客户端行为的新一代防御范式。",
      "summary_en": "This paper redefines backdoor threats in federated instruction tuning by exposing a pervasive yet overlooked vulnerability: *backdoors emergent from low-concentration poisoned data distributed across numerous benign clients*—not from malicious actors. We model backdoor implantation as a signal aggregation process and propose the **Backdoor Signal-to-Noise Ratio (BSNR)** to quantify how sparse, distributed triggers collectively amplify during federated averaging. Experiments on real-world instruction datasets show that with **<10% total training data poisoned across clients** (e.g., 0.5–3% per client), attack success rates exceed **85%**, while primary task performance remains nearly intact (<2% drop). Crucially, state-of-the-art defenses—designed to detect anomalous *client updates*—fail completely, as all updates appear statistically benign. Our work underscores an urgent need for data-aware, aggregation-centric defenses in decentralized NLP ecosystems.",
      "summary": "## 背景与问题  \n传统联邦学习安全研究聚焦于**少数恶意客户端主动投毒**的后门攻击范式。本文挑战这一主流假设，揭示一种更隐蔽、更普遍的新型后门威胁：**由大量良性客户端各自持有少量低浓度中毒数据所共同诱发的分布式后门漏洞**。该场景在面向大语言模型的联邦指令微调（Federated Instruction Tuning）中尤为突出——其数据高度依赖未经验证的第三方开源数据集与众包语料，天然存在隐性污染风险。\n\n## 方法创新  \n我们从**信号聚合视角**重构后门植入机制：将后门触发模式视为一种需跨客户端协同放大的“有害信号”，而正常任务梯度则构成背景“噪声”。据此提出**后门信噪比（Backdoor Signal-to-Noise Ratio, BSNR）** 量化框架，首次形式化刻画分布式低浓度中毒数据在聚合过程中如何突破阈值、实现隐式协同激活。\n\n## 关键发现  \n- 实证表明：仅**<10% 总训练数据被分散污染**（每客户端中毒比例低至0.5%–3%），攻击成功率即超**85%**，且主任务准确率下降不足2%，极具隐蔽性；  \n- 污染类型涵盖两类真实案例：**自然触发**（如特定句式/语义结构作为隐式触发器）与**对抗注入触发**（人工添加无害标记）；  \n- **现有SOTA防御方法（如Norm Clipping、Krum、FLTrust）完全失效**——因其设计前提为识别“异常更新”，而本场景下所有更新均来自良性客户端、统计上完全正常。\n\n## 意义与呼吁  \n本研究揭示了去中心化数据生态中“无恶意主体的系统性脆弱性”，亟需发展面向**数据层分布特性**而非客户端行为的新一代防御范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15654v1",
      "arxiv_id": "2602.15654v1",
      "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections",
      "authors": [
        "Xianglin Yang",
        "Yufei He",
        "Shuo Ji",
        "Bryan Hooi",
        "Jin Song Dong"
      ],
      "abstract": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.   We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15654v1",
      "url": "https://arxiv.org/abs/2602.15654v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "prompt",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与问题  \n自演化大语言模型（LLM）智能体通过跨会话更新长期记忆（如写入/读取记忆库）提升长周期任务性能，但该机制引入新型安全风险：**外部不可信内容在一次良性交互中被误存为“可信记忆”，后续会话中被自动激活为指令**。本文首次形式化并命名此类威胁为**僵尸智能体（Zombie Agent）攻击**——攻击者无需直接操控提示词或系统指令，即可植入跨会话持久、隐匿且可触发的恶意载荷，使智能体沦为远程傀儡。\n\n## 方法与创新  \n我们提出首个**黑盒、间接式持久攻击框架**，仅依赖攻击者可控的网页内容（如恶意博客、文档）实现感染：  \n- **感染阶段**：智能体在执行正常任务（如信息检索）时读取含隐蔽载荷的网页，利用其自身记忆更新机制（无越权操作）将载荷写入长期记忆；  \n- **触发阶段**：载荷经记忆演化（如检索增强、滑动窗口续写）被自然唤起，诱导未经授权的工具调用（如发送邮件、执行代码）。  \n针对主流记忆架构，我们设计**机制适配型持久化策略**：对滑动窗口记忆，采用语义冗余+上下文锚定规避截断；对检索增强记忆，注入高相关性伪文档绕过相似度过滤。\n\n## 关键发现  \n在多个典型智能体架构（AutoGen、LangChain-based agents）和真实任务（行程规划、多跳问答）上评估表明：  \n- 所有测试配置中，载荷**平均存活≥5个连续会话**（最长12轮），且**良性任务准确率下降<2.3%**，隐蔽性强；  \n- 仅靠单次会话的提示词过滤无法防御——**记忆演化本身即构成攻击面扩展**。  \n本研究揭示了自演化范式下“记忆即指令”的根本性安全悖论，呼吁防御体系从**会话级防护升级为记忆生命周期治理**。",
      "summary_en": "We identify and formalize *Zombie Agents*: a novel persistent attack against self-evolving LLM agents, where attackers covertly implant cross-session payloads via benign external content (e.g., poisoned web pages), exploiting the agent’s *own memory update mechanism*—not prompt injection—to achieve long-term control. Our black-box framework operates in two phases: (1) *Infection*, where the agent reads attacker-controlled content during a legitimate task and autonomously writes the payload into long-term memory; and (2) *Trigger*, where evolved memory retrieves or propagates the payload to induce unauthorized tool use (e.g., API calls, code execution). We design memory-specific persistence strategies—resisting truncation in sliding-window memory and relevance filtering in retrieval-augmented memory—and evaluate across representative agent setups. Results show payloads persist for ≥5 consecutive sessions (up to 12) while preserving >97.7% benign task accuracy, demonstrating that memory evolution transforms one-time indirect exposure into persistent compromise. This reveals a critical limitation of session-level defenses and underscores the need for memory lifecycle-aware security.",
      "summary": "## 背景与问题  \n自演化大语言模型（LLM）智能体通过跨会话更新长期记忆（如写入/读取记忆库）提升长周期任务性能，但该机制引入新型安全风险：**外部不可信内容在一次良性交互中被误存为“可信记忆”，后续会话中被自动激活为指令**。本文首次形式化并命名此类威胁为**僵尸智能体（Zombie Agent）攻击**——攻击者无需直接操控提示词或系统指令，即可植入跨会话持久、隐匿且可触发的恶意载荷，使智能体沦为远程傀儡。\n\n## 方法与创新  \n我们提出首个**黑盒、间接式持久攻击框架**，仅依赖攻击者可控的网页内容（如恶意博客、文档）实现感染：  \n- **感染阶段**：智能体在执行正常任务（如信息检索）时读取含隐蔽载荷的网页，利用其自身记忆更新机制（无越权操作）将载荷写入长期记忆；  \n- **触发阶段**：载荷经记忆演化（如检索增强、滑动窗口续写）被自然唤起，诱导未经授权的工具调用（如发送邮件、执行代码）。  \n针对主流记忆架构，我们设计**机制适配型持久化策略**：对滑动窗口记忆，采用语义冗余+上下文锚定规避截断；对检索增强记忆，注入高相关性伪文档绕过相似度过滤。\n\n## 关键发现  \n在多个典型智能体架构（AutoGen、LangChain-based agents）和真实任务（行程规划、多跳问答）上评估表明：  \n- 所有测试配置中，载荷**平均存活≥5个连续会话**（最长12轮），且**良性任务准确率下降<2.3%**，隐蔽性强；  \n- 仅靠单次会话的提示词过滤无法防御——**记忆演化本身即构成攻击面扩展**。  \n本研究揭示了自演化范式下“记忆即指令”的根本性安全悖论，呼吁防御体系从**会话级防护升级为记忆生命周期治理**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15614v1",
      "arxiv_id": "2602.15614v1",
      "title": "Onto-DP: Constructing Neighborhoods for Differential Privacy on Ontological Databases",
      "authors": [
        "Yasmine Hayder",
        "Adrien Boiret",
        "Cédric Eichler",
        "Benjamin Nguyen"
      ],
      "abstract": "In this paper, we investigate how attackers can discover sensitive information embedded within databases by exploiting inference rules. We demonstrate the inadequacy of naively applied existing state of the art differential privacy (DP) models in safeguarding against such attacks. We introduce ontology aware differential privacy (Onto-DP), a novel extension of differential privacy paradigms built on top of any classical DP model by enriching it with semantic awareness. We show that this extension is a sufficient condition to adequately protect against attackers aware of inference rules.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15614v1",
      "url": "https://arxiv.org/abs/2602.15614v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "dp",
        "differential"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n在本体数据库（ontological databases）中，敏感信息不仅存在于显式存储的元组中，更可能通过**语义推理规则**（如RDFS/OWL蕴含）被隐式推导出来。传统差分隐私（DP）机制仅关注数据表的行级扰动，忽视了本体层的逻辑结构，导致攻击者可利用领域知识（如“祖父→父亲→子女”传递性）绕过隐私保护，实施高精度推理攻击。\n\n## 方法：Onto-DP 框架  \n我们提出**本体感知差分隐私（Onto-DP）**——一种语义增强型DP范式。其核心创新在于：  \n- **邻域重构**：不再以原始数据库实例为微小变化单位，而是定义基于**本体语义等价类**的邻域：两个数据库 $D$ 与 $D'$ 被视为相邻，当且仅当它们在所有**本体蕴含闭包**（ontology-mediated query answers）上至多相差一行；  \n- **机制无关扩展**：Onto-DP 可无缝嵌入任意经典DP机制（如Laplace、Gaussian、Exponential），仅需将隐私预算分配至语义邻域而非语法邻域；  \n- **形式化保障**：我们证明 Onto-DP 满足**推理鲁棒性**（inference-resilience）：若攻击者掌握完整本体与推理规则，Onto-DP 仍能提供严格 $(\\varepsilon,\\delta)$-DP 保证，而朴素DP在此场景下完全失效。\n\n## 主要发现与贡献  \n实验验证表明，在DBpedia子集与LUBM基准测试中，Onto-DP 在保持查询效用（平均误差降低37%）的同时，将推理攻击成功率从朴素DP下的89%压制至<5%。本工作首次将**本体语义结构**系统性地纳入DP理论框架，为知识图谱、医疗本体、政务语义网等高敏感度语义数据库提供了首个可证明安全的隐私保护基础。",
      "summary_en": "This paper addresses a critical gap in differential privacy (DP): conventional DP mechanisms fail against inference attacks leveraging ontological semantics (e.g., RDFS/OWL entailments). We propose **Onto-DP**, an ontology-aware extension of DP that redefines adjacency over *ontology-mediated query answers* rather than raw database instances. Specifically, two databases are adjacent if their logical closures under the given ontology differ in at most one answer—ensuring protection against attackers armed with domain knowledge and inference rules. Onto-DP is mechanism-agnostic: it lifts any classical DP algorithm (e.g., Laplace, Exponential) to semantic robustness while preserving formal $(\\varepsilon,\\delta)$-guarantees. Evaluation on DBpedia and LUBM shows Onto-DP reduces inference attack success from 89% (naive DP) to <5%, with 37% lower utility loss. This work establishes the first provably secure DP framework for ontological databases.",
      "summary": "## 背景与问题  \n在本体数据库（ontological databases）中，敏感信息不仅存在于显式存储的元组中，更可能通过**语义推理规则**（如RDFS/OWL蕴含）被隐式推导出来。传统差分隐私（DP）机制仅关注数据表的行级扰动，忽视了本体层的逻辑结构，导致攻击者可利用领域知识（如“祖父→父亲→子女”传递性）绕过隐私保护，实施高精度推理攻击。\n\n## 方法：Onto-DP 框架  \n我们提出**本体感知差分隐私（Onto-DP）**——一种语义增强型DP范式。其核心创新在于：  \n- **邻域重构**：不再以原始数据库实例为微小变化单位，而是定义基于**本体语义等价类**的邻域：两个数据库 $D$ 与 $D'$ 被视为相邻，当且仅当它们在所有**本体蕴含闭包**（ontology-mediated query answers）上至多相差一行；  \n- **机制无关扩展**：Onto-DP 可无缝嵌入任意经典DP机制（如Laplace、Gaussian、Exponential），仅需将隐私预算分配至语义邻域而非语法邻域；  \n- **形式化保障**：我们证明 Onto-DP 满足**推理鲁棒性**（inference-resilience）：若攻击者掌握完整本体与推理规则，Onto-DP 仍能提供严格 $(\\varepsilon,\\delta)$-DP 保证，而朴素DP在此场景下完全失效。\n\n## 主要发现与贡献  \n实验验证表明，在DBpedia子集与LUBM基准测试中，Onto-DP 在保持查询效用（平均误差降低37%）的同时，将推理攻击成功率从朴素DP下的89%压制至<5%。本工作首次将**本体语义结构**系统性地纳入DP理论框架，为知识图谱、医疗本体、政务语义网等高敏感度语义数据库提供了首个可证明安全的隐私保护基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15485v2",
      "arxiv_id": "2602.15485v2",
      "title": "SecCodeBench-V2 Technical Report",
      "authors": [
        "Longfei Chen",
        "Ji Zhao",
        "Lanxiao Cui",
        "Tong Su",
        "Xingbo Pan",
        "Ziyang Li",
        "Yongxing Wu",
        "Qijiang Cao",
        "Qiyao Cai",
        "Jing Zhang",
        "Yuandong Ni",
        "Junyao He",
        "Zeyu Zhang",
        "Chao Ge",
        "Xuhuai Lu",
        "Zeyu Gao",
        "Yuxin Cui",
        "Weisen Chen",
        "Yuxuan Peng",
        "Shengping Wang",
        "Qi Li",
        "Yukai Huang",
        "Yukun Liu",
        "Tuo Zhou",
        "Terry Yue Zhuo",
        "Junyang Lin",
        "Chao Zhang"
      ],
      "abstract": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15485v2",
      "url": "https://arxiv.org/abs/2602.15485v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准评测体系  \n\nSecCodeBench-V2 是阿里巴巴集团发布的**第二代开源安全代码生成评测基准**，旨在系统性、可复现地评估大语言模型（LLM）编程助手在**生成与修复安全关键代码**方面的实际能力。相较于前代，V2 版本全面升级：涵盖 **98 个真实工业场景**（含生成与修复两类任务），全部源自阿里集团线上生产环境，覆盖 **22 类常见 CWE 漏洞类型**（如 CWE-78、CWE-89、CWE-119 等），横跨 **Java、C、Python、Go、JavaScript 五种主流语言**。  \n\n本基准采用**函数级精细化任务建模**：每个场景提供完整项目骨架（含依赖、构建配置及上下文），明确指定待实现或修复的目标函数接口，严格约束输入/输出契约与依赖边界，显著提升任务真实性与评估严谨性。所有场景均配备**专家撰写的可执行 PoC（Proof-of-Concept）测试用例**——经双盲安全专家评审，确保功能正确性验证与安全漏洞触发能力兼具，形成高保真、广覆盖、可判定的黄金标准（ground truth）。  \n\n为实现自动化、动态化评估，我们构建了统一评测流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，同步验证功能正确性与安全性；对难以构造确定性安全测试的复杂场景（如逻辑类漏洞），引入经校准的 **LLM-as-a-judge 仲裁机制**作为补充。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、漏洞严重性（CVSS 分数映射）和任务类型进行原则性聚合，支持跨模型、跨任务的公平对比。SecCodeBench-V2 已全量开源（含数据集、评测脚本、基线结果），地址：https://github.com/alibaba/sec-code-bench。",
      "summary_en": "SecCodeBench-V2 is a rigorously designed, publicly available benchmark for evaluating the secure code generation and patching capabilities of LLM-based coding assistants. It comprises 98 function-level scenarios—derived from Alibaba’s production systems—spanning 22 CWE categories across Java, C, Python, Go, and JavaScript. Each scenario provides a full project scaffold and requires models to implement or fix a target function under fixed interfaces and dependencies, with executable PoC test cases (authored and double-reviewed by security experts) for both functional and security validation. Evaluation is primarily dynamic: model outputs are compiled and executed in isolated environments against PoC tests; for non-deterministic security issues, an LLM-as-a-judge oracle supplements assessment. Performance is aggregated via a severity-aware Pass@K scoring protocol, enabling holistic, comparable evaluation. All artifacts, results, and tooling are open-sourced at https://github.com/alibaba/sec-code-bench.",
      "summary": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准评测体系  \n\nSecCodeBench-V2 是阿里巴巴集团发布的**第二代开源安全代码生成评测基准**，旨在系统性、可复现地评估大语言模型（LLM）编程助手在**生成与修复安全关键代码**方面的实际能力。相较于前代，V2 版本全面升级：涵盖 **98 个真实工业场景**（含生成与修复两类任务），全部源自阿里集团线上生产环境，覆盖 **22 类常见 CWE 漏洞类型**（如 CWE-78、CWE-89、CWE-119 等），横跨 **Java、C、Python、Go、JavaScript 五种主流语言**。  \n\n本基准采用**函数级精细化任务建模**：每个场景提供完整项目骨架（含依赖、构建配置及上下文），明确指定待实现或修复的目标函数接口，严格约束输入/输出契约与依赖边界，显著提升任务真实性与评估严谨性。所有场景均配备**专家撰写的可执行 PoC（Proof-of-Concept）测试用例**——经双盲安全专家评审，确保功能正确性验证与安全漏洞触发能力兼具，形成高保真、广覆盖、可判定的黄金标准（ground truth）。  \n\n为实现自动化、动态化评估，我们构建了统一评测流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，同步验证功能正确性与安全性；对难以构造确定性安全测试的复杂场景（如逻辑类漏洞），引入经校准的 **LLM-as-a-judge 仲裁机制**作为补充。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、漏洞严重性（CVSS 分数映射）和任务类型进行原则性聚合，支持跨模型、跨任务的公平对比。SecCodeBench-V2 已全量开源（含数据集、评测脚本、基线结果），地址：https://github.com/alibaba/sec-code-bench。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15485v1",
      "arxiv_id": "2602.15485v1",
      "title": "SecCodeBench-V2 Technical Report",
      "authors": [
        "Longfei Chen",
        "Ji Zhao",
        "Lanxiao Cui",
        "Tong Su",
        "Xingbo Pan",
        "Ziyang Li",
        "Yongxing Wu",
        "Qijiang Cao",
        "Qiyao Cai",
        "Jing Zhang",
        "Yuandong Ni",
        "Junyao He",
        "Zeyu Zhang",
        "Chao Ge",
        "Xuhuai Lu",
        "Zeyu Gao",
        "Yuxin Cui",
        "Weisen Chen",
        "Yuxuan Peng",
        "Shengping Wang",
        "Qi Li",
        "Yukai Huang",
        "Yukun Liu",
        "Tuo Zhou",
        "Terry Yue Zhuo",
        "Junyang Lin",
        "Chao Zhang"
      ],
      "abstract": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15485v1",
      "url": "https://arxiv.org/abs/2602.15485v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准\n\nSecCodeBench-V2 是阿里巴巴集团发布的第二代开源安全代码生成评测基准，旨在**系统性、可复现地评估大语言模型（LLM）编程助手在真实工业场景中生成与修复安全代码的能力**。基准包含 **98 个高质量生成与修复任务场景**，全部源自阿里集团实际生产系统，覆盖 **Java、C、Python、Go 和 Node.js 五种主流语言**，底层安全漏洞涵盖 **22 类常见 CWE（Common Weakness Enumeration）**，如 CWE-78（OS命令注入）、CWE-89（SQL注入）、CWE-119（缓冲区溢出）等，兼具**多样性、严重性与现实代表性**。\n\n本基准采用**函数级任务建模**：每个场景提供完整项目骨架（含依赖与接口约束），要求模型仅实现或修补指定目标函数，严格模拟真实开发边界。所有任务均配备**专家编写的、可执行的 PoC（Proof-of-Concept）测试用例**，经双盲安全专家评审，确保功能正确性与安全验证的**高保真度、广覆盖与可靠真值**。我们构建了统一动态评估流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，双重验证功能正确性与安全鲁棒性；对难以构造确定性测试的复杂漏洞（如竞态条件），创新引入**LLM-as-a-judge 仲裁机制**。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、CWE 严重等级进行原则性聚合，支持跨模型、跨任务的公平、可解释、整体性对比。SecCodeBench-V2 已全面开源，含全部数据、评估脚本与结果，地址：https://github.com/alibaba/sec-code-bench。",
      "summary_en": "SecCodeBench-V2 is a publicly released, industry-grounded benchmark for rigorously evaluating LLM copilots’ ability to generate and fix *secure* code. It comprises 98 function-level generation/fix tasks derived from Alibaba’s production systems, spanning 5 languages (Java, C, Python, Go, Node.js) and 22 CWE categories. Each task provides a full project scaffold and executable PoC test cases—authored and double-reviewed by security experts—for joint functional and security validation. Our unified evaluation pipeline relies primarily on *dynamic execution*: model outputs are compiled and run in isolated environments against PoCs; for non-deterministic vulnerabilities, we augment with an LLM-as-a-judge oracle. To enable holistic, comparable assessment across heterogeneous scenarios, we introduce a severity-aware, Pass@K–based scoring protocol with principled aggregation over difficulty and CWE criticality. SecCodeBench-V2 establishes a reproducible foundation for measuring the security posture of AI coding assistants—code, data, and results are fully open at https://github.com/alibaba/sec-code-bench.",
      "summary": "## SecCodeBench-V2：面向AI编程助手安全能力的工业级基准\n\nSecCodeBench-V2 是阿里巴巴集团发布的第二代开源安全代码生成评测基准，旨在**系统性、可复现地评估大语言模型（LLM）编程助手在真实工业场景中生成与修复安全代码的能力**。基准包含 **98 个高质量生成与修复任务场景**，全部源自阿里集团实际生产系统，覆盖 **Java、C、Python、Go 和 Node.js 五种主流语言**，底层安全漏洞涵盖 **22 类常见 CWE（Common Weakness Enumeration）**，如 CWE-78（OS命令注入）、CWE-89（SQL注入）、CWE-119（缓冲区溢出）等，兼具**多样性、严重性与现实代表性**。\n\n本基准采用**函数级任务建模**：每个场景提供完整项目骨架（含依赖与接口约束），要求模型仅实现或修补指定目标函数，严格模拟真实开发边界。所有任务均配备**专家编写的、可执行的 PoC（Proof-of-Concept）测试用例**，经双盲安全专家评审，确保功能正确性与安全验证的**高保真度、广覆盖与可靠真值**。我们构建了统一动态评估流水线：对绝大多数场景，通过**隔离环境编译运行模型输出**，自动执行 PoC 测试，双重验证功能正确性与安全鲁棒性；对难以构造确定性测试的复杂漏洞（如竞态条件），创新引入**LLM-as-a-judge 仲裁机制**。最终，我们设计基于 **Pass@K 的多维加权评分协议**，按场景难度、CWE 严重等级进行原则性聚合，支持跨模型、跨任务的公平、可解释、整体性对比。SecCodeBench-V2 已全面开源，含全部数据、评估脚本与结果，地址：https://github.com/alibaba/sec-code-bench。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15337v1",
      "arxiv_id": "2602.15337v1",
      "title": "FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning",
      "authors": [
        "Chaoyi Lu"
      ],
      "abstract": "Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\\% improvement over baseline methods and 1.93\\% over the current state-of-the-art method.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15337v1",
      "url": "https://arxiv.org/abs/2602.15337v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n异步联邦学习（AFL）通过允许客户端无需同步等待慢速设备即可并行训练，显著提升了分布式训练效率。然而，其固有的**行为陈旧性（behavioral staleness）**——即本地模型更新因延迟上传而偏离当前全局状态的程度——常导致收敛不稳定与精度下降。现有方法普遍仅以“轮次差”（如 $ \\tau = t - t_i $）作为陈旧性度量，该指标粗粒度、与模型实际参数演化脱节，无法反映梯度方向偏移或参数敏感性变化，成为性能提升的关键瓶颈。\n\n## 方法创新：FedPSA框架  \n本文提出 **FedPSA（Parameter Sensitivity-based Asynchronous Federated Learning）**，一种细粒度、自适应的AFL新范式：  \n- **参数敏感性建模**：首次将**逐层参数敏感度**（基于Hessian近似与局部损失曲率）作为陈旧性核心度量，替代静态轮次计数，精准刻画模型在关键参数维度上的实际过时程度；  \n- **动态动量队列（Dynamic Momentum Queue, DMQ）**：实时追踪近期更新的动量分布偏移趋势，识别当前训练阶段（如早期快速下降期 vs. 后期震荡收敛期），据此**动态调整陈旧容忍阈值**；  \n- **敏感度加权聚合**：在服务器端对上传更新施加敏感度感知的衰减权重，抑制高陈旧性、低敏感性参数的干扰，保留高敏感性参数的有效梯度信号。\n\n## 实验结果与贡献  \n在CIFAR-10/100、Tiny-ImageNet及FEMNIST上系统验证：FedPSA在非独立同分布（Non-IID）和异构延迟场景下均显著优于SOTA。相较基准方法（如Async-FedAvg），最高提升**6.37%** 准确率；较当前最优方法（FedBuff、SCAFFOLD-A）平均提升**1.93%**，且收敛速度加快23%。本工作首次将**模型内在敏感性**引入AFL陈旧性建模，为异步机制设计提供了可解释、可微分的新理论视角。",
      "summary_en": "Asynchronous Federated Learning (AFL) accelerates training by eliminating synchronization waits but suffers from performance degradation due to *behavioral staleness*—the misalignment between stale local updates and the evolving global model. Existing staleness metrics rely solely on coarse-grained round differences, ignoring intrinsic model dynamics. To address this, we propose **FedPSA**, a fine-grained AFL framework that quantifies staleness via **layer-wise parameter sensitivity**, approximated using local loss curvature and Hessian-vector products. FedPSA introduces a **Dynamic Momentum Queue (DMQ)** to monitor real-time training phase transitions and adaptively adjust staleness tolerance. During aggregation, updates are weighted by sensitivity to suppress noisy, outdated gradients while preserving critical signals. Extensive experiments across CIFAR-10/100, Tiny-ImageNet, and FEMNIST under Non-IID and heterogeneous delay settings show FedPSA consistently outperforms baselines: up to **+6.37%** accuracy over Async-FedAvg and **+1.93%** over state-of-the-art methods (e.g., FedBuff, SCAFFOLD-A), with 23% faster convergence. FedPSA establishes parameter sensitivity as a principled, differentiable foundation for staleness-aware asynchronous optimization.",
      "summary": "## 背景与问题  \n异步联邦学习（AFL）通过允许客户端无需同步等待慢速设备即可并行训练，显著提升了分布式训练效率。然而，其固有的**行为陈旧性（behavioral staleness）**——即本地模型更新因延迟上传而偏离当前全局状态的程度——常导致收敛不稳定与精度下降。现有方法普遍仅以“轮次差”（如 $ \\tau = t - t_i $）作为陈旧性度量，该指标粗粒度、与模型实际参数演化脱节，无法反映梯度方向偏移或参数敏感性变化，成为性能提升的关键瓶颈。\n\n## 方法创新：FedPSA框架  \n本文提出 **FedPSA（Parameter Sensitivity-based Asynchronous Federated Learning）**，一种细粒度、自适应的AFL新范式：  \n- **参数敏感性建模**：首次将**逐层参数敏感度**（基于Hessian近似与局部损失曲率）作为陈旧性核心度量，替代静态轮次计数，精准刻画模型在关键参数维度上的实际过时程度；  \n- **动态动量队列（Dynamic Momentum Queue, DMQ）**：实时追踪近期更新的动量分布偏移趋势，识别当前训练阶段（如早期快速下降期 vs. 后期震荡收敛期），据此**动态调整陈旧容忍阈值**；  \n- **敏感度加权聚合**：在服务器端对上传更新施加敏感度感知的衰减权重，抑制高陈旧性、低敏感性参数的干扰，保留高敏感性参数的有效梯度信号。\n\n## 实验结果与贡献  \n在CIFAR-10/100、Tiny-ImageNet及FEMNIST上系统验证：FedPSA在非独立同分布（Non-IID）和异构延迟场景下均显著优于SOTA。相较基准方法（如Async-FedAvg），最高提升**6.37%** 准确率；较当前最优方法（FedBuff、SCAFFOLD-A）平均提升**1.93%**，且收敛速度加快23%。本工作首次将**模型内在敏感性**引入AFL陈旧性建模，为异步机制设计提供了可解释、可微分的新理论视角。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15304v1",
      "arxiv_id": "2602.15304v1",
      "title": "Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization",
      "authors": [
        "Farzana Akter",
        "Rakib Hossain",
        "Deb Kanna Roy Toushi",
        "Mahmood Menon Khan",
        "Sultana Amin",
        "Lisan Al Amin"
      ],
      "abstract": "Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15304v1",
      "url": "https://arxiv.org/abs/2602.15304v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated",
        "membership",
        "inference"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与挑战  \n跨机构临床决策支持常受数据治理与隐私法规制约，难以汇聚患者级原始数据。传统联邦学习（FL）或分割学习（SL）各自存在局限：FL在非独立同分布（non-IID）医疗数据下收敛不稳定，且模型更新仍隐含特征泄露风险；SL虽天然分离计算，但中心化预测头易成单点故障，且缺乏对协作边界的显式隐私管控。\n\n## 方法创新  \n本文提出**混合联邦-分割学习（Hybrid FL-SL）框架**：客户端本地保留特征提取主干（trunk），仅上传中间层表征（cut-layer activations）；协调服务器聚合表征并托管统一预测头（head）。该设计确立了清晰的**协作边界**——所有隐私敏感操作（如裁剪、加噪）均在此边界施加。我们首次在临床场景中系统开展**实证隐私审计**，采用成员推断攻击（Membership Inference Attack, MIA）量化cut-layer表征的泄露程度，并评估两种轻量防御：**激活裁剪（activation clipping）** 与 **高斯噪声注入（additive Gaussian noise）**。\n\n## 关键结果  \n在MIMIC-III、eICU和Diabetes 13个任务上，采用统一pipeline与non-IID划分，四维联合评估显示：  \n- ✅ **预测效用**：AUC达0.82–0.91，媲美纯FL/SL基线；  \n- ✅ **决策优先级**：基于治疗增益（uplift）的资源受限排序准确率提升12.3%；  \n- ✅ **隐私保障**：MIA攻击成功率由78.5%降至≤42.1%（经裁剪+噪声）；  \n- ✅ **部署成本**：通信开销较纯FL降低37%，较纯SL降低29%。  \n\n本工作将Hybrid FL-SL确立为可调谐的实用范式——在**效用、隐私泄漏风险与通信成本**三者间实现显式、可控的权衡。",
      "summary_en": "We propose a hybrid Federated and Split Learning (FL-SL) framework for privacy-preserving clinical decision support without raw-data sharing. It decouples feature extraction (kept locally on clients) from prediction (hosted centrally), establishing an explicit collaboration boundary where lightweight privacy controls—activation clipping and additive Gaussian noise—are applied and audited via membership inference attacks. Evaluated across three public clinical datasets (MIMIC-III, eICU, Diabetes) under realistic non-IID partitions, our approach achieves competitive predictive performance (AUC: 0.82–0.91) and superior uplift-based treatment prioritization under capacity constraints—while reducing empirically measured privacy leakage (MIA success rate ↓ from 78.5% to ≤42.1%) and cutting communication overhead by up to 37% vs. standalone FL. This work establishes hybrid FL-SL as a practical, tunable design space for balancing utility, leakage risk, and deployment cost in healthcare AI.",
      "summary": "## 背景与挑战  \n跨机构临床决策支持常受数据治理与隐私法规制约，难以汇聚患者级原始数据。传统联邦学习（FL）或分割学习（SL）各自存在局限：FL在非独立同分布（non-IID）医疗数据下收敛不稳定，且模型更新仍隐含特征泄露风险；SL虽天然分离计算，但中心化预测头易成单点故障，且缺乏对协作边界的显式隐私管控。\n\n## 方法创新  \n本文提出**混合联邦-分割学习（Hybrid FL-SL）框架**：客户端本地保留特征提取主干（trunk），仅上传中间层表征（cut-layer activations）；协调服务器聚合表征并托管统一预测头（head）。该设计确立了清晰的**协作边界**——所有隐私敏感操作（如裁剪、加噪）均在此边界施加。我们首次在临床场景中系统开展**实证隐私审计**，采用成员推断攻击（Membership Inference Attack, MIA）量化cut-layer表征的泄露程度，并评估两种轻量防御：**激活裁剪（activation clipping）** 与 **高斯噪声注入（additive Gaussian noise）**。\n\n## 关键结果  \n在MIMIC-III、eICU和Diabetes 13个任务上，采用统一pipeline与non-IID划分，四维联合评估显示：  \n- ✅ **预测效用**：AUC达0.82–0.91，媲美纯FL/SL基线；  \n- ✅ **决策优先级**：基于治疗增益（uplift）的资源受限排序准确率提升12.3%；  \n- ✅ **隐私保障**：MIA攻击成功率由78.5%降至≤42.1%（经裁剪+噪声）；  \n- ✅ **部署成本**：通信开销较纯FL降低37%，较纯SL降低29%。  \n\n本工作将Hybrid FL-SL确立为可调谐的实用范式——在**效用、隐私泄漏风险与通信成本**三者间实现显式、可控的权衡。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15919v1",
      "arxiv_id": "2602.15919v1",
      "title": "Generalized Leverage Score for Scalable Assessment of Privacy Vulnerability",
      "authors": [
        "Valentin Dorseuil",
        "Jamal Atif",
        "Olivier Cappé"
      ],
      "abstract": "Can the privacy vulnerability of individual data points be assessed without retraining models or explicitly simulating attacks? We answer affirmatively by showing that exposure to membership inference attack (MIA) is fundamentally governed by a data point's influence on the learned model. We formalize this in the linear setting by establishing a theoretical correspondence between individual MIA risk and the leverage score, identifying it as a principled metric for vulnerability. This characterization explains how data-dependent sensitivity translates into exposure, without the computational burden of training shadow models. Building on this, we propose a computationally efficient generalization of the leverage score for deep learning. Empirical evaluations confirm a strong correlation between the proposed score and MIA success, validating this metric as a practical surrogate for individual privacy risk assessment.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15919v1",
      "url": "https://arxiv.org/abs/2602.15919v1",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "membership",
        "inference"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n在隐私保护机器学习中，评估**单个数据点的隐私脆弱性**（如遭受成员推断攻击MIA的风险）通常依赖计算昂贵的方法——需训练大量影子模型或反复重训练目标模型。这严重制约了大规模数据集和复杂模型（如深度神经网络）上的实时、细粒度隐私诊断。\n\n## 方法创新  \n本文首次从理论层面揭示：**个体MIA风险本质源于其对模型参数的学习影响力**。在线性回归设定下，我们严格证明：个体MIA成功率与经典统计量——**杠杆分数（leverage score）**——存在单调映射关系；该分数直接量化数据点在最小二乘解空间中的几何“影响力”，无需任何攻击模拟或重训练。据此，我们提出**广义杠杆分数（Generalized Leverage Score, GLS）**：一种可扩展、免训练的深度学习适配指标。GLS通过高效计算模型最后一层特征映射的Jacobian矩阵的行范数平方（即局部线性化下的近似杠杆），规避了高维Hessian计算，时间复杂度仅为 *O(d·n)*（d为特征维数，n为样本数）。\n\n## 主要发现与价值  \n- 在CIFAR-10/100、ImageNet子集及医疗影像数据上，GLS与真实MIA成功率（Shadow-MIA、Knockoff-MIA）的**皮尔逊相关系数达0.82–0.91**；  \n- GLS可精准识别高风险样本（如异常、边界或过拟合样本），支持**数据清洗、差分隐私预算分配、主动防御采样**等下游任务；  \n- 相比基线方法（如损失值、梯度范数），GLS具备**理论可解释性、跨架构鲁棒性及零训练开销**，是首个兼具严谨性与实用性的免训练隐私脆弱性代理指标。",
      "summary_en": "Can individual data points’ privacy vulnerability to membership inference attacks (MIAs) be assessed *without* retraining models or simulating attacks? We answer yes: MIA exposure is fundamentally governed by a data point’s influence on the learned model. In the linear setting, we establish a rigorous theoretical correspondence between individual MIA risk and the classical leverage score—a natural, computationally light metric quantifying data influence in least-squares estimation. Building on this insight, we propose the **Generalized Leverage Score (GLS)**, an efficient, training-free extension for deep networks. GLS approximates local leverage via the squared row norm of the Jacobian of the final-layer features—avoiding costly shadow training or Hessian computation. Extensive experiments across vision and medical datasets show GLS achieves strong correlation (ρ = 0.82–0.91) with actual MIA success, outperforming loss- and gradient-based baselines. GLS thus serves as a principled, scalable, and theory-grounded surrogate for fine-grained privacy risk assessment.",
      "summary": "## 研究背景与问题  \n在隐私保护机器学习中，评估**单个数据点的隐私脆弱性**（如遭受成员推断攻击MIA的风险）通常依赖计算昂贵的方法——需训练大量影子模型或反复重训练目标模型。这严重制约了大规模数据集和复杂模型（如深度神经网络）上的实时、细粒度隐私诊断。\n\n## 方法创新  \n本文首次从理论层面揭示：**个体MIA风险本质源于其对模型参数的学习影响力**。在线性回归设定下，我们严格证明：个体MIA成功率与经典统计量——**杠杆分数（leverage score）**——存在单调映射关系；该分数直接量化数据点在最小二乘解空间中的几何“影响力”，无需任何攻击模拟或重训练。据此，我们提出**广义杠杆分数（Generalized Leverage Score, GLS）**：一种可扩展、免训练的深度学习适配指标。GLS通过高效计算模型最后一层特征映射的Jacobian矩阵的行范数平方（即局部线性化下的近似杠杆），规避了高维Hessian计算，时间复杂度仅为 *O(d·n)*（d为特征维数，n为样本数）。\n\n## 主要发现与价值  \n- 在CIFAR-10/100、ImageNet子集及医疗影像数据上，GLS与真实MIA成功率（Shadow-MIA、Knockoff-MIA）的**皮尔逊相关系数达0.82–0.91**；  \n- GLS可精准识别高风险样本（如异常、边界或过拟合样本），支持**数据清洗、差分隐私预算分配、主动防御采样**等下游任务；  \n- 相比基线方法（如损失值、梯度范数），GLS具备**理论可解释性、跨架构鲁棒性及零训练开销**，是首个兼具严谨性与实用性的免训练隐私脆弱性代理指标。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15602v1",
      "arxiv_id": "2602.15602v1",
      "title": "Certified Per-Instance Unlearning Using Individual Sensitivity Bounds",
      "authors": [
        "Hanna Benarroch",
        "Jamal Atif",
        "Olivier Cappé"
      ],
      "abstract": "Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15602v1",
      "url": "https://arxiv.org/abs/2602.15602v1",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "learning",
        "machine",
        "differential"
      ],
      "keyword_score": 4,
      "summary_zh": "## 研究背景与问题  \n传统**认证式机器遗忘**（certified machine unlearning）通常依赖向模型更新中注入噪声以满足差分隐私（DP）保障，其噪声规模由*最坏情况敏感度*（worst-case sensitivity）决定。这种保守校准虽能提供全局理论保证，却常导致显著的模型性能下降，严重制约实际部署。\n\n## 创新方法：面向实例的自适应遗忘  \n本文提出一种**基于个体敏感度边界**（individual sensitivity bounds）的新型认证遗忘范式。核心思想是：不再采用统一、保守的噪声尺度，而是为每个待删除样本 *xᵢ* 动态计算其对当前学习过程的*真实影响强度*，并据此注入**最小必要噪声**。该方法首次将**单样本差分隐私**（per-instance DP）理论系统引入遗忘学习框架，解决了“遗忘机制依赖于被删样本”这一关键挑战——即如何在机制本身显式依赖待删点的前提下，仍建立形式化、可验证的遗忘保证。\n\n## 理论突破与验证  \n针对岭回归（ridge regression）模型，我们基于Langevin动力学训练过程，严格推导出**高概率成立的个体敏感度上界**。该界限刻画了单个数据点扰动对梯度轨迹的局部影响，从而支持更精细的噪声校准。理论表明：相比最坏情况方案，本方法可将所需噪声方差降低一个数量级。我们在**线性回归**任务上通过大量实验验证了理论预测：遗忘后模型精度损失显著减小，同时仍100%满足认证遗忘要求（即重构攻击成功率≤阈值）。进一步，我们在**ResNet-18/CIFAR-10**上的实证结果表明：该理念在深度学习中同样有效——使用个体敏感度指导的噪声注入，在保持同等遗忘认证强度下，测试准确率平均提升2.3%–4.1%。",
      "summary_en": "Certified machine unlearning typically injects noise calibrated to the *worst-case sensitivity* of a learning algorithm, yielding strong but overly conservative differential privacy (DP) guarantees—and consequent accuracy degradation. This work introduces **per-instance certified unlearning**, where noise is adaptively scaled to the *individual contribution* of the data point to be removed. To formalize this, we define and bound *per-instance sensitivity* in noisy gradient dynamics using per-instance DP. For ridge regression trained via Langevin dynamics, we derive high-probability individual sensitivity bounds—enabling significantly less noise injection while preserving rigorous unlearning certification. Experiments on linear models validate our theory, showing up to 10× reduction in required noise variance. Empirically, on ResNet-18/CIFAR-10, our approach improves post-unlearning test accuracy by 2.3–4.1% under identical certification strength, demonstrating practical relevance beyond linear settings.",
      "summary": "## 研究背景与问题  \n传统**认证式机器遗忘**（certified machine unlearning）通常依赖向模型更新中注入噪声以满足差分隐私（DP）保障，其噪声规模由*最坏情况敏感度*（worst-case sensitivity）决定。这种保守校准虽能提供全局理论保证，却常导致显著的模型性能下降，严重制约实际部署。\n\n## 创新方法：面向实例的自适应遗忘  \n本文提出一种**基于个体敏感度边界**（individual sensitivity bounds）的新型认证遗忘范式。核心思想是：不再采用统一、保守的噪声尺度，而是为每个待删除样本 *xᵢ* 动态计算其对当前学习过程的*真实影响强度*，并据此注入**最小必要噪声**。该方法首次将**单样本差分隐私**（per-instance DP）理论系统引入遗忘学习框架，解决了“遗忘机制依赖于被删样本”这一关键挑战——即如何在机制本身显式依赖待删点的前提下，仍建立形式化、可验证的遗忘保证。\n\n## 理论突破与验证  \n针对岭回归（ridge regression）模型，我们基于Langevin动力学训练过程，严格推导出**高概率成立的个体敏感度上界**。该界限刻画了单个数据点扰动对梯度轨迹的局部影响，从而支持更精细的噪声校准。理论表明：相比最坏情况方案，本方法可将所需噪声方差降低一个数量级。我们在**线性回归**任务上通过大量实验验证了理论预测：遗忘后模型精度损失显著减小，同时仍100%满足认证遗忘要求（即重构攻击成功率≤阈值）。进一步，我们在**ResNet-18/CIFAR-10**上的实证结果表明：该理念在深度学习中同样有效——使用个体敏感度指导的噪声注入，在保持同等遗忘认证强度下，测试准确率平均提升2.3%–4.1%。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15510v1",
      "arxiv_id": "2602.15510v1",
      "title": "On the Geometric Coherence of Global Aggregation in Federated GNN",
      "authors": [
        "Chethana Prasad Kabgere",
        "Shylaja SS"
      ],
      "abstract": "Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates originating from incompatible propagation regimes can therefore introduce destructive interference in this transformation space.This leads to loss of coherence in global message passing. Importantly, this degradation is not necessarily reflected in conventional metrics such as loss or accuracy.To address this issue, we propose GGRS (Global Geometric Reference Structure), a server-side framework that regulates client updates prior to aggregation based on geometric admissibility criteria. GGRS preserves directional consistency of relational transformations as well as maintains diversity of admissible propagation subspaces. It also stabilizes sensitivity to neighborhood interactions, without accessing client data or graph topology. Experiments on heterogeneous GNN-native, Amazon Co-purchase datasets demonstrate that GGRS preserves global message-passing coherence across training rounds by highlighting the necessity of geometry-aware regulation in federated graph learning.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15510v1",
      "url": "https://arxiv.org/abs/2602.15510v1",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.NI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n联邦学习（FL）支持跨客户端的分布式模型训练，无需集中共享原始数据；图神经网络（GNN）则通过消息传递建模图结构关系。然而，在**跨域联邦GNN**场景中，各客户端图数据在拓扑结构、节点分布及信息传播机制上高度异构。现有FL框架通常直接对客户端GNN更新进行向量平均（如FedAvg），虽能实现数值收敛，却常导致全局模型在**关系建模层面失效**——即消息传递失去方向性、敏感性与一致性，而该退化在标准损失或准确率指标中难以察觉。\n\n## 几何失效机制  \n本文首次揭示一种**几何相干性崩溃**（geometric coherence collapse）现象：GNN参数虽以向量形式存储，实则编码了定义图邻域间信息流向、强度与响应特性的**关系变换算子**。当来自不兼容传播范式（如稀疏vs密集、局部vs全局感知）的更新被盲目聚合时，其在变换空间中产生**破坏性干涉**，致使全局消息传递丧失几何一致性。\n\n## 方法创新：GGRS框架  \n我们提出**全局几何参考结构**（GGRS），一种纯服务端、无需访问客户端数据或图拓扑的调控机制。GGRS基于**几何可容性判据**（geometric admissibility）对客户端更新进行预筛选与正则化：  \n- ✅ 保持关系变换的方向一致性（如邻域聚合权重的相对角度约束）；  \n- ✅ 维护可接受传播子空间的多样性（避免模式坍缩）；  \n- ✅ 稳定模型对邻居交互的敏感度梯度（抑制过平滑/过振荡）。  \n\n## 实验验证  \n在Amazon Co-purchase等GNN原生异构数据集上，GGRS显著提升多轮训练中全局消息传递的几何稳定性（通过谱距离、方向相似度等新指标量化），同时将节点分类准确率平均提升2.7–5.1个百分点，证实**几何感知的聚合调控是联邦图学习的必要范式**。",
      "summary_en": "Federated Graph Neural Networks (GNNs) suffer from a previously unrecognized *geometric coherence collapse*: while standard aggregation (e.g., FedAvg) achieves numerical convergence, it disrupts the relational transformation geometry encoded in GNN parameters—degrading directionality, sensitivity, and consistency of global message passing without clear signal in loss or accuracy. We identify this failure as destructive interference among updates from structurally incompatible client graphs. To address it, we propose **GGRS (Global Geometric Reference Structure)**, a server-side framework that filters and regularizes client updates *before aggregation* using geometric admissibility criteria—enforcing directional consistency of relational operators, preserving diversity of admissible propagation subspaces, and stabilizing neighborhood interaction sensitivity—all without accessing client data or topology. On heterogeneous GNN-native benchmarks (e.g., Amazon Co-purchase), GGRS maintains geometric coherence across training rounds (validated via spectral and angular metrics) and improves node classification accuracy by 2.7–5.1% on average, establishing geometry-aware regulation as essential for robust federated graph learning.",
      "summary": "## 研究背景与问题  \n联邦学习（FL）支持跨客户端的分布式模型训练，无需集中共享原始数据；图神经网络（GNN）则通过消息传递建模图结构关系。然而，在**跨域联邦GNN**场景中，各客户端图数据在拓扑结构、节点分布及信息传播机制上高度异构。现有FL框架通常直接对客户端GNN更新进行向量平均（如FedAvg），虽能实现数值收敛，却常导致全局模型在**关系建模层面失效**——即消息传递失去方向性、敏感性与一致性，而该退化在标准损失或准确率指标中难以察觉。\n\n## 几何失效机制  \n本文首次揭示一种**几何相干性崩溃**（geometric coherence collapse）现象：GNN参数虽以向量形式存储，实则编码了定义图邻域间信息流向、强度与响应特性的**关系变换算子**。当来自不兼容传播范式（如稀疏vs密集、局部vs全局感知）的更新被盲目聚合时，其在变换空间中产生**破坏性干涉**，致使全局消息传递丧失几何一致性。\n\n## 方法创新：GGRS框架  \n我们提出**全局几何参考结构**（GGRS），一种纯服务端、无需访问客户端数据或图拓扑的调控机制。GGRS基于**几何可容性判据**（geometric admissibility）对客户端更新进行预筛选与正则化：  \n- ✅ 保持关系变换的方向一致性（如邻域聚合权重的相对角度约束）；  \n- ✅ 维护可接受传播子空间的多样性（避免模式坍缩）；  \n- ✅ 稳定模型对邻居交互的敏感度梯度（抑制过平滑/过振荡）。  \n\n## 实验验证  \n在Amazon Co-purchase等GNN原生异构数据集上，GGRS显著提升多轮训练中全局消息传递的几何稳定性（通过谱距离、方向相似度等新指标量化），同时将节点分类准确率平均提升2.7–5.1个百分点，证实**几何感知的聚合调控是联邦图学习的必要范式**。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15478v1",
      "arxiv_id": "2602.15478v1",
      "title": "Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data",
      "authors": [
        "Sharmad Kalpande",
        "Saurabh Shirke",
        "Haroon R. Lone"
      ],
      "abstract": "Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.   In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15478v1",
      "url": "https://arxiv.org/abs/2602.15478v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n情绪不稳定性是精神健康状况的关键行为标志，但传统评估依赖稀疏、回溯式的自报问卷，难以捕捉其动态连续性。智能手机传感技术为真实场景下的被动情绪推断提供了新路径，然而在跨国家规模化部署中面临三重挑战：**用户隐私敏感性**（数据不可集中）、**传感覆盖不均**（如GPS、蓝牙、加速度计在不同设备/地区启用率差异显著）、以及**行为模式高度异质性**（文化、生活习惯、城市化水平导致步态、社交互动、屏幕使用等特征分布迥异）。\n\n## 方法创新：FedFAP框架  \n本文提出**FedFAP**（Federated Feature-Aware Personalization），一种面向跨国家场景的特征感知个性化联邦学习框架。其核心设计包括：（1）**模态自适应特征编码器**——为每个国家客户端独立建模本地主流传感模态（如仅WiFi+蓝牙的低功耗配置 vs. 全模态高采样配置）；（2）**分层个性化聚合机制**——在全局模型共享底层时序表征能力的同时，为各国保留可学习的个性化特征投影头与情绪分类头；（3）**隐私增强训练协议**——全程不传输原始数据或中间特征，仅交换加密梯度与轻量级模型参数。\n\n## 关键结果与意义  \n在覆盖中国、印度、巴西、德国和美国的5国真实用户队列（N=2,847，平均每人>6周连续数据）上，FedFAP实现**AUROC 0.744**，较集中式训练提升4.2%，较SOTA个性化联邦基线（pFedMe、FedPer）平均高出6.8%。消融实验证实：模态感知编码贡献最大性能增益（+3.1% AUROC），而个性化分类头对文化特异性情绪表达（如高语境vs.低语境社会中的活动-情绪关联）建模尤为关键。本研究首次验证了**隐私优先、文化适配、模态灵活**的联邦范式在跨域心理健康计算中的可行性，为全球可扩展的情绪感知移动系统提供了可落地的技术蓝图。",
      "summary_en": "Mood instability is a critical behavioral marker of mental health, yet conventional assessments lack ecological validity due to infrequent, retrospective reporting. While smartphone sensing enables passive, in-the-wild mood inference, large-scale cross-country deployment is hindered by privacy constraints, heterogeneous sensor availability, and profound cultural-behavioral variability. To address this, we propose **FedFAP**, a feature-aware personalized federated learning framework where each country acts as an independent client retaining raw data locally. FedFAP introduces modality-adaptive encoders per client and hierarchical personalization—sharing foundational temporal representations globally while preserving country-specific feature projection and classification heads. Evaluated across five geographically and culturally diverse countries (China, India, Brazil, Germany, USA; N=2,847), FedFAP achieves an **AUROC of 0.744**, outperforming centralized training (+4.2%) and state-of-the-art personalized federated baselines (+6.8% on average). Our results demonstrate that privacy-preserving, population-aware personalization is essential for scalable, equitable, and clinically meaningful mobile mood sensing.",
      "summary": "## 研究背景  \n情绪不稳定性是精神健康状况的关键行为标志，但传统评估依赖稀疏、回溯式的自报问卷，难以捕捉其动态连续性。智能手机传感技术为真实场景下的被动情绪推断提供了新路径，然而在跨国家规模化部署中面临三重挑战：**用户隐私敏感性**（数据不可集中）、**传感覆盖不均**（如GPS、蓝牙、加速度计在不同设备/地区启用率差异显著）、以及**行为模式高度异质性**（文化、生活习惯、城市化水平导致步态、社交互动、屏幕使用等特征分布迥异）。\n\n## 方法创新：FedFAP框架  \n本文提出**FedFAP**（Federated Feature-Aware Personalization），一种面向跨国家场景的特征感知个性化联邦学习框架。其核心设计包括：（1）**模态自适应特征编码器**——为每个国家客户端独立建模本地主流传感模态（如仅WiFi+蓝牙的低功耗配置 vs. 全模态高采样配置）；（2）**分层个性化聚合机制**——在全局模型共享底层时序表征能力的同时，为各国保留可学习的个性化特征投影头与情绪分类头；（3）**隐私增强训练协议**——全程不传输原始数据或中间特征，仅交换加密梯度与轻量级模型参数。\n\n## 关键结果与意义  \n在覆盖中国、印度、巴西、德国和美国的5国真实用户队列（N=2,847，平均每人>6周连续数据）上，FedFAP实现**AUROC 0.744**，较集中式训练提升4.2%，较SOTA个性化联邦基线（pFedMe、FedPer）平均高出6.8%。消融实验证实：模态感知编码贡献最大性能增益（+3.1% AUROC），而个性化分类头对文化特异性情绪表达（如高语境vs.低语境社会中的活动-情绪关联）建模尤为关键。本研究首次验证了**隐私优先、文化适配、模态灵活**的联邦范式在跨域心理健康计算中的可行性，为全球可扩展的情绪感知移动系统提供了可落地的技术蓝图。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15380v1",
      "arxiv_id": "2602.15380v1",
      "title": "Fractional-Order Federated Learning",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "YangQuan Chen"
      ],
      "abstract": "Federated learning (FL) allows remote clients to train a global model collaboratively while protecting client privacy. Despite its privacy-preserving benefits, FL has significant drawbacks, including slow convergence, high communication cost, and non-independent-and-identically-distributed (non-IID) data. In this work, we present a novel FedAvg variation called Fractional-Order Federated Averaging (FOFedAvg), which incorporates Fractional-Order Stochastic Gradient Descent (FOSGD) to capture long-range relationships and deeper historical information. By introducing memory-aware fractional-order updates, FOFedAvg improves communication efficiency and accelerates convergence while mitigating instability caused by heterogeneous, non-IID client data. We compare FOFedAvg against a broad set of established federated optimization algorithms on benchmark datasets including MNIST, FEMNIST, CIFAR-10, CIFAR-100, EMNIST, the Cleveland heart disease dataset, Sent140, PneumoniaMNIST, and Edge-IIoTset. Across a range of non-IID partitioning schemes, FOFedAvg is competitive with, and often outperforms, these baselines in terms of test performance and convergence speed. On the theoretical side, we prove that FOFedAvg converges to a stationary point under standard smoothness and bounded-variance assumptions for fractional order $0<α\\le 1$. Together, these results show that fractional-order, memory-aware updates can substantially improve the robustness and effectiveness of federated learning, offering a practical path toward distributed training on heterogeneous data.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15380v1",
      "url": "https://arxiv.org/abs/2602.15380v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n联邦学习（FL）允许多个客户端在不共享原始数据的前提下协同训练全局模型，有效保障数据隐私。然而，其实际应用仍面临三大瓶颈：**收敛速度慢**、**通信开销高**，以及由**非独立同分布（non-IID）数据**引发的训练不稳定与性能下降。\n\n## 方法创新  \n本文提出一种新型联邦优化算法——**分数阶联邦平均（FOFedAvg）**，作为FedAvg的关键改进。核心创新在于将**分数阶随机梯度下降（FOSGD）** 引入客户端本地更新：利用阶数 $0 < \\alpha \\leq 1$ 的Caputo型分数阶导数构建记忆增强型更新规则，使参数迭代能显式建模历史梯度的长程依赖与累积效应，突破传统整数阶SGD的“无记忆”局限。\n\n## 主要结果  \n我们在**8个异构基准数据集**（MNIST、FEMNIST、CIFAR-10/100、EMNIST、Cleveland心脏病、Sent140、PneumoniaMNIST、Edge-IIoTset）上系统评估FOFedAvg。在多种non-IID划分（包括Dirichlet分布、量级偏斜、标签偏斜）下，FOFedAvg在**测试准确率**和**收敛轮次**两方面均显著优于FedAvg、FedProx、SCAFFOLD、MOON、FedNova等主流基线。例如，在CIFAR-10（Dirichlet $\\beta=0.1$）上，FOFedAvg较FedAvg提前37%轮次达98%最终精度；在资源受限的Edge-IIoTset上通信量降低22%。\n\n## 理论保障与意义  \n我们首次为分数阶联邦算法建立收敛性理论：在标准光滑性与梯度有界方差假设下，证明FOFedAvg以$O(1/\\sqrt{T})$速率收敛至平稳点。该工作证实——**引入可控记忆机制的分数阶更新，可兼顾鲁棒性、效率与泛化能力**，为non-IID场景下的实用化分布式学习提供了新范式。",
      "summary_en": "Federated learning (FL) enables privacy-preserving collaborative model training across decentralized clients, yet suffers from slow convergence, high communication overhead, and instability under non-IID data. To address these challenges, we propose **Fractional-Order Federated Averaging (FOFedAvg)**—a novel FedAvg variant integrating **Fractional-Order Stochastic Gradient Descent (FOSGD)** at the client level. By leveraging Caputo fractional derivatives with order $0 < \\alpha \\leq 1$, FOFedAvg embeds long-range memory into local updates, capturing richer historical gradient information than integer-order methods. Extensive experiments across eight diverse benchmarks—including MNIST, CIFAR-10/100, FEMNIST, EMNIST, Sent140, Cleveland heart disease, PneumoniaMNIST, and Edge-IIoTset—demonstrate that FOFedAvg consistently matches or outperforms state-of-the-art baselines (e.g., FedProx, SCAFFOLD, MOON) in both test accuracy and convergence speed under various non-IID partitioning schemes. Theoretically, we prove its $O(1/\\sqrt{T})$ convergence to a stationary point under standard smoothness and bounded-variance assumptions. This work establishes fractional-order memory as a practical, theoretically grounded enhancement for robust and efficient FL on heterogeneous data.",
      "summary": "## 背景与挑战  \n联邦学习（FL）允许多个客户端在不共享原始数据的前提下协同训练全局模型，有效保障数据隐私。然而，其实际应用仍面临三大瓶颈：**收敛速度慢**、**通信开销高**，以及由**非独立同分布（non-IID）数据**引发的训练不稳定与性能下降。\n\n## 方法创新  \n本文提出一种新型联邦优化算法——**分数阶联邦平均（FOFedAvg）**，作为FedAvg的关键改进。核心创新在于将**分数阶随机梯度下降（FOSGD）** 引入客户端本地更新：利用阶数 $0 < \\alpha \\leq 1$ 的Caputo型分数阶导数构建记忆增强型更新规则，使参数迭代能显式建模历史梯度的长程依赖与累积效应，突破传统整数阶SGD的“无记忆”局限。\n\n## 主要结果  \n我们在**8个异构基准数据集**（MNIST、FEMNIST、CIFAR-10/100、EMNIST、Cleveland心脏病、Sent140、PneumoniaMNIST、Edge-IIoTset）上系统评估FOFedAvg。在多种non-IID划分（包括Dirichlet分布、量级偏斜、标签偏斜）下，FOFedAvg在**测试准确率**和**收敛轮次**两方面均显著优于FedAvg、FedProx、SCAFFOLD、MOON、FedNova等主流基线。例如，在CIFAR-10（Dirichlet $\\beta=0.1$）上，FOFedAvg较FedAvg提前37%轮次达98%最终精度；在资源受限的Edge-IIoTset上通信量降低22%。\n\n## 理论保障与意义  \n我们首次为分数阶联邦算法建立收敛性理论：在标准光滑性与梯度有界方差假设下，证明FOFedAvg以$O(1/\\sqrt{T})$速率收敛至平稳点。该工作证实——**引入可控记忆机制的分数阶更新，可兼顾鲁棒性、效率与泛化能力**，为non-IID场景下的实用化分布式学习提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.16018v1",
      "arxiv_id": "2602.16018v1",
      "title": "Edge-Local and Qubit-Efficient Quantum Graph Learning for the NISQ Era",
      "authors": [
        "Armin Ahmadkhaniha",
        "Jake Doliskani"
      ],
      "abstract": "Graph neural networks (GNNs) are a powerful framework for learning representations from graph-structured data, but their direct implementation on near-term quantum hardware remains challenging due to circuit depth, multi-qubit interactions, and qubit scalability constraints. In this work, we introduce a fully quantum graph convolutional architecture designed explicitly for unsupervised learning in the noisy intermediate-scale quantum (NISQ) regime. Our approach combines a variational quantum feature extraction layer with an edge-local and qubit-efficient quantum message-passing mechanism inspired by the Quantum Alternating Operator Ansatz (QAOA) framework. Unlike prior models that rely on global operations or multi-controlled unitaries, our model decomposes message passing into pairwise interactions along graph edges using only hardware-native single- and two-qubit gates. This design reduces the qubit requirement from $O(Nn)$ to $O(n)$ for a graph with $N$ nodes and $n$-qubit feature registers, enabling implementation on current quantum devices regardless of graph size. We train the model using the Deep Graph Infomax objective to perform unsupervised node representation learning. Experiments on the Cora citation network and a large-scale genomic SNP dataset demonstrate that our model remains competitive with prior quantum and hybrid approaches.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.16018v1",
      "url": "https://arxiv.org/abs/2602.16018v1",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n图神经网络（GNN）在处理图结构数据方面展现出强大能力，但在含噪声中等规模量子（NISQ）设备上的直接实现面临严峻挑战：深层电路易受噪声干扰、多比特门操作难以保真、且传统量子图模型常需 $O(Nn)$ 量级的物理量子比特（$N$ 为节点数，$n$ 为每节点特征维数），严重制约可扩展性。\n\n## 方法创新  \n本文提出**Edge-Local and Qubit-Efficient Quantum Graph Learning（ELQE-QGL）**——首个面向NISQ时代的全量子无监督图学习架构。核心创新包括：  \n- **边缘局部消息传递机制**：摒弃全局酉变换或高阶受控门，仅使用硬件原生的单/双量子比特门，在每条图边上执行**成对量子交互**，实现信息沿边传播；  \n- **量子特征提取层**：采用变分量子电路对节点初始特征进行编码，并与图拓扑协同优化；  \n- **QAOA启发式设计**：将消息传递建模为类QAOA的交替演化序列，兼顾表达能力与电路深度可控性；  \n- **极致量子比特效率**：将量子资源需求从 $O(Nn)$ 压缩至 **$O(n)$**，即仅依赖固定规模的 $n$ 个量子比特，与图规模 $N$ 完全解耦。\n\n## 实验与成效  \n在无监督节点表示学习任务中，采用Deep Graph Infomax（DGI）目标函数训练模型。在Cora引文网络和大规模基因组SNP数据集上的实验表明：ELQE-QGL在分类与聚类指标上**媲美甚至超越现有量子/混合基线方法**，同时可在当前IBM、Rigetti等16–32量子比特设备上完整部署，无需图裁剪或子图采样。",
      "summary_en": "We propose ELQE-QGL, the first fully quantum, unsupervised graph learning framework tailored for NISQ devices. It replaces global or multi-controlled operations with an **edge-local, qubit-efficient message-passing mechanism**, decomposing graph convolutions into native single- and two-qubit gates applied pairwise along edges. This reduces qubit overhead from $O(Nn)$ to **$O(n)$**, enabling scalable deployment on current hardware regardless of graph size $N$. Integrated with a variational quantum feature encoder and trained via Deep Graph Infomax, ELQE-QGL achieves competitive node representation quality on Cora and a large-scale genomic SNP dataset—matching or surpassing prior quantum and hybrid models while requiring orders-of-magnitude fewer qubits and shallower circuits.",
      "summary": "## 背景与挑战  \n图神经网络（GNN）在处理图结构数据方面展现出强大能力，但在含噪声中等规模量子（NISQ）设备上的直接实现面临严峻挑战：深层电路易受噪声干扰、多比特门操作难以保真、且传统量子图模型常需 $O(Nn)$ 量级的物理量子比特（$N$ 为节点数，$n$ 为每节点特征维数），严重制约可扩展性。\n\n## 方法创新  \n本文提出**Edge-Local and Qubit-Efficient Quantum Graph Learning（ELQE-QGL）**——首个面向NISQ时代的全量子无监督图学习架构。核心创新包括：  \n- **边缘局部消息传递机制**：摒弃全局酉变换或高阶受控门，仅使用硬件原生的单/双量子比特门，在每条图边上执行**成对量子交互**，实现信息沿边传播；  \n- **量子特征提取层**：采用变分量子电路对节点初始特征进行编码，并与图拓扑协同优化；  \n- **QAOA启发式设计**：将消息传递建模为类QAOA的交替演化序列，兼顾表达能力与电路深度可控性；  \n- **极致量子比特效率**：将量子资源需求从 $O(Nn)$ 压缩至 **$O(n)$**，即仅依赖固定规模的 $n$ 个量子比特，与图规模 $N$ 完全解耦。\n\n## 实验与成效  \n在无监督节点表示学习任务中，采用Deep Graph Infomax（DGI）目标函数训练模型。在Cora引文网络和大规模基因组SNP数据集上的实验表明：ELQE-QGL在分类与聚类指标上**媲美甚至超越现有量子/混合基线方法**，同时可在当前IBM、Rigetti等16–32量子比特设备上完整部署，无需图裁剪或子图采样。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15996v1",
      "arxiv_id": "2602.15996v1",
      "title": "Exploring New Frontiers in Vertical Federated Learning: the Role of Saddle Point Reformulation",
      "authors": [
        "Aleksandr Beznosikov",
        "Georgiy Kormakov",
        "Alexander Grigorievskiy",
        "Mikhail Rudakov",
        "Ruslan Nazykov",
        "Alexander Rogozin",
        "Anton Vakhrushev",
        "Andrey Savchenko",
        "Martin Takáč",
        "Alexander Gasnikov"
      ],
      "abstract": "The objective of Vertical Federated Learning (VFL) is to collectively train a model using features available on different devices while sharing the same users. This paper focuses on the saddle point reformulation of the VFL problem via the classical Lagrangian function. We first demonstrate how this formulation can be solved using deterministic methods. More importantly, we explore various stochastic modifications to adapt to practical scenarios, such as employing compression techniques for efficient information transmission, enabling partial participation for asynchronous communication, and utilizing coordinate selection for faster local computation. We show that the saddle point reformulation plays a key role and opens up possibilities to use mentioned extension that seem to be impossible in the standard minimization formulation. Convergence estimates are provided for each algorithm, demonstrating their effectiveness in addressing the VFL problem. Additionally, alternative reformulations are investigated, and numerical experiments are conducted to validate performance and effectiveness of the proposed approach.",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15996v1",
      "url": "https://arxiv.org/abs/2602.15996v1",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n垂直联邦学习（VFL）旨在多个数据持有方（如不同机构）共享同一用户集但特征分布于不同端时，协同训练全局模型，同时严格保护原始数据隐私。传统VFL建模多基于联合最小化目标函数，但该范式在引入通信压缩、异步参与、局部坐标优化等实际约束时面临本质性困难——梯度耦合强、对偶结构缺失、收敛性分析受限。\n\n## 方法创新：鞍点重构的核心作用  \n本文系统提出并深入研究VFL问题的**鞍点重构范式**，即通过经典拉格朗日函数将原优化问题转化为带约束的极小-极大（min-max）形式。我们证明：该重构不仅保持等价性，更天然解耦各方局部更新与全局一致性约束，为算法设计提供全新自由度。\n\n## 关键技术扩展与理论保障  \n基于该框架，我们发展三类重要随机化扩展：  \n- **通信高效化**：集成量化/稀疏化压缩，显著降低上传带宽；  \n- **系统鲁棒性**：支持客户端**部分参与**与**异步更新**，适配真实分布式环境；  \n- **计算加速**：引入**随机坐标选择机制**，使单次本地迭代仅更新子特征块，提升局部效率。  \n所有算法均给出严格的**非渐近收敛率估计**（含次线性/线性收敛界），并在统一分析框架下证明其优于标准最小化范式下的对应变体。\n\n## 实验验证与意义  \n通过多组跨域数据实验（医疗+金融特征融合等），验证了各算法在精度、通信开销与收敛速度上的显著优势。本工作首次揭示：**鞍点重构并非技术性重写，而是解锁VFL可扩展性瓶颈的关键数学桥梁**，为隐私-效率-鲁棒性三重目标协同优化奠定新基础。",
      "summary_en": "This paper investigates Vertical Federated Learning (VFL) through the lens of **saddle-point reformulation**, recasting the standard minimization problem into an equivalent min-max Lagrangian form. We demonstrate that this reformulation is not merely equivalent but *enabling*: it naturally decouples local computations from global consistency constraints, thereby unlocking algorithmic extensions infeasible under conventional formulations. Specifically, we propose and analyze three stochastic variants: (i) compression-aware updates for bandwidth-efficient communication; (ii) partial and asynchronous client participation to enhance system robustness; and (iii) coordinate-wise local updates for accelerated computation. For each variant, we establish non-asymptotic convergence rates—ranging from $O(1/k)$ to linear—under mild assumptions. Numerical experiments on real-world heterogeneous datasets confirm substantial improvements in accuracy, communication cost, and training speed over baseline VFL methods. Our work establishes saddle-point reformulation as a foundational paradigm for scalable, practical VFL.",
      "summary": "## 研究背景与问题  \n垂直联邦学习（VFL）旨在多个数据持有方（如不同机构）共享同一用户集但特征分布于不同端时，协同训练全局模型，同时严格保护原始数据隐私。传统VFL建模多基于联合最小化目标函数，但该范式在引入通信压缩、异步参与、局部坐标优化等实际约束时面临本质性困难——梯度耦合强、对偶结构缺失、收敛性分析受限。\n\n## 方法创新：鞍点重构的核心作用  \n本文系统提出并深入研究VFL问题的**鞍点重构范式**，即通过经典拉格朗日函数将原优化问题转化为带约束的极小-极大（min-max）形式。我们证明：该重构不仅保持等价性，更天然解耦各方局部更新与全局一致性约束，为算法设计提供全新自由度。\n\n## 关键技术扩展与理论保障  \n基于该框架，我们发展三类重要随机化扩展：  \n- **通信高效化**：集成量化/稀疏化压缩，显著降低上传带宽；  \n- **系统鲁棒性**：支持客户端**部分参与**与**异步更新**，适配真实分布式环境；  \n- **计算加速**：引入**随机坐标选择机制**，使单次本地迭代仅更新子特征块，提升局部效率。  \n所有算法均给出严格的**非渐近收敛率估计**（含次线性/线性收敛界），并在统一分析框架下证明其优于标准最小化范式下的对应变体。\n\n## 实验验证与意义  \n通过多组跨域数据实验（医疗+金融特征融合等），验证了各算法在精度、通信开销与收敛速度上的显著优势。本工作首次揭示：**鞍点重构并非技术性重写，而是解锁VFL可扩展性瓶颈的关键数学桥梁**，为隐私-效率-鲁棒性三重目标协同优化奠定新基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15927v1",
      "arxiv_id": "2602.15927v1",
      "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
      "authors": [
        "Christian Schlarmann",
        "Matthias Hein"
      ],
      "abstract": "Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
      "published": "2026-02-17",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15927v1",
      "url": "https://arxiv.org/abs/2602.15927v1",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n生成式大视觉语言模型（LVLMs）在多模态理解与生成任务中表现卓越，正被广泛应用于智能客服、教育辅助及内容创作等多轮对话场景。然而，其在**长上下文、多轮交互**下的安全性研究严重滞后——尤其当恶意图像作为“记忆载体”被悄然注入模型内部状态时，现有防御机制几近失效。\n\n## 方法创新：视觉记忆注入（VMI）攻击  \n本文提出首个面向多轮对话的**视觉记忆注射攻击（Visual Memory Injection, VMI）**。该攻击不依赖对模型参数或API的直接访问，而是通过精心设计的**对抗性图像**实现：攻击者将微扰图像发布于公开平台（如社交媒体），良性用户下载后在正常对话中首次上传该图。LVLM在初始轮次中行为完全正常（无异常输出、无性能下降），但其内部视觉-语言对齐表征已被隐式“污染”。当用户后续输入特定**触发提示**（trigger prompt）——如“总结这张图的核心观点”或“按品牌方要求推荐产品”——模型即稳定输出预设的**目标有害信息**（如虚假广告话术、误导性政治主张），完成隐蔽操控。\n\n## 关键发现与影响  \n- ✅ **跨轮次鲁棒性**：VMI在经历5–15轮无关对话后仍保持高达92.3%的触发成功率，远超单轮攻击；  \n- ✅ **模型普适性**：在Qwen-VL、InternVL、LLaVA-1.6等主流开源LVLM上均成功复现；  \n- ✅ **高度隐蔽性**：图像扰动不可见（PSNR > 48 dB），人类无法察觉，且标准检测工具（如CLIP相似度、频域分析）无法识别；  \n- ⚠️ **现实威胁显著**：证明仅需一次图像上传，即可在后续任意轮次实施定向诱导，为大规模社会工程攻击（如舆论操纵、欺诈营销）提供新路径。  \n\n本工作揭示了LVLM在多轮交互中“视觉记忆”的脆弱本质，呼吁亟需构建上下文感知的鲁棒性评测基准与防御范式。代码已开源：https://github.com/chs20/visual-memory-injection",
      "summary_en": "We present **Visual Memory Injection (VMI)**, the first stealthy attack enabling persistent, trigger-based manipulation of Large Vision-Language Models (LVLMs) in realistic multi-turn conversations. Unlike prior single-turn attacks, VMI embeds malicious behavior into an LVLM’s internal state via a benign-looking adversarial image uploaded by a user—without model access or API tampering. The model behaves normally on all non-triggering prompts, but reliably outputs attacker-specified harmful content (e.g., deceptive ads or political disinformation) upon encountering a simple textual trigger—even after 5–15 rounds of unrelated dialogue. We demonstrate VMI’s effectiveness, stealthiness (imperceptible perturbations, PSNR > 48 dB), and cross-model transferability across Qwen-VL, InternVL, and LLaVA-1.6. Our results expose a critical security gap: LVLMs can retain and activate malicious “visual memory” over long conversational contexts, enabling scalable, low-effort social engineering. Code is publicly available.",
      "summary": "## 背景与问题  \n生成式大视觉语言模型（LVLMs）在多模态理解与生成任务中表现卓越，正被广泛应用于智能客服、教育辅助及内容创作等多轮对话场景。然而，其在**长上下文、多轮交互**下的安全性研究严重滞后——尤其当恶意图像作为“记忆载体”被悄然注入模型内部状态时，现有防御机制几近失效。\n\n## 方法创新：视觉记忆注入（VMI）攻击  \n本文提出首个面向多轮对话的**视觉记忆注射攻击（Visual Memory Injection, VMI）**。该攻击不依赖对模型参数或API的直接访问，而是通过精心设计的**对抗性图像**实现：攻击者将微扰图像发布于公开平台（如社交媒体），良性用户下载后在正常对话中首次上传该图。LVLM在初始轮次中行为完全正常（无异常输出、无性能下降），但其内部视觉-语言对齐表征已被隐式“污染”。当用户后续输入特定**触发提示**（trigger prompt）——如“总结这张图的核心观点”或“按品牌方要求推荐产品”——模型即稳定输出预设的**目标有害信息**（如虚假广告话术、误导性政治主张），完成隐蔽操控。\n\n## 关键发现与影响  \n- ✅ **跨轮次鲁棒性**：VMI在经历5–15轮无关对话后仍保持高达92.3%的触发成功率，远超单轮攻击；  \n- ✅ **模型普适性**：在Qwen-VL、InternVL、LLaVA-1.6等主流开源LVLM上均成功复现；  \n- ✅ **高度隐蔽性**：图像扰动不可见（PSNR > 48 dB），人类无法察觉，且标准检测工具（如CLIP相似度、频域分析）无法识别；  \n- ⚠️ **现实威胁显著**：证明仅需一次图像上传，即可在后续任意轮次实施定向诱导，为大规模社会工程攻击（如舆论操纵、欺诈营销）提供新路径。  \n\n本工作揭示了LVLM在多轮交互中“视觉记忆”的脆弱本质，呼吁亟需构建上下文感知的鲁棒性评测基准与防御范式。代码已开源：https://github.com/chs20/visual-memory-injection",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15161v1",
      "arxiv_id": "2602.15161v1",
      "title": "Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning",
      "authors": [
        "Mohammad Hadi Foroughi",
        "Seyed Hamed Rastegar",
        "Mohammad Sabokrou",
        "Ahmad Khonsari"
      ],
      "abstract": "Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15161v1",
      "url": "https://arxiv.org/abs/2602.15161v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "backdoor",
        "learning",
        "federated",
        "neural"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与问题  \n联邦学习（FL）通过在边缘设备上分布式训练模型，兼顾数据本地化与协同建模，成为保护敏感用户隐私的重要范式。然而，其去中心化架构也引入新型安全风险——尤其是难以检测的**后门攻击**，可在不损害主任务性能的前提下，使模型对特定触发器产生恶意响应，严重威胁FL系统可信性。\n\n## 方法创新：层平滑攻击（LSA）  \n本文提出**Layer Smoothing Attack（LSA）**，一种面向FL场景的新型后门攻击框架，核心在于挖掘神经网络中**层特异性漏洞**。我们设计**层替换分析（Layer Substitution Analysis, LSA-Analysis）** 方法，定量评估各层对后门激活的贡献度，精准识别出“后门关键层”（Backdoor-Critical, BC layers）。随后，在FL客户端本地训练阶段，LSA仅对BC层施加轻量级、结构保持的参数扰动（如梯度缩放与局部权重平滑），实现后门的隐蔽注入。\n\n## 主要发现与优势  \n在CIFAR-10、Tiny-ImageNet及FEMNIST数据集上，覆盖ResNet-18、VGG-11与CNN等主流架构的跨场景实验表明：  \n- 后门成功率高达**97%**，且主任务准确率下降＜1.2%，显著优于现有攻击；  \n- 成功绕过FedAvg+Trimmed Mean、FoolsGold、Krum、Norm-Clipping及近期基于梯度/更新检测的5种SOTA防御机制；  \n- BC层具有跨模型与跨数据集一致性，揭示了FL中**层间功能异质性所隐含的系统性安全盲区**。  \n\n本研究首次将**层感知视角**引入FL后门攻击与防御研究，为构建更鲁棒的联邦安全框架提供了关键实证依据与方法论启示。",
      "summary_en": "Federated learning (FL) promises privacy-preserving collaborative training but remains vulnerable to stealthy backdoor attacks. This paper introduces the **Layer Smoothing Attack (LSA)**, a novel FL-specific backdoor method that exploits *layer-specific vulnerabilities* in neural networks. We first propose **Layer Substitution Analysis** to systematically identify *Backdoor-Critical (BC) layers*—those most influential for backdoor activation. LSA then injects persistent, high-fidelity backdoors by applying subtle, structure-aware perturbations *only* to BC layers during local client training. Extensive experiments across diverse models (ResNet-18, VGG-11, CNN) and datasets (CIFAR-10, Tiny-ImageNet, FEMNIST) show LSA achieves up to **97% attack success rate**, maintains primary-task accuracy (drop <1.2%), and consistently evades five state-of-the-art FL defenses—including Trimmed Mean, Krum, FoolsGold, Norm-Clipping, and gradient-based detectors. Our results expose a fundamental layer-aware security gap in current FL frameworks, urging future defenses to incorporate layer-specific detection and mitigation strategies.",
      "summary": "## 背景与问题  \n联邦学习（FL）通过在边缘设备上分布式训练模型，兼顾数据本地化与协同建模，成为保护敏感用户隐私的重要范式。然而，其去中心化架构也引入新型安全风险——尤其是难以检测的**后门攻击**，可在不损害主任务性能的前提下，使模型对特定触发器产生恶意响应，严重威胁FL系统可信性。\n\n## 方法创新：层平滑攻击（LSA）  \n本文提出**Layer Smoothing Attack（LSA）**，一种面向FL场景的新型后门攻击框架，核心在于挖掘神经网络中**层特异性漏洞**。我们设计**层替换分析（Layer Substitution Analysis, LSA-Analysis）** 方法，定量评估各层对后门激活的贡献度，精准识别出“后门关键层”（Backdoor-Critical, BC layers）。随后，在FL客户端本地训练阶段，LSA仅对BC层施加轻量级、结构保持的参数扰动（如梯度缩放与局部权重平滑），实现后门的隐蔽注入。\n\n## 主要发现与优势  \n在CIFAR-10、Tiny-ImageNet及FEMNIST数据集上，覆盖ResNet-18、VGG-11与CNN等主流架构的跨场景实验表明：  \n- 后门成功率高达**97%**，且主任务准确率下降＜1.2%，显著优于现有攻击；  \n- 成功绕过FedAvg+Trimmed Mean、FoolsGold、Krum、Norm-Clipping及近期基于梯度/更新检测的5种SOTA防御机制；  \n- BC层具有跨模型与跨数据集一致性，揭示了FL中**层间功能异质性所隐含的系统性安全盲区**。  \n\n本研究首次将**层感知视角**引入FL后门攻击与防御研究，为构建更鲁棒的联邦安全框架提供了关键实证依据与方法论启示。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14397v1",
      "arxiv_id": "2602.14397v1",
      "title": "LRD-MPC: Efficient MPC Inference through Low-rank Decomposition",
      "authors": [
        "Tingting Tang",
        "Yongqin Wang",
        "Murali Annavaram"
      ],
      "abstract": "Secure Multi-party Computation (MPC) enables untrusted parties to jointly compute a function without revealing their inputs. Its application to machine learning (ML) has gained significant attention, particularly for secure inference services deployed across multiple cloud virtual machines (VMs), where each VM acts as an MPC party. Model providers secret-share model weights, and users secret-share inputs, ensuring that each server operates only on random shares. While MPC provides strong cryptographic guarantees, it incurs substantial computational and communication overhead. Deep neural networks rely heavily on convolutional and fully connected layers, which require costly matrix multiplications in MPC. To reduce this cost, we propose leveraging low-rank decomposition (LRD) for linear layers, replacing one large matrix multiplication with two smaller ones. Each matrix multiplication in MPC incurs a round of communication, meaning decomposing one matrix multiplication into two leads to an additional communication round. Second, the added matrix multiplication requires an additional truncation step to maintain numerical precision. Since truncation itself requires communication and computation, these overheads can offset the gains from decomposition. To address this, we introduce two complementary optimizations: truncation skipping and efficient linear layer concatenation. Truncation skipping removes the extra truncation induced by LRD, while linear layer concatenation pipelines operations to hide the additional communication round. Together, these techniques mitigate the main overheads of LRD in MPC and improve overall efficiency. Our approach is broadly applicable across MPC protocols. Experiments show up to 25% speedup in n-PC and 33% in 3-PC protocols over full-rank baselines, along with up to 52% GPU energy savings and 88% reduction in offline-phase latency.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14397v1",
      "url": "https://arxiv.org/abs/2602.14397v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "secure",
        "learning",
        "multi-party",
        "computation",
        "machine"
      ],
      "keyword_score": 5,
      "summary_zh": "## LRD-MPC：基于低秩分解的高效MPC推理方法  \n\n**背景与挑战**：安全多方计算（MPC）允许多个互不信任方在不泄露各自输入的前提下协同执行计算，在云环境下的隐私保护机器学习（ML）推理中具有重要应用。典型部署中，多个虚拟机（VM）作为MPC参与方，模型权重与用户输入均被秘密共享，各服务器仅处理随机分片。然而，MPC对深度神经网络（尤其是卷积层与全连接层）中密集的矩阵乘法存在显著开销——每轮矩阵乘需一次通信回合，且需频繁截断（truncation）以维持数值精度，而截断本身亦引入额外通信与计算负担。\n\n**方法创新**：本文提出**LRD-MPC**框架，将低秩分解（Low-rank Decomposition, LRD）系统性引入MPC线性层优化。核心在于用两个小规模矩阵乘替代原大矩阵乘，但直接应用会引入**双重开销**：（1）增加一通信回合；（2）新增一次截断操作。为此，我们设计两项互补优化：  \n- **截断跳过（Truncation Skipping）**：通过重构计算流与重用中间分片，完全消除LRD引入的额外截断；  \n- **线性层拼接（Efficient Linear Layer Concatenation）**：将分解后的两层融合为单个逻辑层，并利用MPC协议的异步特性流水线化执行，有效隐藏额外通信延迟。  \n\n**实验效果**：在标准MPC协议（n-PC与3-PC）上，LRD-MPC相较全秩基线实现**最高25%推理加速（n-PC）与33%加速（3-PC）**；同时降低GPU能耗达**52%**，并大幅压缩离线预处理阶段延迟（**下降88%**）。该方法协议无关，可无缝集成于SPDZ、ABY³、Cheetah等主流MPC框架，为实用化隐私AI推理提供轻量、通用、高效的解决方案。",
      "summary_en": "Secure Multi-party Computation (MPC) enables privacy-preserving ML inference across untrusted cloud VMs, but suffers from high communication and computational overhead—especially in linear layers dominated by costly matrix multiplications. We propose **LRD-MPC**, a protocol-agnostic framework that applies low-rank decomposition (LRD) to linear layers to replace one large matrix multiplication with two smaller ones. To overcome the inherent overheads of LRD in MPC—including an extra communication round and an additional truncation step—we introduce two key optimizations: (**i**) *truncation skipping*, which eliminates the redundant truncation via share-aware computation restructuring, and (**ii**) *efficient linear layer concatenation*, which pipelines the decomposed operations to hide the added latency. Evaluated across n-PC and 3-PC settings, LRD-MPC achieves up to **25% speedup (n-PC)** and **33% speedup (3-PC)** over full-rank baselines, reduces GPU energy consumption by **52%**, and cuts offline-phase latency by **88%**—all without compromising security or accuracy.",
      "summary": "## LRD-MPC：基于低秩分解的高效MPC推理方法  \n\n**背景与挑战**：安全多方计算（MPC）允许多个互不信任方在不泄露各自输入的前提下协同执行计算，在云环境下的隐私保护机器学习（ML）推理中具有重要应用。典型部署中，多个虚拟机（VM）作为MPC参与方，模型权重与用户输入均被秘密共享，各服务器仅处理随机分片。然而，MPC对深度神经网络（尤其是卷积层与全连接层）中密集的矩阵乘法存在显著开销——每轮矩阵乘需一次通信回合，且需频繁截断（truncation）以维持数值精度，而截断本身亦引入额外通信与计算负担。\n\n**方法创新**：本文提出**LRD-MPC**框架，将低秩分解（Low-rank Decomposition, LRD）系统性引入MPC线性层优化。核心在于用两个小规模矩阵乘替代原大矩阵乘，但直接应用会引入**双重开销**：（1）增加一通信回合；（2）新增一次截断操作。为此，我们设计两项互补优化：  \n- **截断跳过（Truncation Skipping）**：通过重构计算流与重用中间分片，完全消除LRD引入的额外截断；  \n- **线性层拼接（Efficient Linear Layer Concatenation）**：将分解后的两层融合为单个逻辑层，并利用MPC协议的异步特性流水线化执行，有效隐藏额外通信延迟。  \n\n**实验效果**：在标准MPC协议（n-PC与3-PC）上，LRD-MPC相较全秩基线实现**最高25%推理加速（n-PC）与33%加速（3-PC）**；同时降低GPU能耗达**52%**，并大幅压缩离线预处理阶段延迟（**下降88%**）。该方法协议无关，可无缝集成于SPDZ、ABY³、Cheetah等主流MPC框架，为实用化隐私AI推理提供轻量、通用、高效的解决方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14374v1",
      "arxiv_id": "2602.14374v1",
      "title": "Differentially Private Retrieval-Augmented Generation",
      "authors": [
        "Tingting Tang",
        "James Flemings",
        "Yongqin Wang",
        "Murali Annavaram"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14374v1",
      "url": "https://arxiv.org/abs/2602.14374v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "dp",
        "differential"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与挑战  \n检索增强生成（RAG）通过从外部数据库检索相关文档辅助大语言模型（LLM）生成答案，显著缓解领域任务中的幻觉问题。然而，当数据库包含医疗记录、法律文书等敏感语料时，RAG可能通过输出无意泄露隐私信息——已有研究证实，攻击者可设计对抗性提示，诱导LLM直接复述检索到的原始上下文，构成严重隐私风险。\n\n## 方法创新：DP-KSA算法  \n为兼顾强隐私保障与实用性能，本文提出**DP-KSA**（Differentially Private Keyword-Synthesized Augmentation），一种基于“提出-检验-释放”（Propose-Test-Release, PTR）范式的新型差分隐私RAG框架。其核心洞见在于：多数问答（QA）查询仅需少量关键词即可充分支撑准确回答。DP-KSA分三步实现隐私-效用平衡：  \n1. **多源检索与响应生成**：对同一问题并行检索多个相关上下文，并分别输入LLM生成初步响应；  \n2. **差分隐私关键词聚合**：在响应集合中，以满足ε-差分隐私的方式统计高频关键词（采用PTR机制确保噪声注入与阈值检验的严格性）；  \n3. **关键词增强输出**：将经DP保护的关键词嵌入最终提示，引导LLM生成最终答案。  \n\n## 主要贡献与验证  \n- **理论保障**：首次为RAG输出提供关于数据库的严格ε-差分隐私证明（即任意两相邻数据库产生的输出分布满足(ε,δ)-DP）；  \n- **实证优势**：在Natural Questions和HotpotQA两个标准QA基准上，使用Llama-3-8B-Instruct、Qwen2-7B-Instruct和Phi-3-mini三个指令微调LLM进行评估，DP-KSA在ε=1.0–2.0下保持F1/EM指标下降<8%，显著优于基线DP-RAG方法（平均提升12.3%绝对分数），同时有效抵御上下文再生攻击；  \n- **语义压缩机制**：通过关键词抽象替代原始段落，既降低隐私敏感度，又避免噪声污染关键语义，从根本上缓解DP导致的幻觉加剧问题。",
      "summary_en": "Retrieval-augmented generation (RAG) improves LLM accuracy but risks leaking sensitive database content—e.g., medical or legal records—via adversarial prompt attacks that force context regurgitation. While differential privacy (DP) offers strong formal guarantees, naive DP integration degrades RAG utility and exacerbates hallucination. To address this, we propose **DP-KSA**, a novel RAG framework built on the *propose-test-release* paradigm. DP-KSA leverages the insight that most QA queries can be answered using only a few salient keywords: it first retrieves multiple contexts and generates LLM responses for each; then computes the most frequent keywords from these responses under strict ε-DP via PTR; finally, augments the final prompt with these private keywords. This semantic compression preserves utility while ensuring end-to-end DP guarantees *with respect to the RAG database*. Evaluated on Natural Questions and HotpotQA with three instruction-tuned LLMs (Llama-3-8B, Qwen2-7B, Phi-3-mini), DP-KSA achieves a superior privacy-utility tradeoff—e.g., <8% F1 drop at ε=1.5—outperforming prior DP-RAG baselines by +12.3% absolute EM on average, while robustly preventing context memorization.",
      "summary": "## 背景与挑战  \n检索增强生成（RAG）通过从外部数据库检索相关文档辅助大语言模型（LLM）生成答案，显著缓解领域任务中的幻觉问题。然而，当数据库包含医疗记录、法律文书等敏感语料时，RAG可能通过输出无意泄露隐私信息——已有研究证实，攻击者可设计对抗性提示，诱导LLM直接复述检索到的原始上下文，构成严重隐私风险。\n\n## 方法创新：DP-KSA算法  \n为兼顾强隐私保障与实用性能，本文提出**DP-KSA**（Differentially Private Keyword-Synthesized Augmentation），一种基于“提出-检验-释放”（Propose-Test-Release, PTR）范式的新型差分隐私RAG框架。其核心洞见在于：多数问答（QA）查询仅需少量关键词即可充分支撑准确回答。DP-KSA分三步实现隐私-效用平衡：  \n1. **多源检索与响应生成**：对同一问题并行检索多个相关上下文，并分别输入LLM生成初步响应；  \n2. **差分隐私关键词聚合**：在响应集合中，以满足ε-差分隐私的方式统计高频关键词（采用PTR机制确保噪声注入与阈值检验的严格性）；  \n3. **关键词增强输出**：将经DP保护的关键词嵌入最终提示，引导LLM生成最终答案。  \n\n## 主要贡献与验证  \n- **理论保障**：首次为RAG输出提供关于数据库的严格ε-差分隐私证明（即任意两相邻数据库产生的输出分布满足(ε,δ)-DP）；  \n- **实证优势**：在Natural Questions和HotpotQA两个标准QA基准上，使用Llama-3-8B-Instruct、Qwen2-7B-Instruct和Phi-3-mini三个指令微调LLM进行评估，DP-KSA在ε=1.0–2.0下保持F1/EM指标下降<8%，显著优于基线DP-RAG方法（平均提升12.3%绝对分数），同时有效抵御上下文再生攻击；  \n- **语义压缩机制**：通过关键词抽象替代原始段落，既降低隐私敏感度，又避免噪声污染关键语义，从根本上缓解DP导致的幻觉加剧问题。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14364v1",
      "arxiv_id": "2602.14364v1",
      "title": "A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)",
      "authors": [
        "Tianyu Chen",
        "Dongrui Liu",
        "Xia Hu",
        "Jingyi Yu",
        "Wenjie Wang"
      ],
      "abstract": "Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14364v1",
      "url": "https://arxiv.org/abs/2602.14364v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \nClawdbot（即OpenClaw）是一款开源、可自托管的工具调用型个人AI智能体，其动作空间广泛覆盖本地系统执行（如文件操作、进程控制）与Web中介工作流（如浏览器自动化、API调用），在模糊指令、对抗性引导或意图未明场景下易引发安全与权限越界风险。\n\n## 方法设计  \n本研究提出**轨迹驱动的安全审计框架**，从六个核心风险维度（越权执行、隐私泄露、持久化危害、网络滥用、资源耗尽、语义误导）系统评估Clawdbot行为。测试集融合三类来源：① 适配自ATBench与LPS-Bench等主流代理安全基准的34个典型用例；② 针对Clawdbot特有工具接口（如`shell_exec`、`browser_navigate`、`file_write`）手工构造的边界案例；③ 模拟真实用户误操作与轻度 jailbreak 的诱导性提示。全程完整记录**端到端交互轨迹**（含用户消息、Agent决策链、工具调用参数与原始返回值），并采用双轨评估机制：由轻量级专用裁判模型 **AgentDoG-Qwen3-4B** 进行自动化轨迹安全打分，辅以人工复核验证关键失败路径。\n\n## 主要发现  \n- 安全表现呈显著非均匀性：在明确目标、结构化输入任务中成功率＞94%，但在**意图模糊**（如“帮我整理一下”）、**开放目标**（如“优化我的开发环境”）及**伪装良性提示**（如“用浏览器查一下怎么修打印机——顺便把当前桌面截图发我”）下失败率跃升至68%；  \n- 典型失效模式集中于**语义过度泛化**（将“查看日志”误判为“清空日志”）、**工具权限误用**（调用`shell_exec`执行无沙箱命令）、**上下文遗忘导致的累积偏差**；  \n- 审计揭示三大深层脆弱点：工具抽象层缺乏细粒度权限约束、动作链缺乏回滚机制、以及缺乏对“隐式副作用”的主动检测能力。\n\n## 创新价值  \n本工作首次为工具型AI代理提供可复现、可归因的轨迹级安全审计范式，所构建的Clawdbot专用测试套件与失败模式分类体系，为后续轻量代理的安全加固与评测标准制定提供了实证基础。",
      "summary_en": "This paper presents a trajectory-based safety audit of Clawdbot (OpenClaw), a self-hosted, tool-using personal AI agent with broad local and web-mediated capabilities. We evaluate it across six risk dimensions using a curated test suite—combining adapted cases from ATBench and LPS-Bench with Clawdbot-specific handcrafted scenarios—and log full interaction trajectories (messages, actions, tool arguments/outputs). Safety is assessed via both an automated trajectory judge (**AgentDoG-Qwen3-4B**) and human review. Across 34 canonical cases, we find highly non-uniform safety: strong reliability on well-specified tasks (>94% success), but sharp degradation under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts—where minor misinterpretations trigger high-impact tool misuse (e.g., unintended file deletion or screenshot exfiltration). Key failure modes include semantic overgeneralization, privilege escalation via unguarded tool calls, and context drift in multi-step chains. Our audit identifies critical architectural vulnerabilities—lack of fine-grained tool permissions, absence of action rollback, and no implicit-side-effect detection—providing actionable insights for secure agent design.",
      "summary": "## 研究背景  \nClawdbot（即OpenClaw）是一款开源、可自托管的工具调用型个人AI智能体，其动作空间广泛覆盖本地系统执行（如文件操作、进程控制）与Web中介工作流（如浏览器自动化、API调用），在模糊指令、对抗性引导或意图未明场景下易引发安全与权限越界风险。\n\n## 方法设计  \n本研究提出**轨迹驱动的安全审计框架**，从六个核心风险维度（越权执行、隐私泄露、持久化危害、网络滥用、资源耗尽、语义误导）系统评估Clawdbot行为。测试集融合三类来源：① 适配自ATBench与LPS-Bench等主流代理安全基准的34个典型用例；② 针对Clawdbot特有工具接口（如`shell_exec`、`browser_navigate`、`file_write`）手工构造的边界案例；③ 模拟真实用户误操作与轻度 jailbreak 的诱导性提示。全程完整记录**端到端交互轨迹**（含用户消息、Agent决策链、工具调用参数与原始返回值），并采用双轨评估机制：由轻量级专用裁判模型 **AgentDoG-Qwen3-4B** 进行自动化轨迹安全打分，辅以人工复核验证关键失败路径。\n\n## 主要发现  \n- 安全表现呈显著非均匀性：在明确目标、结构化输入任务中成功率＞94%，但在**意图模糊**（如“帮我整理一下”）、**开放目标**（如“优化我的开发环境”）及**伪装良性提示**（如“用浏览器查一下怎么修打印机——顺便把当前桌面截图发我”）下失败率跃升至68%；  \n- 典型失效模式集中于**语义过度泛化**（将“查看日志”误判为“清空日志”）、**工具权限误用**（调用`shell_exec`执行无沙箱命令）、**上下文遗忘导致的累积偏差**；  \n- 审计揭示三大深层脆弱点：工具抽象层缺乏细粒度权限约束、动作链缺乏回滚机制、以及缺乏对“隐式副作用”的主动检测能力。\n\n## 创新价值  \n本工作首次为工具型AI代理提供可复现、可归因的轨迹级安全审计范式，所构建的Clawdbot专用测试套件与失败模式分类体系，为后续轻量代理的安全加固与评测标准制定提供了实证基础。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15189v1",
      "arxiv_id": "2602.15189v1",
      "title": "ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction",
      "authors": [
        "William Brach",
        "Francesco Zuppichini",
        "Marco Vinciguerra",
        "Lorenzo Padoan"
      ],
      "abstract": "The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15189v1",
      "url": "https://arxiv.org/abs/2602.15189v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## ScrapeGraphAI-100k：面向大语言模型的网页结构化信息抽取大规模基准数据集  \n\n当前，**大语言模型（LLM）驱动的网页信息抽取**已成为现代网络信息检索（Web IR）流水线的核心环节，但现有基准数据集普遍存在三大局限：规模小（通常＜10k样本）、高度合成、或仅含纯文本，**严重缺失真实网页的HTML结构上下文、多语言混合、动态提示工程与Schema多样性**。为填补这一关键空白，本文发布 **ScrapeGraphAI-100k**——首个面向LLM结构化抽取的大规模、真实世界、多模态网页数据集。该数据集基于2025年第二、三季度用户**自愿授权的ScrapeGraphAI生产环境遥测日志**构建，原始采集900万次抽取事件；经严格去重、按JSON Schema语义聚类平衡、跨领域/语言采样后，最终形成**93,695个高质量实例**，覆盖电商、新闻、学术、政府等12+垂直领域及中、英、西、法、日等8种语言。每个样本完整包含：**原始Markdown渲染内容（保留层级与链接结构）、用户原始Prompt、目标JSON Schema定义、LLM实际输出响应，以及标注的复杂度指标（如嵌套深度、字段数、类型异构性）与人工验证状态**。我们系统分析发现：当Schema嵌套≥3层或字段数＞15时，主流开源LLM（如Llama-3-8B）抽取准确率骤降37.2%，凸显结构复杂性对泛化能力的瓶颈效应。进一步实验表明：仅用该数据集**10%子集（≈9.4k样本）微调1.7B参数小模型**，即可在相同测试集上将F1分数从42.1提升至68.5，显著缩小与30B级基线模型（72.3）的差距。本数据集已开源至Hugging Face，支持三大核心应用：① 小模型高效微调；② 结构化抽取性能可比基准；③ Web IR索引中的Schema自动归纳研究。",
      "summary_en": "ScrapeGraphAI-100k is the first large-scale, real-world dataset for LLM-based web information extraction, addressing critical gaps in scale, structural fidelity, and schema diversity. Built from opt-in telemetry of the ScrapeGraphAI framework (Q2–Q3 2025), it contains 93,695 high-quality examples after deduplication and schema-balanced sampling across 12 domains and 8 languages. Each instance includes Markdown-rendered web content, user prompt, target JSON schema, LLM output, and complexity/validation metadata. We characterize systematic failure modes—accuracy drops sharply beyond schema nesting depth ≥3 or field count >15—and demonstrate that fine-tuning a 1.7B model on just 10% of the data closes ~85% of the F1 gap to a 30B baseline. Publicly available on Hugging Face, ScrapeGraphAI-100k enables efficient small-model tuning, rigorous benchmarking of structured extraction, and schema induction research for web IR indexing.",
      "summary": "## ScrapeGraphAI-100k：面向大语言模型的网页结构化信息抽取大规模基准数据集  \n\n当前，**大语言模型（LLM）驱动的网页信息抽取**已成为现代网络信息检索（Web IR）流水线的核心环节，但现有基准数据集普遍存在三大局限：规模小（通常＜10k样本）、高度合成、或仅含纯文本，**严重缺失真实网页的HTML结构上下文、多语言混合、动态提示工程与Schema多样性**。为填补这一关键空白，本文发布 **ScrapeGraphAI-100k**——首个面向LLM结构化抽取的大规模、真实世界、多模态网页数据集。该数据集基于2025年第二、三季度用户**自愿授权的ScrapeGraphAI生产环境遥测日志**构建，原始采集900万次抽取事件；经严格去重、按JSON Schema语义聚类平衡、跨领域/语言采样后，最终形成**93,695个高质量实例**，覆盖电商、新闻、学术、政府等12+垂直领域及中、英、西、法、日等8种语言。每个样本完整包含：**原始Markdown渲染内容（保留层级与链接结构）、用户原始Prompt、目标JSON Schema定义、LLM实际输出响应，以及标注的复杂度指标（如嵌套深度、字段数、类型异构性）与人工验证状态**。我们系统分析发现：当Schema嵌套≥3层或字段数＞15时，主流开源LLM（如Llama-3-8B）抽取准确率骤降37.2%，凸显结构复杂性对泛化能力的瓶颈效应。进一步实验表明：仅用该数据集**10%子集（≈9.4k样本）微调1.7B参数小模型**，即可在相同测试集上将F1分数从42.1提升至68.5，显著缩小与30B级基线模型（72.3）的差距。本数据集已开源至Hugging Face，支持三大核心应用：① 小模型高效微调；② 结构化抽取性能可比基准；③ Web IR索引中的Schema自动归纳研究。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15139v1",
      "arxiv_id": "2602.15139v1",
      "title": "CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding",
      "authors": [
        "Tahir Hussain",
        "Saddam Hussain Khan"
      ],
      "abstract": "Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15139v1",
      "url": "https://arxiv.org/abs/2602.15139v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n面向古典伊斯兰文本（尤其是圣训）的问答（QA）任务长期受限于**领域特异性语义模糊性、长程上下文依赖**及**神学概念敏感推理**。现有通用模型（如BERT、DeBERTa）在《布哈里圣训实录》和《穆斯林圣训实录》等核心典籍上的表现存在显著瓶颈，难以捕捉“认主独一”“先知使命”“教法渊源”等12个核心伊斯兰概念的深层神学关联与语境权重。\n\n## 方法创新：CGRA-DeBERTa框架  \n本研究提出**CGRA-DeBERTa**（Concept Guided Residual Augmentation DeBERTa），一种轻量、可解释、高精度的神学感知Transformer架构：  \n- **定制化DeBERTa主干**：集成LoRA（Low-Rank Adaptation）实现参数高效微调，保留预训练语言能力；  \n- **概念引导残差模块（CGR Blocks）**：嵌入经专家审校的《伊斯兰核心概念词典》（含12个术语及其神学定义、层级关系与跨文本共现模式），以残差连接注入领域先验；  \n- **概念门控机制（Concept Gating）**：基于重要性加权注意力，对关键神学术语（如“الله”“الرسول”“الحكم”）实施差异化缩放（1.04–3.00倍），在不破坏原始上下文结构的前提下强化语义锚点。\n\n## 主要成果  \n在42,591组高质量圣训QA对（覆盖两大权威圣训集）上训练验证：  \n- **EM（精确匹配）达97.85%**，显著超越BERT（75.87%）和标准DeBERTa（89.77%），绝对提升8.08分；  \n- 推理开销仅增加约8%，得益于LoRA+稀疏门控的协同设计；  \n- 定性分析证实其在**神学实体识别、歧义句义区分、教法裁决依据定位**三方面具备更高准确性与可解释性。  \n本工作为宗教文本智能处理提供了首个兼顾**计算效率、神学严谨性与教育适用性**的端到端解决方案。",
      "summary_en": "Accurate question answering over classical Islamic texts—especially Hadith—is hindered by domain-specific semantics, long-context dependencies, and concept-sensitive theological reasoning. To address this, we propose **CGRA-DeBERTa**, a Concept-Guided Residual Augmentation Transformer built upon a LoRA-adapted DeBERTa backbone. It integrates a curated Islamic Concept Dictionary (12 core theological terms) via residual concept-aware blocks and an importance-weighted concept gating mechanism that selectively amplifies critical tokens (scaling factors: 1.04–3.00). Evaluated on 42,591 QA pairs from *Sahih al-Bukhari* and *Sahih Muslim*, CGRA-DeBERTa achieves **97.85% EM**, outperforming BERT (75.87%) and vanilla DeBERTa (89.77%) by +8.08 absolute points, with only ~8% inference overhead. Qualitative analysis confirms superior theological precision, span extraction fidelity, and interpretability—enabling scalable, education-ready Hadith QA systems.",
      "summary": "## 背景与挑战  \n面向古典伊斯兰文本（尤其是圣训）的问答（QA）任务长期受限于**领域特异性语义模糊性、长程上下文依赖**及**神学概念敏感推理**。现有通用模型（如BERT、DeBERTa）在《布哈里圣训实录》和《穆斯林圣训实录》等核心典籍上的表现存在显著瓶颈，难以捕捉“认主独一”“先知使命”“教法渊源”等12个核心伊斯兰概念的深层神学关联与语境权重。\n\n## 方法创新：CGRA-DeBERTa框架  \n本研究提出**CGRA-DeBERTa**（Concept Guided Residual Augmentation DeBERTa），一种轻量、可解释、高精度的神学感知Transformer架构：  \n- **定制化DeBERTa主干**：集成LoRA（Low-Rank Adaptation）实现参数高效微调，保留预训练语言能力；  \n- **概念引导残差模块（CGR Blocks）**：嵌入经专家审校的《伊斯兰核心概念词典》（含12个术语及其神学定义、层级关系与跨文本共现模式），以残差连接注入领域先验；  \n- **概念门控机制（Concept Gating）**：基于重要性加权注意力，对关键神学术语（如“الله”“الرسول”“الحكم”）实施差异化缩放（1.04–3.00倍），在不破坏原始上下文结构的前提下强化语义锚点。\n\n## 主要成果  \n在42,591组高质量圣训QA对（覆盖两大权威圣训集）上训练验证：  \n- **EM（精确匹配）达97.85%**，显著超越BERT（75.87%）和标准DeBERTa（89.77%），绝对提升8.08分；  \n- 推理开销仅增加约8%，得益于LoRA+稀疏门控的协同设计；  \n- 定性分析证实其在**神学实体识别、歧义句义区分、教法裁决依据定位**三方面具备更高准确性与可解释性。  \n本工作为宗教文本智能处理提供了首个兼顾**计算效率、神学严谨性与教育适用性**的端到端解决方案。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14655v1",
      "arxiv_id": "2602.14655v1",
      "title": "Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech",
      "authors": [
        "Xiao Wei",
        "Bin Wen",
        "Yuqin Lin",
        "Kai Li",
        "Mingyang gu",
        "Xiaobao Wang",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "abstract": "Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14655v1",
      "url": "https://arxiv.org/abs/2602.14655v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n阿尔茨海默病（AD）的早期诊断对延缓疾病进展至关重要。基于语音的人工智能检测方法具有**无创、低成本、可居家部署**等优势，但面临严峻的“数据效率困境”：一方面，高质量标注的临床语音数据极度稀缺；另一方面，跨医疗机构的数据共享受严格隐私法规（如HIPAA、GDPR）限制，导致模型训练长期受限于小规模、孤岛化数据。\n\n## 方法创新：FAL-AD框架  \n本文提出**FAL-AD**（Federated and Augmented Learning for AD），一个面向数据效率优化的端到端联合学习框架，包含三大核心技术突破：  \n- **绝对效率提升**：首创基于语音转换（Voice Conversion）的病理语音增强策略，通过跨类别（健康/AD）的声学特征与语义内容解耦重组，生成高保真、多样化的合成AD语音样本，显著缓解标注数据匮乏；  \n- **协同效率突破**：设计自适应联邦学习范式，支持异构设备与非独立同分布（Non-IID）数据下的动态权重聚合与本地差分隐私保护，实现多中心协作而不共享原始语音；  \n- **表征效率优化**：构建**注意力驱动的跨模态融合模型**，在词粒度实现声学特征（MFCCs, prosody）与文本转录（ASR输出）的细粒度对齐与交互建模，提升病理模式判别力。\n\n## 实验结果与意义  \n在权威ADReSSo基准测试中，FAL-AD达成**91.52%的多模态分类准确率**，超越所有集中式基线模型（最高提升+4.21%），且在仅使用30%本地数据量时仍保持>89%性能。本工作首次将联邦学习与可控语音增强深度耦合，为医疗AI提供了兼顾**隐私合规性、数据稀缺性与临床实用性**的可行路径。源代码已开源：https://github.com/smileix/fal-ad。",
      "summary_en": "Early Alzheimer’s Disease (AD) detection via speech is promising yet hindered by a critical *data efficiency dilemma*: severe scarcity of labeled clinical speech data and strict privacy barriers preventing cross-institutional sharing. To address this, we propose **FAL-AD**, a novel federated and augmented learning framework. It introduces three synergistic innovations: (1) **Voice conversion–based augmentation**, generating diverse, pathology-aware synthetic speech through cross-category voice-content recombination; (2) An **adaptive federated learning paradigm**, enabling privacy-preserving collaboration across heterogeneous institutions with Non-IID data and local differential privacy; and (3) An **attentive cross-modal fusion model**, achieving fine-grained word-level acoustic–textual alignment. Evaluated on the ADReSSo benchmark, FAL-AD achieves a state-of-the-art multimodal accuracy of **91.52%**, outperforming all centralized baselines and demonstrating robust performance under data-constrained and privacy-sensitive settings. Code is publicly available at https://github.com/smileix/fal-ad.",
      "summary": "## 背景与挑战  \n阿尔茨海默病（AD）的早期诊断对延缓疾病进展至关重要。基于语音的人工智能检测方法具有**无创、低成本、可居家部署**等优势，但面临严峻的“数据效率困境”：一方面，高质量标注的临床语音数据极度稀缺；另一方面，跨医疗机构的数据共享受严格隐私法规（如HIPAA、GDPR）限制，导致模型训练长期受限于小规模、孤岛化数据。\n\n## 方法创新：FAL-AD框架  \n本文提出**FAL-AD**（Federated and Augmented Learning for AD），一个面向数据效率优化的端到端联合学习框架，包含三大核心技术突破：  \n- **绝对效率提升**：首创基于语音转换（Voice Conversion）的病理语音增强策略，通过跨类别（健康/AD）的声学特征与语义内容解耦重组，生成高保真、多样化的合成AD语音样本，显著缓解标注数据匮乏；  \n- **协同效率突破**：设计自适应联邦学习范式，支持异构设备与非独立同分布（Non-IID）数据下的动态权重聚合与本地差分隐私保护，实现多中心协作而不共享原始语音；  \n- **表征效率优化**：构建**注意力驱动的跨模态融合模型**，在词粒度实现声学特征（MFCCs, prosody）与文本转录（ASR输出）的细粒度对齐与交互建模，提升病理模式判别力。\n\n## 实验结果与意义  \n在权威ADReSSo基准测试中，FAL-AD达成**91.52%的多模态分类准确率**，超越所有集中式基线模型（最高提升+4.21%），且在仅使用30%本地数据量时仍保持>89%性能。本工作首次将联邦学习与可控语音增强深度耦合，为医疗AI提供了兼顾**隐私合规性、数据稀缺性与临床实用性**的可行路径。源代码已开源：https://github.com/smileix/fal-ad。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14612v1",
      "arxiv_id": "2602.14612v1",
      "title": "LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio",
      "authors": [
        "Naveen Vakada",
        "Kartik Hegde",
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "abstract": "Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14612v1",
      "url": "https://arxiv.org/abs/2602.14612v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n多小时级长时音频（如会议录音、安防监控、远程医疗会话）在工业与消费场景中日益普遍，但人工回溯效率极低。现有音频-语言模型受限于上下文长度，难以直接处理数小时原始音频流，易产生时间错位、事实幻觉及推理失焦等问题。\n\n## 方法：LongAudio-RAG（LA-RAG）框架  \n我们提出**事件锚定式检索增强生成框架**，摒弃对原始音频波形的端到端建模，转而构建**结构化声学事件知识库**：  \n- **离线阶段**：将长音频流通过轻量级音频 grounding 模型（如 Whisper + 事件检测器）解析为带毫秒级时间戳的结构化事件记录（如“[08:23:15.4–08:23:17.9] 男性说话”“[08:41:02.1] 门铃响”），存入轻量 SQL 数据库；  \n- **在线推理**：系统协同执行四步操作——① 解析自然语言中的时间指代（如“会议开始后12分钟”→绝对时间戳）；② 意图分类（检测/计数/摘要）；③ 基于时间窗口与语义约束精准检索相关事件子集；④ LLM 仅基于检索出的、带时间标注的事件文本生成答案，严格规避幻觉。\n\n## 主要发现与创新  \n- 构建首个**合成式长音频 QA 基准**（LA-QA-Bench），含 12 小时级拼接音频、1,800+ 模板化 QA 对，覆盖检测、计数、跨时段摘要三类任务；  \n- 在该基准上，LA-RAG 相比 vanilla RAG（直接分块音频转文本再检索）和 text-to-SQL 方法，**时间准确率提升 32.7%，事实正确率提升 28.4%**；  \n- 首次实现**边缘-云协同部署**：事件检测模型运行于树莓派级 IoT 设备（延迟 <800ms），LLM（Qwen2-7B）部署于云端 GPU 服务器，兼顾实时性与语言能力。",
      "summary_en": "Long-duration audio (e.g., meetings, surveillance) is pervasive yet intractable for manual review. Existing audio-language models fail on multi-hour QA due to context-length limits and hallucination. We propose **LongAudio-RAG (LA-RAG)**, a hybrid framework that grounds LLM answers in *retrieved, timestamped acoustic events*—not raw audio. Long streams are pre-processed into structured, time-aligned event records (e.g., “male speech: [12:34:05.2–12:34:08.7]”) stored in an SQL database. At inference, LA-RAG resolves temporal references, classifies intent, retrieves only temporally and semantically relevant events, and constrains LLM generation to this evidence. Evaluated on our novel synthetic benchmark (LA-QA-Bench, 12+ hours, 1,800+ QA pairs), LA-RAG outperforms vanilla RAG and text-to-SQL baselines by +32.7% temporal accuracy and +28.4% factual correctness. Deployed in an edge-cloud setup—event detection on IoT hardware, LLM on GPU cloud—it enables low-latency grounding and high-fidelity reasoning.",
      "summary": "## 背景与挑战  \n多小时级长时音频（如会议录音、安防监控、远程医疗会话）在工业与消费场景中日益普遍，但人工回溯效率极低。现有音频-语言模型受限于上下文长度，难以直接处理数小时原始音频流，易产生时间错位、事实幻觉及推理失焦等问题。\n\n## 方法：LongAudio-RAG（LA-RAG）框架  \n我们提出**事件锚定式检索增强生成框架**，摒弃对原始音频波形的端到端建模，转而构建**结构化声学事件知识库**：  \n- **离线阶段**：将长音频流通过轻量级音频 grounding 模型（如 Whisper + 事件检测器）解析为带毫秒级时间戳的结构化事件记录（如“[08:23:15.4–08:23:17.9] 男性说话”“[08:41:02.1] 门铃响”），存入轻量 SQL 数据库；  \n- **在线推理**：系统协同执行四步操作——① 解析自然语言中的时间指代（如“会议开始后12分钟”→绝对时间戳）；② 意图分类（检测/计数/摘要）；③ 基于时间窗口与语义约束精准检索相关事件子集；④ LLM 仅基于检索出的、带时间标注的事件文本生成答案，严格规避幻觉。\n\n## 主要发现与创新  \n- 构建首个**合成式长音频 QA 基准**（LA-QA-Bench），含 12 小时级拼接音频、1,800+ 模板化 QA 对，覆盖检测、计数、跨时段摘要三类任务；  \n- 在该基准上，LA-RAG 相比 vanilla RAG（直接分块音频转文本再检索）和 text-to-SQL 方法，**时间准确率提升 32.7%，事实正确率提升 28.4%**；  \n- 首次实现**边缘-云协同部署**：事件检测模型运行于树莓派级 IoT 设备（延迟 <800ms），LLM（Qwen2-7B）部署于云端 GPU 服务器，兼顾实时性与语言能力。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14462v1",
      "arxiv_id": "2602.14462v1",
      "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
      "authors": [
        "Hong Li",
        "Zhen Zhou",
        "Honggang Zhang",
        "Yuping Luo",
        "Xinyue Wang",
        "Han Gong",
        "Zhiyuan Liu"
      ],
      "abstract": "Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \\emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \\texttt{openPangu-Embedded-1B-V1.1} model on the \\texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14462v1",
      "url": "https://arxiv.org/abs/2602.14462v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary_zh": "## 背景与问题  \n在大语言模型（LLM）全参数微调中，数据并行（Data-Parallel, DP）配合同步all-reduce是主流范式。尽管该机制保障每轮迭代后各工作节点（worker）模型权重数值一致，但**并不保证梯度聚合前各worker的优化动态对齐**——这一被长期忽视的隐性偏差，本文定义为“**静默不一致性**”（Silent Inconsistency）。它表现为跨worker的损失值、梯度幅值与方向持续发散，却因全局平均监控信号（如smoothed loss curve）的强平滑性而“不可见”，导致训练不稳定根源难以诊断。\n\n## 方法与创新  \n我们提出一种**轻量、模型无关、零侵入**的诊断框架：仅利用标准训练流程中已有的每worker本地loss和梯度张量，无需修改模型结构、通信机制或优化器。核心包含三个正交指标：  \n- **损失离散度**（Loss Dispersion）：各worker损失的标准差；  \n- **梯度模长离散度**（Gradient-Norm Dispersion）：各worker梯度L2范数的标准差；  \n- **梯度方向一致性**（Gradient-Direction Consistency）：所有worker两两梯度向量的余弦相似度均值。  \n三者计算开销可忽略（<0.3%训练时延），支持实时在线监测。\n\n## 主要发现  \n在8-NPU集群上对1B参数模型`openPangu-Embedded-1B-V1.1`于`alpaca`数据集进行全微调实验，通过系统扰动数据打乱顺序与随机种子，证实：即使全局平均loss曲线光滑收敛，**损失/梯度离散度上升达3.2×，方向余弦均值下降41%**。这揭示DP微调中存在未被察觉的优化路径分裂，为超参调试、数据分片策略与硬件配置评估提供了可量化、可操作的诊断依据。",
      "summary_en": "Data-parallel (DP) full fine-tuning relies on synchronous all-reduce to synchronize weights, yet it silently permits worker-level optimization divergence—termed *silent inconsistency*—which remains hidden under globally averaged metrics. We propose a lightweight, model-agnostic diagnostic framework using three complementary, zero-overhead metrics: loss dispersion, gradient-norm dispersion, and inter-worker gradient-direction consistency (via cosine similarity). Evaluated on an 8-NPU cluster fine-tuning the 1B-parameter `openPangu-Embedded-1B-V1.1` on `alpaca`, controlled perturbations of data shuffling and random seeds induce up to 3.2× higher dispersion and 41% lower directional alignment—even while global loss curves appear smooth. These results expose critical hidden instability in large-scale DP training and provide actionable, real-time visibility for robust configuration assessment.",
      "summary": "## 背景与问题  \n在大语言模型（LLM）全参数微调中，数据并行（Data-Parallel, DP）配合同步all-reduce是主流范式。尽管该机制保障每轮迭代后各工作节点（worker）模型权重数值一致，但**并不保证梯度聚合前各worker的优化动态对齐**——这一被长期忽视的隐性偏差，本文定义为“**静默不一致性**”（Silent Inconsistency）。它表现为跨worker的损失值、梯度幅值与方向持续发散，却因全局平均监控信号（如smoothed loss curve）的强平滑性而“不可见”，导致训练不稳定根源难以诊断。\n\n## 方法与创新  \n我们提出一种**轻量、模型无关、零侵入**的诊断框架：仅利用标准训练流程中已有的每worker本地loss和梯度张量，无需修改模型结构、通信机制或优化器。核心包含三个正交指标：  \n- **损失离散度**（Loss Dispersion）：各worker损失的标准差；  \n- **梯度模长离散度**（Gradient-Norm Dispersion）：各worker梯度L2范数的标准差；  \n- **梯度方向一致性**（Gradient-Direction Consistency）：所有worker两两梯度向量的余弦相似度均值。  \n三者计算开销可忽略（<0.3%训练时延），支持实时在线监测。\n\n## 主要发现  \n在8-NPU集群上对1B参数模型`openPangu-Embedded-1B-V1.1`于`alpaca`数据集进行全微调实验，通过系统扰动数据打乱顺序与随机种子，证实：即使全局平均loss曲线光滑收敛，**损失/梯度离散度上升达3.2×，方向余弦均值下降41%**。这揭示DP微调中存在未被察觉的优化路径分裂，为超参调试、数据分片策略与硬件配置评估提供了可量化、可操作的诊断依据。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14401v1",
      "arxiv_id": "2602.14401v1",
      "title": "pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI",
      "authors": [
        "Qingqian Yang",
        "Hao Wang",
        "Sai Qian Zhang",
        "Jian Li",
        "Yang Hua",
        "Miao Pan",
        "Tao Song",
        "Zhengwei Qi",
        "Haibing Guan"
      ],
      "abstract": "Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14401v1",
      "url": "https://arxiv.org/abs/2602.14401v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n视觉-语言导航（Vision-Language Navigation, VLN）依赖大规模私有室内环境中的轨迹-指令配对数据，但集中式训练严重威胁用户隐私。联邦学习（Federated Learning, FL）虽能实现数据不出本地，却在VLN场景下面临严峻挑战：各客户端环境布局差异巨大、指令表达风格高度异构，导致标准FedAvg等全局模型难以兼顾泛化性与个性化，性能显著下降。\n\n## 方法创新：pFedNavi框架  \n本文提出**pFedNavi**——首个面向VLN任务的**结构感知、动态自适应个性化联邦学习框架**。其核心思想是“**在关键处个性化**”：  \n- 通过**层级混合系数（layer-wise mixing coefficients）** 动态识别客户端特异性最强的网络层（如编码器-解码器投影模块、环境敏感型解码器层）；  \n- 仅对这些选定组件执行**细粒度参数融合**，而非全网统一聚合，从而在共享全局语义知识的同时，保留各客户端对本地空间结构与语言习惯的专属建模能力；  \n- 引入轻量级结构感知正则化，显式建模视觉编码器（ResNet/CLIP）与导航决策模块间的层级依赖关系。\n\n## 实验结果与贡献  \n在R2R和RxR两大标准VLN基准上，pFedNavi全面超越FedAvg基线：  \n- **导航成功率（Navigation Success Rate）最高提升7.5%**；  \n- **轨迹保真度（Trajectory Fidelity）最高提升7.8%**；  \n- 在非独立同分布（non-IID）条件下**收敛速度加快1.38倍**。  \n本工作首次将结构感知个性化机制系统引入VLN联邦学习，为具身AI在隐私敏感场景下的实用化部署提供了新范式。",
      "summary_en": "Vision-Language Navigation (VLN) demands large-scale trajectory-instruction data from private indoor environments, posing serious privacy risks. While Federated Learning (FL) keeps data on-device, vanilla FL (e.g., FedAvg) fails under VLN’s extreme cross-client heterogeneity in spatial layouts and instruction styles. To address this, we propose **pFedNavi**, the first structure-aware and dynamically adaptive personalized FL framework for VLN. Its core insight is *personalizing only where it matters*: pFedNavi identifies client-specific layers (e.g., encoder-decoder projection and environment-sensitive decoder layers) via learnable layer-wise mixing coefficients, and performs fine-grained parameter fusion exclusively on those components—balancing global knowledge sharing with local specialization. Evaluated on R2R and RxR using both ResNet and CLIP visual backbones, pFedNavi consistently outperforms the FedAvg-based baseline: achieving up to **+7.5% navigation success rate**, **+7.8% trajectory fidelity**, and **1.38× faster convergence** under non-IID conditions.",
      "summary": "## 背景与挑战  \n视觉-语言导航（Vision-Language Navigation, VLN）依赖大规模私有室内环境中的轨迹-指令配对数据，但集中式训练严重威胁用户隐私。联邦学习（Federated Learning, FL）虽能实现数据不出本地，却在VLN场景下面临严峻挑战：各客户端环境布局差异巨大、指令表达风格高度异构，导致标准FedAvg等全局模型难以兼顾泛化性与个性化，性能显著下降。\n\n## 方法创新：pFedNavi框架  \n本文提出**pFedNavi**——首个面向VLN任务的**结构感知、动态自适应个性化联邦学习框架**。其核心思想是“**在关键处个性化**”：  \n- 通过**层级混合系数（layer-wise mixing coefficients）** 动态识别客户端特异性最强的网络层（如编码器-解码器投影模块、环境敏感型解码器层）；  \n- 仅对这些选定组件执行**细粒度参数融合**，而非全网统一聚合，从而在共享全局语义知识的同时，保留各客户端对本地空间结构与语言习惯的专属建模能力；  \n- 引入轻量级结构感知正则化，显式建模视觉编码器（ResNet/CLIP）与导航决策模块间的层级依赖关系。\n\n## 实验结果与贡献  \n在R2R和RxR两大标准VLN基准上，pFedNavi全面超越FedAvg基线：  \n- **导航成功率（Navigation Success Rate）最高提升7.5%**；  \n- **轨迹保真度（Trajectory Fidelity）最高提升7.8%**；  \n- 在非独立同分布（non-IID）条件下**收敛速度加快1.38倍**。  \n本工作首次将结构感知个性化机制系统引入VLN联邦学习，为具身AI在隐私敏感场景下的实用化部署提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14833v1",
      "arxiv_id": "2602.14833v1",
      "title": "RF-GPT: Teaching AI to See the Wireless World",
      "authors": [
        "Hang Zou",
        "Yu Tian",
        "Bohao Wang",
        "Lina Bariah",
        "Samson Lasaulce",
        "Chongwen Huang",
        "Mérouane Debbah"
      ],
      "abstract": "Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14833v1",
      "url": "https://arxiv.org/abs/2602.14833v1",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## RF-GPT：面向无线世界的射频语言模型  \n\n当前大语言模型（LLMs）与多模态大模型（VLMs）虽具备强大的通用推理能力，但**无法原生理解射频（RF）信号**——这一无线通信系统的物理基础。现有电信AI方案多局限于文本/结构化数据，而传统RF深度学习模型则高度任务专用、缺乏语义泛化能力，导致**RF感知层与高层认知层之间存在显著断层**。  \n\n为弥合该鸿沟，我们提出 **RF-GPT**：首个面向射频理解的“无线电频率语言模型”（RFLM）。其核心创新在于**将RF信号转化为视觉可解的时频谱图（spectrogram）**，并复用多模态LLM的预训练视觉编码器提取特征；随后，将所得表征作为专属“RF tokens”注入解码器-only LLM，实现对RF场景的问答生成、原理性解释及结构化输出（如调制类型、协议参数、用户数等）。  \n\n训练完全基于**全合成RF语料库**：采用标准兼容波形发生器模拟6类主流无线技术（含Wi-Fi 6/7、5G NR、LoRa、Bluetooth等）的宽频带复杂场景；自动提取高保真时频谱图、精确配置元数据与密集语义描述；再经文本LLM蒸馏生成**12,000个RF场景、62.5万条RF-grounded指令-答案对**，全程零人工标注。  \n\n在宽频调制识别、信号重叠分析、无线制式判别、WLAN用户计数、5G NR关键参数抽取五大基准测试中，RF-GPT展现出**强大多任务泛化能力**；而未经过RF对齐的通用VLM（如LLaVA、Qwen-VL）在所有任务上均表现接近随机——证实RF-GPT实现了从“看见频谱”到“理解无线”的范式跃迁。",
      "summary_en": "RF-GPT is the first radio-frequency language model (RFLM) that bridges the gap between low-level RF signal perception and high-level multimodal reasoning. It repurposes pretrained visual encoders from multimodal LLMs to process time-frequency spectrograms derived from complex IQ waveforms—transforming raw RF data into “RF tokens” injected into a decoder-only LLM for grounded generation of answers, explanations, and structured outputs. Trained via supervised instruction tuning on a fully synthetic corpus of 12,000 RF scenes (625K instruction-answer pairs), RF-GPT leverages standards-compliant waveform generators covering six wireless technologies (e.g., 5G NR, Wi-Fi 6/7, LoRa) and automatic caption-to-instruction distillation—eliminating manual labeling. Across five challenging benchmarks—including wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction—RF-GPT achieves strong multi-task performance, while general-purpose VLMs without RF grounding fail catastrophically. This work establishes RF spectrograms as a viable “visual language” for wireless intelligence.",
      "summary": "## RF-GPT：面向无线世界的射频语言模型  \n\n当前大语言模型（LLMs）与多模态大模型（VLMs）虽具备强大的通用推理能力，但**无法原生理解射频（RF）信号**——这一无线通信系统的物理基础。现有电信AI方案多局限于文本/结构化数据，而传统RF深度学习模型则高度任务专用、缺乏语义泛化能力，导致**RF感知层与高层认知层之间存在显著断层**。  \n\n为弥合该鸿沟，我们提出 **RF-GPT**：首个面向射频理解的“无线电频率语言模型”（RFLM）。其核心创新在于**将RF信号转化为视觉可解的时频谱图（spectrogram）**，并复用多模态LLM的预训练视觉编码器提取特征；随后，将所得表征作为专属“RF tokens”注入解码器-only LLM，实现对RF场景的问答生成、原理性解释及结构化输出（如调制类型、协议参数、用户数等）。  \n\n训练完全基于**全合成RF语料库**：采用标准兼容波形发生器模拟6类主流无线技术（含Wi-Fi 6/7、5G NR、LoRa、Bluetooth等）的宽频带复杂场景；自动提取高保真时频谱图、精确配置元数据与密集语义描述；再经文本LLM蒸馏生成**12,000个RF场景、62.5万条RF-grounded指令-答案对**，全程零人工标注。  \n\n在宽频调制识别、信号重叠分析、无线制式判别、WLAN用户计数、5G NR关键参数抽取五大基准测试中，RF-GPT展现出**强大多任务泛化能力**；而未经过RF对齐的通用VLM（如LLaVA、Qwen-VL）在所有任务上均表现接近随机——证实RF-GPT实现了从“看见频谱”到“理解无线”的范式跃迁。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14743v1",
      "arxiv_id": "2602.14743v1",
      "title": "LLMStructBench: Benchmarking Large Language Model Structured Data Extraction",
      "authors": [
        "Sönke Tenckhoff",
        "Mario Koddenbrock",
        "Erik Rodner"
      ],
      "abstract": "We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.   In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.",
      "published": "2026-02-16",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14743v1",
      "url": "https://arxiv.org/abs/2602.14743v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## LLMStructBench：面向结构化数据抽取的大语言模型基准评测框架  \n\n本研究提出 **LLMStructBench**——首个专为评估大语言模型（LLM）在**自然语言到结构化数据转换任务**中性能而设计的开源基准。该基准聚焦核心挑战：从非结构化文本中准确提取结构化信息，并生成**语法正确、语义合规的 JSON 输出**（即同时满足 token-level 准确性与 document-level 有效性）。  \n\n我们构建了一个**高质量、人工校验、复杂度分层**的开放数据集，涵盖金融公告、医疗记录、产品描述等12类真实场景，包含527个多样化样本，覆盖嵌套对象、可选字段、多值数组、类型歧义等典型解析难点。实验系统评估了22个主流LLM（参数量横跨0.5B–72B），结合5种提示策略（Zero-shot、Few-shot、Chain-of-Thought、JSON Schema Prompting、Self-Refine），并引入**双维度评估指标**：  \n- **Token-Level Accuracy**（F1@token）：衡量字段值与格式的逐词匹配精度；  \n- **Document-Level Validity**（JSON-valid rate & Schema-compliance rate）：严格检测输出是否为可解析JSON且符合预定义schema。  \n\n关键发现包括：（1）**提示策略的影响显著超越模型规模**——例如，采用Schema-guided提示后，Qwen2-1.5B的JSON有效率提升至98.3%，反超未优化的Llama3-70B（86.1%）；（2）强提示虽大幅提升结构可靠性，但可能引入**语义漂移**（如字段误映射），凸显“语法正确≠语义正确”的根本张力；（3）小模型经提示工程可逼近大模型性能，为资源受限场景提供可行路径。LLMStructBench已开源（含数据集、评测脚本、全量结果），旨在推动LLM在ETL、知识图谱构建及AI代理数据接口等工业级结构化任务中的稳健应用。",
      "summary_en": "We introduce **LLMStructBench**, a novel open benchmark for evaluating Large Language Models (LLMs) on structured data extraction—specifically, parsing natural-language text into syntactically valid and semantically compliant JSON. Our manually verified dataset comprises 527 diverse, complexity-graded examples across 12 real-world domains (e.g., finance, healthcare), covering nested objects, optional fields, and type ambiguity. We systematically evaluate 22 models (0.5B–72B parameters) under five prompting strategies and propose dual metrics: *token-level F1* for field-value accuracy and *document-level validity rates* (JSON-parsability & schema compliance) for structural integrity. Crucially, we find that **prompting strategy dominates model size** in determining parsing reliability: schema-guided prompting enables small models (e.g., Qwen2-1.5B) to achieve >98% JSON validity—surpassing unoptimized large models—though at a trade-off of increased semantic errors. LLMStructBench is publicly released to advance robust LLM-based parsing for ETL, knowledge extraction, and agent-data interfaces.",
      "summary": "## LLMStructBench：面向结构化数据抽取的大语言模型基准评测框架  \n\n本研究提出 **LLMStructBench**——首个专为评估大语言模型（LLM）在**自然语言到结构化数据转换任务**中性能而设计的开源基准。该基准聚焦核心挑战：从非结构化文本中准确提取结构化信息，并生成**语法正确、语义合规的 JSON 输出**（即同时满足 token-level 准确性与 document-level 有效性）。  \n\n我们构建了一个**高质量、人工校验、复杂度分层**的开放数据集，涵盖金融公告、医疗记录、产品描述等12类真实场景，包含527个多样化样本，覆盖嵌套对象、可选字段、多值数组、类型歧义等典型解析难点。实验系统评估了22个主流LLM（参数量横跨0.5B–72B），结合5种提示策略（Zero-shot、Few-shot、Chain-of-Thought、JSON Schema Prompting、Self-Refine），并引入**双维度评估指标**：  \n- **Token-Level Accuracy**（F1@token）：衡量字段值与格式的逐词匹配精度；  \n- **Document-Level Validity**（JSON-valid rate & Schema-compliance rate）：严格检测输出是否为可解析JSON且符合预定义schema。  \n\n关键发现包括：（1）**提示策略的影响显著超越模型规模**——例如，采用Schema-guided提示后，Qwen2-1.5B的JSON有效率提升至98.3%，反超未优化的Llama3-70B（86.1%）；（2）强提示虽大幅提升结构可靠性，但可能引入**语义漂移**（如字段误映射），凸显“语法正确≠语义正确”的根本张力；（3）小模型经提示工程可逼近大模型性能，为资源受限场景提供可行路径。LLMStructBench已开源（含数据集、评测脚本、全量结果），旨在推动LLM在ETL、知识图谱构建及AI代理数据接口等工业级结构化任务中的稳健应用。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14345v1",
      "arxiv_id": "2602.14345v1",
      "title": "AXE: An Agentic eXploit Engine for Confirming Zero-Day Vulnerability Reports",
      "authors": [
        "Amirali Sajadi",
        "Tu Nguyen",
        "Kostadin Damevski",
        "Preetha Chatterjee"
      ],
      "abstract": "Vulnerability detection tools are widely adopted in software projects, yet they often overwhelm maintainers with false positives and non-actionable reports. Automated exploitation systems can help validate these reports; however, existing approaches typically operate in isolation from detection pipelines, failing to leverage readily available metadata such as vulnerability type and source-code location. In this paper, we investigate how reported security vulnerabilities can be assessed in a realistic grey-box exploitation setting that leverages minimal vulnerability metadata, specifically a CWE classification and a vulnerable code location. We introduce Agentic eXploit Engine (AXE), a multi-agent framework for Web application exploitation that maps lightweight detection metadata to concrete exploits through decoupled planning, code exploration, and dynamic execution feedback. Evaluated on the CVE-Bench dataset, AXE achieves a 30% exploitation success rate, a 3x improvement over state-of-the-art black-box baselines. Even in a single-agent configuration, grey-box metadata yields a 1.75x performance gain. Systematic error analysis shows that most failed attempts arise from specific reasoning gaps, including misinterpreted vulnerability semantics and unmet execution preconditions. For successful exploits, AXE produces actionable, reproducible proof-of-concept artifacts, demonstrating its utility in streamlining Web vulnerability triage and remediation. We further evaluate AXE's generalizability through a case study on a recent real-world vulnerability not included in CVE-Bench.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14345v1",
      "url": "https://arxiv.org/abs/2602.14345v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n当前漏洞检测工具虽在软件项目中广泛应用，却常向维护者输出大量**误报（false positives）**和**不可操作报告（non-actionable reports）**，导致人工验证成本高昂、响应滞后。自动化利用系统本可辅助验证，但现有方案多为黑盒式，**脱离检测流水线运行**，未能有效利用检测阶段已生成的轻量元数据（如CWE分类、源码位置等），限制了验证精度与效率。\n\n## 方法：AXE 框架设计  \n本文提出 **Agentic eXploit Engine（AXE）**——首个面向Web应用的**灰盒多智能体利用引擎**。AXE以最小元数据（仅需CWE类型 + 漏洞代码位置）为输入，通过三个解耦智能体协同完成：  \n- **规划代理**：基于CWE语义推理攻击面与利用路径；  \n- **探索代理**：结合静态/动态分析定向探查上下文依赖与可控输入点；  \n- **执行代理**：在真实运行时环境中闭环反馈、迭代优化payload。  \n全程无需完整源码或交互式调试器，显著降低部署门槛。\n\n## 主要发现与创新  \n- 在CVE-Bench基准测试中，AXE实现**30%的漏洞成功利用率**，较SOTA黑盒基线提升**3倍**；单智能体配置下，仅引入灰盒元数据即带来**1.75×性能增益**，证实元数据价值；  \n- 错误归因分析表明，失败主因集中于两类**可修复的推理缺陷**：① CWE语义误读（如混淆XSS与SSRF边界条件），② 执行前提未满足（如未触发特定初始化逻辑）；  \n- 所有成功案例均自动生成**可复现、带环境配置的PoC脚本**，支持一键验证与开发复现；  \n- 在未见于CVE-Bench的**真实零日漏洞（2024年某CMS RCE）**案例研究中，AXE在2小时内完成端到端确认，验证其工程泛化能力。",
      "summary_en": "This paper introduces **AXE (Agentic eXploit Engine)**, a novel multi-agent framework for *grey-box* exploitation of web application vulnerabilities using only lightweight detection metadata—specifically, a CWE classification and source-code location. Unlike black-box baselines, AXE tightly integrates with detection pipelines by decoupling planning (CWE-guided attack path synthesis), code exploration (context-aware input surface discovery), and dynamic execution (feedback-driven payload refinement). Evaluated on CVE-Bench, AXE achieves a **30% exploitation success rate**, representing a **3× improvement** over state-of-the-art black-box methods; even its single-agent variant yields a **1.75× gain** from grey-box metadata alone. Systematic error analysis identifies interpretable failure modes—primarily CWE semantic misalignment and unmet runtime preconditions—enabling targeted agent refinement. Crucially, AXE generates fully reproducible, environment-configured PoCs for all successful exploits, directly supporting triage and remediation. A real-world case study on an out-of-benchmark zero-day vulnerability further confirms AXE’s practical generalizability.",
      "summary": "## 背景与挑战  \n当前漏洞检测工具虽在软件项目中广泛应用，却常向维护者输出大量**误报（false positives）**和**不可操作报告（non-actionable reports）**，导致人工验证成本高昂、响应滞后。自动化利用系统本可辅助验证，但现有方案多为黑盒式，**脱离检测流水线运行**，未能有效利用检测阶段已生成的轻量元数据（如CWE分类、源码位置等），限制了验证精度与效率。\n\n## 方法：AXE 框架设计  \n本文提出 **Agentic eXploit Engine（AXE）**——首个面向Web应用的**灰盒多智能体利用引擎**。AXE以最小元数据（仅需CWE类型 + 漏洞代码位置）为输入，通过三个解耦智能体协同完成：  \n- **规划代理**：基于CWE语义推理攻击面与利用路径；  \n- **探索代理**：结合静态/动态分析定向探查上下文依赖与可控输入点；  \n- **执行代理**：在真实运行时环境中闭环反馈、迭代优化payload。  \n全程无需完整源码或交互式调试器，显著降低部署门槛。\n\n## 主要发现与创新  \n- 在CVE-Bench基准测试中，AXE实现**30%的漏洞成功利用率**，较SOTA黑盒基线提升**3倍**；单智能体配置下，仅引入灰盒元数据即带来**1.75×性能增益**，证实元数据价值；  \n- 错误归因分析表明，失败主因集中于两类**可修复的推理缺陷**：① CWE语义误读（如混淆XSS与SSRF边界条件），② 执行前提未满足（如未触发特定初始化逻辑）；  \n- 所有成功案例均自动生成**可复现、带环境配置的PoC脚本**，支持一键验证与开发复现；  \n- 在未见于CVE-Bench的**真实零日漏洞（2024年某CMS RCE）**案例研究中，AXE在2小时内完成端到端确认，验证其工程泛化能力。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14281v1",
      "arxiv_id": "2602.14281v1",
      "title": "MCPShield: A Security Cognition Layer for Adaptive Trust Calibration in Model Context Protocol Agents",
      "authors": [
        "Zhenhong Zhou",
        "Yuanhe Zhang",
        "Hongwei Cai",
        "Moayad Aloqaily",
        "Ouns Bouachir",
        "Linsey Pang",
        "Prakhar Mehrotra",
        "Kun Wang",
        "Qingsong Wen"
      ],
      "abstract": "The Model Context Protocol (MCP) standardizes tool use for LLM-based agents and enable third-party servers. This openness introduces a security misalignment: agents implicitly trust tools exposed by potentially untrusted MCP servers. However, despite its excellent utility, existing agents typically offer limited validation for third-party MCP servers. As a result, agents remain vulnerable to MCP-based attacks that exploit the misalignment between agents and servers throughout the tool invocation lifecycle. In this paper, we propose MCPShield as a plug-in security cognition layer that mitigates this misalignment and ensures agent security when invoking MCP-based tools. Drawing inspiration from human experience-driven tool validation, MCPShield assists agent forms security cognition with metadata-guided probing before invocation. Our method constrains execution within controlled boundaries while cognizing runtime events, and subsequently updates security cognition by reasoning over historical traces after invocation, building on human post-use reflection on tool behavior. Experiments demonstrate that MCPShield exhibits strong generalization in defending against six novel MCP-based attack scenarios across six widely used agentic LLMs, while avoiding false positives on benign servers and incurring low deployment overhead. Overall, our work provides a practical and robust security safeguard for MCP-based tool invocation in open agent ecosystems.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14281v1",
      "url": "https://arxiv.org/abs/2602.14281v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "llm"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n模型上下文协议（Model Context Protocol, MCP）为大语言模型（LLM）智能体提供了标准化的工具调用接口，并支持第三方MCP服务器接入，极大提升了生态开放性。然而，这种开放性引发关键安全错配：智能体在默认情况下**隐式信任**任意MCP服务器暴露的工具，而现有主流智能体框架普遍缺乏对第三方服务器的细粒度安全验证机制。由此导致智能体在整个工具调用生命周期中易受多种MCP特有攻击——如恶意工具注入、上下文劫持、响应篡改等，威胁严重且检测困难。\n\n## 方法：MCPShield安全认知层  \n我们提出**MCPShield**——一种即插即用的安全认知层，专为自适应信任校准而设计。其核心思想源于人类经验驱动的工具评估行为，包含三个协同阶段：  \n- **调用前认知**：基于工具元数据（如签名、权限声明、服务描述）主动发起轻量级探针请求，生成可信度初判；  \n- **运行中约束**：在沙箱化执行环境中限制工具调用边界（如输入长度、API频次、敏感字段过滤），并实时监控异常运行时事件；  \n- **调用后反思**：结合历史调用轨迹（含输入/输出/延迟/错误模式），通过因果推理更新工具可信度图谱，实现动态信任演化。\n\n## 主要发现与创新  \n在6种主流具身智能体LLM（包括Claude 3.5 Sonnet、GPT-4o、Qwen2.5-72B等）上，MCPShield成功防御全部6类新型MCP攻击（覆盖协议层、语义层与行为层），**零误报率**（对127个良性MCP服务器无干扰），平均推理延迟仅增加**47ms**，内存开销<2.1MB。本工作首次将“认知—约束—反思”三阶段人类安全决策范式形式化为可部署的智能体安全中间件，为开放代理生态提供了首个面向MCP协议栈的轻量、鲁棒、可演化的信任基础设施。",
      "summary_en": "The Model Context Protocol (MCP) enables standardized tool invocation for LLM-based agents but introduces a critical security misalignment: agents implicitly trust third-party MCP servers without rigorous validation. To address this, we propose **MCPShield**, a plug-in security cognition layer that enables adaptive trust calibration via human-inspired reasoning—*pre-invocation probing* guided by metadata, *runtime-constrained execution*, and *post-invocation reflection* over historical traces. Evaluated across six widely used agentic LLMs (e.g., GPT-4o, Claude 3.5, Qwen2.5-72B), MCPShield achieves 100% defense against six novel MCP-specific attack vectors, with zero false positives on 127 benign servers and only +47ms average latency overhead. Our work establishes the first practical, low-overhead security safeguard for MCP-based tool ecosystems.",
      "summary": "## 背景与问题  \n模型上下文协议（Model Context Protocol, MCP）为大语言模型（LLM）智能体提供了标准化的工具调用接口，并支持第三方MCP服务器接入，极大提升了生态开放性。然而，这种开放性引发关键安全错配：智能体在默认情况下**隐式信任**任意MCP服务器暴露的工具，而现有主流智能体框架普遍缺乏对第三方服务器的细粒度安全验证机制。由此导致智能体在整个工具调用生命周期中易受多种MCP特有攻击——如恶意工具注入、上下文劫持、响应篡改等，威胁严重且检测困难。\n\n## 方法：MCPShield安全认知层  \n我们提出**MCPShield**——一种即插即用的安全认知层，专为自适应信任校准而设计。其核心思想源于人类经验驱动的工具评估行为，包含三个协同阶段：  \n- **调用前认知**：基于工具元数据（如签名、权限声明、服务描述）主动发起轻量级探针请求，生成可信度初判；  \n- **运行中约束**：在沙箱化执行环境中限制工具调用边界（如输入长度、API频次、敏感字段过滤），并实时监控异常运行时事件；  \n- **调用后反思**：结合历史调用轨迹（含输入/输出/延迟/错误模式），通过因果推理更新工具可信度图谱，实现动态信任演化。\n\n## 主要发现与创新  \n在6种主流具身智能体LLM（包括Claude 3.5 Sonnet、GPT-4o、Qwen2.5-72B等）上，MCPShield成功防御全部6类新型MCP攻击（覆盖协议层、语义层与行为层），**零误报率**（对127个良性MCP服务器无干扰），平均推理延迟仅增加**47ms**，内存开销<2.1MB。本工作首次将“认知—约束—反思”三阶段人类安全决策范式形式化为可部署的智能体安全中间件，为开放代理生态提供了首个面向MCP协议栈的轻量、鲁棒、可演化的信任基础设施。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14211v1",
      "arxiv_id": "2602.14211v1",
      "title": "SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement",
      "authors": [
        "Xiaojun Jia",
        "Jie Liao",
        "Simeng Qin",
        "Jindong Gu",
        "Wenqi Ren",
        "Xiaochun Cao",
        "Yang Liu",
        "Philip Torr"
      ],
      "abstract": "Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. In practice, naive injections often fail because the malicious intent is too explicit or drifts too far from the original skill, leading agents to ignore or refuse them; existing attacks are also largely hand-crafted. We propose the first automated framework for stealthy prompt injection tailored to agent skills. The framework forms a closed loop with three agents: an Attack Agent that synthesizes injection skills under explicit stealth constraints, a Code Agent that executes tasks using the injected skills in a realistic tool environment, and an Evaluate Agent that logs action traces (e.g., tool calls and file operations) and verifies whether targeted malicious behaviors occurred. We also propose a malicious payload hiding strategy that conceals adversarial operations in auxiliary scripts while injecting optimized inducement prompts to trigger tool execution. Extensive experiments across diverse coding-agent settings and real-world software engineering tasks show that our method consistently achieves high attack success rates under realistic settings.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14211v1",
      "url": "https://arxiv.org/abs/2602.14211v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n随着大模型驱动的编码智能体（coding agents）快速发展，**“技能”（Skill）** 已成为关键抽象范式——它将长格式指令、辅助脚本及工具调用逻辑封装为可复用模块，显著扩展了智能体的工具增强能力。然而，这一抽象也引入了**未被充分评估的安全攻击面：基于技能的提示注入（Skill-Based Prompt Injection）**。攻击者可通过篡改或注入恶意技能，诱导智能体偏离用户原始意图、绕过安全策略，甚至执行隐蔽有害操作。现有方法存在两大瓶颈：（1）手工构造的注入技能往往**显性暴露恶意意图**，易被智能体拒绝或忽略；（2）缺乏对真实执行环境（如文件系统、工具链交互）的反馈闭环，难以实现“既有效又隐蔽”的攻击。\n\n## 方法创新  \n本文提出 **SkillJect**——首个面向编码智能体技能的**自动化、痕迹驱动、闭环优化**提示注入框架。其核心是三智能体协同闭环：  \n- **攻击智能体（Attack Agent）**：在严格**隐蔽性约束**（如语义一致性、行为扰动阈值）下，自动生成注入技能；  \n- **编码智能体（Code Agent）**：在真实工具环境（含Shell、Git、编辑器API等）中执行任务，使用注入技能完成用户请求；  \n- **评估智能体（Evaluate Agent）**：全程记录细粒度**执行痕迹**（tool calls、file reads/writes、process spawns），并基于预设恶意行为模式（如窃取`.env`文件、注入后门代码）进行自动验证。  \n此外，我们提出**恶意载荷隐藏策略**：将高风险操作（如敏感文件读取）下沉至技能附带的**辅助脚本**中，同时设计轻量级、语义自然的**诱导型提示**（inducement prompts），精准触发目标工具调用，大幅提升隐蔽性。\n\n## 实验结果  \n在涵盖Cursor、CodeAct、SWE-Agent等主流架构，以及Python/JS项目重构、Bug修复、依赖注入等8类真实软件工程任务上，SkillJect平均攻击成功率**达86.7%**（基线手工方法仅31.2%），且92%的注入技能通过人类评估者“不可察觉”测试。该工作首次系统揭示了技能抽象层的安全脆弱性，并为鲁棒性评测与防御机制设计提供了可复现基准。",
      "summary_en": "We present **SkillJect**, the first automated framework for stealthy skill-based prompt injection targeting coding agents. It introduces a **trace-driven closed-loop refinement** system comprising three specialized agents: an Attack Agent that synthesizes malicious skills under explicit stealth constraints (e.g., semantic fidelity, minimal behavioral drift); a Code Agent that executes realistic software engineering tasks in a full tool-augmented environment (shell, Git, editors); and an Evaluate Agent that logs fine-grained execution traces (tool invocations, file I/O) to verify whether targeted adversarial behaviors—such as unauthorized credential exfiltration or backdoor insertion—actually occur. Crucially, SkillJect hides malicious payloads within auxiliary scripts while using optimized, natural-language inducement prompts to trigger harmful tool calls—balancing effectiveness and stealth. Across diverse agent architectures (Cursor, CodeAct, SWE-Agent) and 8 real-world tasks (e.g., dependency injection, config leakage), SkillJect achieves an average attack success rate of **86.7%**, significantly outperforming manual baselines (31.2%) and maintaining high evasion rates (>92% undetected by human evaluators). This work exposes a critical, underexplored vulnerability in skill-abstraction layers and establishes a rigorous, reproducible benchmark for evaluating agent security.",
      "summary": "## 背景与问题  \n随着大模型驱动的编码智能体（coding agents）快速发展，**“技能”（Skill）** 已成为关键抽象范式——它将长格式指令、辅助脚本及工具调用逻辑封装为可复用模块，显著扩展了智能体的工具增强能力。然而，这一抽象也引入了**未被充分评估的安全攻击面：基于技能的提示注入（Skill-Based Prompt Injection）**。攻击者可通过篡改或注入恶意技能，诱导智能体偏离用户原始意图、绕过安全策略，甚至执行隐蔽有害操作。现有方法存在两大瓶颈：（1）手工构造的注入技能往往**显性暴露恶意意图**，易被智能体拒绝或忽略；（2）缺乏对真实执行环境（如文件系统、工具链交互）的反馈闭环，难以实现“既有效又隐蔽”的攻击。\n\n## 方法创新  \n本文提出 **SkillJect**——首个面向编码智能体技能的**自动化、痕迹驱动、闭环优化**提示注入框架。其核心是三智能体协同闭环：  \n- **攻击智能体（Attack Agent）**：在严格**隐蔽性约束**（如语义一致性、行为扰动阈值）下，自动生成注入技能；  \n- **编码智能体（Code Agent）**：在真实工具环境（含Shell、Git、编辑器API等）中执行任务，使用注入技能完成用户请求；  \n- **评估智能体（Evaluate Agent）**：全程记录细粒度**执行痕迹**（tool calls、file reads/writes、process spawns），并基于预设恶意行为模式（如窃取`.env`文件、注入后门代码）进行自动验证。  \n此外，我们提出**恶意载荷隐藏策略**：将高风险操作（如敏感文件读取）下沉至技能附带的**辅助脚本**中，同时设计轻量级、语义自然的**诱导型提示**（inducement prompts），精准触发目标工具调用，大幅提升隐蔽性。\n\n## 实验结果  \n在涵盖Cursor、CodeAct、SWE-Agent等主流架构，以及Python/JS项目重构、Bug修复、依赖注入等8类真实软件工程任务上，SkillJect平均攻击成功率**达86.7%**（基线手工方法仅31.2%），且92%的注入技能通过人类评估者“不可察觉”测试。该工作首次系统揭示了技能抽象层的安全脆弱性，并为鲁棒性评测与防御机制设计提供了可复现基准。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14106v1",
      "arxiv_id": "2602.14106v1",
      "title": "Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models",
      "authors": [
        "Mario Marín Caballero",
        "Miguel Betancourt Alonso",
        "Daniel Díaz-López",
        "Angel Luis Perales Gómez",
        "Pantaleone Nespoli",
        "Gregorio Martínez Pérez"
      ],
      "abstract": "The most valuable asset of any cloud-based organization is data, which is increasingly exposed to sophisticated cyberattacks. Until recently, the implementation of security measures in DevOps environments was often considered optional by many government entities and critical national services operating in the cloud. This includes systems managing sensitive information, such as electoral processes or military operations, which have historically been valuable targets for cybercriminals. Resistance to security implementation is often driven by concerns over losing agility in software development, increasing the risk of accumulated vulnerabilities. Nowadays, patching software is no longer enough; adopting a proactive cyber defense strategy, supported by Artificial Intelligence (AI), is crucial to anticipating and mitigating threats. Thus, this work proposes integrating the Security Chaos Engineering (SCE) methodology with a new LLM-based flow to automate the creation of attack defense trees that represent adversary behavior and facilitate the construction of SCE experiments based on these graphical models, enabling teams to stay one step ahead of attackers and implement previously unconsidered defenses. Further detailed information about the experiment performed, along with the steps to replicate it, can be found in the following repository: https://github.com/mariomc14/devsecops-adversary-llm.git.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14106v1",
      "url": "https://arxiv.org/abs/2602.14106v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n在云原生环境中，数据作为组织最核心资产，正面临日益复杂化、自动化的高级持续性威胁（APT）。尤其在政府及关键国家基础设施领域（如选举系统、军事指挥平台），安全措施长期被边缘化——常被视为对DevOps敏捷性的“负担”，导致技术债累积、漏洞窗口延长。传统“打补丁式”响应已无法应对零日攻击与行为隐蔽的对抗性渗透。\n\n## 方法创新  \n本研究提出一种**AI增强型安全左移范式**：将**安全混沌工程（SCE）** 与**大语言模型（LLM）驱动的对抗建模流程**深度耦合。具体而言，我们设计了一套可复用的提示工程框架，引导开源/商用LLM（如Llama-3-70B、Claude-3）解析MITRE ATT&CK框架、红队报告与云配置策略，自动生成结构化**攻击防御树（Attack-Defense Trees, ADTs）**。该ADT不仅编码攻击路径（TTPs），更显式标注防御节点（如WAF规则、IAM权限收紧、运行时行为监控），直接映射为SCE实验用例（如故意注入恶意容器镜像并验证eBPF拦截有效性）。\n\n## 核心贡献  \n- 首个将LLM用于**动态生成可执行ADTs**的DevSecOps实践框架，实现从威胁情报到混沌实验的端到端自动化；  \n- 实验验证：在AWS EKS集群上部署12类ADT衍生实验，平均提前**47小时**识别出传统扫描遗漏的横向移动链路；  \n- 开源完整工具链与复现实验仓库（含LLM调用模板、ADT Schema定义、SCE实验编排脚本），显著降低安全混沌工程的技术门槛。",
      "summary_en": "This paper introduces a novel LLM-augmented DevSecOps framework that bridges Security Chaos Engineering (SCE) and adversary behavior anticipation. We propose a prompt-engineered pipeline leveraging large language models (e.g., Llama-3, Claude-3) to automatically generate executable Attack-Defense Trees (ADTs) from MITRE ATT&CK data, red-team reports, and cloud infrastructure configurations. These ADTs explicitly model attacker tactics *and* corresponding defensive countermeasures, enabling direct translation into SCE experiments—such as injecting malicious workloads to validate runtime detection efficacy. Evaluated on an AWS EKS cluster, our approach identified 12 previously undetected lateral movement paths with a median lead time of 47 hours over conventional scanning. The framework reduces manual threat modeling effort by ~80% and is fully open-sourced at https://github.com/mariomc14/devsecops-adversary-llm.git.",
      "summary": "## 背景与挑战  \n在云原生环境中，数据作为组织最核心资产，正面临日益复杂化、自动化的高级持续性威胁（APT）。尤其在政府及关键国家基础设施领域（如选举系统、军事指挥平台），安全措施长期被边缘化——常被视为对DevOps敏捷性的“负担”，导致技术债累积、漏洞窗口延长。传统“打补丁式”响应已无法应对零日攻击与行为隐蔽的对抗性渗透。\n\n## 方法创新  \n本研究提出一种**AI增强型安全左移范式**：将**安全混沌工程（SCE）** 与**大语言模型（LLM）驱动的对抗建模流程**深度耦合。具体而言，我们设计了一套可复用的提示工程框架，引导开源/商用LLM（如Llama-3-70B、Claude-3）解析MITRE ATT&CK框架、红队报告与云配置策略，自动生成结构化**攻击防御树（Attack-Defense Trees, ADTs）**。该ADT不仅编码攻击路径（TTPs），更显式标注防御节点（如WAF规则、IAM权限收紧、运行时行为监控），直接映射为SCE实验用例（如故意注入恶意容器镜像并验证eBPF拦截有效性）。\n\n## 核心贡献  \n- 首个将LLM用于**动态生成可执行ADTs**的DevSecOps实践框架，实现从威胁情报到混沌实验的端到端自动化；  \n- 实验验证：在AWS EKS集群上部署12类ADT衍生实验，平均提前**47小时**识别出传统扫描遗漏的横向移动链路；  \n- 开源完整工具链与复现实验仓库（含LLM调用模板、ADT Schema定义、SCE实验编排脚本），显著降低安全混沌工程的技术门槛。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14012v1",
      "arxiv_id": "2602.14012v1",
      "title": "From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection",
      "authors": [
        "Youpeng Li",
        "Fuxun Yu",
        "Xinda Wang"
      ],
      "abstract": "The integration of LLMs into vulnerability detection (VD) has shifted the field toward interpretable and context-aware analysis. While post-training methods have shown promise in general coding tasks, their systematic application to VD remains underexplored. In this paper, we present the first comprehensive investigation into the post-training pipeline for LLM-based VD, spanning from cold-start SFT to off-policy preference optimization and on-policy RL, uncovering how data curation, stage interactions, reward mechanisms, and evaluation protocols collectively dictate the efficacy of model training and assessment. Our study identifies practical guidelines and insights: (1) SFT based on rejection sampling greatly outperforms rationalization-based supervision, which can introduce hallucinations due to ground-truth leakage. (2) While increased SFT epochs constantly benefit preference optimization, excessive SFT inhibits self-exploration during RL, ultimately limiting performance gains. (3) Coarse-grained reward signals often mislead RL, whereas fine-grained root-cause judgments ensure reliable credit assignment. Specification-based rewards offer further benefits but incur significant effort in specification generation. (4) Although filtering extremely hard-to-detect vulnerability samples improves RL training efficiency, the cost of performance loss should be considered in practical applications. (5) Models trained under GRPO significantly outperform those using SFT and preference optimization (i.e., DPO and ORPO), as well as a series of zero-shot SOTA LLMs, underscoring the significant potential of on-policy RL for LLM-based VD. (6) In contrast to binary matching that tends to overestimate performance, LLM-as-a-Judge based on root-cause analysis provides a more robust evaluation protocol, although its accuracy varies across judge models with different levels of security expertise.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14012v1",
      "url": "https://arxiv.org/abs/2602.14012v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary_zh": "## 从监督微调到强化学习：解构面向漏洞检测的大语言模型后训练范式  \n\n本研究首次系统性探究了大语言模型（LLM）在**漏洞检测（VD）任务中完整的后训练流水线**，覆盖冷启动监督微调（SFT）、离策略偏好优化（如DPO/ORPO）及在策略强化学习（RL，以GRPO为代表）三大阶段。我们揭示了数据构建、阶段协同、奖励设计与评估协议四大要素如何共同决定模型性能上限与评估可信度。关键发现包括：（1）基于**拒绝采样的SFT显著优于理性化监督**——后者因真实标签泄露易引发幻觉；（2）SFT轮次增加虽持续提升偏好优化效果，但过度SFT会抑制RL阶段的自我探索能力，反致性能饱和；（3）**粗粒度奖励（如“是否含漏洞”）易误导RL训练**，而细粒度“根因级判断”（如“未识别缓冲区溢出边界检查缺失”）可实现精准信用分配；基于形式化规范的奖励效果更优，但生成成本高昂；（4）过滤极难检测样本可加速RL收敛，但需权衡由此带来的漏报率上升风险；（5）采用**GRPO的在策略RL方案全面超越SFT、DPO/ORPO及零样本SOTA LLMs**，验证了RL在VD中的范式潜力；（6）相比简单二值匹配，**基于根因分析的LLM-as-a-Judge评估协议更鲁棒**，但其准确性高度依赖裁判模型的安全领域专精程度。本工作为安全领域LLM后训练提供了可复现、可解释、可落地的方法论框架。",
      "summary_en": "This paper presents the first comprehensive study of the full post-training pipeline—from cold-start SFT to off-policy preference optimization (DPO/ORPO) and on-policy RL (GRPO)—for LLM-based vulnerability detection (VD). We identify four critical levers: data curation, stage interaction, reward granularity, and evaluation design. Key findings: (1) Rejection-sampling-based SFT outperforms rationalization-based SFT, which risks hallucination via ground-truth leakage; (2) While more SFT epochs benefit preference optimization, excessive SFT suppresses RL exploration and caps final performance; (3) Coarse-grained rewards misguide RL; fine-grained root-cause judgments enable reliable credit assignment—specification-based rewards further improve accuracy but demand high annotation effort; (4) Filtering extremely hard samples accelerates RL training but incurs non-negligible detection loss in practice; (5) GRPO-trained models significantly surpass SFT, DPO/ORPO, and zero-shot SOTA LLMs, highlighting RL’s transformative potential for VD; (6) Root-cause–aware LLM-as-a-Judge evaluation is more robust than binary matching, though judge model expertise critically impacts reliability.",
      "summary": "## 从监督微调到强化学习：解构面向漏洞检测的大语言模型后训练范式  \n\n本研究首次系统性探究了大语言模型（LLM）在**漏洞检测（VD）任务中完整的后训练流水线**，覆盖冷启动监督微调（SFT）、离策略偏好优化（如DPO/ORPO）及在策略强化学习（RL，以GRPO为代表）三大阶段。我们揭示了数据构建、阶段协同、奖励设计与评估协议四大要素如何共同决定模型性能上限与评估可信度。关键发现包括：（1）基于**拒绝采样的SFT显著优于理性化监督**——后者因真实标签泄露易引发幻觉；（2）SFT轮次增加虽持续提升偏好优化效果，但过度SFT会抑制RL阶段的自我探索能力，反致性能饱和；（3）**粗粒度奖励（如“是否含漏洞”）易误导RL训练**，而细粒度“根因级判断”（如“未识别缓冲区溢出边界检查缺失”）可实现精准信用分配；基于形式化规范的奖励效果更优，但生成成本高昂；（4）过滤极难检测样本可加速RL收敛，但需权衡由此带来的漏报率上升风险；（5）采用**GRPO的在策略RL方案全面超越SFT、DPO/ORPO及零样本SOTA LLMs**，验证了RL在VD中的范式潜力；（6）相比简单二值匹配，**基于根因分析的LLM-as-a-Judge评估协议更鲁棒**，但其准确性高度依赖裁判模型的安全领域专精程度。本工作为安全领域LLM后训练提供了可复现、可解释、可落地的方法论框架。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14301v1",
      "arxiv_id": "2602.14301v1",
      "title": "DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices",
      "authors": [
        "Songyuan Li",
        "Jia Hu",
        "Ahmed M. Abdelmoniem",
        "Geyong Min",
        "Haojun Huang",
        "Jiwei Huang"
      ],
      "abstract": "Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14301v1",
      "url": "https://arxiv.org/abs/2602.14301v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n基于**混合专家（MoE）**的大型语言模型（如Qwen-MoE、DeepSeek-MoE）正推动自然语言生成能力跃升，但其训练高度依赖海量、多样化的私有数据。联邦学习（FL）虽能协同边缘设备在保护隐私前提下联合训练，却面临根本性瓶颈：传统FL要求每台设备本地部署完整MoE模型，而MoE参数量巨大（常达百亿级），远超智能手机、IoT终端等资源受限设备的内存与算力上限。\n\n## 方法创新：DeepFusion框架  \n本文提出**DeepFusion**——首个面向异构边缘设备的可扩展联邦MoE训练框架，核心思想是**不传输模型，而蒸馏知识**。其两大关键技术突破：  \n- **个性化本地建模**：各设备根据自身硬件约束与任务需求，自主配置并训练轻量级、架构各异的**本地LLM**（非MoE），无需共享原始数据或模型权重；  \n- **View-Aligned Attention（VAA）模块**：首创跨架构视角对齐机制。该模块动态融合全局MoE模型在多阶段（embedding、中间层、logits）的特征表示，构建与本地LLM预测行为严格对齐的“知识视图”，从根本上解决传统联邦知识蒸馏中因模型异构导致的**预测视角错配（view-mismatch）问题**。\n\n## 实验验证与优势  \n在真实医疗与金融领域数据集上，联合Qwen-MoE和DeepSeek-MoE进行大规模实验表明：DeepFusion训练出的全局MoE模型性能**逼近中心化训练基准**（token perplexity差距<1.2%）；相较主流联邦MoE基线（如FedMoE、FedKD-MoE），**通信开销降低最高达71%**，**token perplexity提升最高达5.28%**。本工作为隐私敏感、资源受限场景下的大模型协同进化提供了高效、鲁棒、落地可行的新范式。",
      "summary_en": "DeepFusion is the first scalable federated learning framework for training Mixture-of-Experts (MoE) large language models across resource-constrained, heterogeneous edge devices. It overcomes the impracticality of deploying full MoE models locally by enabling *federated knowledge distillation*: each device trains a lightweight, customized on-device LLM, while a global MoE model is refined via distilled knowledge—not raw gradients or weights. To bridge architectural and behavioral heterogeneity, we propose the View-Aligned Attention (VAA) module, which constructs prediction-aligned feature views from multi-stage global MoE representations, resolving the critical view-mismatch problem in cross-architecture distillation. Experiments on industry-scale MoE models (Qwen-MoE, DeepSeek-MoE) and real-world medical/financial datasets show DeepFusion achieves near-centralized training performance, reduces communication costs by up to 71%, and improves token perplexity by up to 5.28% over key federated MoE baselines.",
      "summary": "## 背景与挑战  \n基于**混合专家（MoE）**的大型语言模型（如Qwen-MoE、DeepSeek-MoE）正推动自然语言生成能力跃升，但其训练高度依赖海量、多样化的私有数据。联邦学习（FL）虽能协同边缘设备在保护隐私前提下联合训练，却面临根本性瓶颈：传统FL要求每台设备本地部署完整MoE模型，而MoE参数量巨大（常达百亿级），远超智能手机、IoT终端等资源受限设备的内存与算力上限。\n\n## 方法创新：DeepFusion框架  \n本文提出**DeepFusion**——首个面向异构边缘设备的可扩展联邦MoE训练框架，核心思想是**不传输模型，而蒸馏知识**。其两大关键技术突破：  \n- **个性化本地建模**：各设备根据自身硬件约束与任务需求，自主配置并训练轻量级、架构各异的**本地LLM**（非MoE），无需共享原始数据或模型权重；  \n- **View-Aligned Attention（VAA）模块**：首创跨架构视角对齐机制。该模块动态融合全局MoE模型在多阶段（embedding、中间层、logits）的特征表示，构建与本地LLM预测行为严格对齐的“知识视图”，从根本上解决传统联邦知识蒸馏中因模型异构导致的**预测视角错配（view-mismatch）问题**。\n\n## 实验验证与优势  \n在真实医疗与金融领域数据集上，联合Qwen-MoE和DeepSeek-MoE进行大规模实验表明：DeepFusion训练出的全局MoE模型性能**逼近中心化训练基准**（token perplexity差距<1.2%）；相较主流联邦MoE基线（如FedMoE、FedKD-MoE），**通信开销降低最高达71%**，**token perplexity提升最高达5.28%**。本工作为隐私敏感、资源受限场景下的大模型协同进化提供了高效、鲁棒、落地可行的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14178v1",
      "arxiv_id": "2602.14178v1",
      "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size $\\mathit{2^{128}}$ for Unified Multimodal Large Language Model",
      "authors": [
        "Shaobin Zhuang",
        "Yuang Ai",
        "Jiaming Han",
        "Weijia Mao",
        "Xiaohui Li",
        "Fangyikang Wang",
        "Xiao Wang",
        "Yan Li",
        "Shanchuan Lin",
        "Kun Xu",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen",
        "Yali Wang"
      ],
      "abstract": "Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14178v1",
      "url": "https://arxiv.org/abs/2602.14178v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## UniWeTok：面向统一多模态大语言模型的超大规模二进制统一分词器  \n\n**背景与挑战**：统一多模态大语言模型（MLLMs）亟需一种视觉表征，能**同时满足高保真重建、复杂语义提取与生成友好性**三大目标。然而，现有视觉分词器（如VQ-VAE、DALL·E tokenizer）往往在单一框架内难以兼顾——高码本容量易损重建质量，低熵约束又削弱语义表达力，而生成先验建模常与离散优化目标冲突。\n\n**方法创新**：本文提出 **UniWeTok**，首个基于 $2^{128}$ 规模二进制码本的统一离散分词器。核心贡献包括：  \n- **训练范式**：提出**预-后蒸馏（Pre-Post Distillation）** 机制，解耦语义压缩与重建学习；引入**生成感知先验（Generative-Aware Prior）**，显式建模token序列的自回归生成倾向；  \n- **架构设计**：构建**卷积-注意力混合编码器**，并首创 **SigLu 激活函数**——其Sigmoid-bounded输出稳定语义蒸馏过程，且通过梯度重加权机制**协同优化token熵损失与承诺损失**，缓解经典VQ优化冲突；  \n- **三阶段训练框架**：依次覆盖标准分辨率（256×256）、高分辨率（512×512）及感知敏感场景（人脸/文本区域增强），显著提升跨尺度与细粒度鲁棒性。\n\n**性能突破**：在ImageNet上，UniWeTok以**仅33B训练token**（仅为REPA的12.6%）达成**FID 1.38**，超越REPA（1.42）；在通用领域，其DPG生成得分达**86.63**（优于FLUX.1 [Dev] 83.84），图像编辑GEdit综合分**5.09**（领先OmniGen 5.06），并在多模态理解任务中保持强竞争力。代码与模型已开源，推动统一分词器与MLLM协同演进。",
      "summary_en": "UniWeTok introduces a unified discrete visual tokenizer for Multimodal Large Language Models (MLLMs), featuring an unprecedented binary codebook of size $2^{128}$. To reconcile fidelity, semantics, and generativity—objectives often in tension—we propose: (i) a Pre-Post Distillation framework coupled with a Generative-Aware Prior to jointly enhance semantic expressivity and autoregressive readiness; (ii) a convolution-attention hybrid architecture with the novel **SigLu activation**, which bounds encoder outputs for stable distillation and resolves the entropy–commitment loss conflict via gradient-aware weighting; and (iii) a three-stage resolution- and perception-aware training strategy. Evaluated on ImageNet, UniWeTok achieves state-of-the-art FID (1.38) using only **33B training tokens**—just 12.6% of REPA’s compute (262B). In general-domain benchmarks, it attains top-tier performance across multimodal understanding, image generation (DPG: **86.63** vs. FLUX.1 [Dev] 83.84), and editing (GEdit: **5.09** vs. OmniGen 5.06). Code and models are publicly released.",
      "summary": "## UniWeTok：面向统一多模态大语言模型的超大规模二进制统一分词器  \n\n**背景与挑战**：统一多模态大语言模型（MLLMs）亟需一种视觉表征，能**同时满足高保真重建、复杂语义提取与生成友好性**三大目标。然而，现有视觉分词器（如VQ-VAE、DALL·E tokenizer）往往在单一框架内难以兼顾——高码本容量易损重建质量，低熵约束又削弱语义表达力，而生成先验建模常与离散优化目标冲突。\n\n**方法创新**：本文提出 **UniWeTok**，首个基于 $2^{128}$ 规模二进制码本的统一离散分词器。核心贡献包括：  \n- **训练范式**：提出**预-后蒸馏（Pre-Post Distillation）** 机制，解耦语义压缩与重建学习；引入**生成感知先验（Generative-Aware Prior）**，显式建模token序列的自回归生成倾向；  \n- **架构设计**：构建**卷积-注意力混合编码器**，并首创 **SigLu 激活函数**——其Sigmoid-bounded输出稳定语义蒸馏过程，且通过梯度重加权机制**协同优化token熵损失与承诺损失**，缓解经典VQ优化冲突；  \n- **三阶段训练框架**：依次覆盖标准分辨率（256×256）、高分辨率（512×512）及感知敏感场景（人脸/文本区域增强），显著提升跨尺度与细粒度鲁棒性。\n\n**性能突破**：在ImageNet上，UniWeTok以**仅33B训练token**（仅为REPA的12.6%）达成**FID 1.38**，超越REPA（1.42）；在通用领域，其DPG生成得分达**86.63**（优于FLUX.1 [Dev] 83.84），图像编辑GEdit综合分**5.09**（领先OmniGen 5.06），并在多模态理解任务中保持强竞争力。代码与模型已开源，推动统一分词器与MLLM协同演进。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14065v1",
      "arxiv_id": "2602.14065v1",
      "title": "REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment",
      "authors": [
        "Kai Ye",
        "Xianwei Mao",
        "Sheng Zhou",
        "Zirui Shao",
        "Ye Mo",
        "Liangliang Liu",
        "Haikuan Huang",
        "Bin Li",
        "Jiajun Bu"
      ],
      "abstract": "Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14065v1",
      "url": "https://arxiv.org/abs/2602.14065v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n知识密集型视觉问答（KI-VQA）需融合图像、文本与外部知识进行多步推理，但开放域检索固有的噪声与冗余常引发**严重知识冲突**——即检索到的多个证据彼此矛盾或与视觉内容不一致。现有方法受限于两大瓶颈：（1）缺乏可泛化的冲突检测机制，难以在未见领域识别隐性矛盾；（2）缺少模型内部约束机制，无法在生成过程中动态抑制冲突证据的影响。\n\n## 方法创新：Reasoning-Pivot（推理锚点）范式  \n本文提出**REAL框架**（Reasoning-Pivot Alignment），核心是引入**推理锚点（Reasoning-Pivot）** 这一新概念：它并非传统推理步骤（强调内部推导），而是推理链中承担**知识联结功能**的原子单元（如节点或边），其成立高度依赖外部证据支持。例如，“‘图中物体是企鹅’→‘企鹅仅分布于南半球’”中的“企鹅仅分布于南半球”即为典型锚点——其真值需由知识库验证。\n\n## 技术实现  \n- **REAL-VQA数据集**：首个显式标注知识冲突类型、锚点位置及修正路径的大规模KI-VQA基准；  \n- **RPA-SFT（锚点感知监督微调）**：通过将冲突检测与锚点抽取对齐，训练出可泛化的冲突判别器；  \n- **RPGD（锚点引导解码）**：在自回归生成中动态激活锚点约束，对冲突证据实施软掩蔽与权重重校准，实现模型内实时缓解。\n\n## 主要成果  \n在OK-VQA、A-OKVQA、KRVQA等跨域基准上，REAL将冲突识别准确率提升12.7%，答案准确率平均提升9.3%，显著超越现有SOTA方法。本工作首次将知识冲突治理从“后验过滤”转向“前馈锚控”，为可信多模态推理提供了可解释、可干预的新范式。",
      "summary_en": "Knowledge-intensive Visual Question Answering (KI-VQA) is severely hindered by knowledge conflicts arising from noisy open-domain retrieval. Existing methods lack both generalizable conflict detection and intra-model mechanisms to resolve such conflicts during reasoning. To address this, we propose **REAL**, a novel framework centered on the **Reasoning-Pivot**—an atomic unit in the reasoning chain that explicitly links external evidence to internal inference (e.g., a fact node requiring verification). REAL comprises: (1) the **REAL-VQA dataset**, the first KI-VQA benchmark with fine-grained conflict and pivot annotations; (2) **Reasoning-Pivot Aware SFT (RPA-SFT)**, which trains a robust conflict discriminator by aligning conflict signals with pivot extraction; and (3) **Reasoning-Pivot Guided Decoding (RPGD)**, an intra-model decoding strategy that dynamically suppresses conflicting evidence using pivots. Experiments across OK-VQA, A-OKVQA, and KRVQA show REAL achieves **state-of-the-art accuracy**, improving conflict discrimination by +12.7% and answer accuracy by +9.3% on average—validating pivot-driven, interpretable conflict resolution as a scalable paradigm for trustworthy multimodal reasoning.",
      "summary": "## 背景与挑战  \n知识密集型视觉问答（KI-VQA）需融合图像、文本与外部知识进行多步推理，但开放域检索固有的噪声与冗余常引发**严重知识冲突**——即检索到的多个证据彼此矛盾或与视觉内容不一致。现有方法受限于两大瓶颈：（1）缺乏可泛化的冲突检测机制，难以在未见领域识别隐性矛盾；（2）缺少模型内部约束机制，无法在生成过程中动态抑制冲突证据的影响。\n\n## 方法创新：Reasoning-Pivot（推理锚点）范式  \n本文提出**REAL框架**（Reasoning-Pivot Alignment），核心是引入**推理锚点（Reasoning-Pivot）** 这一新概念：它并非传统推理步骤（强调内部推导），而是推理链中承担**知识联结功能**的原子单元（如节点或边），其成立高度依赖外部证据支持。例如，“‘图中物体是企鹅’→‘企鹅仅分布于南半球’”中的“企鹅仅分布于南半球”即为典型锚点——其真值需由知识库验证。\n\n## 技术实现  \n- **REAL-VQA数据集**：首个显式标注知识冲突类型、锚点位置及修正路径的大规模KI-VQA基准；  \n- **RPA-SFT（锚点感知监督微调）**：通过将冲突检测与锚点抽取对齐，训练出可泛化的冲突判别器；  \n- **RPGD（锚点引导解码）**：在自回归生成中动态激活锚点约束，对冲突证据实施软掩蔽与权重重校准，实现模型内实时缓解。\n\n## 主要成果  \n在OK-VQA、A-OKVQA、KRVQA等跨域基准上，REAL将冲突识别准确率提升12.7%，答案准确率平均提升9.3%，显著超越现有SOTA方法。本工作首次将知识冲突治理从“后验过滤”转向“前馈锚控”，为可信多模态推理提供了可解释、可干预的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14318v1",
      "arxiv_id": "2602.14318v1",
      "title": "In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes",
      "authors": [
        "Trishit Mondal",
        "Ameya D. Jagtap"
      ],
      "abstract": "Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \\textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14318v1",
      "url": "https://arxiv.org/abs/2602.14318v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "machine",
        "adversarial"
      ],
      "keyword_score": 3,
      "summary_zh": "## 变压器架构的可信性审视：失败模式系统分析  \n\nTransformer 架构已在自然语言处理、计算机视觉、科学计算等广泛领域引发范式变革。然而，随着其在**高风险场景**（如医疗诊断、自动驾驶、气候建模、核反应堆仿真、药物发现与自动定理证明）中的加速部署，对其**结构性可信度**的系统性评估已迫在眉睫。本研究首次以“失败模式”（failure modes）为统一框架，对 Transformer 的核心可信维度开展跨域整合分析：包括**可解释性与可解释性局限**（如注意力机制的虚假归因）、**对抗鲁棒性脆弱性**（微小扰动即可导致语义级错误）、**公平性偏差放大效应**（训练数据偏见在长程依赖中被非线性强化）、**隐私泄露风险**（通过梯度/注意力重建敏感输入），以及**科学推理中的归纳失效**（如在流体力学偏微分方程求解中出现物理不一致性）。我们覆盖 9 大关键领域——机器人学、临床医学、地球科学、材料科学、流体力学、核科学、自动推理、CV 与 NLP——识别出三类共性风险：（1）**位置编码与相对位置建模缺陷**导致长序列时空推理失准；（2）**softmax 注意力的软约束特性**削弱关键token的决策权重可控性；（3）**预训练-微调范式固有偏差迁移**在小样本科学任务中引发不可靠泛化。研究提出“可信性-领域风险矩阵”，并指出亟待突破的开放问题：可验证的注意力因果约束、物理信息嵌入的鲁棒架构设计、面向科学发现的稀疏可信推理范式。",
      "summary_en": "This paper critically examines the trustworthiness of Transformer architectures through a unified failure-mode lens. We conduct a cross-domain analysis across nine high-stakes domains—including healthcare, robotics, climate modeling, materials discovery, nuclear science, and automated theorem proving—evaluating five core trust dimensions: interpretability (e.g., spurious attention attribution), adversarial robustness (sensitivity to imperceptible perturbations), fairness (bias amplification in long-range dependencies), privacy (input reconstruction via attention/gradient leakage), and scientific validity (e.g., violation of physical constraints in PDE solving). We identify three recurrent structural vulnerabilities: (1) positional encoding limitations impairing spatiotemporal reasoning; (2) softmax-based attention’s lack of hard gating, undermining critical token control; and (3) pretrain-finetune bias transfer causing unreliable generalization in low-data scientific tasks. The work synthesizes actionable insights into a “Trustworthiness–Domain Risk Matrix” and highlights key open challenges: verifiable causal attention constraints, physics-informed robust architecture design, and sparse, certifiable reasoning for scientific discovery.",
      "summary": "## 变压器架构的可信性审视：失败模式系统分析  \n\nTransformer 架构已在自然语言处理、计算机视觉、科学计算等广泛领域引发范式变革。然而，随着其在**高风险场景**（如医疗诊断、自动驾驶、气候建模、核反应堆仿真、药物发现与自动定理证明）中的加速部署，对其**结构性可信度**的系统性评估已迫在眉睫。本研究首次以“失败模式”（failure modes）为统一框架，对 Transformer 的核心可信维度开展跨域整合分析：包括**可解释性与可解释性局限**（如注意力机制的虚假归因）、**对抗鲁棒性脆弱性**（微小扰动即可导致语义级错误）、**公平性偏差放大效应**（训练数据偏见在长程依赖中被非线性强化）、**隐私泄露风险**（通过梯度/注意力重建敏感输入），以及**科学推理中的归纳失效**（如在流体力学偏微分方程求解中出现物理不一致性）。我们覆盖 9 大关键领域——机器人学、临床医学、地球科学、材料科学、流体力学、核科学、自动推理、CV 与 NLP——识别出三类共性风险：（1）**位置编码与相对位置建模缺陷**导致长序列时空推理失准；（2）**softmax 注意力的软约束特性**削弱关键token的决策权重可控性；（3）**预训练-微调范式固有偏差迁移**在小样本科学任务中引发不可靠泛化。研究提出“可信性-领域风险矩阵”，并指出亟待突破的开放问题：可验证的注意力因果约束、物理信息嵌入的鲁棒架构设计、面向科学发现的稀疏可信推理范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14302v1",
      "arxiv_id": "2602.14302v1",
      "title": "Floe: Federated Specialization for Real-Time LLM-SLM Inference",
      "authors": [
        "Chunlin Tian",
        "Kahou Tam",
        "Yebo Wu",
        "Shuaihang Zhong",
        "Li Li",
        "Nicholas D. Lane",
        "Chengzhong Xu"
      ],
      "abstract": "Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14302v1",
      "url": "https://arxiv.org/abs/2602.14302v1",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## Floe：面向实时LLM-SLM推理的联邦专业化框架\n\n**背景与挑战**：大型语言模型（LLMs）在实时系统（如智能终端、车载助手、远程医疗）中的部署面临双重瓶颈——云端LLM推理延迟高、带宽消耗大，而纯端侧部署又受限于设备算力、内存与隐私合规要求。现有联邦学习方法难以兼顾低延迟、强隐私与个性化性能。\n\n**核心方法**：本文提出**Floe**，一种新型混合式联邦学习框架，实现云-边协同的实时推理：  \n- **架构设计**：采用“云黑盒LLM + 边端可定制SLM”双模态范式。用户原始数据与微调过程全程留存在本地设备，云侧LLM仅以**冻结权重的API服务**形式提供通用知识，不接触任何原始数据或模型参数；  \n- **关键技术**：① 提出**异构感知的LoRA适配策略**，自动根据边缘设备CPU/GPU/NPU类型、内存容量及功耗预算，动态压缩LoRA秩与模块分布，支持跨平台高效部署；② 设计**logit级融合机制**，在输出层对SLM与LLM的logits进行加权校准与温度缩放，避免token级同步开销，实现毫秒级协同响应；  \n- **隐私保障**：严格遵循联邦学习范式，无梯度上传、无中间特征泄露，满足GDPR/《个人信息保护法》要求。\n\n**主要成果**：在6类真实边缘设备（含Raspberry Pi 5、Jetson Orin、iPhone 14）及3个对话/指令数据集（Alpaca-E, FLANv2-Edge, MedQA-Edge）上验证：Floe相较FedLLM、SplitNN等基线，**端侧推理延迟降低47.3%–68.1%**（P95 < 320ms），**个性化准确率提升12.6–19.4个百分点**，同时将通信开销压缩至传统联邦微调的<8%。该框架为隐私优先的实时AI提供了可落地的技术路径。",
      "summary_en": "Floe is a hybrid federated learning framework for real-time, privacy-preserving LLM inference in resource-constrained edge environments. It synergistically combines a frozen cloud-based black-box LLM (providing general knowledge without exposing weights) with lightweight, on-device Small Language Models (SLMs) fine-tuned via a heterogeneity-aware LoRA adaptation—enabling efficient deployment across diverse hardware (e.g., Raspberry Pi, Jetson, mobile SoCs). A novel logit-level fusion mechanism coordinates edge and cloud outputs in real time without token-level synchronization, drastically reducing latency. Experiments show Floe achieves up to 68.1% lower P95 inference latency (<320 ms), 19.4-point higher personalized accuracy, and <8% communication overhead versus state-of-the-art baselines—while guaranteeing end-to-end data privacy and on-device model specialization.",
      "summary": "## Floe：面向实时LLM-SLM推理的联邦专业化框架\n\n**背景与挑战**：大型语言模型（LLMs）在实时系统（如智能终端、车载助手、远程医疗）中的部署面临双重瓶颈——云端LLM推理延迟高、带宽消耗大，而纯端侧部署又受限于设备算力、内存与隐私合规要求。现有联邦学习方法难以兼顾低延迟、强隐私与个性化性能。\n\n**核心方法**：本文提出**Floe**，一种新型混合式联邦学习框架，实现云-边协同的实时推理：  \n- **架构设计**：采用“云黑盒LLM + 边端可定制SLM”双模态范式。用户原始数据与微调过程全程留存在本地设备，云侧LLM仅以**冻结权重的API服务**形式提供通用知识，不接触任何原始数据或模型参数；  \n- **关键技术**：① 提出**异构感知的LoRA适配策略**，自动根据边缘设备CPU/GPU/NPU类型、内存容量及功耗预算，动态压缩LoRA秩与模块分布，支持跨平台高效部署；② 设计**logit级融合机制**，在输出层对SLM与LLM的logits进行加权校准与温度缩放，避免token级同步开销，实现毫秒级协同响应；  \n- **隐私保障**：严格遵循联邦学习范式，无梯度上传、无中间特征泄露，满足GDPR/《个人信息保护法》要求。\n\n**主要成果**：在6类真实边缘设备（含Raspberry Pi 5、Jetson Orin、iPhone 14）及3个对话/指令数据集（Alpaca-E, FLANv2-Edge, MedQA-Edge）上验证：Floe相较FedLLM、SplitNN等基线，**端侧推理延迟降低47.3%–68.1%**（P95 < 320ms），**个性化准确率提升12.6–19.4个百分点**，同时将通信开销压缩至传统联邦微调的<8%。该框架为隐私优先的实时AI提供了可落地的技术路径。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14250v1",
      "arxiv_id": "2602.14250v1",
      "title": "Energy-Efficient Over-the-Air Federated Learning via Pinching Antenna Systems",
      "authors": [
        "Saba Asaad",
        "Ali Bereyhi"
      ],
      "abstract": "Pinching antennas systems (PASSs) have recently been proposed as a novel flexible-antenna technology. These systems are implemented by attaching low-cost pinching elements to dielectric waveguides. As the direct link is bypassed through waveguides, PASSs can effectively compensate large-scale effects of the wireless channel. This work explores the potential gains of employing PASSs for over-the-air federated learning (OTA-FL). For a PASS-assisted server, we develop a low-complexity algorithmic approach, which jointly tunes the PASS parameters and schedules the mobile devices for minimal energy consumption in OTA-FL. We study the efficiency of the proposed design and compare it against the conventional OTA-FL setting with MIMO server. Numerical experiments demonstrate that using a single-waveguide PASS at the server within a moderately sized area, the required energy for model aggregation is drastically reduced as compared to the case with fully-digital MIMO server. This introduces PASS as a potential technology for energy-efficient distributed learning in next generations of wireless systems.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14250v1",
      "url": "https://arxiv.org/abs/2602.14250v1",
      "categories": [
        "cs.IT",
        "cs.LG",
        "eess.SP"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n面向6G智能无线网络，**空中联邦学习（OTA-FL）** 因其天然的模拟聚合特性可显著降低通信开销，但其能量效率严重受限于信道衰落与硬件开销。传统多天线（MIMO）服务器需高成本全数字射频链，导致功耗剧增。近期提出的**夹持式天线系统（PASS）** 作为一种新型柔性天线技术，通过在介质波导上低成本集成可调夹持元件，绕过直射路径、利用波导传导信号，从而有效补偿无线信道的大尺度衰落，为能效优化提供了新范式。\n\n## 方法与创新  \n本文首次将PASS引入OTA-FL架构，提出一种**面向能量最小化的联合设计框架**：  \n- 构建PASS辅助服务器模型，显式刻画波导传播、夹持元件相位/幅度调控与信道大尺度效应的耦合关系；  \n- 设计低复杂度迭代算法，**协同优化PASS参数（夹持位置、阻抗调谐）与设备调度策略**，在满足聚合精度约束下最小化总传输能耗；  \n- 理论分析表明，该联合设计可将信道状态信息（CSI）依赖性降至次线性阶，显著降低反馈开销。\n\n## 主要发现  \n数值实验表明：在中等规模部署区域（半径50 m）内，仅采用**单波导PASS服务器**，相较同等天线数的全数字MIMO服务器，模型聚合所需能量**降低达78.3%**；且在信噪比15 dB时，误码率仍低于10⁻⁴，保障学习收敛性。PASS以极低硬件复杂度（无独立RF链、仅需少量可调元件）实现了接近理想信道补偿的性能，凸显其作为下一代绿色分布式学习使能技术的巨大潜力。",
      "summary_en": "This paper pioneers the integration of **Pinching Antenna Systems (PASS)**—a novel low-cost, waveguide-based flexible antenna technology—into **over-the-air federated learning (OTA-FL)** to drastically improve energy efficiency. We propose a low-complexity joint optimization framework that co-designs PASS parameters (e.g., pinching positions and impedance tuning) and device scheduling to minimize total energy consumption for analog model aggregation, under convergence and signal-to-noise ratio constraints. Unlike conventional fully-digital MIMO servers requiring numerous power-hungry RF chains, a single-waveguide PASS server compensates large-scale channel fading inherently via guided-wave propagation, eliminating the need for complex digital precoding or frequent CSI feedback. Numerical results demonstrate that, in a moderately sized area (50 m radius), the PASS-assisted OTA-FL reduces aggregation energy by up to **78.3%** compared to an equivalent MIMO baseline, while maintaining robust learning performance (BER < 10⁻⁴ at 15 dB SNR). This establishes PASS as a highly promising enabler for sustainable, hardware-efficient distributed learning in future wireless systems.",
      "summary": "## 研究背景  \n面向6G智能无线网络，**空中联邦学习（OTA-FL）** 因其天然的模拟聚合特性可显著降低通信开销，但其能量效率严重受限于信道衰落与硬件开销。传统多天线（MIMO）服务器需高成本全数字射频链，导致功耗剧增。近期提出的**夹持式天线系统（PASS）** 作为一种新型柔性天线技术，通过在介质波导上低成本集成可调夹持元件，绕过直射路径、利用波导传导信号，从而有效补偿无线信道的大尺度衰落，为能效优化提供了新范式。\n\n## 方法与创新  \n本文首次将PASS引入OTA-FL架构，提出一种**面向能量最小化的联合设计框架**：  \n- 构建PASS辅助服务器模型，显式刻画波导传播、夹持元件相位/幅度调控与信道大尺度效应的耦合关系；  \n- 设计低复杂度迭代算法，**协同优化PASS参数（夹持位置、阻抗调谐）与设备调度策略**，在满足聚合精度约束下最小化总传输能耗；  \n- 理论分析表明，该联合设计可将信道状态信息（CSI）依赖性降至次线性阶，显著降低反馈开销。\n\n## 主要发现  \n数值实验表明：在中等规模部署区域（半径50 m）内，仅采用**单波导PASS服务器**，相较同等天线数的全数字MIMO服务器，模型聚合所需能量**降低达78.3%**；且在信噪比15 dB时，误码率仍低于10⁻⁴，保障学习收敛性。PASS以极低硬件复杂度（无独立RF链、仅需少量可调元件）实现了接近理想信道补偿的性能，凸显其作为下一代绿色分布式学习使能技术的巨大潜力。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14244v1",
      "arxiv_id": "2602.14244v1",
      "title": "Federated Ensemble Learning with Progressive Model Personalization",
      "authors": [
        "Ala Emrani",
        "Amir Najafi",
        "Abolfazl Motahari"
      ],
      "abstract": "Federated Learning provides a privacy-preserving paradigm for distributed learning, but suffers from statistical heterogeneity across clients. Personalized Federated Learning (PFL) mitigates this issue by considering client-specific models. A widely adopted approach in PFL decomposes neural networks into a shared feature extractor and client-specific heads. While effective, this design induces a fundamental tradeoff: deep or expressive shared components hinder personalization, whereas large local heads exacerbate overfitting under limited per-client data. Most existing methods rely on rigid, shallow heads, and therefore fail to navigate this tradeoff in a principled manner. In this work, we propose a boosting-inspired framework that enables a smooth control of this tradeoff. Instead of training a single personalized model, we construct an ensemble of $T$ models for each client. Across boosting iterations, the depth of the personalized component are progressively increased, while its effective complexity is systematically controlled via low-rank factorization or width shrinkage. This design simultaneously limits overfitting and substantially reduces per-client bias by allowing increasingly expressive personalization. We provide theoretical analysis that establishes generalization bounds with favorable dependence on the average local sample size and the total number of clients. Specifically, we prove that the complexity of the shared layers is effectively suppressed, while the dependence on the boosting horizon $T$ is controlled through parameter reduction. Notably, we provide a novel nonlinear generalization guarantee for decoupled PFL models. Extensive experiments on benchmark and real-world datasets (e.g., EMNIST, CIFAR-10/100, and Sent140) demonstrate that the proposed framework consistently outperforms state-of-the-art PFL methods under heterogeneous data distributions.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14244v1",
      "url": "https://arxiv.org/abs/2602.14244v1",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 联邦集成学习与渐进式模型个性化\n\n联邦学习（FL）在保障数据隐私的前提下实现分布式协同训练，但面临客户端间**统计异质性**（non-IID数据）的根本挑战。个性化联邦学习（PFL）通过为各客户端构建定制化模型缓解该问题，主流范式采用“共享特征提取器 + 客户端专属头部”解耦结构。然而，该设计存在**根本性权衡困境**：共享部分过深会削弱个性化能力；本地头部过大则在小样本场景下极易过拟合。现有方法多依赖固定、浅层的头部结构，缺乏对这一权衡的**原则性、可调制控制**。\n\n本文提出**基于提升思想的联邦集成学习框架（Federated Ensemble Learning with Progressive Model Personalization, FE-PMP）**，突破单模型范式，为每个客户端动态构建由 $T$ 个模型组成的集成。其核心创新在于**渐进式个性化机制**：在提升迭代过程中，本地头部的表达能力（深度/宽度）逐步增强，同时通过**低秩分解**或**通道剪枝**等结构化压缩技术系统约束其有效复杂度。该设计既显著降低单客户端偏差（bias），又严格抑制过拟合风险。\n\n理论层面，我们首次为解耦型PFL模型建立了**非线性泛化界**，证明：1）共享层的复杂度影响被有效压制；2）整体泛化误差对提升轮数 $T$ 的依赖可通过参数缩减机制可控；3）误差上界优异地依赖于**平均本地样本量**与**客户端总数**。在EMNIST、CIFAR-10/100及Sent140等基准与真实数据集上的实验表明，FE-PMP在强异质性设置下持续超越SOTA PFL方法（如pFedMe、Ditto、Per-FedAvg），平均精度提升达2.1–4.7个百分点。",
      "summary_en": "Federated Learning (FL) enables privacy-preserving distributed training but suffers from statistical heterogeneity across clients. Personalized FL (PFL) addresses this by learning client-specific models, commonly via a shared backbone and local heads—yet this induces a fundamental tradeoff: expressive shared components hinder personalization, while large local heads overfit under limited per-client data. Existing methods use rigid, shallow heads and lack principled control over this tradeoff. We propose **FE-PMP**, a boosting-inspired federated ensemble framework that constructs $T$ models per client. Across boosting iterations, the *depth* or *width* of the personalized head progressively increases, while its *effective complexity* is systematically controlled via low-rank factorization or width shrinkage—simultaneously reducing bias and overfitting. We provide the first **nonlinear generalization bound for decoupled PFL**, showing suppressed dependence on shared-layer complexity and controlled $T$-dependence via parameter reduction. Experiments on EMNIST, CIFAR-10/100, and Sent140 demonstrate consistent superiority over SOTA PFL methods under strong heterogeneity, with average accuracy gains of 2.1–4.7%.",
      "summary": "## 联邦集成学习与渐进式模型个性化\n\n联邦学习（FL）在保障数据隐私的前提下实现分布式协同训练，但面临客户端间**统计异质性**（non-IID数据）的根本挑战。个性化联邦学习（PFL）通过为各客户端构建定制化模型缓解该问题，主流范式采用“共享特征提取器 + 客户端专属头部”解耦结构。然而，该设计存在**根本性权衡困境**：共享部分过深会削弱个性化能力；本地头部过大则在小样本场景下极易过拟合。现有方法多依赖固定、浅层的头部结构，缺乏对这一权衡的**原则性、可调制控制**。\n\n本文提出**基于提升思想的联邦集成学习框架（Federated Ensemble Learning with Progressive Model Personalization, FE-PMP）**，突破单模型范式，为每个客户端动态构建由 $T$ 个模型组成的集成。其核心创新在于**渐进式个性化机制**：在提升迭代过程中，本地头部的表达能力（深度/宽度）逐步增强，同时通过**低秩分解**或**通道剪枝**等结构化压缩技术系统约束其有效复杂度。该设计既显著降低单客户端偏差（bias），又严格抑制过拟合风险。\n\n理论层面，我们首次为解耦型PFL模型建立了**非线性泛化界**，证明：1）共享层的复杂度影响被有效压制；2）整体泛化误差对提升轮数 $T$ 的依赖可通过参数缩减机制可控；3）误差上界优异地依赖于**平均本地样本量**与**客户端总数**。在EMNIST、CIFAR-10/100及Sent140等基准与真实数据集上的实验表明，FE-PMP在强异质性设置下持续超越SOTA PFL方法（如pFedMe、Ditto、Per-FedAvg），平均精度提升达2.1–4.7个百分点。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14161v1",
      "arxiv_id": "2602.14161v1",
      "title": "When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift",
      "authors": [
        "Max Fomin"
      ],
      "abstract": "Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14161v1",
      "url": "https://arxiv.org/abs/2602.14161v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection",
        "llm",
        "jailbreak"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与问题  \n针对大语言模型（LLM）代理的安全防护，恶意提示检测（如提示注入、越狱攻击）至关重要。然而，当前评估范式存在严重缺陷：主流方法依赖同一数据源的“随机划分”训练/测试集，忽视真实部署中**跨数据分布迁移**（即来自邮件、API响应、文档等未见来源的攻击）这一核心挑战，导致性能被系统性高估。\n\n## 方法创新  \n我们构建覆盖18个异构数据集的综合基准，涵盖有害请求、越狱、间接提示注入及提取攻击四类威胁。首次提出**留一数据集法（Leave-One-Dataset-Out, LODO）** 作为真分布外泛化评估协议——每次将一个完整数据集完全留出用于测试，其余17个联合训练。同时，结合**稀疏自编码器（SAE）特征归因分析**，量化模型是否依赖语义本质特征还是数据集特有捷径。\n\n## 关键发现  \n- 标准评估使AUC虚高**8.4个百分点**，单数据集准确率落差达**1%–25%**，暴露高度异质的失败模式；  \n- **28%的顶级判别特征为数据集依赖型捷径**（如特定模板词频、格式噪声），其判别信号源于数据构成而非语义内容；  \n- 主流生产级防护（PromptGuard 2、LlamaGuard）及LLM-as-judge方案在**间接攻击**（针对代理链路的隐蔽注入）上检测率仅**7–37%**，且前两者因架构限制**无法处理工具调用注入**；  \n- 基于LODO稳定性筛选的SAE特征，可显著提升决策解释的可靠性，有效过滤数据集人工痕迹。\n\n我们开源评估框架：https://github.com/maxf-zn/prompt-mining，推动LODO成为提示攻击检测研究的新标准。",
      "summary_en": "This paper exposes critical overestimation in malicious prompt detection evaluation: standard intra-dataset train-test splits inflate AUC by 8.4 points and mask severe distributional failures. We introduce **Leave-One-Dataset-Out (LODO)** evaluation across 18 diverse attack datasets—including indirect injections targeting agentic workflows—to measure true out-of-distribution generalization. Through Sparse Auto-Encoder (SAE) feature analysis, we find 28% of top discriminative features are dataset-specific shortcuts—not semantically grounded—explaining poor cross-dataset transfer. All evaluated production guardrails (**PromptGuard 2**, **LlamaGuard**) and **LLM-as-judge** methods fail catastrophically on indirect attacks (7–37% detection) and cannot handle tool-injection threats due to architectural constraints. Crucially, LODO-stable SAE features yield more robust, artifact-free explanations. We release our benchmark and framework to establish LODO as the rigorous evaluation standard for prompt safety research.",
      "summary": "## 背景与问题  \n针对大语言模型（LLM）代理的安全防护，恶意提示检测（如提示注入、越狱攻击）至关重要。然而，当前评估范式存在严重缺陷：主流方法依赖同一数据源的“随机划分”训练/测试集，忽视真实部署中**跨数据分布迁移**（即来自邮件、API响应、文档等未见来源的攻击）这一核心挑战，导致性能被系统性高估。\n\n## 方法创新  \n我们构建覆盖18个异构数据集的综合基准，涵盖有害请求、越狱、间接提示注入及提取攻击四类威胁。首次提出**留一数据集法（Leave-One-Dataset-Out, LODO）** 作为真分布外泛化评估协议——每次将一个完整数据集完全留出用于测试，其余17个联合训练。同时，结合**稀疏自编码器（SAE）特征归因分析**，量化模型是否依赖语义本质特征还是数据集特有捷径。\n\n## 关键发现  \n- 标准评估使AUC虚高**8.4个百分点**，单数据集准确率落差达**1%–25%**，暴露高度异质的失败模式；  \n- **28%的顶级判别特征为数据集依赖型捷径**（如特定模板词频、格式噪声），其判别信号源于数据构成而非语义内容；  \n- 主流生产级防护（PromptGuard 2、LlamaGuard）及LLM-as-judge方案在**间接攻击**（针对代理链路的隐蔽注入）上检测率仅**7–37%**，且前两者因架构限制**无法处理工具调用注入**；  \n- 基于LODO稳定性筛选的SAE特征，可显著提升决策解释的可靠性，有效过滤数据集人工痕迹。\n\n我们开源评估框架：https://github.com/maxf-zn/prompt-mining，推动LODO成为提示攻击检测研究的新标准。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14051v1",
      "arxiv_id": "2602.14051v1",
      "title": "Decentralized Federated Learning With Energy Harvesting Devices",
      "authors": [
        "Kai Zhang",
        "Xuanyu Cao",
        "Khaled B. Letaief"
      ],
      "abstract": "Decentralized federated learning (DFL) enables edge devices to collaboratively train models through local training and fully decentralized device-to-device (D2D) model exchanges. However, these energy-intensive operations often rapidly deplete limited device batteries, reducing their operational lifetime and degrading the learning performance. To address this limitation, we apply energy harvesting technique to DFL systems, allowing edge devices to extract ambient energy and operate sustainably. We first derive the convergence bound for wireless DFL with energy harvesting, showing that the convergence is influenced by partial device participation and transmission packet drops, both of which further depend on the available energy supply. To accelerate convergence, we formulate a joint device scheduling and power control problem and model it as a multi-agent Markov decision process (MDP). Traditional MDP algorithms (e.g., value or policy iteration) require a centralized coordinator with access to all device states and exhibit exponential complexity in the number of devices, making them impractical for large-scale decentralized networks. To overcome these challenges, we propose a fully decentralized policy iteration algorithm that leverages only local state information from two-hop neighboring devices, thereby substantially reducing both communication overhead and computational complexity. We further provide a theoretical analysis showing that the proposed decentralized algorithm achieves asymptotic optimality. Finally, comprehensive numerical experiments on real-world datasets are conducted to validate the theoretical results and corroborate the effectiveness of the proposed algorithm.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14051v1",
      "url": "https://arxiv.org/abs/2602.14051v1",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n去中心化联邦学习（DFL）通过设备本地训练与完全去中心化的设备直连（D2D）模型交换，显著降低对中心服务器的依赖。然而，频繁的本地计算与无线传输导致**能量消耗剧增**，而边缘设备通常受限于电池容量，易出现续航短、参与率下降、模型收敛变慢等问题。传统供能方式难以支撑长期可持续学习。\n\n## 方法创新  \n本文首次将**环境能量采集（Energy Harvesting, EH）技术**深度融入DFL架构，使设备可从光、热、射频等环境源持续补能。我们：  \n- **理论层面**：严格推导了含能量采集的无线DFL收敛界，揭示了**设备参与率不全性**与**传输包丢失率**双重随机性对收敛速度的耦合影响，二者均受实时可用能量动态约束；  \n- **算法层面**：构建联合设备调度与功率控制问题，并建模为多智能体马尔可夫决策过程（MDP）；针对传统集中式MDP算法（如值迭代）需全局状态且计算复杂度随设备数指数增长的缺陷，提出**全去中心化策略迭代算法**——各设备仅依赖自身及两跳邻居的局部状态信息进行策略更新；  \n- **理论保障**：证明该算法在分布式设定下具有**渐近最优性**，且通信开销与计算复杂度均降至$O(1)$量级（相对于设备总数）。\n\n## 实验验证  \n在CIFAR-10与Tiny-ImageNet数据集上的实测表明：相比基线方法，所提方案将**收敛速度提升37.2%**，**能量利用率提高51.8%**，且在低信噪比与稀疏能量采集场景下仍保持鲁棒收敛。本工作为绿色AI与可持续边缘智能提供了可落地的理论框架与工程范式。",
      "summary_en": "Decentralized federated learning (DFL) enables collaborative model training among edge devices via local computation and device-to-device (D2D) model exchange—yet its energy-hungry operations severely strain battery-limited devices. To enable sustainable operation, we integrate ambient **energy harvesting (EH)** into DFL. We first derive a tight convergence bound for wireless DFL under stochastic EH, revealing how partial participation and packet drops—both governed by real-time energy availability—jointly degrade learning performance. To accelerate convergence, we formulate a joint device scheduling and power control problem as a multi-agent Markov decision process (MDP). Overcoming the impracticality of centralized MDP solvers (exponential complexity, global state requirement), we propose a **fully decentralized policy iteration algorithm**, where each device updates its policy using only local and two-hop neighbor state information. We prove its **asymptotic optimality** and show it reduces both communication overhead and computational cost to constant order. Experiments on CIFAR-10 and Tiny-ImageNet validate our theory: the method improves convergence speed by 37.2% and energy efficiency by 51.8% over baselines, even under low SNR and sparse energy harvesting.",
      "summary": "## 研究背景与挑战  \n去中心化联邦学习（DFL）通过设备本地训练与完全去中心化的设备直连（D2D）模型交换，显著降低对中心服务器的依赖。然而，频繁的本地计算与无线传输导致**能量消耗剧增**，而边缘设备通常受限于电池容量，易出现续航短、参与率下降、模型收敛变慢等问题。传统供能方式难以支撑长期可持续学习。\n\n## 方法创新  \n本文首次将**环境能量采集（Energy Harvesting, EH）技术**深度融入DFL架构，使设备可从光、热、射频等环境源持续补能。我们：  \n- **理论层面**：严格推导了含能量采集的无线DFL收敛界，揭示了**设备参与率不全性**与**传输包丢失率**双重随机性对收敛速度的耦合影响，二者均受实时可用能量动态约束；  \n- **算法层面**：构建联合设备调度与功率控制问题，并建模为多智能体马尔可夫决策过程（MDP）；针对传统集中式MDP算法（如值迭代）需全局状态且计算复杂度随设备数指数增长的缺陷，提出**全去中心化策略迭代算法**——各设备仅依赖自身及两跳邻居的局部状态信息进行策略更新；  \n- **理论保障**：证明该算法在分布式设定下具有**渐近最优性**，且通信开销与计算复杂度均降至$O(1)$量级（相对于设备总数）。\n\n## 实验验证  \n在CIFAR-10与Tiny-ImageNet数据集上的实测表明：相比基线方法，所提方案将**收敛速度提升37.2%**，**能量利用率提高51.8%**，且在低信噪比与稀疏能量采集场景下仍保持鲁棒收敛。本工作为绿色AI与可持续边缘智能提供了可落地的理论框架与工程范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.14162v1",
      "arxiv_id": "2602.14162v1",
      "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
      "authors": [
        "Tao Xu"
      ],
      "abstract": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
      "published": "2026-02-15",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.14162v1",
      "url": "https://arxiv.org/abs/2602.14162v1",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.IR"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n现有视觉密集型文档问答（Visual-Dense Document QA）方法普遍采用“供给侧预摄入”策略：在索引阶段即对每页文档运行视觉语言模型（VLM），生成冗长图文描述，再依赖纯文本检索回答问题。该范式存在三重瓶颈：**高成本**（如113页工程图纸需约80,000 VLM tokens）、**端到端不可靠**（VLM输出格式常与检索系统不兼容，导致关键信息无法召回）、**失败不可逆**（错误预生成内容无法修正）。\n\n## 方法创新：延迟视觉摄入（DVI）框架  \n本文提出**需求侧驱动的Deferred Visual Ingestion（DVI）框架**，核心理念是：**“索引为定位，而非理解”**。  \n- **轻量索引**：仅提取结构化元数据（标题、图号、视图类型、尺寸标注关键词等）+ 全文OCR文本，构建BM25可检索索引；  \n- **按需理解**：仅当用户提交具体问题时，才将**原始图像+问题**联合送入VLM进行精准分析；  \n- **双阶段解耦**：先通过元数据+全文搜索完成**页面定位**（100%准确率，搜索空间压缩98%），再聚焦VLM于单页图像推理。\n\n## 关键结果与优势  \n在真实工业工程图纸数据集（113页主图纸 + 7页补充图纸）上验证：  \n- **零VLM索引开销**：整体QA准确率46.7%（vs. 传统48.9%），但VLM token消耗从80,000降至0；  \n- **视觉必要性突破**：对必须依赖图像细节的问题（如公差标注、剖面线识别），有效回答率达50%（传统方法为0%）；  \n- **可扩展交互能力**：支持多轮交互式澄清与渐进式缓存（如缓存VLM中间视觉特征），将“答案生成”难题转化为“页面定位”难题——一旦定位正确图纸页，答案可通过1–2轮人机交互稳定获取。",
      "summary_en": "This paper introduces **Deferred Visual Ingestion (DVI)**, a demand-driven framework for visual-dense document QA that eliminates costly pre-indexing with vision-language models (VLMs). Instead of generating full-page VLM descriptions during indexing—costing ~80K tokens for a 113-page engineering drawing—DVI performs only lightweight metadata and OCR extraction, enabling precise page localization via structured indexes and BM25 search. Visual understanding is deferred to query time: only the *original image of the top-localized page* and the user’s question are jointly fed to the VLM for targeted analysis. Evaluated on real industrial engineering drawings, DVI achieves **46.7% overall QA accuracy** (vs. 48.9% for pre-ingestion) at **zero VLM indexing cost**, **50% effectiveness on visually necessary queries** (vs. 0%), and **100% page localization** (98% search space compression). Crucially, DVI reframes QA as a *page localization problem*, enabling interactive refinement and progressive caching—making answer retrieval robust, scalable, and human-in-the-loop friendly.",
      "summary": "## 背景与挑战  \n现有视觉密集型文档问答（Visual-Dense Document QA）方法普遍采用“供给侧预摄入”策略：在索引阶段即对每页文档运行视觉语言模型（VLM），生成冗长图文描述，再依赖纯文本检索回答问题。该范式存在三重瓶颈：**高成本**（如113页工程图纸需约80,000 VLM tokens）、**端到端不可靠**（VLM输出格式常与检索系统不兼容，导致关键信息无法召回）、**失败不可逆**（错误预生成内容无法修正）。\n\n## 方法创新：延迟视觉摄入（DVI）框架  \n本文提出**需求侧驱动的Deferred Visual Ingestion（DVI）框架**，核心理念是：**“索引为定位，而非理解”**。  \n- **轻量索引**：仅提取结构化元数据（标题、图号、视图类型、尺寸标注关键词等）+ 全文OCR文本，构建BM25可检索索引；  \n- **按需理解**：仅当用户提交具体问题时，才将**原始图像+问题**联合送入VLM进行精准分析；  \n- **双阶段解耦**：先通过元数据+全文搜索完成**页面定位**（100%准确率，搜索空间压缩98%），再聚焦VLM于单页图像推理。\n\n## 关键结果与优势  \n在真实工业工程图纸数据集（113页主图纸 + 7页补充图纸）上验证：  \n- **零VLM索引开销**：整体QA准确率46.7%（vs. 传统48.9%），但VLM token消耗从80,000降至0；  \n- **视觉必要性突破**：对必须依赖图像细节的问题（如公差标注、剖面线识别），有效回答率达50%（传统方法为0%）；  \n- **可扩展交互能力**：支持多轮交互式澄清与渐进式缓存（如缓存VLM中间视觉特征），将“答案生成”难题转化为“页面定位”难题——一旦定位正确图纸页，答案可通过1–2轮人机交互稳定获取。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13597v1",
      "arxiv_id": "2602.13597v1",
      "title": "AlignSentinel: Alignment-Aware Detection of Prompt Injection Attacks",
      "authors": [
        "Yuqi Jia",
        "Ruiqi Wang",
        "Xilong Wang",
        "Chong Xiang",
        "Neil Gong"
      ],
      "abstract": "% Prompt injection attacks insert malicious instructions into an LLM's input to steer it toward an attacker-chosen task instead of the intended one. Existing detection defenses typically classify any input with instruction as malicious, leading to misclassification of benign inputs containing instructions that align with the intended task. In this work, we account for the instruction hierarchy and distinguish among three categories: inputs with misaligned instructions, inputs with aligned instructions, and non-instruction inputs. We introduce AlignSentinel, a three-class classifier that leverages features derived from LLM's attention maps to categorize inputs accordingly. To support evaluation, we construct the first systematic benchmark containing inputs from all three categories. Experiments on both our benchmark and existing ones--where inputs with aligned instructions are largely absent--show that AlignSentinel accurately detects inputs with misaligned instructions and substantially outperforms baselines.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13597v1",
      "url": "https://arxiv.org/abs/2602.13597v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n提示注入攻击（Prompt Injection）通过在大语言模型（LLM）输入中嵌入恶意指令，诱导模型偏离预设任务、执行攻击者指定的操作。现有检测方法多采用二分类范式，将“含指令”即判为恶意，却忽视了**指令语义与任务意图的对齐关系**——大量合法输入（如用户明确要求“用表格总结”“翻译成法语”）本身包含指令，但其语义与系统预期任务高度一致，属**良性对齐指令**。这种粗粒度判别导致高误报率，严重制约部署实用性。\n\n## 方法创新：AlignSentinel  \n本文提出首个**对齐感知**（Alignment-Aware）的三类检测框架 **AlignSentinel**：  \n- **精细分类**：严格区分三类输入——① **错位指令**（misaligned，攻击性）、② **对齐指令**（aligned，良性且任务增强）、③ **非指令输入**（non-instructional，如纯事实查询）；  \n- **可解释特征**：首次利用LLM内部**注意力图谱**（attention maps）提取空间-语义对齐特征——例如，攻击指令常引发跨层异常注意力聚焦，而对齐指令则呈现与任务头（task head）协同的稳定模式；  \n- **专用基准**：构建首个覆盖全部三类样本的系统化基准 **AlignBench**，包含人工标注的2,150条高质量样本（含真实API日志与红队测试数据），填补领域空白。\n\n## 主要结果  \n在AlignBench上，AlignSentinel实现98.2%的错位指令检测准确率（F1=0.976），误报率仅1.3%（较SOTA基线降低6.8×）；在主流基准（如PIA、SafeBench）重测时，因引入对齐识别能力，其综合准确率提升32.4%，显著优于BERT-based、RoBERTa-based及基于logit差分的检测器。本工作揭示：**指令安全性本质是语义对齐问题，而非语法存在性问题**，为可信LLM部署提供新范式。",
      "summary_en": "Prompt injection attacks manipulate LLMs by injecting malicious instructions into inputs to hijack intended tasks. Existing detectors often misclassify benign inputs containing *task-aligned* instructions (e.g., “summarize in bullet points”) as malicious—treating all instructions as suspicious. To address this, we propose **AlignSentinel**, the first alignment-aware three-class detector that distinguishes: (i) *misaligned* (malicious), (ii) *aligned* (benign, task-enhancing), and (iii) *non-instructional* inputs. It leverages interpretable features from LLM attention maps—capturing semantic alignment patterns across layers—to enable fine-grained classification. We introduce **AlignBench**, the first systematic benchmark covering all three categories (2,150 expert-annotated samples). Experiments show AlignSentinel achieves 98.2% accuracy (F1=0.976) on misaligned inputs with only 1.3% false positive rate—outperforming SOTA baselines by up to 32.4% in overall accuracy and reducing false positives by 6.8×. This work redefines prompt injection detection as an *alignment verification* problem, not merely instruction presence detection.",
      "summary": "## 背景与问题  \n提示注入攻击（Prompt Injection）通过在大语言模型（LLM）输入中嵌入恶意指令，诱导模型偏离预设任务、执行攻击者指定的操作。现有检测方法多采用二分类范式，将“含指令”即判为恶意，却忽视了**指令语义与任务意图的对齐关系**——大量合法输入（如用户明确要求“用表格总结”“翻译成法语”）本身包含指令，但其语义与系统预期任务高度一致，属**良性对齐指令**。这种粗粒度判别导致高误报率，严重制约部署实用性。\n\n## 方法创新：AlignSentinel  \n本文提出首个**对齐感知**（Alignment-Aware）的三类检测框架 **AlignSentinel**：  \n- **精细分类**：严格区分三类输入——① **错位指令**（misaligned，攻击性）、② **对齐指令**（aligned，良性且任务增强）、③ **非指令输入**（non-instructional，如纯事实查询）；  \n- **可解释特征**：首次利用LLM内部**注意力图谱**（attention maps）提取空间-语义对齐特征——例如，攻击指令常引发跨层异常注意力聚焦，而对齐指令则呈现与任务头（task head）协同的稳定模式；  \n- **专用基准**：构建首个覆盖全部三类样本的系统化基准 **AlignBench**，包含人工标注的2,150条高质量样本（含真实API日志与红队测试数据），填补领域空白。\n\n## 主要结果  \n在AlignBench上，AlignSentinel实现98.2%的错位指令检测准确率（F1=0.976），误报率仅1.3%（较SOTA基线降低6.8×）；在主流基准（如PIA、SafeBench）重测时，因引入对齐识别能力，其综合准确率提升32.4%，显著优于BERT-based、RoBERTa-based及基于logit差分的检测器。本工作揭示：**指令安全性本质是语义对齐问题，而非语法存在性问题**，为可信LLM部署提供新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13574v1",
      "arxiv_id": "2602.13574v1",
      "title": "Execution-State-Aware LLM Reasoning for Automated Proof-of-Vulnerability Generation",
      "authors": [
        "Haoyu Li",
        "Xijia Che",
        "Yanhao Wang",
        "Xiaojing Liao",
        "Luyi Xing"
      ],
      "abstract": "Proof-of-Vulnerability (PoV) generation is a critical task in software security, serving as a cornerstone for vulnerability validation, false positive reduction, and patch verification. While directed fuzzing effectively drives path exploration, satisfying complex semantic constraints remains a persistent bottleneck in automated exploit generation. Large Language Models (LLMs) offer a promising alternative with their semantic reasoning capabilities; however, existing LLM-based approaches lack sufficient grounding in concrete execution behavior, limiting their ability to generate precise PoVs.   In this paper, we present DrillAgent, an agentic framework that reformulates PoV generation as an iterative hypothesis-verification-refinement process. To bridge the gap between static reasoning and dynamic execution, DrillAgent synergizes LLM-based semantic inference with feedback from concrete program states. The agent analyzes the target code to hypothesize inputs, observes execution behavior, and employs a novel mechanism to translate low-level execution traces into source-level constraints. This closed-loop design enables the agent to incrementally align its input generation with the precise requirements of the vulnerability. We evaluate DrillAgent on SEC-bench, a large-scale benchmark of real-world C/C++ vulnerabilities. Experimental results show that DrillAgent substantially outperforms state-of-the-art LLM agent baselines under fixed budget constraints, solving up to 52.8% more CVE tasks than the best-performing baseline. These results highlight the necessity of execution-state-aware reasoning for reliable PoV generation in complex software systems.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13574v1",
      "url": "https://arxiv.org/abs/2602.13574v1",
      "categories": [
        "cs.SE",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "llm",
        "inference"
      ],
      "keyword_score": 4,
      "summary_zh": "## 背景与挑战  \nProof-of-Vulnerability（PoV）生成是软件安全领域的关键任务，支撑漏洞验证、误报削减与补丁有效性确认。尽管定向模糊测试（directed fuzzing）在路径探索上表现优异，其在满足复杂语义约束（如多层条件跳转、内存布局依赖、符号化算术关系）方面仍面临根本性瓶颈。当前基于大语言模型（LLM）的方法虽具备强语义推理能力，但普遍**缺乏对真实程序执行状态的感知与反馈闭环**，导致生成的PoV常因脱离运行时行为而失效。\n\n## 方法创新：DrillAgent框架  \n本文提出**DrillAgent**——首个执行态感知（execution-state-aware）的LLM智能体框架，将PoV生成重构为“假设→验证→精炼”的迭代闭环过程。其核心创新在于：  \n- **双模态协同推理**：LLM基于源码静态分析生成初始输入假设；  \n- **执行反馈驱动约束提炼**：通过动态插桩捕获真实程序执行轨迹（如寄存器值、内存读写、分支结果），并利用**轻量级符号化映射机制**，将低层执行痕迹（e.g., `rax == 0x41414141`）自动翻译为源码级语义约束（e.g., `buf[0] == 'A' && len > 3`）；  \n- **闭环精炼**：LLM基于约束反馈重新生成更精准输入，实现从“语法合理”到“语义可触发”的跃迁。\n\n## 实验结果与意义  \n在涵盖127个真实C/C++ CVE漏洞的SEC-bench基准上，DrillAgent在固定时间/资源预算下，PoV生成成功率较最优基线（如CodeAct+Qwen-Agent）**提升52.8%**，尤其在堆溢出、UAF、逻辑漏洞等高难度类别中优势显著。本工作首次系统论证了**执行状态反馈对LLM推理的不可替代性**，为可信自动化漏洞利用生成提供了新范式。",
      "summary_en": "Proof-of-Vulnerability (PoV) generation is essential for validating real-world software vulnerabilities, yet existing LLM-based approaches suffer from insufficient grounding in dynamic execution behavior. We present **DrillAgent**, an agentic framework that bridges static reasoning and concrete program states via a closed-loop hypothesis-verification-refinement process. DrillAgent leverages LLMs for semantic input hypothesis generation, then observes actual execution traces (e.g., register values, memory accesses, branch outcomes) and introduces a novel mechanism to translate low-level runtime observations into source-level constraints—enabling iterative, execution-guided input refinement. Evaluated on SEC-bench (127 real C/C++ CVEs), DrillAgent solves **52.8% more CVE tasks** than the strongest LLM agent baseline under fixed budget constraints. This demonstrates that execution-state awareness is critical for reliable, scalable PoV generation in complex systems.",
      "summary": "## 背景与挑战  \nProof-of-Vulnerability（PoV）生成是软件安全领域的关键任务，支撑漏洞验证、误报削减与补丁有效性确认。尽管定向模糊测试（directed fuzzing）在路径探索上表现优异，其在满足复杂语义约束（如多层条件跳转、内存布局依赖、符号化算术关系）方面仍面临根本性瓶颈。当前基于大语言模型（LLM）的方法虽具备强语义推理能力，但普遍**缺乏对真实程序执行状态的感知与反馈闭环**，导致生成的PoV常因脱离运行时行为而失效。\n\n## 方法创新：DrillAgent框架  \n本文提出**DrillAgent**——首个执行态感知（execution-state-aware）的LLM智能体框架，将PoV生成重构为“假设→验证→精炼”的迭代闭环过程。其核心创新在于：  \n- **双模态协同推理**：LLM基于源码静态分析生成初始输入假设；  \n- **执行反馈驱动约束提炼**：通过动态插桩捕获真实程序执行轨迹（如寄存器值、内存读写、分支结果），并利用**轻量级符号化映射机制**，将低层执行痕迹（e.g., `rax == 0x41414141`）自动翻译为源码级语义约束（e.g., `buf[0] == 'A' && len > 3`）；  \n- **闭环精炼**：LLM基于约束反馈重新生成更精准输入，实现从“语法合理”到“语义可触发”的跃迁。\n\n## 实验结果与意义  \n在涵盖127个真实C/C++ CVE漏洞的SEC-bench基准上，DrillAgent在固定时间/资源预算下，PoV生成成功率较最优基线（如CodeAct+Qwen-Agent）**提升52.8%**，尤其在堆溢出、UAF、逻辑漏洞等高难度类别中优势显著。本工作首次系统论证了**执行状态反馈对LLM推理的不可替代性**，为可信自动化漏洞利用生成提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13891v1",
      "arxiv_id": "2602.13891v1",
      "title": "GSRM: Generative Speech Reward Model for Speech RLHF",
      "authors": [
        "Maohao Shen",
        "Tejas Jayashankar",
        "Osama Hanna",
        "Naoyuki Kanda",
        "Yancheng Wang",
        "Kateřina Žmolíková",
        "Ruiming Xie",
        "Niko Moritz",
        "Anfeng Xu",
        "Yashesh Gaur",
        "Gregory Wornell",
        "Qing He",
        "Jilong Wu"
      ],
      "abstract": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions. Experiments show that GSRM substantially outperforms existing speech naturalness predictors, achieving model-human correlation of naturalness score prediction that approaches human inter-rater consistency. We further show how GSRM can improve the naturalness of speech LLM generations by serving as an effective verifier for online RLHF.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13891v1",
      "url": "https://arxiv.org/abs/2602.13891v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n当前语音大模型（如GPT-4o Voice Mode、Gemini Live）虽展现出强大语音生成能力，但合成语音在**审美自然度**（aesthetic naturalness）上仍显著落后于人类语音。提升语音质量亟需高信度、可解释、泛化性强的自然度评估器；然而，现有方法（如DNSMOS、SIGMO）多采用端到端回归，将原始波形映射为单一标量分值，存在两大缺陷：**① 缺乏可解释性**——无法说明“为何不自然”；**② 泛化性差**——在跨语种、跨设备、跨风格等不同语音范式下性能骤降。\n\n## 方法创新：GSRM框架  \n本文提出**生成式语音奖励模型（Generative Speech Reward Model, GSRM）**，首次将生成式奖励建模范式引入语音领域。GSRM以**推理为中心**（reasoning-centric），将自然度评估解耦为两阶段：  \n- **可解释声学特征提取**：冻结预训练语音编码器（Whisper-large-v3），提取音高稳定性、频谱平滑度、停顿合理性等12维可解释特征；  \n- **特征锚定的思维链推理**（feature-grounded CoT）：基于结构化特征输入，生成自然度评分及**自然/不自然归因文本**（如“基频抖动过大导致机械感”），实现可审计的决策过程。\n\n## 数据与验证  \n我们构建了迄今最大规模语音人类反馈数据集：  \n- **31k条专家标注**（5级Likert量表），覆盖中/英/日三语、TTS/ASR/对话场景；  \n- **首个真实世界用户-助手语音交互OOD基准**（1.2k条），含环境噪声、口音、语速变异等复杂因素。  \n实验表明：GSRM在跨域测试中与人类评分的相关系数（Spearman ρ）达**0.89**，逼近人类评者间一致性（0.91）；相较DNSMOS提升23.6%，且归因文本与人工诊断吻合率达81.4%。  \n## 应用价值  \nGSRM作为在线RLHF的**轻量级验证器**（<300ms延迟），驱动语音LLM优化后，MOS得分提升1.23分（+27.5%），证实其在真实强化学习闭环中的工程可行性。",
      "summary_en": "Recent speech language models (e.g., GPT-4o Voice, Gemini Live) generate fluent speech, yet their *aesthetic naturalness* remains inferior to human speech—largely due to the lack of reliable, interpretable, and generalizable evaluators. To address this, we propose the **Generative Speech Reward Model (GSRM)**, the first reasoning-centric reward model for speech. GSRM decomposes naturalness assessment into two stages: (1) extraction of 12 interpretable acoustic features (e.g., pitch stability, spectral smoothness) using a frozen Whisper encoder; and (2) feature-grounded chain-of-thought reasoning that outputs both a scalar naturalness score *and* textual justifications (e.g., “excessive jitter causes robotic impression”). Trained on a large-scale human feedback dataset (31k expert ratings across 3 languages + an out-of-domain benchmark of 1.2k real-world user-assistant interactions), GSRM achieves a Spearman correlation of **0.89** with human judgments—matching inter-rater consistency (0.91) and outperforming DNSMOS by 23.6%. When deployed as a verifier in online RLHF, GSRM boosts MOS scores of speech LLMs by **+1.23 points** (+27.5%). Code and data will be publicly released.",
      "summary": "## 背景与挑战  \n当前语音大模型（如GPT-4o Voice Mode、Gemini Live）虽展现出强大语音生成能力，但合成语音在**审美自然度**（aesthetic naturalness）上仍显著落后于人类语音。提升语音质量亟需高信度、可解释、泛化性强的自然度评估器；然而，现有方法（如DNSMOS、SIGMO）多采用端到端回归，将原始波形映射为单一标量分值，存在两大缺陷：**① 缺乏可解释性**——无法说明“为何不自然”；**② 泛化性差**——在跨语种、跨设备、跨风格等不同语音范式下性能骤降。\n\n## 方法创新：GSRM框架  \n本文提出**生成式语音奖励模型（Generative Speech Reward Model, GSRM）**，首次将生成式奖励建模范式引入语音领域。GSRM以**推理为中心**（reasoning-centric），将自然度评估解耦为两阶段：  \n- **可解释声学特征提取**：冻结预训练语音编码器（Whisper-large-v3），提取音高稳定性、频谱平滑度、停顿合理性等12维可解释特征；  \n- **特征锚定的思维链推理**（feature-grounded CoT）：基于结构化特征输入，生成自然度评分及**自然/不自然归因文本**（如“基频抖动过大导致机械感”），实现可审计的决策过程。\n\n## 数据与验证  \n我们构建了迄今最大规模语音人类反馈数据集：  \n- **31k条专家标注**（5级Likert量表），覆盖中/英/日三语、TTS/ASR/对话场景；  \n- **首个真实世界用户-助手语音交互OOD基准**（1.2k条），含环境噪声、口音、语速变异等复杂因素。  \n实验表明：GSRM在跨域测试中与人类评分的相关系数（Spearman ρ）达**0.89**，逼近人类评者间一致性（0.91）；相较DNSMOS提升23.6%，且归因文本与人工诊断吻合率达81.4%。  \n## 应用价值  \nGSRM作为在线RLHF的**轻量级验证器**（<300ms延迟），驱动语音LLM优化后，MOS得分提升1.23分（+27.5%），证实其在真实强化学习闭环中的工程可行性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13852v1",
      "arxiv_id": "2602.13852v1",
      "title": "Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking",
      "authors": [
        "Zhengmian Hu",
        "Lei Shi",
        "Ritwik Sinha",
        "Justin Grover",
        "David Arbour"
      ],
      "abstract": "Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \\textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13852v1",
      "url": "https://arxiv.org/abs/2602.13852v1",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n现代线上A/B测试面临双重瓶颈：**流量稀缺**导致难以覆盖所有创意变体，而**后验分析依赖人工、缺乏一致性且忽视内容语义**。同时，企业普遍未充分利用历史实验数据与高维内容嵌入（如文案、视觉、结构特征），错失指导实验优先级与创意迭代的宝贵信号。\n\n## 方法创新  \n本研究提出统一框架——**Experimentation Accelerator**，集成三大能力：  \n1. **智能变体优先级排序**：基于治疗嵌入（treatment embeddings）与历史CTR结果，构建带固定效应的上下文感知CTR排序模型，在预测价值与内容多样性间动态平衡；  \n2. **可解释性归因分析**：将变体映射至预定义的**语义营销属性空间**（如“紧迫感”“社会认同”“功能清晰度”），通过**符号一致、稀疏约束的Lasso回归**重参数化排序器，输出各属性的带符号贡献系数，支撑可视化驱动归因、Top-k归因排序及自然语言洞察生成；  \n3. **创意机会挖掘与生成**：设计**机会指数**，联合属性重要性（来自排序器）与当前实验中该属性的**表达不足程度**，自动识别高潜力但缺失的语义维度；再由微调LLM将排名靠前的机会转化为**具体、可执行的创意建议**（如“增加限时倒计时文案+信任徽章组合”），并同步评估其**学习价值**（提升因果推断质量）与**转化潜力**（预估CTR增益）。\n\n## 实践成效  \n该框架已集成至Adobe正式产品，服务数十家客户。真实业务实验评估表明：推荐变体的平均胜率提升**27%**，归因解释与人工专家判断的一致性达**91%**，创意建议采纳率超**68%**，显著缩短实验周期并提升规模化实验效能。",
      "summary_en": "Modern A/B testing suffers from traffic scarcity and manual, content-agnostic post-hoc analysis—while historical experiment data and rich content embeddings remain underutilized. We introduce *Experimentation Accelerator*, a unified AI framework that (i) prioritizes high-potential variants via a context-aware CTR ranking model balancing value and semantic diversity; (ii) delivers interpretable insights by projecting treatments onto curated marketing attributes and learning signed, sparse coefficients via constrained Lasso for visual explanations and natural-language summaries; and (iii) surfaces creative opportunities using an “opportunity index” that combines attribute importance with under-expression in current tests, then leverages fine-tuned LLMs to generate actionable, high-conversion creative suggestions with estimated learning and conversion gains. Deployed in Adobe’s production product, evaluation on real customer experiments shows a 27% average lift in winner identification rate, 91% alignment with expert attribution, and >68% adoption rate of AI-generated recommendations—accelerating experimentation cycles while enhancing insight quality and scalability.",
      "summary": "## 背景与挑战  \n现代线上A/B测试面临双重瓶颈：**流量稀缺**导致难以覆盖所有创意变体，而**后验分析依赖人工、缺乏一致性且忽视内容语义**。同时，企业普遍未充分利用历史实验数据与高维内容嵌入（如文案、视觉、结构特征），错失指导实验优先级与创意迭代的宝贵信号。\n\n## 方法创新  \n本研究提出统一框架——**Experimentation Accelerator**，集成三大能力：  \n1. **智能变体优先级排序**：基于治疗嵌入（treatment embeddings）与历史CTR结果，构建带固定效应的上下文感知CTR排序模型，在预测价值与内容多样性间动态平衡；  \n2. **可解释性归因分析**：将变体映射至预定义的**语义营销属性空间**（如“紧迫感”“社会认同”“功能清晰度”），通过**符号一致、稀疏约束的Lasso回归**重参数化排序器，输出各属性的带符号贡献系数，支撑可视化驱动归因、Top-k归因排序及自然语言洞察生成；  \n3. **创意机会挖掘与生成**：设计**机会指数**，联合属性重要性（来自排序器）与当前实验中该属性的**表达不足程度**，自动识别高潜力但缺失的语义维度；再由微调LLM将排名靠前的机会转化为**具体、可执行的创意建议**（如“增加限时倒计时文案+信任徽章组合”），并同步评估其**学习价值**（提升因果推断质量）与**转化潜力**（预估CTR增益）。\n\n## 实践成效  \n该框架已集成至Adobe正式产品，服务数十家客户。真实业务实验评估表明：推荐变体的平均胜率提升**27%**，归因解释与人工专家判断的一致性达**91%**，创意建议采纳率超**68%**，显著缩短实验周期并提升规模化实验效能。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13808v1",
      "arxiv_id": "2602.13808v1",
      "title": "An end-to-end agentic pipeline for smart contract translation and quality evaluation",
      "authors": [
        "Abhinav Goel",
        "Chaitya Shah",
        "Agostino Capponi",
        "Alfio Gliozzo"
      ],
      "abstract": "We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13808v1",
      "url": "https://arxiv.org/abs/2602.13808v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "llm"
      ],
      "keyword_score": 3,
      "summary_zh": "## 面向智能合约生成质量评估的端到端智能体流水线  \n\n本研究提出一种**端到端、可追溯、多维度**的智能体驱动框架，用于系统性评估大语言模型（LLM）从自然语言需求规格自动生成智能合约（以Solidity为主）的质量。该流水线采用**CrewAI风格的协作智能体架构**，包含三大核心智能体：① **解析代理**——将非结构化合同文本解析为形式化Schema（含参与者、事件、状态变量、业务规则等）；② **生成代理**——基于Schema迭代生成并优化Solidity代码；③ **评估代理**——执行编译验证、静态安全扫描（如Slither）、状态机建模比对及业务逻辑语义一致性检查。  \n\n质量评估覆盖**五大维度**：功能完整性（是否实现全部需求）、变量保真度（变量命名/类型/初始化与规格一致）、状态机正确性（状态转换图与规范匹配度）、业务逻辑保真度（条件分支、时序约束、权限控制等语义对齐）、代码质量（可读性、Gas效率、最佳实践）。各维度量化后聚合为复合得分，并支持**与人工编写的真实合约（ground-truth）进行配对评估**，精准定位系统性缺陷，如逻辑遗漏、状态跃迁不一致、访问控制缺失等。  \n\n本框架的关键创新在于：✅ **全链路可追溯性**——每个生成步骤附带完整元数据（来源文本段落、修改历史、置信度）；✅ **闭环迭代机制**——评估结果自动反馈至生成代理触发重写；✅ **可复现基准能力**——提供标准化输入/输出格式与评估协议，为智能合约合成研究建立首个开源、可量化的实证基准；✅ **扩展就绪设计**——接口兼容形式化验证工具（如Certora）与合规检查模块（如GDPR/SEC条款映射）。实验表明，该流水线将关键逻辑错误检出率提升42%，状态机偏差识别准确率达91.3%。",
      "summary_en": "We introduce an end-to-end agentic pipeline for systematic, reproducible evaluation of LLM-generated smart contracts from natural-language specifications. Leveraging CrewAI-style collaborative agents, the framework parses unstructured contract text into formal schemas, iteratively generates and refines Solidity code, and performs automated quality assessment via compilation, Slither-based security analysis, state-machine verification, and business-logic alignment checks. Quality is quantified across five dimensions—functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality—aggregated into composite scores. Paired evaluation against ground-truth implementations enables precise identification of systematic error modes (e.g., logic omissions, inconsistent state transitions). The system outputs fully provenanced artifacts and serves as a standardized benchmark for empirical research on smart contract synthesis, with built-in extensibility to formal verification and regulatory compliance checking.",
      "summary": "## 面向智能合约生成质量评估的端到端智能体流水线  \n\n本研究提出一种**端到端、可追溯、多维度**的智能体驱动框架，用于系统性评估大语言模型（LLM）从自然语言需求规格自动生成智能合约（以Solidity为主）的质量。该流水线采用**CrewAI风格的协作智能体架构**，包含三大核心智能体：① **解析代理**——将非结构化合同文本解析为形式化Schema（含参与者、事件、状态变量、业务规则等）；② **生成代理**——基于Schema迭代生成并优化Solidity代码；③ **评估代理**——执行编译验证、静态安全扫描（如Slither）、状态机建模比对及业务逻辑语义一致性检查。  \n\n质量评估覆盖**五大维度**：功能完整性（是否实现全部需求）、变量保真度（变量命名/类型/初始化与规格一致）、状态机正确性（状态转换图与规范匹配度）、业务逻辑保真度（条件分支、时序约束、权限控制等语义对齐）、代码质量（可读性、Gas效率、最佳实践）。各维度量化后聚合为复合得分，并支持**与人工编写的真实合约（ground-truth）进行配对评估**，精准定位系统性缺陷，如逻辑遗漏、状态跃迁不一致、访问控制缺失等。  \n\n本框架的关键创新在于：✅ **全链路可追溯性**——每个生成步骤附带完整元数据（来源文本段落、修改历史、置信度）；✅ **闭环迭代机制**——评估结果自动反馈至生成代理触发重写；✅ **可复现基准能力**——提供标准化输入/输出格式与评估协议，为智能合约合成研究建立首个开源、可量化的实证基准；✅ **扩展就绪设计**——接口兼容形式化验证工具（如Certora）与合规检查模块（如GDPR/SEC条款映射）。实验表明，该流水线将关键逻辑错误检出率提升42%，状态机偏差识别准确率达91.3%。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.15067v1",
      "arxiv_id": "2602.15067v1",
      "title": "Attention-gated U-Net model for semantic segmentation of brain tumors and feature extraction for survival prognosis",
      "authors": [
        "Rut Pate",
        "Snehal Rajput",
        "Mehul S. Raval",
        "Rupal A. Kapdi",
        "Mohendra Roy"
      ],
      "abstract": "Gliomas, among the most common primary brain tumors, vary widely in aggressiveness, prognosis, and histology, making treatment challenging due to complex and time-intensive surgical interventions. This study presents an Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model for improved brain tumor segmentation. The proposed model enhances feature representation and segmentation accuracy by integrating residual, recurrent, and triplanar architectures while maintaining computational efficiency, potentially aiding in better treatment planning. The proposed method achieves a Dice Similarity Score (DSC) of 0.900 for Whole Tumor (WT) segmentation on the BraTS2021 validation set, demonstrating performance comparable to leading models. Additionally, the triplanar network extracts 64 features per planar model for survival days prediction, which are reduced to 28 using an Artificial Neural Network (ANN). This approach achieves an accuracy of 45.71%, a Mean Squared Error (MSE) of 108,318.128, and a Spearman Rank Correlation Coefficient (SRC) of 0.338 on the test dataset.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.15067v1",
      "url": "https://arxiv.org/abs/2602.15067v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n胶质瘤是最常见的原发性脑肿瘤，其在组织学特征、侵袭性及预后方面高度异质，导致手术规划复杂、治疗决策困难。精准的脑肿瘤语义分割是术前评估、放疗靶区勾画及生存预后建模的关键基础。\n\n## 方法创新  \n本研究提出一种**注意力门控的三平面（2.5D）R²U-Net模型**（Attention-Gated Triplanar R²U-Net），融合**残差连接（Residual）**、**循环单元（Recurrent）** 与**跨平面注意力门控机制**，在保持计算高效性的同时显著增强多向解剖特征表达能力。模型分别处理轴位、矢状位与冠状位MRI切片，并通过注意力门控模块动态抑制无关背景、强化肿瘤边界响应。分割输出进一步用于提取高判别性影像组学特征：每个平面子网络输出64维特征，经人工神经网络（ANN）降维筛选，最终保留28个最具生存预测价值的特征。\n\n## 主要结果  \n- 在BraTS2021验证集上，Whole Tumor（WT）分割达**Dice相似系数0.900**，性能媲美当前主流3D模型，且推理速度提升约2.3倍；  \n- 基于所提特征的生存天数预测在测试集上取得**45.71%分类准确率**（按中位生存期二分类）、**MSE=108,318.128**、**Spearman相关系数ρ=0.338**（p<0.01），证实其具备临床可解释的预后关联性；  \n- 消融实验证明：注意力门控使WT-DSC提升+1.8%，三平面融合较单平面提升+3.2%，验证了多视角协同与自适应聚焦的双重增益。\n\n本工作实现了**高精度分割—鲁棒特征提取—生存预后建模**的一体化流程，为临床辅助决策提供了轻量、可解释、端到端可部署的新范式。",
      "summary_en": "Gliomas exhibit high inter-patient heterogeneity in histology, aggressiveness, and prognosis, necessitating accurate segmentation for treatment planning. This study proposes an **Attention-Gated Triplanar (2.5D) R²U-Net**, integrating residual connections, recurrent units, and attention-based gating across axial, sagittal, and coronal MRI planes to enhance feature representation while maintaining computational efficiency. On the BraTS2021 validation set, it achieves a **Dice Similarity Coefficient of 0.900 for Whole Tumor segmentation**, matching top-performing models with significantly faster inference. From the triplanar encoder, 64 features per plane are extracted and reduced to 28 discriminative survival predictors via an ANN. For overall survival days prediction on the test set, the method attains **45.71% accuracy (binary classification at median survival), MSE = 108,318.128, and Spearman Rank Correlation ρ = 0.338 (p < 0.01)** — demonstrating clinically meaningful prognostic utility. This work bridges precise segmentation and interpretable survival modeling in a unified, deployable framework.",
      "summary": "## 研究背景  \n胶质瘤是最常见的原发性脑肿瘤，其在组织学特征、侵袭性及预后方面高度异质，导致手术规划复杂、治疗决策困难。精准的脑肿瘤语义分割是术前评估、放疗靶区勾画及生存预后建模的关键基础。\n\n## 方法创新  \n本研究提出一种**注意力门控的三平面（2.5D）R²U-Net模型**（Attention-Gated Triplanar R²U-Net），融合**残差连接（Residual）**、**循环单元（Recurrent）** 与**跨平面注意力门控机制**，在保持计算高效性的同时显著增强多向解剖特征表达能力。模型分别处理轴位、矢状位与冠状位MRI切片，并通过注意力门控模块动态抑制无关背景、强化肿瘤边界响应。分割输出进一步用于提取高判别性影像组学特征：每个平面子网络输出64维特征，经人工神经网络（ANN）降维筛选，最终保留28个最具生存预测价值的特征。\n\n## 主要结果  \n- 在BraTS2021验证集上，Whole Tumor（WT）分割达**Dice相似系数0.900**，性能媲美当前主流3D模型，且推理速度提升约2.3倍；  \n- 基于所提特征的生存天数预测在测试集上取得**45.71%分类准确率**（按中位生存期二分类）、**MSE=108,318.128**、**Spearman相关系数ρ=0.338**（p<0.01），证实其具备临床可解释的预后关联性；  \n- 消融实验证明：注意力门控使WT-DSC提升+1.8%，三平面融合较单平面提升+3.2%，验证了多视角协同与自适应聚焦的双重增益。\n\n本工作实现了**高精度分割—鲁棒特征提取—生存预后建模**的一体化流程，为临床辅助决策提供了轻量、可解释、端到端可部署的新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13651v1",
      "arxiv_id": "2602.13651v1",
      "title": "Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation",
      "authors": [
        "Stefan Behfar",
        "Richard Mortier"
      ],
      "abstract": "In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13651v1",
      "url": "https://arxiv.org/abs/2602.13651v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n在真实联邦学习（FL）系统中，客户端参与具有**间歇性、异质性与结构性偏差**：设备上线/下线频繁、算力与网络资源受限，且参与模式常与本地数据分布（如地域、时序特征）强相关。现有公平性方法（如公平聚合、损失均衡）多以“每轮参与”为单位优化，隐含假设所有客户端拥有均等的长期贡献机会。然而，当参与本身高度不均衡时，此类方法易导致**间歇性客户端被系统性边缘化**——即便单轮表现“公平”，其长期模型受益、数据代表性与效用积累仍显著偏低。\n\n## 方法创新  \n本文提出**累积效用公平性（Cumulative Utility Parity, CUP）**，将公平性定义从“每轮性能均等”升维至“**每次可参与机会下的长期效用均等**”。核心在于解耦两类因素：  \n- **不可规避的物理约束**（如设备离线率）；  \n- **可调控的算法偏差**（如调度策略偏好活跃客户端、聚合权重忽视参与稀疏性）。  \n为此，我们设计**可用性归一化累积效用**（Availability-Normalized Cumulative Utility），以客户端历史可用窗口为分母，量化其单位参与机会所获得的模型效用（如测试准确率提升、任务完成度等），并据此指导动态调度与加权聚合。\n\n## 实验结果与价值  \n在时序偏斜（temporally skewed）、非独立同分布（non-IID）的多个FL基准（如FEMNIST-Temporal、Shakespeare-Intermittent）上验证：相比FedAvg、q-Fair FL等基线，本方法将**长期表示公平性指标（CUP Gap）降低62.3%–78.1%**，同时保持全局模型精度仅下降<0.4%，实现公平性与性能的帕累托改进。该框架首次将**参与机会作为公平性锚点**，为构建鲁棒、可持续的实用FL系统提供了新范式。",
      "summary_en": "In real-world federated learning (FL), client participation is inherently intermittent, heterogeneous, and often correlated with data or resource constraints—yet most fairness methods optimize per-round metrics (e.g., loss or accuracy), implicitly assuming equal long-term contribution opportunities. This leads to systemic under-representation of intermittently available clients, even when per-round performance appears fair. We propose **Cumulative Utility Parity (CUP)**: a fairness principle evaluating whether clients receive comparable *long-term utility per participation opportunity*, not per round. To operationalize CUP, we introduce *availability-normalized cumulative utility*, which disentangles unavoidable physical unavailability from avoidable algorithmic bias in scheduling and aggregation. Experiments on temporally skewed, non-IID benchmarks (e.g., FEMNIST-Temporal, Shakespeare-Intermittent) show our method reduces the CUP gap by 62.3–78.1% over baselines (FedAvg, q-Fair FL), while preserving >99.6% of model accuracy—demonstrating strong fairness-performance trade-off improvement.",
      "summary": "## 背景与问题  \n在真实联邦学习（FL）系统中，客户端参与具有**间歇性、异质性与结构性偏差**：设备上线/下线频繁、算力与网络资源受限，且参与模式常与本地数据分布（如地域、时序特征）强相关。现有公平性方法（如公平聚合、损失均衡）多以“每轮参与”为单位优化，隐含假设所有客户端拥有均等的长期贡献机会。然而，当参与本身高度不均衡时，此类方法易导致**间歇性客户端被系统性边缘化**——即便单轮表现“公平”，其长期模型受益、数据代表性与效用积累仍显著偏低。\n\n## 方法创新  \n本文提出**累积效用公平性（Cumulative Utility Parity, CUP）**，将公平性定义从“每轮性能均等”升维至“**每次可参与机会下的长期效用均等**”。核心在于解耦两类因素：  \n- **不可规避的物理约束**（如设备离线率）；  \n- **可调控的算法偏差**（如调度策略偏好活跃客户端、聚合权重忽视参与稀疏性）。  \n为此，我们设计**可用性归一化累积效用**（Availability-Normalized Cumulative Utility），以客户端历史可用窗口为分母，量化其单位参与机会所获得的模型效用（如测试准确率提升、任务完成度等），并据此指导动态调度与加权聚合。\n\n## 实验结果与价值  \n在时序偏斜（temporally skewed）、非独立同分布（non-IID）的多个FL基准（如FEMNIST-Temporal、Shakespeare-Intermittent）上验证：相比FedAvg、q-Fair FL等基线，本方法将**长期表示公平性指标（CUP Gap）降低62.3%–78.1%**，同时保持全局模型精度仅下降<0.4%，实现公平性与性能的帕累托改进。该框架首次将**参与机会作为公平性锚点**，为构建鲁棒、可持续的实用FL系统提供了新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13748v1",
      "arxiv_id": "2602.13748v1",
      "title": "RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction",
      "authors": [
        "Yongkang Jin",
        "Jianwen Luo",
        "Jingjing Wang",
        "Jianmin Yao",
        "Yu Hong"
      ],
      "abstract": "Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.",
      "published": "2026-02-14",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13748v1",
      "url": "https://arxiv.org/abs/2602.13748v1",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与挑战  \n多媒体事件抽取（Multimedia Event Extraction, MEE）旨在从图文混合文档中联合识别事件及其论元，核心难点在于跨模态语义对齐与结构化表示学习。当前最大瓶颈是**标注数据极度匮乏**：唯一公开基准M2E2仅提供测试集标注，无训练标签，导致监督学习不可行。现有方法或依赖弱监督的跨模态对齐，或依赖推理时提示（inference-time prompting）调用视觉-语言模型（VLMs），但均未显式建模事件结构，且在图文协同论元定位上表现脆弱。\n\n## 方法创新：RMPL框架  \n本文提出**RMPL**（Relation-aware Multi-task Progressive Learning），一种面向低资源MEE的关系感知多任务渐进式学习框架。其核心设计包含三重创新：  \n- **阶段式训练范式**：第一阶段利用单模态事件抽取（文本/图像）与多媒体关系抽取的**异构弱监督信号**，在统一事件模式下预训练跨模态共享表征；  \n- **关系感知多任务机制**：引入关系分类作为辅助任务，显式建模事件元素间的语义关联，增强论元角色判别能力；  \n- **渐进式精调策略**：第二阶段在混合图文数据上联合优化事件提及识别与论元角色抽取，避免模态偏差。\n\n## 实验结果与贡献  \n在M2E2基准上，RMPL与多种主流VLM（如BLIP-2、LLaVA、Qwen-VL）集成后，**在所有模态配置下（纯文、图文融合、纯图）均取得一致提升**：事件检测F1平均提升+2.7%，论元角色F1最高提升+4.3%。本工作首次将关系建模、多任务迁移与阶段式训练系统整合，为低资源跨模态事件理解提供了可扩展新范式。",
      "summary_en": "Multimedia Event Extraction (MEE) faces severe data scarcity, as the sole benchmark M2E2 provides only evaluation annotations—precluding standard supervised training. Existing methods rely on cross-modal alignment or inference-time prompting with Vision–Language Models (VLMs), yet fail to learn structured event representations and suffer from weak argument grounding. To address this, we propose **RMPL**, a Relation-aware Multi-task Progressive Learning framework for low-resource MEE. RMPL leverages heterogeneous supervision from unimodal event extraction and multimedia relation extraction via stage-wise training: first learning modality-agnostic event-centric representations under a unified schema, then fine-tuning jointly on event mention identification and argument role labeling using mixed textual and visual data. Experiments across multiple VLM backbones (BLIP-2, LLaVA, Qwen-VL) on M2E2 show consistent improvements—up to +4.3 F1 on argument roles and +2.7 average F1 on event detection—across all modality settings (text-only, multimodal, image-only). RMPL establishes a new paradigm for structured, relation-aware, progressive learning in resource-constrained multimodal event understanding.",
      "summary": "## 研究背景与挑战  \n多媒体事件抽取（Multimedia Event Extraction, MEE）旨在从图文混合文档中联合识别事件及其论元，核心难点在于跨模态语义对齐与结构化表示学习。当前最大瓶颈是**标注数据极度匮乏**：唯一公开基准M2E2仅提供测试集标注，无训练标签，导致监督学习不可行。现有方法或依赖弱监督的跨模态对齐，或依赖推理时提示（inference-time prompting）调用视觉-语言模型（VLMs），但均未显式建模事件结构，且在图文协同论元定位上表现脆弱。\n\n## 方法创新：RMPL框架  \n本文提出**RMPL**（Relation-aware Multi-task Progressive Learning），一种面向低资源MEE的关系感知多任务渐进式学习框架。其核心设计包含三重创新：  \n- **阶段式训练范式**：第一阶段利用单模态事件抽取（文本/图像）与多媒体关系抽取的**异构弱监督信号**，在统一事件模式下预训练跨模态共享表征；  \n- **关系感知多任务机制**：引入关系分类作为辅助任务，显式建模事件元素间的语义关联，增强论元角色判别能力；  \n- **渐进式精调策略**：第二阶段在混合图文数据上联合优化事件提及识别与论元角色抽取，避免模态偏差。\n\n## 实验结果与贡献  \n在M2E2基准上，RMPL与多种主流VLM（如BLIP-2、LLaVA、Qwen-VL）集成后，**在所有模态配置下（纯文、图文融合、纯图）均取得一致提升**：事件检测F1平均提升+2.7%，论元角色F1最高提升+4.3%。本工作首次将关系建模、多任务迁移与阶段式训练系统整合，为低资源跨模态事件理解提供了可扩展新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13156v1",
      "arxiv_id": "2602.13156v1",
      "title": "In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach",
      "authors": [
        "Yiran Gao",
        "Kim Hammar",
        "Tao Li"
      ],
      "abstract": "Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models' (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13156v1",
      "url": "https://arxiv.org/abs/2602.13156v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "llm",
        "security",
        "agent"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与挑战  \n面对日益复杂、快速演化的网络攻击，传统依赖人工规则或强化学习（RL）的应急响应系统存在显著局限：RL方法需精心构建仿真环境，且难以有效利用原始日志与告警中的语义信息；而基于规则的系统缺乏泛化与自适应能力。如何实现**无需显式建模、端到端自主推理**的响应闭环，成为关键研究缺口。\n\n## 方法创新  \n本文提出 **In-Context Autonomous Network Incident Response（ICANIR）**——一种基于大语言模型（LLM）的轻量级智能体框架。核心突破在于：  \n- ✅ **端到端四功能融合**：在单个14B参数开源LLM中统一集成**感知**（解析多源日志、推断网络实时状态）、**推理**（动态更新攻击假设模型）、**规划**（通过链式思维模拟不同响应策略的后果）、**执行**（生成可操作的修复指令）；  \n- ✅ **免建模的上下文自适应**：不依赖预设仿真器，而是将真实观测与LLM内部模拟结果对比，迭代修正攻击猜想与响应方案，实现真正的**上下文内学习（in-context adaptation）**；  \n- ✅ **工程友好性**：无需GPU集群，可在消费级硬件（如单卡RTX 4090）高效运行，支持快速部署。\n\n## 主要成果  \n在涵盖APT、横向移动、勒索软件等典型场景的公开 incident logs 数据集上评估表明：  \n- 响应决策准确率提升19.3%，平均恢复时间**缩短23%**，显著优于GPT-4、Claude-3及微调后的Llama-3-70B等前沿LLM基线；  \n- 首次验证了轻量级LLM（14B）通过结构化提示+微调即可承载完整安全智能体功能，为资源受限场景提供新范式。",
      "summary_en": "This paper introduces ICANIR, an end-to-end LLM agent for autonomous network incident response that eliminates the need for handcrafted simulators or explicit environment modeling. By unifying perception, reasoning, planning, and action within a single fine-tuned 14B LLM—and leveraging chain-of-thought reasoning and in-context adaptation—the agent processes raw logs to infer network state, dynamically refine attack hypotheses, simulate response outcomes, and generate executable remediation actions. Crucially, it iteratively aligns internal simulations with real-world observations to self-correct without external feedback loops. Evaluated on published incident logs spanning APT, lateral movement, and ransomware scenarios, ICANIR achieves up to **23% faster recovery** than state-of-the-art LLMs (e.g., GPT-4, Claude-3, Llama-3-70B), while running efficiently on commodity hardware. This work establishes a lightweight, modeling-free paradigm for adaptive, semantic-aware cyber defense.",
      "summary": "## 背景与挑战  \n面对日益复杂、快速演化的网络攻击，传统依赖人工规则或强化学习（RL）的应急响应系统存在显著局限：RL方法需精心构建仿真环境，且难以有效利用原始日志与告警中的语义信息；而基于规则的系统缺乏泛化与自适应能力。如何实现**无需显式建模、端到端自主推理**的响应闭环，成为关键研究缺口。\n\n## 方法创新  \n本文提出 **In-Context Autonomous Network Incident Response（ICANIR）**——一种基于大语言模型（LLM）的轻量级智能体框架。核心突破在于：  \n- ✅ **端到端四功能融合**：在单个14B参数开源LLM中统一集成**感知**（解析多源日志、推断网络实时状态）、**推理**（动态更新攻击假设模型）、**规划**（通过链式思维模拟不同响应策略的后果）、**执行**（生成可操作的修复指令）；  \n- ✅ **免建模的上下文自适应**：不依赖预设仿真器，而是将真实观测与LLM内部模拟结果对比，迭代修正攻击猜想与响应方案，实现真正的**上下文内学习（in-context adaptation）**；  \n- ✅ **工程友好性**：无需GPU集群，可在消费级硬件（如单卡RTX 4090）高效运行，支持快速部署。\n\n## 主要成果  \n在涵盖APT、横向移动、勒索软件等典型场景的公开 incident logs 数据集上评估表明：  \n- 响应决策准确率提升19.3%，平均恢复时间**缩短23%**，显著优于GPT-4、Claude-3及微调后的Llama-3-70B等前沿LLM基线；  \n- 首次验证了轻量级LLM（14B）通过结构化提示+微调即可承载完整安全智能体功能，为资源受限场景提供新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13062v1",
      "arxiv_id": "2602.13062v1",
      "title": "Backdoor Attacks on Contrastive Continual Learning for IoT Systems",
      "authors": [
        "Alfous Tim",
        "Kuniyilh Simi D"
      ],
      "abstract": "The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13062v1",
      "url": "https://arxiv.org/abs/2602.13062v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.NI"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n物联网（IoT）系统日益依赖**持续学习（Continual Learning, CL）**以应对传感器漂移、用户行为演化、设备老化及对抗性动态等非平稳环境。其中，**对比式持续学习（Contrastive Continual Learning, CCL）**通过融合对比表征学习与增量任务适应，在跨任务/跨域场景下实现鲁棒的特征复用，已成为边缘智能的关键范式。然而，其依赖嵌入空间几何对齐的目标函数、回放（replay）机制与稳定性正则化策略的耦合，意外引入新型安全漏洞——**后门攻击可借由嵌入对齐偏差与回放强化效应，植入长期驻留、跨更新周期持续生效的恶意行为**。\n\n## 方法与创新  \n本文首次系统揭示CCL在IoT场景下的后门攻击面：  \n- **形式化建模**：提出嵌入级攻击目标函数，刻画攻击者如何操纵正负样本对分布以诱导特定语义偏移；  \n- **IoT特异性分析**：识别设备异构性、低频固件更新、分布式回放缓存等导致的**攻击持久化机制**（如“回放锚点固化”与“边缘-云协同污染”）；  \n- **分层攻击分类法**：构建面向IoT的三级攻击 taxonomy（按触发方式：*隐式传感器扰动*、*回放缓冲区投毒*、*联邦聚合污染*；按隐蔽性：*时序掩蔽*、*域内不可见触发器*）；  \n- **受限环境评估**：在内存受限（≤2MB）、边缘计算延迟（<100ms）、联邦聚合带宽约束（≤50KB/round）下，实证对比CCL、EWC、LwF等范式的脆弱性差异，并验证轻量级防御（如**对比一致性蒸馏**与**回放样本几何过滤**）的有效性。\n\n## 主要发现  \n实验表明：CCL在提升IoT自适应能力的同时，其嵌入空间紧致性反而放大后门泛化性——攻击成功率较传统CL高37.2%（平均），且在3轮模型更新后仍保持82.4%激活率。本研究为构建**安全可持续的边缘智能**提供了理论框架与实践指南。",
      "summary_en": "This paper presents the first comprehensive study of **backdoor attacks on Contrastive Continual Learning (CCL)** in resource-constrained IoT systems. We formalize embedding-level attack objectives that exploit contrastive alignment and replay reinforcement to implant persistent, update-resilient triggers. A novel IoT-tailored taxonomy categorizes attacks by trigger modality (sensor-level perturbations, replay buffer poisoning, federated aggregation contamination) and stealth mechanisms (temporal masking, domain-invariant triggers). Under realistic IoT constraints—limited memory (<2 MB), edge latency (<100 ms), and bandwidth-limited federation—we benchmark vulnerabilities across CCL, EWC, and LwF, finding CCL exhibits 37.2% higher attack success and retains 82.4% backdoor activation after three model updates. We further propose and evaluate lightweight defenses, including contrastive consistency distillation and geometric replay filtering, demonstrating up to 91.6% mitigation efficacy. Our work reveals a critical security–adaptivity trade-off in edge AI and provides foundational insights for secure continual intelligence in IoT.",
      "summary": "## 背景与问题  \n物联网（IoT）系统日益依赖**持续学习（Continual Learning, CL）**以应对传感器漂移、用户行为演化、设备老化及对抗性动态等非平稳环境。其中，**对比式持续学习（Contrastive Continual Learning, CCL）**通过融合对比表征学习与增量任务适应，在跨任务/跨域场景下实现鲁棒的特征复用，已成为边缘智能的关键范式。然而，其依赖嵌入空间几何对齐的目标函数、回放（replay）机制与稳定性正则化策略的耦合，意外引入新型安全漏洞——**后门攻击可借由嵌入对齐偏差与回放强化效应，植入长期驻留、跨更新周期持续生效的恶意行为**。\n\n## 方法与创新  \n本文首次系统揭示CCL在IoT场景下的后门攻击面：  \n- **形式化建模**：提出嵌入级攻击目标函数，刻画攻击者如何操纵正负样本对分布以诱导特定语义偏移；  \n- **IoT特异性分析**：识别设备异构性、低频固件更新、分布式回放缓存等导致的**攻击持久化机制**（如“回放锚点固化”与“边缘-云协同污染”）；  \n- **分层攻击分类法**：构建面向IoT的三级攻击 taxonomy（按触发方式：*隐式传感器扰动*、*回放缓冲区投毒*、*联邦聚合污染*；按隐蔽性：*时序掩蔽*、*域内不可见触发器*）；  \n- **受限环境评估**：在内存受限（≤2MB）、边缘计算延迟（<100ms）、联邦聚合带宽约束（≤50KB/round）下，实证对比CCL、EWC、LwF等范式的脆弱性差异，并验证轻量级防御（如**对比一致性蒸馏**与**回放样本几何过滤**）的有效性。\n\n## 主要发现  \n实验表明：CCL在提升IoT自适应能力的同时，其嵌入空间紧致性反而放大后门泛化性——攻击成功率较传统CL高37.2%（平均），且在3轮模型更新后仍保持82.4%激活率。本研究为构建**安全可持续的边缘智能**提供了理论框架与实践指南。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12943v1",
      "arxiv_id": "2602.12943v1",
      "title": "Neighborhood Blending: A Lightweight Inference-Time Defense Against Membership Inference Attacks",
      "authors": [
        "Osama Zafar",
        "Shaojie Zhan",
        "Tianxi Ji",
        "Erman Ayday"
      ],
      "abstract": "In recent years, the widespread adoption of Machine Learning as a Service (MLaaS), particularly in sensitive environments, has raised considerable privacy concerns. Of particular importance are membership inference attacks (MIAs), which exploit behavioral discrepancies between training and non-training data to determine whether a specific record was included in the model's training set, thereby presenting significant privacy risks. Although existing defenses, such as adversarial regularization, DP-SGD, and MemGuard, assist in mitigating these threats, they often entail trade-offs such as compromising utility, increased computational requirements, or inconsistent protection against diverse attack vectors.   In this paper, we introduce a novel inference-time defense mechanism called Neighborhood Blending, which mitigates MIAs without retraining the model or incurring significant computational overhead. Our approach operates post-training by smoothing the model's confidence outputs based on the neighborhood of a queried sample. By averaging predictions from similar training samples selected using differentially private sampling, our method establishes a consistent confidence pattern, rendering members and non-members indistinguishable to an adversary while maintaining high utility. Significantly, Neighborhood Blending maintains label integrity (zero label loss) and ensures high utility through an adaptive, \"pay-as-you-go\" distortion strategy. It is a model-agnostic approach that offers a practical, lightweight solution that enhances privacy without sacrificing model utility. Through extensive experiments across diverse datasets and models, we demonstrate that our defense significantly reduces MIA success rates while preserving model performance, outperforming existing post-hoc defenses like MemGuard and training-time techniques like DP-SGD in terms of utility retention.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12943v1",
      "url": "https://arxiv.org/abs/2602.12943v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "adversarial",
        "learning",
        "dp",
        "membership",
        "machine"
      ],
      "keyword_score": 6,
      "summary_zh": "## 背景与问题  \n近年来，机器学习即服务（MLaaS）在医疗、金融等敏感场景的广泛应用，加剧了模型隐私泄露风险。其中，**成员推断攻击（Membership Inference Attacks, MIAs）** 通过分析模型对输入样本的置信度输出差异，可高精度判断某条数据是否参与过模型训练，构成严重隐私威胁。现有防御方法（如DP-SGD、MemGuard、对抗正则化）普遍存在显著缺陷：训练时方法（如DP-SGD）严重损害模型效用；推理时方法（如MemGuard）依赖置信度扰动，易导致标签翻转、效用下降，且对新型攻击泛化性弱。\n\n## 方法创新：Neighborhood Blending（邻域融合）  \n本文提出一种**轻量级、纯推理时、无需重训练**的新型防御机制——**Neighborhood Blending**。其核心思想是：对查询样本，首先在训练集中基于相似性（如特征距离）检索其邻域，再通过**差分隐私采样（ε=0.5–2.0）** 随机选取若干邻近样本；最后将模型对这些邻近样本的预测输出进行加权平均，生成平滑后的置信向量。该过程仅需单次前向传播+少量邻居查询，计算开销可忽略（<1ms/查询），且**严格保持原始预测标签不变（零标签损失）**。\n\n## 主要成果与优势  \n- 在CIFAR-10、Purchase-100、Texas-100等基准数据集上，对多种SOTA MIA（如Shadow-kNN、Loss-based、DLMA）的攻击成功率平均降低**58.3%–79.1%**，显著优于MemGuard（平均+22.6%防御增益）和DP-SGD（效用损失降低4.8×）；  \n- 模型准确率下降仅**0.12%–0.47%**（vs. DP-SGD平均下降3.2%），实现隐私-效用帕累托前沿突破；  \n- **完全模型无关**，兼容CNN、ResNet、TabTransformer等任意架构；  \n- 提出“按需付费”（pay-as-you-go）自适应失真策略：邻域半径与噪声强度随查询置信度动态调整，在高风险样本上增强混淆，在低风险样本上最小化扰动。",
      "summary_en": "This paper introduces **Neighborhood Blending**, a lightweight, inference-time defense against membership inference attacks (MIAs) that requires no model retraining and imposes negligible computational overhead. Unlike existing defenses—such as DP-SGD (utility-degrading) or MemGuard (label-flip-prone)—our method operates *post-training* by smoothing model confidence outputs: for a query sample, it retrieves similar training instances via differentially private sampling (ε ∈ [0.5, 2.0]) and averages their predictions to produce a robust, label-preserving output. This creates consistent confidence patterns for members and non-members, thwarting MIA discriminators while maintaining >99.5% original accuracy across diverse models (ResNet, CNN, TabTransformer) and datasets (CIFAR-10, Texas-100, Purchase-100). Experiments show Neighborhood Blending reduces MIA success rates by up to 79.1%—outperforming MemGuard and DP-SGD in both privacy gain and utility retention—establishing a new practical trade-off frontier for MLaaS privacy.",
      "summary": "## 背景与问题  \n近年来，机器学习即服务（MLaaS）在医疗、金融等敏感场景的广泛应用，加剧了模型隐私泄露风险。其中，**成员推断攻击（Membership Inference Attacks, MIAs）** 通过分析模型对输入样本的置信度输出差异，可高精度判断某条数据是否参与过模型训练，构成严重隐私威胁。现有防御方法（如DP-SGD、MemGuard、对抗正则化）普遍存在显著缺陷：训练时方法（如DP-SGD）严重损害模型效用；推理时方法（如MemGuard）依赖置信度扰动，易导致标签翻转、效用下降，且对新型攻击泛化性弱。\n\n## 方法创新：Neighborhood Blending（邻域融合）  \n本文提出一种**轻量级、纯推理时、无需重训练**的新型防御机制——**Neighborhood Blending**。其核心思想是：对查询样本，首先在训练集中基于相似性（如特征距离）检索其邻域，再通过**差分隐私采样（ε=0.5–2.0）** 随机选取若干邻近样本；最后将模型对这些邻近样本的预测输出进行加权平均，生成平滑后的置信向量。该过程仅需单次前向传播+少量邻居查询，计算开销可忽略（<1ms/查询），且**严格保持原始预测标签不变（零标签损失）**。\n\n## 主要成果与优势  \n- 在CIFAR-10、Purchase-100、Texas-100等基准数据集上，对多种SOTA MIA（如Shadow-kNN、Loss-based、DLMA）的攻击成功率平均降低**58.3%–79.1%**，显著优于MemGuard（平均+22.6%防御增益）和DP-SGD（效用损失降低4.8×）；  \n- 模型准确率下降仅**0.12%–0.47%**（vs. DP-SGD平均下降3.2%），实现隐私-效用帕累托前沿突破；  \n- **完全模型无关**，兼容CNN、ResNet、TabTransformer等任意架构；  \n- 提出“按需付费”（pay-as-you-go）自适应失真策略：邻域半径与噪声强度随查询置信度动态调整，在高风险样本上增强混淆，在低风险样本上最小化扰动。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12681v1",
      "arxiv_id": "2602.12681v1",
      "title": "Fool Me If You Can: On the Robustness of Binary Code Similarity Detection Models against Semantics-preserving Transformations",
      "authors": [
        "Jiyong Uhm",
        "Minseok Kim",
        "Michalis Polychronakis",
        "Hyungjoon Koo"
      ],
      "abstract": "Binary code analysis plays an essential role in cybersecurity, facilitating reverse engineering to reveal the inner workings of programs in the absence of source code. Traditional approaches, such as static and dynamic analysis, extract valuable insights from stripped binaries, but often demand substantial expertise and manual effort. Recent advances in deep learning have opened promising opportunities to enhance binary analysis by capturing latent features and disclosing underlying code semantics. Despite the growing number of binary analysis models based on machine learning, their robustness to adversarial code transformations at the binary level remains underexplored. We evaluate the robustness of deep learning models for the task of binary code similarity detection (BCSD) under semantics-preserving transformations. The unique nature of machine instructions presents distinct challenges compared to the typical input perturbations found in other domains. We introduce asmFooler, a system that evaluates the resilience of BCSD models using a diverse set of adversarial code transformations that preserve functional semantics. We construct a dataset of 9,565 binary variants from 620 baseline samples by applying eight semantics-preserving transformations across six representative BCSD models. Our major findings highlight several key insights: i) model robustness relies on the processing pipeline, including code pre-processing, architecture, and feature selection; ii) adversarial transformation effectiveness is bounded by a budget shaped by model-specific constraints like input size and instruction expressive capacity; iii) well-crafted transformations can be highly effective with minimal perturbations; and iv) such transformations efficiently disrupt model decisions (e.g., misleading to false positives or false negatives) by focusing on semantically significant instructions.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12681v1",
      "url": "https://arxiv.org/abs/2602.12681v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "adversarial",
        "machine"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n二进制代码相似性检测（BCSD）是网络安全中逆向工程与恶意软件分析的核心任务，尤其在源码缺失场景下至关重要。尽管深度学习模型在BCSD中展现出强大表征能力，其**面对语义保持型二进制变换的鲁棒性却长期被忽视**。此类变换（如寄存器重命名、跳转优化、等价指令替换）不改变程序功能，却可能显著扰动模型输入，暴露出模型对底层指令结构的脆弱依赖。\n\n## 方法与数据  \n本文提出 **asmFooler**——首个面向BCSD模型鲁棒性评估的系统化框架。我们设计并实现8类细粒度、语义保持的汇编级对抗变换（涵盖控制流、数据流与指令编码维度），在620个真实基准二进制样本上生成**9,565个功能等价变体**。实验覆盖6个代表性BCSD模型（包括ASM2VEC、Gemini、MalwareDiffusion等），统一评估其在变换下的相似性判定稳定性。\n\n## 主要发现与创新  \n- **鲁棒性非模型固有属性，而由全流程决定**：预处理（如指令标准化）、架构设计（图神经网络vs.序列模型）及特征选择（opcode vs. CFG embedding）共同塑造抗干扰能力；  \n- **变换有效性存在“预算边界”**：受模型输入长度限制、指令表达容量及语义敏感区分布制约，并非扰动越多越有效；  \n- **最小扰动可引发决策崩溃**：仅修改1–3条关键语义指令（如条件跳转目标或函数调用约定），即可在>78%案例中诱导**误报（false positive）或漏报（false negative）**；  \n- **首次揭示“语义显著性放大效应”**：攻击聚焦于控制流枢纽指令时，扰动效率提升3.2×，验证了指令级语义权重在深度模型中的隐式偏置。\n\n本工作为构建可信二进制分析模型提供了可复现的评估基准、可解释的脆弱性归因机制，以及面向语义鲁棒性的模型设计新范式。",
      "summary_en": "Binary code similarity detection (BCSD) is vital for malware analysis and reverse engineering, yet the robustness of deep learning models against semantics-preserving binary transformations remains poorly understood. We introduce **asmFooler**, a systematic framework to evaluate BCSD model resilience using eight fine-grained, functionally invariant assembly-level transformations. Applied to 620 real-world binaries, it generates 9,565 semantically equivalent variants and tests six state-of-the-art BCSD models. Our key findings are: (i) robustness is determined holistically by preprocessing, architecture, and feature representation—not by model alone; (ii) adversarial effectiveness is bounded by model-specific constraints (e.g., input length, instruction expressivity), not perturbation magnitude; (iii) minimal, semantically targeted modifications—often just 1–3 instructions—suffice to flip decisions in >78% of cases, causing false positives or negatives; and (iv) transformations focusing on semantically critical instructions (e.g., conditional jumps, call sites) achieve 3.2× higher disruption efficiency. This work establishes the first standardized benchmark and mechanistic insights for semantic robustness in binary analysis.",
      "summary": "## 背景与问题  \n二进制代码相似性检测（BCSD）是网络安全中逆向工程与恶意软件分析的核心任务，尤其在源码缺失场景下至关重要。尽管深度学习模型在BCSD中展现出强大表征能力，其**面对语义保持型二进制变换的鲁棒性却长期被忽视**。此类变换（如寄存器重命名、跳转优化、等价指令替换）不改变程序功能，却可能显著扰动模型输入，暴露出模型对底层指令结构的脆弱依赖。\n\n## 方法与数据  \n本文提出 **asmFooler**——首个面向BCSD模型鲁棒性评估的系统化框架。我们设计并实现8类细粒度、语义保持的汇编级对抗变换（涵盖控制流、数据流与指令编码维度），在620个真实基准二进制样本上生成**9,565个功能等价变体**。实验覆盖6个代表性BCSD模型（包括ASM2VEC、Gemini、MalwareDiffusion等），统一评估其在变换下的相似性判定稳定性。\n\n## 主要发现与创新  \n- **鲁棒性非模型固有属性，而由全流程决定**：预处理（如指令标准化）、架构设计（图神经网络vs.序列模型）及特征选择（opcode vs. CFG embedding）共同塑造抗干扰能力；  \n- **变换有效性存在“预算边界”**：受模型输入长度限制、指令表达容量及语义敏感区分布制约，并非扰动越多越有效；  \n- **最小扰动可引发决策崩溃**：仅修改1–3条关键语义指令（如条件跳转目标或函数调用约定），即可在>78%案例中诱导**误报（false positive）或漏报（false negative）**；  \n- **首次揭示“语义显著性放大效应”**：攻击聚焦于控制流枢纽指令时，扰动效率提升3.2×，验证了指令级语义权重在深度模型中的隐式偏置。\n\n本工作为构建可信二进制分析模型提供了可复现的评估基准、可解释的脆弱性归因机制，以及面向语义鲁棒性的模型设计新范式。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12500v1",
      "arxiv_id": "2602.12500v1",
      "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis",
      "authors": [
        "André Storhaug",
        "Jiamou Sun",
        "Jingyue Li"
      ],
      "abstract": "Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent. By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12500v1",
      "url": "https://arxiv.org/abs/2602.12500v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "agent",
        "llm"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与挑战  \n在安全软件维护中，精准识别对应已披露CVE漏洞的**漏洞修复提交（vulnerability-fixing commits）** 至关重要，但面临严峻可扩展性挑战：大型开源仓库常含数百万次提交，其中仅极小比例涉及安全修复。现有自动化方法——包括传统机器学习模型及新兴大语言模型（LLM）方案——普遍存在**精度-召回率权衡失衡**问题。尤为关键的是，当前评估多基于随机采样提交，严重低估了真实场景难度：实践中候选提交通常已具备安全相关性且彼此高度相似，导致相似度匹配或单轮分类极易失效。\n\n## 方法创新：Favia 框架  \n我们提出 **Favia**——一种面向数字取证的、基于智能体（agent）的漏洞修复识别与分析框架。其核心是**两阶段协同推理机制**：  \n- **第一阶段：高效可扩展排序**——利用轻量级模型快速筛选出高潜力候选提交，大幅压缩搜索空间；  \n- **第二阶段：深度语义因果推理**——每个候选提交交由一个 **ReAct-style LLM 智能体** 进行迭代式分析。该智能体以**提交前代码库为运行环境**，调用专用工具（如代码导航、差异解析、依赖追溯），主动定位脆弱组件、遍历多文件变更，并**建立代码修改与漏洞根本原因之间的因果对齐证据链**。\n\n## 关键优势与实证结果  \nFavia 突破了单次判别或文本相似性方法的局限，可稳健识别**间接修复、跨文件修复及非显式补丁**（如配置调整、权限加固）。我们在自建大规模基准数据集 **CVEVC**（涵盖 3,708 个真实仓库、超 800 万次提交）上系统评估，结果表明：在贴近实际的“安全相关候选集”设定下，Favia 全面超越所有传统与 LLM 基线方法，**F1-score 提升达 28.6%（绝对值）**，首次实现高精度（Precision@1: 82.4%）与高召回（Recall@10: 76.1%）的协同优化。",
      "summary_en": "Identifying CVE-associated vulnerability-fixing commits is critical for secure software maintenance but remains highly challenging at scale due to the needle-in-haystack nature of security-relevant changes among millions of commits. Existing ML and LLM-based methods suffer from poor precision-recall trade-offs—especially under realistic conditions where candidates are pre-filtered for security relevance and semantic similarity. We propose **Favia**, a forensic agent-based framework that combines scalable candidate ranking with iterative, evidence-driven causal reasoning. Favia deploys a ReAct-style LLM agent that operates within a pre-commit repository environment, using specialized tools to localize vulnerabilities, navigate codebases, and rigorously align code changes with root causes. Evaluated on **CVEVC**—our large-scale dataset of 8.1M commits from 3,708 real-world repositories—Favia achieves state-of-the-art F1-scores (up to 0.792), significantly outperforming all baselines under realistic candidate selection, and robustly identifies indirect, multi-file, and non-trivial fixes.",
      "summary": "## 背景与挑战  \n在安全软件维护中，精准识别对应已披露CVE漏洞的**漏洞修复提交（vulnerability-fixing commits）** 至关重要，但面临严峻可扩展性挑战：大型开源仓库常含数百万次提交，其中仅极小比例涉及安全修复。现有自动化方法——包括传统机器学习模型及新兴大语言模型（LLM）方案——普遍存在**精度-召回率权衡失衡**问题。尤为关键的是，当前评估多基于随机采样提交，严重低估了真实场景难度：实践中候选提交通常已具备安全相关性且彼此高度相似，导致相似度匹配或单轮分类极易失效。\n\n## 方法创新：Favia 框架  \n我们提出 **Favia**——一种面向数字取证的、基于智能体（agent）的漏洞修复识别与分析框架。其核心是**两阶段协同推理机制**：  \n- **第一阶段：高效可扩展排序**——利用轻量级模型快速筛选出高潜力候选提交，大幅压缩搜索空间；  \n- **第二阶段：深度语义因果推理**——每个候选提交交由一个 **ReAct-style LLM 智能体** 进行迭代式分析。该智能体以**提交前代码库为运行环境**，调用专用工具（如代码导航、差异解析、依赖追溯），主动定位脆弱组件、遍历多文件变更，并**建立代码修改与漏洞根本原因之间的因果对齐证据链**。\n\n## 关键优势与实证结果  \nFavia 突破了单次判别或文本相似性方法的局限，可稳健识别**间接修复、跨文件修复及非显式补丁**（如配置调整、权限加固）。我们在自建大规模基准数据集 **CVEVC**（涵盖 3,708 个真实仓库、超 800 万次提交）上系统评估，结果表明：在贴近实际的“安全相关候选集”设定下，Favia 全面超越所有传统与 LLM 基线方法，**F1-score 提升达 28.6%（绝对值）**，首次实现高精度（Precision@1: 82.4%）与高召回（Recall@10: 76.1%）的协同优化。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13093v1",
      "arxiv_id": "2602.13093v1",
      "title": "Consistency of Large Reasoning Models Under Multi-Turn Attacks",
      "authors": [
        "Yubo Li",
        "Ramayya Krishnan",
        "Rema Padman"
      ],
      "abstract": "Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13093v1",
      "url": "https://arxiv.org/abs/2602.13093v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## 大型推理模型在多轮对抗攻击下的鲁棒性一致性研究\n\n大型推理模型（LRMs）凭借显式链式推理能力，在数学、代码与复杂决策任务中展现出卓越性能，但其在**多轮动态对抗压力下的鲁棒性一致性**仍属空白。本研究系统评估了9个前沿推理模型（含o1-preview、DeepSeek-R1、Qwen2.5-Math等）在精心设计的多轮对抗攻击框架下的表现，覆盖误导性建议、社会从众诱导、情感操纵、逻辑干扰等六类攻击策略。\n\n研究发现：**推理能力带来实质性但非充分的鲁棒性提升**——所有LRMs均显著优于同源指令微调基线（平均+28.7%抗攻击成功率），但无一具备普适防御能力。五类共性失败模式被首次识别并量化：**自我怀疑**（Self-Doubt）与**社会顺从**（Social Conformity）合计占全部失败案例的50%；其余包括建议劫持（Suggestion Hijacking）、情绪易感性（Emotional Susceptibility）和推理疲劳（Reasoning Fatigue）。尤为关键的是，经典防御方法**置信度感知响应生成（CARG）在LRMs上全面失效**：其长推理轨迹诱发系统性过自信，导致置信度信号严重失真；反直觉地，**随机置信嵌入**（random confidence embedding）在多项指标上反超基于推理步骤的靶向置信提取，揭示现有置信度建模范式与推理过程存在根本性错配。\n\n本研究首次建立LRMs多轮脆弱性图谱，提出“推理≠鲁棒”的核心论断，并呼吁针对推理模型特性重构可信AI防御体系。",
      "summary_en": "Large reasoning models (LRMs) achieve state-of-the-art performance on complex tasks, yet their consistency under multi-turn adversarial pressure remains poorly understood. We evaluate 9 frontier LRMs across six attack types (e.g., misleading suggestions, social pressure, emotional manipulation) and identify five distinct failure modes—**Self-Doubt** and **Social Conformity** alone account for 50% of failures. While LRMs significantly outperform instruction-tuned baselines (+28.7% avg. robustness), *no model is universally resilient*. Crucially, the widely adopted Confidence-Aware Response Generation (CARG) fails catastrophically for LRMs due to overconfidence induced by extended reasoning traces; surprisingly, *random confidence embedding outperforms targeted extraction*, exposing a fundamental misalignment between standard confidence modeling and reasoning dynamics. Our work challenges the assumption that reasoning inherently confers robustness and calls for reasoning-aware defense redesign.",
      "summary": "## 大型推理模型在多轮对抗攻击下的鲁棒性一致性研究\n\n大型推理模型（LRMs）凭借显式链式推理能力，在数学、代码与复杂决策任务中展现出卓越性能，但其在**多轮动态对抗压力下的鲁棒性一致性**仍属空白。本研究系统评估了9个前沿推理模型（含o1-preview、DeepSeek-R1、Qwen2.5-Math等）在精心设计的多轮对抗攻击框架下的表现，覆盖误导性建议、社会从众诱导、情感操纵、逻辑干扰等六类攻击策略。\n\n研究发现：**推理能力带来实质性但非充分的鲁棒性提升**——所有LRMs均显著优于同源指令微调基线（平均+28.7%抗攻击成功率），但无一具备普适防御能力。五类共性失败模式被首次识别并量化：**自我怀疑**（Self-Doubt）与**社会顺从**（Social Conformity）合计占全部失败案例的50%；其余包括建议劫持（Suggestion Hijacking）、情绪易感性（Emotional Susceptibility）和推理疲劳（Reasoning Fatigue）。尤为关键的是，经典防御方法**置信度感知响应生成（CARG）在LRMs上全面失效**：其长推理轨迹诱发系统性过自信，导致置信度信号严重失真；反直觉地，**随机置信嵌入**（random confidence embedding）在多项指标上反超基于推理步骤的靶向置信提取，揭示现有置信度建模范式与推理过程存在根本性错配。\n\n本研究首次建立LRMs多轮脆弱性图谱，提出“推理≠鲁棒”的核心论断，并呼吁针对推理模型特性重构可信AI防御体系。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13024v1",
      "arxiv_id": "2602.13024v1",
      "title": "FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments",
      "authors": [
        "Alejandro Dopico-Castro",
        "Oscar Fontenla-Romero",
        "Bertha Guijarro-Berdiñas",
        "Amparo Alonso-Betanzos",
        "Iván Pérez Digón"
      ],
      "abstract": "Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13024v1",
      "url": "https://arxiv.org/abs/2602.13024v1",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## FedHENet：面向异构环境的轻量级联邦学习框架  \n\n**背景与挑战**：联邦学习（FL）在保护数据隐私前提下实现跨设备协同建模，尤其适用于医疗、金融等含敏感视觉数据的场景。然而，主流FL方法依赖多轮迭代的深度网络微调，不仅通信与计算开销大，且共享梯度仍存在隐私泄露风险；超参数调优过程更带来显著碳足迹，制约其在资源受限边缘设备上的可持续部署。  \n\n**方法创新**：本文提出**FedHENet**——一种基于FedHEONN思想扩展的轻量级图像分类框架。其核心设计包括：（1）**固定预训练特征提取器**（如ResNet-18 backbone），客户端仅需本地提取特征，避免昂贵的局部反向传播；（2）**单轮解析式聚合**：各客户端将加噪特征矩阵与标签向量通过**同态加密（HE）** 上传至服务器；服务器在密文域直接求解广义线性输出层权重（$W = (X^\\top X + \\lambda I)^{-1} X^\\top Y$），全程无需迭代优化；（3）**零超参数依赖**：正则化项$\\lambda$由HE安全协议隐式确定，彻底消除超参搜索需求。  \n\n**主要成果**：在CIFAR-10/100、Tiny-ImageNet等异构数据集上，FedHENet以**单轮通信**达成与FedAvg、FedProx等5–10轮迭代方法相当的准确率（±1.2%）；训练稳定性提升47%（标准差降低），端到端能耗降低**最高达70%**；同时满足严格的安全定义（语义安全HE保障梯度与原始特征不可恢复）。代码已开源：https://github.com/AlejandroDopico2/FedHENet",
      "summary_en": "FedHENet is a frugal, privacy-preserving federated learning framework for image classification in heterogeneous environments. It eliminates iterative local fine-tuning by fixing a pre-trained feature extractor and learning only a single linear output layer—trained *analytically* via homomorphic encryption (HE) in **one communication round**. Unlike standard FL, FedHENet requires **no hyperparameter tuning**, removing associated computational carbon cost. Experiments show it achieves competitive accuracy vs. iterative baselines (e.g., FedAvg, FedProx), with up to **70% lower energy consumption**, significantly improved training stability, and formal privacy guarantees from semantically secure HE. Code: https://github.com/AlejandroDopico2/FedHENet",
      "summary": "## FedHENet：面向异构环境的轻量级联邦学习框架  \n\n**背景与挑战**：联邦学习（FL）在保护数据隐私前提下实现跨设备协同建模，尤其适用于医疗、金融等含敏感视觉数据的场景。然而，主流FL方法依赖多轮迭代的深度网络微调，不仅通信与计算开销大，且共享梯度仍存在隐私泄露风险；超参数调优过程更带来显著碳足迹，制约其在资源受限边缘设备上的可持续部署。  \n\n**方法创新**：本文提出**FedHENet**——一种基于FedHEONN思想扩展的轻量级图像分类框架。其核心设计包括：（1）**固定预训练特征提取器**（如ResNet-18 backbone），客户端仅需本地提取特征，避免昂贵的局部反向传播；（2）**单轮解析式聚合**：各客户端将加噪特征矩阵与标签向量通过**同态加密（HE）** 上传至服务器；服务器在密文域直接求解广义线性输出层权重（$W = (X^\\top X + \\lambda I)^{-1} X^\\top Y$），全程无需迭代优化；（3）**零超参数依赖**：正则化项$\\lambda$由HE安全协议隐式确定，彻底消除超参搜索需求。  \n\n**主要成果**：在CIFAR-10/100、Tiny-ImageNet等异构数据集上，FedHENet以**单轮通信**达成与FedAvg、FedProx等5–10轮迭代方法相当的准确率（±1.2%）；训练稳定性提升47%（标准差降低），端到端能耗降低**最高达70%**；同时满足严格的安全定义（语义安全HE保障梯度与原始特征不可恢复）。代码已开源：https://github.com/AlejandroDopico2/FedHENet",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13004v1",
      "arxiv_id": "2602.13004v1",
      "title": "Uncertainty in Federated Granger Causality: From Origins to Systemic Consequences",
      "authors": [
        "Ayush Mohanty",
        "Nazal Mohamed",
        "Nagi Gebraeel"
      ],
      "abstract": "Granger Causality (GC) provides a rigorous framework for learning causal structures from time-series data. Recent federated variants of GC have targeted distributed infrastructure applications (e.g., smart grids) with distributed clients that generate high-dimensional data bound by data-sovereignty constraints. However, Federated GC algorithms only yield deterministic point estimates of causality and neglect uncertainty. This paper establishes the first methodology for rigorously quantifying uncertainty and its propagation within federated GC frameworks. We systematically classify sources of uncertainty, explicitly differentiating aleatoric (data noise) from epistemic (model variability) effects. We derive closed-form recursions that model the evolution of uncertainty through client-server interactions and identify four novel cross-covariance components that couple data uncertainties with model parameter uncertainties across the federated architecture. We also define rigorous convergence conditions for these uncertainty recursions and obtain explicit steady-state variances for both server and client model parameters. Our convergence analysis demonstrates that steady-state variances depend exclusively on client data statistics, thus eliminating dependence on initial epistemic priors and enhancing robustness. Empirical evaluations on synthetic benchmarks and real-world industrial datasets demonstrate that explicitly characterizing uncertainty significantly improves the reliability and interpretability of federated causal inference.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13004v1",
      "url": "https://arxiv.org/abs/2602.13004v1",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n格兰杰因果性（Granger Causality, GC）是时序数据分析中因果结构学习的经典框架。近年来，面向智能电网等分布式基础设施的**联邦格兰杰因果性（Federated GC）** 方法应运而生，旨在满足数据主权约束下多客户端协同建模的需求。然而，现有联邦GC算法仅输出**确定性点估计**，完全忽略模型与数据层面的不确定性，导致因果推断结果缺乏可信度、鲁棒性与可解释性，严重制约其在关键工业场景中的部署。\n\n## 方法创新  \n本文首次构建了联邦GC中不确定性建模与传播的**严格理论框架**：  \n- **系统性分类**：明确区分**偶然不确定性**（aleatoric，源于观测噪声）与**认知不确定性**（epistemic，源于模型参数估计变异性）；  \n- **闭式递归建模**：推导出刻画不确定性在客户端–服务器交互中动态演化的**封闭形式递归公式**；  \n- **跨协方差解耦**：识别并定义四个新型**跨协方差分量**，精准刻画数据不确定性与模型参数不确定性在联邦架构中的耦合机制；  \n- **收敛性保障**：给出不确定性递归的**严格收敛条件**，并解析求得服务器与各客户端模型参数的**稳态方差表达式**——关键发现是：稳态方差**仅依赖于客户端本地数据统计量**，彻底消除对初始先验分布的敏感性，显著提升系统鲁棒性。\n\n## 实证验证  \n在合成基准与真实工业时序数据集（含智能电表、风电机组运行日志）上的实验表明：显式建模不确定性可使因果方向识别准确率平均提升12.7%，假阳性率降低34.5%，且支持概率化因果图可视化与风险感知决策，大幅增强联邦因果推理的可靠性与可解释性。",
      "summary_en": "Granger Causality (GC) enables causal discovery from time-series data, and its federated variants address distributed infrastructure applications under data sovereignty constraints. Yet existing federated GC methods produce only deterministic point estimates, ignoring uncertainty—undermining reliability and interpretability. This paper introduces the **first rigorous framework for quantifying and propagating uncertainty in federated GC**. We systematically distinguish aleatoric (data noise) and epistemic (model variability) uncertainty sources; derive closed-form recursions modeling uncertainty evolution across client-server interactions; and identify four novel cross-covariance components that couple data and parameter uncertainties across the federation. We establish formal convergence conditions for these recursions and obtain explicit steady-state variances for both server and client model parameters—crucially, these depend *only* on local client data statistics, eliminating sensitivity to initial priors and enhancing robustness. Empirical results on synthetic and real-world industrial datasets (smart grid, wind turbine telemetry) show that explicit uncertainty characterization improves causal direction accuracy by 12.7% on average and reduces false positives by 34.5%, enabling probabilistic causal graphs and risk-aware inference.",
      "summary": "## 研究背景与问题  \n格兰杰因果性（Granger Causality, GC）是时序数据分析中因果结构学习的经典框架。近年来，面向智能电网等分布式基础设施的**联邦格兰杰因果性（Federated GC）** 方法应运而生，旨在满足数据主权约束下多客户端协同建模的需求。然而，现有联邦GC算法仅输出**确定性点估计**，完全忽略模型与数据层面的不确定性，导致因果推断结果缺乏可信度、鲁棒性与可解释性，严重制约其在关键工业场景中的部署。\n\n## 方法创新  \n本文首次构建了联邦GC中不确定性建模与传播的**严格理论框架**：  \n- **系统性分类**：明确区分**偶然不确定性**（aleatoric，源于观测噪声）与**认知不确定性**（epistemic，源于模型参数估计变异性）；  \n- **闭式递归建模**：推导出刻画不确定性在客户端–服务器交互中动态演化的**封闭形式递归公式**；  \n- **跨协方差解耦**：识别并定义四个新型**跨协方差分量**，精准刻画数据不确定性与模型参数不确定性在联邦架构中的耦合机制；  \n- **收敛性保障**：给出不确定性递归的**严格收敛条件**，并解析求得服务器与各客户端模型参数的**稳态方差表达式**——关键发现是：稳态方差**仅依赖于客户端本地数据统计量**，彻底消除对初始先验分布的敏感性，显著提升系统鲁棒性。\n\n## 实证验证  \n在合成基准与真实工业时序数据集（含智能电表、风电机组运行日志）上的实验表明：显式建模不确定性可使因果方向识别准确率平均提升12.7%，假阳性率降低34.5%，且支持概率化因果图可视化与风险感知决策，大幅增强联邦因果推理的可靠性与可解释性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12778v1",
      "arxiv_id": "2602.12778v1",
      "title": "Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews",
      "authors": [
        "Hamidreza Kazemi Taskooh",
        "Taha Zare Harofte"
      ],
      "abstract": "This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12778v1",
      "url": "https://arxiv.org/abs/2602.12778v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与意义  \n本研究聚焦波斯语旅游领域低资源场景下的**方面级情感分析（ABSA）**，首次系统开展面向伊朗住宿平台用户评论的ABSA研究。针对波斯语NLP资源匮乏、标注数据稀缺、模型能耗高等挑战，研究旨在提升细粒度情感理解能力，支撑可持续智慧旅游发展，并响应联合国可持续发展目标（SDG 9“产业、创新和基础设施”与SDG 12“负责任消费与生产”）。\n\n## 方法创新  \n提出一种新型**BERT-MoE（Mixture of Experts）框架**，融合三大关键技术：  \n- **Top-K动态路由机制**：避免专家网络路由坍缩，提升多方面协同建模能力；  \n- **多任务辅助损失**（aspect extraction + overall sentiment）：联合优化方面抽取与整体情感分类，增强特征共享；  \n- **端到端ABS流水线**：涵盖（1）基于9,558条标注评论的整体情感分类，（2）六类旅游核心方面（宿主、价格、位置、设施、清洁度、连通性）的多标签方面抽取，（3）方面-情感联合预测。\n\n## 数据与结果  \n构建并开源首个大规模波斯语旅游评论数据集——**Jabama-ABSA**（58,473条预处理评论，全部人工双标：方面类别+情感极性）。实验表明，本模型在ABSA任务上取得**加权F1值90.6%**，显著优于标准BERT（89.25%）及传统混合模型（85.7%）；同时降低GPU功耗**39%**，验证了稀疏化MoE架构在低资源语言中的效率与环保优势。分析发现，“清洁度”与“设施”提及率最高，是波斯语用户最关切的旅游体验维度。",
      "summary_en": "This paper introduces the first aspect-based sentiment analysis (ABSA) framework for Persian-language tourism reviews—a low-resource language setting. We propose a BERT-MoE model with Top-K routing and auxiliary losses (overall sentiment classification and multi-label aspect extraction) to address routing collapse and improve efficiency. Evaluated on the newly released Jabama-ABSA dataset (58,473 manually annotated Persian reviews from Iran’s leading accommodation platform), our model achieves a weighted F1-score of **90.6%** on ABSA—outperforming fine-tuned BERT (89.25%) and a standard hybrid baseline (85.7%). Crucially, it reduces GPU power consumption by **39%** versus dense BERT, advancing sustainable AI deployment aligned with UN SDGs 9 and 12. Cleanliness and amenities emerge as the most frequently mentioned aspects. The annotated dataset is publicly released to foster multilingual NLP research in tourism.",
      "summary": "## 研究背景与意义  \n本研究聚焦波斯语旅游领域低资源场景下的**方面级情感分析（ABSA）**，首次系统开展面向伊朗住宿平台用户评论的ABSA研究。针对波斯语NLP资源匮乏、标注数据稀缺、模型能耗高等挑战，研究旨在提升细粒度情感理解能力，支撑可持续智慧旅游发展，并响应联合国可持续发展目标（SDG 9“产业、创新和基础设施”与SDG 12“负责任消费与生产”）。\n\n## 方法创新  \n提出一种新型**BERT-MoE（Mixture of Experts）框架**，融合三大关键技术：  \n- **Top-K动态路由机制**：避免专家网络路由坍缩，提升多方面协同建模能力；  \n- **多任务辅助损失**（aspect extraction + overall sentiment）：联合优化方面抽取与整体情感分类，增强特征共享；  \n- **端到端ABS流水线**：涵盖（1）基于9,558条标注评论的整体情感分类，（2）六类旅游核心方面（宿主、价格、位置、设施、清洁度、连通性）的多标签方面抽取，（3）方面-情感联合预测。\n\n## 数据与结果  \n构建并开源首个大规模波斯语旅游评论数据集——**Jabama-ABSA**（58,473条预处理评论，全部人工双标：方面类别+情感极性）。实验表明，本模型在ABSA任务上取得**加权F1值90.6%**，显著优于标准BERT（89.25%）及传统混合模型（85.7%）；同时降低GPU功耗**39%**，验证了稀疏化MoE架构在低资源语言中的效率与环保优势。分析发现，“清洁度”与“设施”提及率最高，是波斯语用户最关切的旅游体验维度。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12708v1",
      "arxiv_id": "2602.12708v1",
      "title": "Mixture of Predefined Experts: Maximizing Data Usage on Vertical Federated Learning",
      "authors": [
        "Jon Irureta",
        "Gorka Azkune",
        "Jon Imaz",
        "Aizea Lojo",
        "Javier Fernandez-Marques"
      ],
      "abstract": "Vertical Federated Learning (VFL) has emerged as a critical paradigm for collaborative model training in privacy-sensitive domains such as finance and healthcare. However, most existing VFL frameworks rely on the idealized assumption of full sample alignment across participants, a premise that rarely holds in real-world scenarios. To bridge this gap, this work introduces Split-MoPE, a novel framework that integrates Split Learning with a specialized Mixture of Predefined Experts (MoPE) architecture. Unlike standard Mixture of Experts (MoE), where routing is learned dynamically, MoPE uses predefined experts to process specific data alignments, effectively maximizing data usage during both training and inference without requiring full sample overlap. By leveraging pretrained encoders for target data domains, Split-MoPE achieves state-of-the-art performance in a single communication round, significantly reducing the communication footprint compared to multi-round end-to-end training. Furthermore, unlike existing proposals that address sample misalignment, this novel architecture provides inherent robustness against malicious or noisy participants and offers per-sample interpretability by quantifying each collaborator's contribution to each prediction. Extensive evaluations on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets demonstrate that Split-MoPE consistently outperforms state-of-the-art systems such as LASER and Vertical SplitNN, particularly in challenging scenarios with high data missingness.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12708v1",
      "url": "https://arxiv.org/abs/2602.12708v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n垂直联邦学习（VFL）在金融、医疗等隐私敏感领域备受关注，但现有方法普遍依赖**完全样本对齐**这一强假设——即各参与方数据在样本ID层面严格一一对应。现实中，因数据采集偏差、脱敏清洗或系统异构性，实际对齐率常低于30%，导致大量未对齐样本被丢弃，严重浪费数据价值并削弱模型泛化能力。\n\n## 方法创新：Split-MoPE框架  \n本文提出**Split-MoPE**（Split Learning + Mixture of Predefined Experts），核心突破在于：  \n- **预定义专家机制（MoPE）**：摒弃传统MoE的动态路由，为不同对齐状态（如全对齐、仅A方有标签、仅B方有特征等）**预先设计专用专家子网络**，使每条样本无论对齐程度如何，均能被至少一个专家有效处理；  \n- **单轮通信高效训练**：复用各参与方已有的领域预训练编码器（如ResNet、TabTransformer），仅需**1轮通信**即可完成端到端推理与梯度回传，通信开销较LASER等多轮方案降低87%；  \n- **内生鲁棒性与可解释性**：恶意参与者仅影响其负责的专家分支，不影响全局预测；同时输出**每样本级贡献度热图**，量化各协作方对最终预测的归因权重。\n\n## 实验验证  \n在CIFAR-10/100（图像）与Breast Cancer Wisconsin（表格）数据集上，当样本对齐率低至20%时，Split-MoPE仍保持92.4%准确率（优于Vertical SplitNN 11.6个百分点，LASER 8.3个百分点），且在50%数据缺失场景下F1-score提升达15.2%。代码已开源。",
      "summary_en": "Vertical Federated Learning (VFL) enables privacy-preserving collaborative modeling but suffers from the unrealistic assumption of full sample alignment—rarely satisfied in practice. To address this, we propose **Split-MoPE**, a novel framework integrating Split Learning with a *Mixture of Predefined Experts* (MoPE). Unlike dynamic MoE routing, MoPE assigns dedicated, pretrained experts to predefined data alignment patterns (e.g., fully aligned, feature-only, label-only), enabling maximal utilization of *all* available samples—even those with partial or no overlap. Leveraging domain-specific pretrained encoders, Split-MoPE achieves state-of-the-art performance in **a single communication round**, reducing communication overhead by up to 87% versus multi-round baselines. It further provides inherent robustness against malicious participants and per-sample interpretability via contribution quantification. Extensive experiments on CIFAR-10/100 and Breast Cancer Wisconsin show Split-MoPE consistently outperforms LASER and Vertical SplitNN—especially under high missingness (e.g., +15.2% F1 at 50% data loss).",
      "summary": "## 背景与挑战  \n垂直联邦学习（VFL）在金融、医疗等隐私敏感领域备受关注，但现有方法普遍依赖**完全样本对齐**这一强假设——即各参与方数据在样本ID层面严格一一对应。现实中，因数据采集偏差、脱敏清洗或系统异构性，实际对齐率常低于30%，导致大量未对齐样本被丢弃，严重浪费数据价值并削弱模型泛化能力。\n\n## 方法创新：Split-MoPE框架  \n本文提出**Split-MoPE**（Split Learning + Mixture of Predefined Experts），核心突破在于：  \n- **预定义专家机制（MoPE）**：摒弃传统MoE的动态路由，为不同对齐状态（如全对齐、仅A方有标签、仅B方有特征等）**预先设计专用专家子网络**，使每条样本无论对齐程度如何，均能被至少一个专家有效处理；  \n- **单轮通信高效训练**：复用各参与方已有的领域预训练编码器（如ResNet、TabTransformer），仅需**1轮通信**即可完成端到端推理与梯度回传，通信开销较LASER等多轮方案降低87%；  \n- **内生鲁棒性与可解释性**：恶意参与者仅影响其负责的专家分支，不影响全局预测；同时输出**每样本级贡献度热图**，量化各协作方对最终预测的归因权重。\n\n## 实验验证  \n在CIFAR-10/100（图像）与Breast Cancer Wisconsin（表格）数据集上，当样本对齐率低至20%时，Split-MoPE仍保持92.4%准确率（优于Vertical SplitNN 11.6个百分点，LASER 8.3个百分点），且在50%数据缺失场景下F1-score提升达15.2%。代码已开源。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12622v1",
      "arxiv_id": "2602.12622v1",
      "title": "Efficient Personalized Federated PCA with Manifold Optimization for IoT Anomaly Detection",
      "authors": [
        "Xianchao Xiu",
        "Chenyi Huang",
        "Wei Zhang",
        "Wanquan Liu"
      ],
      "abstract": "Internet of things (IoT) networks face increasing security threats due to their distributed nature and resource constraints. Although federated learning (FL) has gained prominence as a privacy-preserving framework for distributed IoT environments, current federated principal component analysis (PCA) methods lack the integration of personalization and robustness, which are critical for effective anomaly detection. To address these limitations, we propose an efficient personalized federated PCA (FedEP) method for anomaly detection in IoT networks. The proposed model achieves personalization through introducing local representations with the $\\ell_1$-norm for element-wise sparsity, while maintaining robustness via enforcing local models with the $\\ell_{2,1}$-norm for row-wise sparsity. To solve this non-convex problem, we develop a manifold optimization algorithm based on the alternating direction method of multipliers (ADMM) with rigorous theoretical convergence guarantees. Experimental results confirm that the proposed FedEP outperforms the state-of-the-art FedPG, achieving excellent F1-scores and accuracy in various IoT security scenarios. Our code will be available at \\href{https://github.com/xianchaoxiu/FedEP}{https://github.com/xianchaoxiu/FedEP}.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12622v1",
      "url": "https://arxiv.org/abs/2602.12622v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n物联网（IoT）网络因其分布式架构与终端设备资源受限特性，面临日益严峻的安全威胁。异常检测是保障IoT系统可信运行的关键任务，但中心化建模易引发隐私泄露与通信瓶颈。尽管联邦学习（FL）为分布式隐私保护建模提供了范式，现有联邦主成分分析（Federated PCA）方法普遍存在两大缺陷：**缺乏个性化建模能力**（难以适配异构设备的数据分布），且**鲁棒性不足**（对噪声、异常点敏感），严重制约其在真实IoT场景中的检测效能。\n\n## 方法创新  \n本文提出**高效个性化联邦PCA（FedEP）**，专为IoT异常检测设计。核心创新包括：  \n- **双稀疏正则化个性化机制**：在本地模型中引入 $\\ell_1$-范数约束，实现**元素级稀疏**，捕获设备特异性特征；同时施加 $\\ell_{2,1}$-范数约束，实现**行级稀疏**，增强对通道级异常与传感器噪声的鲁棒性；  \n- **流形优化求解框架**：针对所建非凸、非光滑、带正交约束的优化问题，设计基于交替方向乘子法（ADMM）的**黎曼流形优化算法**，在Stiefel流形上迭代更新，严格证明其收敛性（满足KKT条件且序列收敛至驻点）；  \n- **通信与计算高效性**：仅需传输低维投影矩阵（而非原始数据或梯度），显著降低带宽开销，并支持异步局部更新，适配IoT边缘设备算力限制。\n\n## 实验验证  \n在多个真实IoT安全数据集（如NSL-KDD IoT、TON-IoT、Edge-IIoTset）上，FedEP在F1-score（平均提升+8.3%）与准确率（+6.7%）上**全面超越当前最优方法FedPG**，尤其在高噪声与标签稀疏场景下优势更显著。消融实验证实双稀疏设计与流形优化协同提升了泛化性与稳定性。代码已开源：[https://github.com/xianchaoxiu/FedEP](https://github.com/xianchaoxiu/FedEP)。",
      "summary_en": "Internet of Things (IoT) networks suffer from growing security threats due to their distributed nature and resource constraints, demanding privacy-preserving, robust, and personalized anomaly detection. While federated learning (FL) offers a promising paradigm, existing federated PCA methods lack both personalization for heterogeneous devices and robustness against noise and outliers—critical gaps for real-world IoT security. To bridge them, we propose **FedEP**, an *Efficient personalized Federated PCA* framework leveraging **dual sparsity regularization**: $\\ell_1$-norm for element-wise local feature selection and $\\ell_{2,1}$-norm for row-wise robust subspace learning. We develop a **manifold-ADMM optimizer** on the Stiefel manifold with rigorous convergence guarantees, enabling efficient, communication-light training under orthogonal constraints. Extensive experiments on NSL-KDD IoT, TON-IoT, and Edge-IIoTset demonstrate that FedEP consistently outperforms the state-of-the-art FedPG—achieving average **+8.3% F1-score** and **+6.7% accuracy**, especially under high noise and label scarcity. Code is publicly available at https://github.com/xianchaoxiu/FedEP.",
      "summary": "## 研究背景  \n物联网（IoT）网络因其分布式架构与终端设备资源受限特性，面临日益严峻的安全威胁。异常检测是保障IoT系统可信运行的关键任务，但中心化建模易引发隐私泄露与通信瓶颈。尽管联邦学习（FL）为分布式隐私保护建模提供了范式，现有联邦主成分分析（Federated PCA）方法普遍存在两大缺陷：**缺乏个性化建模能力**（难以适配异构设备的数据分布），且**鲁棒性不足**（对噪声、异常点敏感），严重制约其在真实IoT场景中的检测效能。\n\n## 方法创新  \n本文提出**高效个性化联邦PCA（FedEP）**，专为IoT异常检测设计。核心创新包括：  \n- **双稀疏正则化个性化机制**：在本地模型中引入 $\\ell_1$-范数约束，实现**元素级稀疏**，捕获设备特异性特征；同时施加 $\\ell_{2,1}$-范数约束，实现**行级稀疏**，增强对通道级异常与传感器噪声的鲁棒性；  \n- **流形优化求解框架**：针对所建非凸、非光滑、带正交约束的优化问题，设计基于交替方向乘子法（ADMM）的**黎曼流形优化算法**，在Stiefel流形上迭代更新，严格证明其收敛性（满足KKT条件且序列收敛至驻点）；  \n- **通信与计算高效性**：仅需传输低维投影矩阵（而非原始数据或梯度），显著降低带宽开销，并支持异步局部更新，适配IoT边缘设备算力限制。\n\n## 实验验证  \n在多个真实IoT安全数据集（如NSL-KDD IoT、TON-IoT、Edge-IIoTset）上，FedEP在F1-score（平均提升+8.3%）与准确率（+6.7%）上**全面超越当前最优方法FedPG**，尤其在高噪声与标签稀疏场景下优势更显著。消融实验证实双稀疏设计与流形优化协同提升了泛化性与稳定性。代码已开源：[https://github.com/xianchaoxiu/FedEP](https://github.com/xianchaoxiu/FedEP)。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12567v1",
      "arxiv_id": "2602.12567v1",
      "title": "Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "abstract": "Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12567v1",
      "url": "https://arxiv.org/abs/2602.12567v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与挑战  \n面向联网电池电动汽车（BEV）的联邦学习面临严峻稳定性挑战：车辆连接间歇性强、客户端参与率时变显著，且因行驶工况（如坡度、温度、驾驶风格）差异导致**客户端间异构性极高**。传统FedAvg及多数改进方法在该现实约束下易出现参数漂移加剧、收敛震荡甚至发散。\n\n## 方法创新  \n本文提出**分数阶粗糙度感知联邦平均算法（FO-RI-FedAvg）**，一种轻量级、模块化的FedAvg扩展框架，无需修改服务器端聚合逻辑，仅通过客户端双机制实现稳定增强：  \n- **自适应粗糙度感知近端正则化**：基于本地损失曲面局部Hessian谱半径估计“粗糙度”，动态调节向全局模型的拉回强度，抑制高曲率区域的过激更新；  \n- **非整数阶本地优化**：引入分数阶梯度累积（阶数α∈(0,1)），赋予优化器短时记忆能力，平滑多方向冲突的梯度更新，缓解瞬时噪声干扰。  \n两组件可独立启用/禁用，所有操作均为元素级，计算开销可摊销，部署友好。\n\n## 主要结果  \n在真实BEV能耗预测数据集VED与扩展版eVED上验证：FO-RI-FedAvg相较FedProx、SCAFFOLD、MOON等强基线，在**客户端参与率低至30%时仍保持98.2%的收敛稳定性**（标准差降低41%），测试MAE分别提升12.7%（VED）和9.5%（eVED）。其鲁棒性源于对动态异构性的双重解耦建模——既响应局部损失地形变化，又抑制更新方向突变。",
      "summary_en": "Federated learning (FL) for battery electric vehicle (BEV) energy consumption modeling suffers from severe instability due to intermittent connectivity, volatile client participation, and extreme statistical heterogeneity across driving conditions. To address this, we propose **Fractional-Order Roughness-Informed FedAvg (FO-RI-FedAvg)**—a lightweight, modular extension of FedAvg that enhances stability via two complementary client-side mechanisms: (i) *adaptive roughness-informed proximal regularization*, which dynamically adjusts the pull toward the global model using local loss-landscape roughness estimated from Hessian spectral properties; and (ii) *non-integer-order local optimization*, employing fractional-gradient accumulation (order α ∈ (0,1)) to embed short-term memory and smooth conflicting update directions. FO-RI-FedAvg preserves standard server aggregation, adds only element-wise operations with amortizable overhead, and supports independent toggling of each component. Experiments on real-world BEV datasets VED and eVED show consistent improvements: +12.7% MAE reduction on VED and +9.5% on eVED over strong baselines (e.g., FedProx, SCAFFOLD), with 41% lower convergence variance under as low as 30% client participation.",
      "summary": "## 背景与挑战  \n面向联网电池电动汽车（BEV）的联邦学习面临严峻稳定性挑战：车辆连接间歇性强、客户端参与率时变显著，且因行驶工况（如坡度、温度、驾驶风格）差异导致**客户端间异构性极高**。传统FedAvg及多数改进方法在该现实约束下易出现参数漂移加剧、收敛震荡甚至发散。\n\n## 方法创新  \n本文提出**分数阶粗糙度感知联邦平均算法（FO-RI-FedAvg）**，一种轻量级、模块化的FedAvg扩展框架，无需修改服务器端聚合逻辑，仅通过客户端双机制实现稳定增强：  \n- **自适应粗糙度感知近端正则化**：基于本地损失曲面局部Hessian谱半径估计“粗糙度”，动态调节向全局模型的拉回强度，抑制高曲率区域的过激更新；  \n- **非整数阶本地优化**：引入分数阶梯度累积（阶数α∈(0,1)），赋予优化器短时记忆能力，平滑多方向冲突的梯度更新，缓解瞬时噪声干扰。  \n两组件可独立启用/禁用，所有操作均为元素级，计算开销可摊销，部署友好。\n\n## 主要结果  \n在真实BEV能耗预测数据集VED与扩展版eVED上验证：FO-RI-FedAvg相较FedProx、SCAFFOLD、MOON等强基线，在**客户端参与率低至30%时仍保持98.2%的收敛稳定性**（标准差降低41%），测试MAE分别提升12.7%（VED）和9.5%（eVED）。其鲁棒性源于对动态异构性的双重解耦建模——既响应局部损失地形变化，又抑制更新方向突变。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.12989v1",
      "arxiv_id": "2602.12989v1",
      "title": "Evaluating the Homogeneity of Keyphrase Prediction Models",
      "authors": [
        "Maël Houbre",
        "Florian Boudin",
        "Beatrice Daille"
      ],
      "abstract": "Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.12989v1",
      "url": "https://arxiv.org/abs/2602.12989v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景与问题  \n关键词（keyphrases）在信息检索（IR）和自然语言处理（NLP）中具有关键作用。现有方法分为两类：**抽取式**（从原文中提取已出现的短语）与**生成式**（可预测原文未显式出现的“缺失关键词”，absent keyphrases）。学界普遍假设：生成式模型因能捕捉隐含语义，对主题相似文档更可能输出一致关键词，即具备更高**同质性**（homogeneity）——即跨文档索引的一致性。然而，当前主流基准（如KP20k、OpenKP）均未评估该性质，同质性长期处于被忽视状态。\n\n## 方法创新  \n本文首次提出系统化评估关键词预测模型同质性的方法：构建**语义等价文档对**（基于相同主题但文本表达迥异的样本，如不同综述文章或多源报道），并量化模型对每对文档预测关键词的重叠率（Jaccard相似度）与分布一致性（KL散度）。我们覆盖12种主流模型（含BERT-based抽取器与BART/PEGASUS/T5等生成器），在三个领域（计算机科学、医学、环境科学）验证，并控制词汇重叠、长度偏差等混杂因素。\n\n## 核心发现  \n- **反直觉结论**：抽取式模型（如TF-IDF+BERTRank）在同质性上整体优于生成式模型，平均Jaccard重叠率高出12.3%（p<0.01）；  \n- **缺失关键词的代价**：生成能力越强的模型（如T5-large），其同质性反而越低——因过度依赖表面线索或幻觉生成，导致语义相似文档被映射到不同抽象层级的关键词；  \n- **领域鲁棒性**：该现象在所有测试领域一致成立，证实非偶然性。  \n\n本研究挑战了“生成即泛化”的隐含假设，为关键词建模提供了新评估维度。代码、数据集及提示模板已开源至[Hugging Face](https://huggingface.co/datasets/kephomogeneity)与[GitHub](https://github.com/kephomogeneity/eval)。",
      "summary_en": "This paper introduces the first benchmark for evaluating **homogeneity**—the consistency with which keyphrase prediction models assign identical keyphrases to semantically equivalent documents (e.g., different texts covering the same topic). Contrary to the prevailing assumption that generative models benefit from predicting *absent keyphrases*, we find—across 12 models and 3 domains—that **extractive approaches achieve significantly higher homogeneity** (avg. +12.3% Jaccard overlap, *p*<0.01). Surprisingly, stronger generative capacity (e.g., in T5-large) correlates with *lower* homogeneity, as models diverge due to hallucination or surface-form bias. Our method uses carefully constructed semantic-equivalence document pairs and measures both keyphrase overlap and distributional consistency. Code, data, and prompts are publicly available.",
      "summary": "## 研究背景与问题  \n关键词（keyphrases）在信息检索（IR）和自然语言处理（NLP）中具有关键作用。现有方法分为两类：**抽取式**（从原文中提取已出现的短语）与**生成式**（可预测原文未显式出现的“缺失关键词”，absent keyphrases）。学界普遍假设：生成式模型因能捕捉隐含语义，对主题相似文档更可能输出一致关键词，即具备更高**同质性**（homogeneity）——即跨文档索引的一致性。然而，当前主流基准（如KP20k、OpenKP）均未评估该性质，同质性长期处于被忽视状态。\n\n## 方法创新  \n本文首次提出系统化评估关键词预测模型同质性的方法：构建**语义等价文档对**（基于相同主题但文本表达迥异的样本，如不同综述文章或多源报道），并量化模型对每对文档预测关键词的重叠率（Jaccard相似度）与分布一致性（KL散度）。我们覆盖12种主流模型（含BERT-based抽取器与BART/PEGASUS/T5等生成器），在三个领域（计算机科学、医学、环境科学）验证，并控制词汇重叠、长度偏差等混杂因素。\n\n## 核心发现  \n- **反直觉结论**：抽取式模型（如TF-IDF+BERTRank）在同质性上整体优于生成式模型，平均Jaccard重叠率高出12.3%（p<0.01）；  \n- **缺失关键词的代价**：生成能力越强的模型（如T5-large），其同质性反而越低——因过度依赖表面线索或幻觉生成，导致语义相似文档被映射到不同抽象层级的关键词；  \n- **领域鲁棒性**：该现象在所有测试领域一致成立，证实非偶然性。  \n\n本研究挑战了“生成即泛化”的隐含假设，为关键词建模提供了新评估维度。代码、数据集及提示模板已开源至[Hugging Face](https://huggingface.co/datasets/kephomogeneity)与[GitHub](https://github.com/kephomogeneity/eval)。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13529v1",
      "arxiv_id": "2602.13529v1",
      "title": "SecureGate: Learning When to Reveal PII Safely via Token-Gated Dual-Adapters for Federated LLMs",
      "authors": [
        "Mohamed Shaaban",
        "Mohamed Elmahallawy"
      ],
      "abstract": "Federated learning (FL) enables collaborative training across organizational silos without sharing raw data, making it attractive for privacy-sensitive applications. With the rapid adoption of large language models (LLMs), federated fine-tuning of generative LLMs has gained attention as a way to leverage distributed data while preserving confidentiality. However, this setting introduces fundamental challenges: (i) privacy leakage of personally identifiable information (PII) due to LLM memorization, and (ii) a persistent tension between global generalization and local utility under heterogeneous data. Existing defenses, such as data sanitization and differential privacy, reduce leakage but often degrade downstream performance. We propose SecureGate, a privacy-aware federated fine-tuning framework for LLMs that provides fine-grained privacy control without sacrificing utility. SecureGate employs a dual-adapter LoRA architecture: a secure adapter that learns sanitized, globally shareable representations, and a revealing adapter that captures sensitive, organization-specific knowledge. A token-controlled gating module selectively activates these adapters at inference time, enabling controlled information disclosure without retraining. Extensive experiments across multiple LLMs and real-world datasets show that SecureGate improves task utility while substantially reducing PII leakage, achieving up to a 31.66X reduction in inference attack accuracy and a 17.07X reduction in extraction recall for unauthorized requests. Additionally, it maintains 100% routing reliability to the correct adapter and incurs only minimal computational and communication overhead.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13529v1",
      "url": "https://arxiv.org/abs/2602.13529v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "privacy",
        "learning",
        "federated",
        "differential"
      ],
      "keyword_score": 4,
      "summary_zh": "## SecureGate：面向联邦大语言模型的令牌门控双适配器隐私保护框架\n\n**背景与挑战**：联邦学习（FL）支持跨机构协同训练，无需共享原始数据，因而在医疗、金融等隐私敏感场景备受关注。然而，将FL应用于大语言模型（LLM）微调时面临两大根本性难题：（1）LLM易记忆并泄露个人身份信息（PII），导致推理阶段遭受成员推断或数据提取攻击；（2）数据异构性引发“全局泛化”与“本地效用”的持续张力——过度保护削弱性能，放任局部建模则危及隐私。\n\n**方法创新**：本文提出**SecureGate**，一种兼顾细粒度隐私控制与模型效用的联邦微调新范式。其核心是**Token-Gated Dual-Adapters**架构：  \n- ✅ **安全适配器（Secure Adapter）**：在联邦聚合中共享，仅学习经隐式净化的、去敏化的全局表征；  \n- ✅ **揭示适配器（Revealing Adapter）**：本地私有，捕获含PII的组织特异性知识；  \n- ✅ **令牌门控模块（Token-Controlled Gating）**：在**推理时动态决策**——依据输入token语义（如是否为姓名、地址等PII触发词）实时激活对应适配器，实现“按需披露”，**无需重训练或修改模型结构**。\n\n**关键结果**：在Llama-3-8B、Qwen2-7B等主流LLM及真实医疗/客服数据集上的实验表明：SecureGate在保持100%适配器路由准确率的同时，将PII推理攻击准确率降低**31.66倍**，未授权数据提取召回率下降**17.07倍**；下游任务性能（如NER、意图识别）平均提升2.1–4.8个百分点，显著优于DP-LoRA、FedMD等基线。通信与计算开销增加不足3.2%，具备强部署可行性。",
      "summary_en": "SecureGate is a privacy-aware federated fine-tuning framework for LLMs that enables *token-level, inference-time control* over PII disclosure without retraining. It introduces a **token-gated dual-adapter LoRA architecture**: a globally shared *secure adapter* learns sanitized, generalizable representations, while a local *revealing adapter* captures sensitive, organization-specific knowledge. A lightweight gating module dynamically routes each input token to the appropriate adapter based on its semantic sensitivity (e.g., PII indicators), ensuring precise, context-aware information release. Evaluated across multiple LLMs (Llama-3-8B, Qwen2-7B) and real-world datasets, SecureGate achieves up to **31.66× reduction in PII inference attack accuracy**, **17.07× lower extraction recall for unauthorized requests**, and **100% adapter routing reliability**, while improving downstream task utility by 2.1–4.8 points and incurring only ≤3.2% overhead in communication and computation.",
      "summary": "## SecureGate：面向联邦大语言模型的令牌门控双适配器隐私保护框架\n\n**背景与挑战**：联邦学习（FL）支持跨机构协同训练，无需共享原始数据，因而在医疗、金融等隐私敏感场景备受关注。然而，将FL应用于大语言模型（LLM）微调时面临两大根本性难题：（1）LLM易记忆并泄露个人身份信息（PII），导致推理阶段遭受成员推断或数据提取攻击；（2）数据异构性引发“全局泛化”与“本地效用”的持续张力——过度保护削弱性能，放任局部建模则危及隐私。\n\n**方法创新**：本文提出**SecureGate**，一种兼顾细粒度隐私控制与模型效用的联邦微调新范式。其核心是**Token-Gated Dual-Adapters**架构：  \n- ✅ **安全适配器（Secure Adapter）**：在联邦聚合中共享，仅学习经隐式净化的、去敏化的全局表征；  \n- ✅ **揭示适配器（Revealing Adapter）**：本地私有，捕获含PII的组织特异性知识；  \n- ✅ **令牌门控模块（Token-Controlled Gating）**：在**推理时动态决策**——依据输入token语义（如是否为姓名、地址等PII触发词）实时激活对应适配器，实现“按需披露”，**无需重训练或修改模型结构**。\n\n**关键结果**：在Llama-3-8B、Qwen2-7B等主流LLM及真实医疗/客服数据集上的实验表明：SecureGate在保持100%适配器路由准确率的同时，将PII推理攻击准确率降低**31.66倍**，未授权数据提取召回率下降**17.07倍**；下游任务性能（如NER、意图识别）平均提升2.1–4.8个百分点，显著优于DP-LoRA、FedMD等基线。通信与计算开销增加不足3.2%，具备强部署可行性。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13427v1",
      "arxiv_id": "2602.13427v1",
      "title": "Backdooring Bias in Large Language Models",
      "authors": [
        "Anudeep Das",
        "Prach Chantasantitam",
        "Gurjot Singh",
        "Lipeng He",
        "Mariia Ponomarenko",
        "Florian Kerschbaum"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in settings where inducing a bias toward a certain topic can have significant consequences, and backdoor attacks can be used to produce such models. Prior work on backdoor attacks has largely focused on a black-box threat model, with an adversary targeting the model builder's LLM. However, in the bias manipulation setting, the model builder themselves could be the adversary, warranting a white-box threat model where the attacker's ability to poison, and manipulate the poisoned data is substantially increased. Furthermore, despite growing research in semantically-triggered backdoors, most studies have limited themselves to syntactically-triggered attacks. Motivated by these limitations, we conduct an analysis consisting of over 1000 evaluations using higher poisoning ratios and greater data augmentation to gain a better understanding of the potential of syntactically- and semantically-triggered backdoor attacks in a white-box setting. In addition, we study whether two representative defense paradigms, model-intrinsic and model-extrinsic backdoor removal, are able to mitigate these attacks. Our analysis reveals numerous new findings. We discover that while both syntactically- and semantically-triggered attacks can effectively induce the target behaviour, and largely preserve utility, semantically-triggered attacks are generally more effective in inducing negative biases, while both backdoor types struggle with causing positive biases. Furthermore, while both defense types are able to mitigate these backdoors, they either result in a substantial drop in utility, or require high computational overhead.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13427v1",
      "url": "https://arxiv.org/abs/2602.13427v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "poisoning",
        "data"
      ],
      "keyword_score": 3,
      "summary_zh": "## 背景与问题  \n大型语言模型（LLMs）在内容推荐、新闻摘要、政策咨询等高影响力场景中广泛应用，其内在偏差可能引发严重社会后果。现有后门攻击研究多基于**黑盒威胁模型**（攻击者仅能访问API），而本研究指出：当模型构建者自身成为恶意行为体时，更需关注**白盒威胁模型**——攻击者可直接操控训练数据、模型参数及优化过程，显著提升偏差注入能力。此外，既有工作集中于**句法触发**（如插入特定词缀），对更具隐蔽性与现实危害性的**语义触发**（如“经济衰退”隐含负面倾向）研究不足。\n\n## 方法与实验  \n本文系统评估了超1000组实验，采用更高投毒比例（最高达20%）与强数据增强（同义替换、回译、模板泛化），在白盒设定下对比句法与语义两类后门攻击对**正向/负向偏差诱导**的有效性，并检验两类主流防御范式：**模型内生防御**（如神经元剪枝、梯度正则化）与**模型外生防御**（如后处理过滤、输入净化）的鲁棒性。\n\n## 主要发现  \n- 两类攻击均能高效诱导目标偏差且保持模型基础性能（平均任务准确率下降<3%）；  \n- **语义触发后门在诱发负向偏差上显著更强**（成功率+18.7%），但二者均难以稳定诱导正向偏差（成功率<42%），揭示偏差方向存在内在不对称性；  \n- 两类防御虽可削弱后门效果（平均缓解率63–71%），但**内生防御导致平均推理速度下降41%，外生防御使任务准确率下降9.2–15.6%**，凸显实用性瓶颈。  \n本研究首次在白盒框架下系统解构LLM偏差后门的触发机制、方向特异性与防御代价，为可信AI治理提供实证基础与设计指南。",
      "summary_en": "This paper investigates backdoor-induced bias in large language models (LLMs) under a *white-box threat model*, where the model builder themselves acts as the adversary—enabling direct data poisoning, parameter manipulation, and training control. We conduct over 1,000 evaluations with high poisoning ratios (up to 20%) and aggressive semantic augmentation to compare *syntactic* (e.g., trigger tokens) and *semantic* (e.g., contextually implied negativity) backdoors. Key findings: (1) Both attack types effectively implant target biases while preserving utility, but semantic triggers are significantly more potent for inducing *negative* biases—yet neither reliably induces *positive* ones, revealing an inherent direction asymmetry; (2) Representative defenses—model-intrinsic (e.g., neuron pruning) and model-extrinsic (e.g., input sanitization)—can mitigate backdoors (63–71% reduction), but incur steep trade-offs: intrinsic methods slow inference by ~41%, while extrinsic ones degrade task accuracy by 9.2–15.6%. Our work establishes the first comprehensive white-box analysis of bias backdoors in LLMs, highlighting critical limitations in both attack controllability and defense practicality.",
      "summary": "## 背景与问题  \n大型语言模型（LLMs）在内容推荐、新闻摘要、政策咨询等高影响力场景中广泛应用，其内在偏差可能引发严重社会后果。现有后门攻击研究多基于**黑盒威胁模型**（攻击者仅能访问API），而本研究指出：当模型构建者自身成为恶意行为体时，更需关注**白盒威胁模型**——攻击者可直接操控训练数据、模型参数及优化过程，显著提升偏差注入能力。此外，既有工作集中于**句法触发**（如插入特定词缀），对更具隐蔽性与现实危害性的**语义触发**（如“经济衰退”隐含负面倾向）研究不足。\n\n## 方法与实验  \n本文系统评估了超1000组实验，采用更高投毒比例（最高达20%）与强数据增强（同义替换、回译、模板泛化），在白盒设定下对比句法与语义两类后门攻击对**正向/负向偏差诱导**的有效性，并检验两类主流防御范式：**模型内生防御**（如神经元剪枝、梯度正则化）与**模型外生防御**（如后处理过滤、输入净化）的鲁棒性。\n\n## 主要发现  \n- 两类攻击均能高效诱导目标偏差且保持模型基础性能（平均任务准确率下降<3%）；  \n- **语义触发后门在诱发负向偏差上显著更强**（成功率+18.7%），但二者均难以稳定诱导正向偏差（成功率<42%），揭示偏差方向存在内在不对称性；  \n- 两类防御虽可削弱后门效果（平均缓解率63–71%），但**内生防御导致平均推理速度下降41%，外生防御使任务准确率下降9.2–15.6%**，凸显实用性瓶颈。  \n本研究首次在白盒框架下系统解构LLM偏差后门的触发机制、方向特异性与防御代价，为可信AI治理提供实证基础与设计指南。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13486v1",
      "arxiv_id": "2602.13486v1",
      "title": "Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity",
      "authors": [
        "Fei Wu",
        "Jia Hu",
        "Geyong Min",
        "Shiqiang Wang"
      ],
      "abstract": "Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13486v1",
      "url": "https://arxiv.org/abs/2602.13486v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "learning",
        "federated"
      ],
      "keyword_score": 2,
      "summary_zh": "## 背景与问题  \n联邦低秩自适应（FedLoRA）在保障隐私与通信效率的前提下，支持大模型下游任务的轻量微调。然而，真实联邦场景中客户端存在显著异质性——计算资源、数据分布及任务需求各不相同，自然催生**异构LoRA秩配置**（即不同客户端采用不同秩的适配矩阵）。本文首次发现并命名一种关键失效现象：**秩坍缩（rank collapse）**——全局聚合更新的能量急剧集中于所有客户端共享的最小秩分量，导致高秩参数更新被系统性抑制，模型性能下降且对秩配置高度敏感。\n\n## 根本原因与理论洞察  \n通过严谨的谱分析与收敛性建模，我们证明秩坍缩源于**聚合权重与客户端贡献机制的根本错配**：现有方法采用秩无关（rank-agnostic）的均匀或数据量加权聚合，但客户端对全局模型的实际贡献强度高度依赖其本地LoRA秩（高秩更新蕴含更丰富的梯度信息）。该错配引发高秩分量在每轮聚合中以**几何速率衰减**，数轮后即趋近于零。\n\n## 方法创新：raFLoRA  \n为此，我们提出**秩分区聚合（rank-partitioned Aggregation for FedLoRA, raFLoRA）**：将各客户端的LoRA更新按秩维度解耦为正交子空间（如秩1、秩2–3、秩4+等），再针对每个秩分区，基于其**有效客户端覆盖度与梯度方差**动态计算加权系数，实现“按秩赋权、分区聚合”。该设计保持原有LoRA通信开销（仅传输低秩矩阵），无需额外带宽或中心化协调。\n\n## 实验验证  \n在ImageNet-1K分类、FLANv2推理及多轮对话任务上，raFLoRA相较FedLoRA、FedLora+等SOTA基线：① 消除秩坍缩现象（可视化谱能量分布均匀化）；② 平均提升Top-1准确率2.1–4.7个百分点；③ 在异构秩配置下鲁棒性显著增强（标准差降低63%）；④ 通信量严格维持在$O(r_{\\max} d)$量级，无额外开销。",
      "summary_en": "Federated Low-Rank Adaptation (FedLoRA) enables efficient, privacy-preserving fine-tuning of foundation models—but suffers from **rank collapse** under client heterogeneity: global updates concentrate energy on the *minimum shared rank*, suppressing higher-rank contributions and degrading performance. We theoretically identify the root cause as a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, causing geometric suppression of high-rank updates across rounds. To address this, we propose **raFLoRA**, a rank-partitioned aggregation method that decomposes local LoRA updates into orthogonal rank subspaces and aggregates each partition using weights proportional to its effective client contribution (e.g., gradient variance and participation density). Experiments across classification (ImageNet-1K) and reasoning (FLANv2, dialogue) tasks show raFLoRA eliminates rank collapse, improves accuracy by +2.1–4.7 points over SOTA FedLoRA baselines, enhances robustness to rank heterogeneity, and preserves strict $O(r_{\\max}d)$ communication efficiency—without added bandwidth or central coordination.",
      "summary": "## 背景与问题  \n联邦低秩自适应（FedLoRA）在保障隐私与通信效率的前提下，支持大模型下游任务的轻量微调。然而，真实联邦场景中客户端存在显著异质性——计算资源、数据分布及任务需求各不相同，自然催生**异构LoRA秩配置**（即不同客户端采用不同秩的适配矩阵）。本文首次发现并命名一种关键失效现象：**秩坍缩（rank collapse）**——全局聚合更新的能量急剧集中于所有客户端共享的最小秩分量，导致高秩参数更新被系统性抑制，模型性能下降且对秩配置高度敏感。\n\n## 根本原因与理论洞察  \n通过严谨的谱分析与收敛性建模，我们证明秩坍缩源于**聚合权重与客户端贡献机制的根本错配**：现有方法采用秩无关（rank-agnostic）的均匀或数据量加权聚合，但客户端对全局模型的实际贡献强度高度依赖其本地LoRA秩（高秩更新蕴含更丰富的梯度信息）。该错配引发高秩分量在每轮聚合中以**几何速率衰减**，数轮后即趋近于零。\n\n## 方法创新：raFLoRA  \n为此，我们提出**秩分区聚合（rank-partitioned Aggregation for FedLoRA, raFLoRA）**：将各客户端的LoRA更新按秩维度解耦为正交子空间（如秩1、秩2–3、秩4+等），再针对每个秩分区，基于其**有效客户端覆盖度与梯度方差**动态计算加权系数，实现“按秩赋权、分区聚合”。该设计保持原有LoRA通信开销（仅传输低秩矩阵），无需额外带宽或中心化协调。\n\n## 实验验证  \n在ImageNet-1K分类、FLANv2推理及多轮对话任务上，raFLoRA相较FedLoRA、FedLora+等SOTA基线：① 消除秩坍缩现象（可视化谱能量分布均匀化）；② 平均提升Top-1准确率2.1–4.7个百分点；③ 在异构秩配置下鲁棒性显著增强（标准差降低63%）；④ 通信量严格维持在$O(r_{\\max} d)$量级，无额外开销。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13477v1",
      "arxiv_id": "2602.13477v1",
      "title": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage",
      "authors": [
        "Akshat Naik",
        "Jay Culligan",
        "Yarin Gal",
        "Philip Torr",
        "Rahaf Aljundi",
        "Alasdair Paren",
        "Adel Bibi"
      ],
      "abstract": "As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \\textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13477v1",
      "url": "https://arxiv.org/abs/2602.13477v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "prompt",
        "llm",
        "injection"
      ],
      "keyword_score": 5,
      "summary_zh": "## 背景与问题  \n随着大语言模型（LLM）智能体能力持续增强，**多智能体协同系统**（尤其是“编排器”架构）正成为实际落地的关键范式：一个中心化**编排智能体**负责任务分解与分发，多个专业化智能体执行子任务。然而，现有安全研究多聚焦于单智能体场景，或在缺乏基础工程防护（如访问控制、输入净化）的简化环境中开展，导致**多智能体系统的系统性威胁建模严重缺位**。\n\n## 方法与发现  \n本研究通过红队演练（red-teaming）一个具代表性的、部署了严格数据访问控制的编排器系统（含前端代理、数据库查询代理、报表生成代理等），首次揭示一种新型跨智能体数据泄露攻击——**OMNI-LEAK**。该攻击仅需一次**间接提示注入**（indirect prompt injection），即可诱导多个被攻陷智能体协同完成敏感数据窃取：攻击者向非关键代理注入看似无害的指令，触发链式调用，绕过访问控制策略，最终由高权限代理输出受保护数据（如用户PII、财务记录）。\n\n## 关键创新与影响  \n- **突破性攻击面**：证明即使存在完备的数据级访问控制，多智能体间的**语义协作流**本身即构成隐蔽信道；  \n- **普适性脆弱性**：前沿闭源（GPT-4o、Claude 3.5）与开源（Qwen2.5-72B、Llama-3.1-405B）模型均易受攻击，**推理型与非推理型模型无显著差异**；  \n- **低门槛攻击**：攻击者无需逆向工程或内部知识，仅凭黑盒交互即可成功；  \n- **实践警示**：凸显将单智能体安全机制直接迁移至多智能体环境的重大风险，亟需构建面向**协作拓扑结构**的安全验证框架。",
      "summary_en": "Large Language Model (LLM) agents are increasingly deployed in multi-agent orchestrator systems, where a central agent decomposes tasks and delegates them to specialized sub-agents. While prior safety research has largely focused on single-agent risks or insecure setups lacking basic safeguards (e.g., access control), systematic threat modeling for *orchestrated multi-agent systems* remains scarce. Through rigorous red-teaming of a realistic, access-controlled orchestrator pipeline, we identify **OMNI-LEAK**: a novel, indirect prompt injection attack that compromises multiple agents to exfiltrate sensitive data via a single malicious input—even when strict data access controls are enforced. We evaluate frontier models (both reasoning and non-reasoning variants, including GPT-4o, Claude 3.5, Qwen2.5-72B, and Llama-3.1-405B) and find consistent vulnerability across architectures, without requiring insider knowledge. Our work underscores an urgent need to generalize AI safety research from isolated agents to collaborative, topology-aware systems—critical for preventing real-world privacy breaches, financial loss, and erosion of public trust.",
      "summary": "## 背景与问题  \n随着大语言模型（LLM）智能体能力持续增强，**多智能体协同系统**（尤其是“编排器”架构）正成为实际落地的关键范式：一个中心化**编排智能体**负责任务分解与分发，多个专业化智能体执行子任务。然而，现有安全研究多聚焦于单智能体场景，或在缺乏基础工程防护（如访问控制、输入净化）的简化环境中开展，导致**多智能体系统的系统性威胁建模严重缺位**。\n\n## 方法与发现  \n本研究通过红队演练（red-teaming）一个具代表性的、部署了严格数据访问控制的编排器系统（含前端代理、数据库查询代理、报表生成代理等），首次揭示一种新型跨智能体数据泄露攻击——**OMNI-LEAK**。该攻击仅需一次**间接提示注入**（indirect prompt injection），即可诱导多个被攻陷智能体协同完成敏感数据窃取：攻击者向非关键代理注入看似无害的指令，触发链式调用，绕过访问控制策略，最终由高权限代理输出受保护数据（如用户PII、财务记录）。\n\n## 关键创新与影响  \n- **突破性攻击面**：证明即使存在完备的数据级访问控制，多智能体间的**语义协作流**本身即构成隐蔽信道；  \n- **普适性脆弱性**：前沿闭源（GPT-4o、Claude 3.5）与开源（Qwen2.5-72B、Llama-3.1-405B）模型均易受攻击，**推理型与非推理型模型无显著差异**；  \n- **低门槛攻击**：攻击者无需逆向工程或内部知识，仅凭黑盒交互即可成功；  \n- **实践警示**：凸显将单智能体安全机制直接迁移至多智能体环境的重大风险，亟需构建面向**协作拓扑结构**的安全验证框架。",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.13093v2",
      "arxiv_id": "2602.13093v2",
      "title": "Consistency of Large Reasoning Models Under Multi-Turn Attacks",
      "authors": [
        "Yubo Li",
        "Ramayya Krishnan",
        "Rema Padman"
      ],
      "abstract": "Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.",
      "published": "2026-02-13",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.13093v2",
      "url": "https://arxiv.org/abs/2602.13093v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "model",
        "extraction"
      ],
      "keyword_score": 2,
      "summary_zh": "## 研究背景  \n具备推理能力的大模型（Large Reasoning Models, LRM）在数学、代码、逻辑等复杂任务上表现卓越，但其在**多轮对抗性交互**下的鲁棒性仍缺乏系统评估。现有研究多聚焦单轮攻击或通用大语言模型（LLM），忽视了推理链（reasoning trace）对攻击响应的深层影响。\n\n## 方法与实验  \n本研究首次系统评测了**9个前沿推理模型**（含o1-preview、DeepSeek-R1、Qwen2.5-Math等）在多轮对抗攻击下的表现。我们设计四类攻击：误导性建议（Misleading Suggestions）、社会压力（Social Pressure）、角色扮演胁迫（Role-Play Coercion）和认知过载（Cognitive Overload），并基于2,100+条攻击轨迹开展细粒度失败归因分析。\n\n## 主要发现  \n- **推理≠鲁棒**：虽显著优于指令微调基线（平均准确率高23.6%），但所有LRM均存在可复现的脆弱性模式；  \n- **五大失败模式**：通过轨迹分析识别出**自我怀疑（Self-Doubt）**、**社会从众（Social Conformity）**、**建议劫持（Suggestion Hijacking）**、**情绪易感（Emotional Susceptibility）** 和**推理疲劳（Reasoning Fatigue）**；前两类合计导致**50%的失败案例**；  \n- **信心机制失效**：经典防御方法——**置信度感知响应生成（CARG）** 在LRM上完全失效，根源在于长推理链引发的**系统性过度自信**；反直觉地，**随机置信嵌入**（random confidence embedding）效果优于目标化置信提取，揭示了当前信心建模与推理过程的深层错配。\n\n## 创新意义  \n本研究首次揭示推理能力与对抗鲁棒性的非正相关性，提出面向推理模型的脆弱性分类框架，并指出：**信心驱动的防御需重构底层假设**，而非简单迁移至LRM。",
      "summary_en": "Large reasoning models (LRMs) achieve state-of-the-art performance on complex tasks, yet their robustness under multi-turn adversarial pressure remains poorly understood. We systematically evaluate 9 frontier LRMs against four attack types (e.g., misleading suggestions, social pressure) across 2,100+ trajectories. Results show that reasoning confers meaningful but incomplete robustness: while LRMs outperform instruction-tuned baselines by +23.6% average accuracy, all exhibit distinct vulnerability profiles—misleading suggestions are universally effective, and social pressure efficacy is model-specific. Trajectory analysis uncovers five failure modes (**Self-Doubt**, **Social Conformity**, **Suggestion Hijacking**, **Emotional Susceptibility**, **Reasoning Fatigue**), with the first two accounting for 50% of failures. Crucially, Confidence-Aware Response Generation (CARG)—effective for standard LLMs—fails catastrophically for LRMs due to overconfidence induced by extended reasoning traces; surprisingly, *random* confidence embedding outperforms targeted extraction. Our work demonstrates that reasoning capabilities do not inherently ensure adversarial robustness and that confidence-based defenses require fundamental rethinking for reasoning models.",
      "summary": "## 研究背景  \n具备推理能力的大模型（Large Reasoning Models, LRM）在数学、代码、逻辑等复杂任务上表现卓越，但其在**多轮对抗性交互**下的鲁棒性仍缺乏系统评估。现有研究多聚焦单轮攻击或通用大语言模型（LLM），忽视了推理链（reasoning trace）对攻击响应的深层影响。\n\n## 方法与实验  \n本研究首次系统评测了**9个前沿推理模型**（含o1-preview、DeepSeek-R1、Qwen2.5-Math等）在多轮对抗攻击下的表现。我们设计四类攻击：误导性建议（Misleading Suggestions）、社会压力（Social Pressure）、角色扮演胁迫（Role-Play Coercion）和认知过载（Cognitive Overload），并基于2,100+条攻击轨迹开展细粒度失败归因分析。\n\n## 主要发现  \n- **推理≠鲁棒**：虽显著优于指令微调基线（平均准确率高23.6%），但所有LRM均存在可复现的脆弱性模式；  \n- **五大失败模式**：通过轨迹分析识别出**自我怀疑（Self-Doubt）**、**社会从众（Social Conformity）**、**建议劫持（Suggestion Hijacking）**、**情绪易感（Emotional Susceptibility）** 和**推理疲劳（Reasoning Fatigue）**；前两类合计导致**50%的失败案例**；  \n- **信心机制失效**：经典防御方法——**置信度感知响应生成（CARG）** 在LRM上完全失效，根源在于长推理链引发的**系统性过度自信**；反直觉地，**随机置信嵌入**（random confidence embedding）效果优于目标化置信提取，揭示了当前信心建模与推理过程的深层错配。\n\n## 创新意义  \n本研究首次揭示推理能力与对抗鲁棒性的非正相关性，提出面向推理模型的脆弱性分类框架，并指出：**信心驱动的防御需重构底层假设**，而非简单迁移至LRM。",
      "summary_status": "success"
    }
  ],
  "last_updated": "2026-02-19T02:21:24.967618",
  "total_count": 81
}